content,label,url
"今日はWebブラウザ上でエディターやファイル管理などを使いながら、サーバー上で直接開発できるツールを紹介したいと思います。
これは codeanywhere

Javascript、PHP、HTML、または他の72言語でカスタマイズされた開発環境を簡単にセットアップできます。
コンテナまたはリモートサーバ上でbuilt-inターミナルコンソールを使用してコマンドが実行出来る。（SSHを使用）
共有リンクを使用してファイル、フォルダ、プロジェクトをURLだけで共有できます
FTP、SFTP、Dropbox、Googleドライブなど・・・どこでもリモートで編集できます。 
Git、Bitbucket、GitHub・・・既存のGitリポジトリに簡単に接続できます。
リビジョンを使用して確認できます。
コンピュータをきれいにし、衝突のない状態で速く走らせます。
ブラウザからコードを作成できるだけではなく、iOSやAndroid搭載の携帯電話やタブレットからもコードを作成できます。

クラウドベースの開発には多くの利点があります。
ブラウザだけで他のソフトウェアをインストールする必要はありません。
基本的には、ブラウザのIDE上で仮想マシンを立上てroot〜sshシェルアクセスを使用してからコードを作成することができます。
プロセスを簡単に見てみましょう。
まず、メールアドレス（例：Gmailアカウント）でサインアップする必要があります。
この段階で必要なのは、無料の層だけです。
より多くのスペースを必要とすると、いくつかの異なるサブスクリプションの1つにアップグレードできます。

次に、「コンテナ」を作成する。 
フリーティアでは、一度に1つに制限されます。
UbuntuまたはCentosのいずれかで操作する、いくつかの異なる事前定義スタックがあります。 
スタックには、Java、C ++、Node、.NET Core、PHPなどのプログラミングが含まれています。

コンテナが作成されたら、今度は完全なsudoアクセス権を持つsshシェルがあります。
C必要なファイルを整理し、パッケージマネージャを実行します。

エディタでは、プロジェクト階層全体にアクセスできます。

最新のIDE機能、コードの完成、コードの折り畳み、構文の色付けなどが含まれています。
Hands-On Video


",False,https://qiita.com//kyawphyonaing/items/43eb7adddf9f509106d3
"

まえがき
ここは普通科高校卒業，工業大学在籍の情報学部女子大生の備忘録です
専攻はセキュリティ系です
なんせ普通科高校出身がゆえ，プログラマー思考が身についてないんです
大学で使う主言語はC
Pythonはほとんど使いません
Qiitaでは自分がPythonの勉強をするに当たっての導入方法，失敗談，成功例などを載せて行こうと思います
できるならばコメント等で掲載している内容とは違った方法やアドバイス，間違い等を言っていただき，私だけでなく見てくださる皆さんと勉強ができたらと…
CCENTの勉強もしなきゃなので更新頻度はめちゃめちゃいいわけではないです
現在大学2年後期のため授業課題等でいっぱいいっぱい…()
それでは，まじめなようでまじめでない，ポンコツのためのポンコツ備忘録のはじまりはじまり～

そもそも『ぱいそん』ってどんな言語？

さまざまな分野のアプリケーションで使われている、極めてパワフルな動的プログラミング言語
COM、.NET、CORBAなどのオブジェクトと組み合わせることができます
Windows、Linux/Unix、OS/2、Mac、Amigaなど多くのOSで使うことが可能
わたしが特に魅力的だと思った点はユーザーフレンドリーであるということです
まえがきにも書いたとおりプログラマー思考が身についていない自分にとって，ユーザーフレンドリーであることはとても重要
初めてPythonに触れたのはProgateという，初心者向けに作られているプログラミング学習サイトでした
別にProgateの回し者とかではないですよ()
個人的にPythonはC言語よりも分かりやすくて学習しやすいと思いました
Pythonの英語版公式サイト
日本語版公式サイトはこちら

Pythonの導入
続いてPythonのインストール方法についてです
ここは簡単，誰でもできます
猫にはできません，PCが使えてこの記事を読める人であればできます，たぶん
『ここ』からパッケージがインストールできます
バージョンはそれぞれ必要に応じて選択してください
今回はリリースされて少し経っていますがPython3.7.0をインストールしようと思います
Downloadをクリックするとこのような画面が出てきます(2018/11/15現在)

わたしはこの中の「Windows x86-64 web-based installer」を選択しました
どれが良かったのか分からないし，違いもよく分からないし…
インストーラーを起動します
多分こんなアイコンしてるはず

起動するとこのような画面が出現します

64bit版ですね
インストールが終わるまでのんびり待ちましょう

インストールが終わりました
とりあえず閉じましょう
英語が全く読めません
分からないので余計なことはしません
何もしてないのに壊れました～！なんてことにはなりたくないし…

Winキーを押して「P」の欄にこんなのが出てたら成功です！
おめでとうございます！しゅごい！


かんそう
くそねみなので今日はここまで
とりあえずインストールできれば満点！！！！
また進展があったら書きます(･_･)
ちゃんと勉強します()
よく見たら日本語版サイトに色々載ってますね…
そんでもってお腹空きました眠いです
マツキヨに売ってるきなこ餅せんべい美味しいので食べてみてください

参考文献
Pythonとは
https://www.python.jp/pages/about.html
",False,https://qiita.com//C74n/items/d27be0468171d245d067
"IT関連の勉強に興味を持ち始めたので、広く浅く基礎知識を得られそうな基本情報技術者試験を受けようと思いました。
不定期ではありますが、関連する記事を書こうと思っています。

はじめに
この記事は、私が基本情報技術者試験に向けての勉強の理解を深めるために書いています。
図の挿入などはしないので、わかりにくい場所が多々あると思いますがご了承ください(本や過去問の解説見た方が良いと思います)。

離散数学1

10進数から2進数への変換
10進数であらわされた数を商が0になるまで2で割り続ける。
最後の計算の余りを頭として順に並べる。

10進小数から2進小数への変換
10進少数の小数点以下の数を2倍する。
積の1の位の数が求める2進小数の少数第一位。
求めた積の小数点以下の数を2倍する。
積の1の位の数が求める2進小数の小数点第二位。
以下同様

整数の表現方法
符号付きの場合、最上位ビット(MSB)はプラスのとき0、マイナスのとき1。
完全補数とは、ある数と足したときに桁が上がる最小の数
<2進数の完全補数の求め方>
補数を求める数より1桁多く、MSBが1、それ以外0の数から引く。
(例 1000000 - 110100 = 001100)
不完全補数とは、ある数の桁数における最大の数
不完全補数 = 完全補数 - 1

小数点数の表現方法
固定少数点数表示は、整数部と小数部に分け、その間に小数点があるとみなして数を扱う。
表現できる数の範囲は狭いが、高速な演算処理が可能。
(例) 0.1011(2) = (1 x 1/2) + (1 x 0/4) + (1 x 1/8) + (1 x 1/16) = 0.6875(10)
浮動小数点表示は、指数表現で表される表現方法。
固定少数点数表示よりも表現できる範囲は広いが、演算処理が比較的低速。
符号を1ビット、指数を3ビット、仮数を4ビットで表すと以下のようになる
(例) 2.5(10) = 10.1(2) = +0.101 x (2^2) = 00101010
この例では符号+(0)、指数2(010)、仮数101(1010)で表される。
限られたビット列で数値を高い精度で表現するために、仮数の最初の桁を0以外の数(2進数なら1)にすることを正規化という。

演算

算術演算
減算では引く数を2の補数を利用し加算に置き換える。

シフト演算
論理シフト演算
ビット列を左右にシフトする(ずらす)演算で、MSBやLSBからあふれた値は捨て、空いたビットには0を挿入する。符号ビットの値は保持されない。
145(10)を左へ1ビットシフトすると100100010(2)となり290(10)となるが、9ビット目は無視されるため00100010(2)つまり34(10)。
逆に右へ1ビットシフトすると01001000(2)となり72(1)となる。
算術シフト演算
負数を考慮したシフト演算で、符号ビット以外のビットをシフトする。
どちらの演算も左1ビットシフトで2倍、右1ビットシフトで1/2倍。ただし小数点以下切り捨て。

誤差

情報落ちによる誤差
浮動小数点演算において、絶対値の大きな数と小さな数の加減算で、絶対値の小さな数の有効桁の一部(or全部)が反映されないことで生じる誤差

桁落ちによる誤差
値がほぼ等しい浮動小数点数同士の減算において、有効桁数が大幅に減ってしまうことで生じる誤差

打切り誤差
演算結果が無限小数となる場合に、一定の桁数で打ち切ったことが原因で発生する誤差

丸め誤差
数表現の桁数の限度により、最下位桁より小さい部分について切り捨て切り上げ四捨五入によって生じる誤差

オーバーフロー and アンダーフロー
用語参照

絶対誤差と相対誤差
真の値と演算結果の差を誤差という。
絶対誤差は誤差の絶対値。
相対誤差 = 絶対誤差 / 真の値

用語
・MSB(Most Significant Bit)...最上位ビット。左端ビットともいわれる
・LSB(Least Significant Bit)...最下位ビット。右端ビットともいわれる
・桁あふれ...表現できる数値範囲を超えてしまい、正しい数値にならないこと。このうち表現できる最大値を超えることをオーバーフロー、最小値を超えることをアンダーフローという

その他気になったこと
・IEEE 754(IEEE Standard for Floating-Point Arithmetic)は浮動小数点数の計算で最も広く採用されている標準規格であり、多くのプロセッサなどのソフトウェア・ハードウェアで実装されている。
単精度、倍精度、拡張単精度、拡張倍精度の4つの形式を定義している。
単精度は32ビットで表現(指数部8、仮数部23ビット)
",False,https://qiita.com//Programshi/items/90e77a3339a6eb6ea338
"

前提
この記事は以下の項目全てに当てはまる方に推奨する言語として、
JavaScriptを推奨してほしくない理由を述べます。

プログラミング初心者
特段目的はないがいずれかのプログラムをちゃんと勉強したい人
独学


JavaScriptが初心者に向かない理由
JavaScriptはWebエンジン上で動作するスクリプト言語です。
C/C++/Javaなどのコンパイル言語ではないため、
初心者には環境構築も容易で、特段HTMLやCSSも簡易なものであれば、
難しいといったことはないから初心者にオススメできる。
確かに数年前はそうでした。
今もそう捉えようと思えばそう捉えられないことはないです。
じゃあなぜ推奨しないか。
初心者がきちんとJavaScriptを勉強しようとするには、
JavaScriptに関する情報量が多すぎるからです。
以下にその事例をあげます。

jQueryの甘い蜜
一度JavaScriptをきちんと勉強しようとすれば、
十中八九初心者は、技術記事やアドバイスなどで
この単語を見かけると思います。
jQueryはJavaScriptのライブラリで、
主にDOM（Document Object Model）を操作する処理や、
Ajax（非同期なクライアントとサーバとの間の通信）処理を容易に、
ブラウザエンジンごとの仕様を気にかけることなく行うことができます。
jQueryを否定するわけではありませんが、
昨今のWebエンジンはブラウザエンジンごとの仕様が以前より
改善されてきているため、jQueryを用いる場面もかなり減ってきています。
ですがこれらの事実は勉強したからわかる話であって、
JavaScript初学者は、
技術記事でよくみかけるであろう煽り文句とともに、
jQueryの直感的な操作のみに焦点を当ててしまい、
それが便利でとても魅力的なものに見えてしまう。
クライアントサーバモデルやAjaxという
言葉にも触れられる機会は少ないでしょう。

数々のライブラリやフレームワークの乱立
jQueryの甘い蜜を、
なんとか啓蒙記事を見るなどして乗り越えたら、
次に直面するのがこの問題。
Node.js？　Vue.js？　React.js？　AngularJS？　 ES5？　ES6？
はてなの連続です。
どれが一体JavaScriptと呼ばれるものなのでしょうか。
言語仕様という言葉もわからない初心者にとって
この問題はとても罪深いです。
ES5/ES6にしても、
どれがES5に基づく技術記事で、どれがES6に基づく技術記事なのか。
なかなか初心者には判断が難しい。
最近、私もReact.jsのバージョンによる仕様が変わったことで、
推奨される記述方法がガラッと変わってしまい驚きました。
初心者にとってはもうはちゃめちゃです。
自分なら多分オーバーヒートしますね。

プログラミング初心者に教えるにあたって
ECMAScriptというJavaScriptの標準規格に、
独学でたどり着いた人が中にはいるかもしれません。
ですがJavaScriptの言語仕様にたどり着くことさえ、
初心者にとっては難しいと容易に想像できます。
また、初心者になんらかのプログラミング言語を勧める場合、
自分が経験した言語で、ある程度の道しるべを示すべきだと思います。
最後に、こういう記事を書いていて、
世に出すのであれば正確な情報、
わかりやすい情報を提供することが重要だと思いました。
",False,https://qiita.com//olt/items/ded3010c755314286231
"

はじめに
皆さん、正規表現は使いこなせていますか？
私は全く使いこなせません。
今回は、私自身が正規表現を学ぶ過程でお世話になった・利用したサイトやツールをまとめます。

対象読者

正規表現って何それおいしいの？な人
正規表現は普段の検索くらいには使うよ！な人
正規表現マスターしてます！な人


正規表現(regular expression)とは
正規表現 - Wikipedia
特定の文字列を表現するためのパターンです。
通常の文字(a-z や 0-9)に加えて、. や * などの特殊文字(メタキャラクタ)から構成されます。

どういう時に使うの？

単純に文字列を検索したい時
特定のパターンの文字列を検索・置換したい時


メールアドレス
URL
クレジットカード番号
郵便番号
電話番号
HTMLタグ 等々...



上記以外にも様々な使いみちが存在します。
プログラミングをする上で覚えておくと便利になること間違いなし！

正規表現を学べる記事・サイト

サルにもわかる正規表現入門

初心者向けの入門記事を扱うサイト。

正規表現とは？
メタ文字の種類
パターンマッチの基礎

などなど、基本的な部分が網羅されており、分かりやすいです。

初心者歓迎！手と目で覚える正規表現入門・その１「さまざまな形式の電話番号を検索しよう」

@jnchito さんの入門記事です。
シリーズ編となっており、全4編構成です。
身近な電話番号を題材にした導入から

各種メタ文字の役割
キャプチャ
エスケープ
先読み、後読み
後方参照

など、様々な部分を扱っており、とても参考になります。

エディタで学ぶ正規表現入門 (全18回)

ドットインストールの入門記事です。
エディタ( Atom )を用いて正規表現を学べます。
動画形式になっており、実際に手を動かしながら学ぶことが出来ます。

RegexOne - Learn Regular Expressions -

全編英語の入門サイトです。

基礎レッスン(全14レッスン)
発展レッスン(全8レッスン)

から成り、初心者 - 中級者向けとなっています。
Solution ボタンを押すことで、即座に解説を確認でき、学習が進めやすい印象です。

Solve Regex | HackerRank

こちらも英語サイトです。


Easy, Medium, Hard 等の難易度や、カテゴリで検索が出来る。
 アカウント登録・ログインすることで、ポイントやランク付等がされたり、進行状況を保存出来たりする。

等の特徴があります。

Regex Crossword

正規表現を使ったクロスワードゲームが遊べるサイトです。
チュートリアルから、多角形を使ったクロスワードなど、初心者から上級者まで楽しめる作りとなっています。
最後まで解ければ、かなり正規表現が分かるようになっている筈です！

正規表現に関するツール

Rubular: a Ruby regular expression editor and tester

Ruby ベースの正規表現エディタで、オンラインでサクっと正規表現を試すことが出来ます。
画面下部にクイック・リファレンスが出ていたり、キャプチャを分かりやすく表示してくれるので非常に重宝します。


Refiddle

こちらもオンラインで使用できる正規表現エディタです。
Rubularを異なる点としては
- JavaScript
- Ruby
- .NET
から言語を選択することが出来ます。

Online regex tester and debugger

こちらのエディタでの最大の特徴は、マッチに掛かった時間とステップ数を表示してくれる点です。
あまり詳細には述べませんが、正規表現にもパフォーマンスがあります。
参考: 正規表現のパフォーマンスの話をされても全くピンと来なかった僕は、backtrackに出会いました。
* は貪欲マッチと呼ばれ、乱用するとステップ数の増加に繋がります。
自分が作った正規表現が意図した動きをするのか？という観点以外にも、パフォーマンスの観点を持つことが大切です。
このエディタでは、パフォーマンスやステップ数を計測することが出来るため、チューニングする際に使っています。



最後に
正規表現は、シンプルな使い方から非常に高度なパターンマッチまで、多様な使い方が出来ます。
正規表現を学び、普段の開発をより豊かにしちゃいましょう！

お願い
良いツール・サイトが見つかり次第、随時更新していきますが、

こんなサイトもおすすめだよ！
こんなツールもあるよ！

という方がいらっしゃいましたら、是非コメントお願いします🙏
",False,https://qiita.com//teinen_qiita/items/806aba936a8ed1f7654e
"

はじめに
いまさらGitコマンドの解説記事を新しく書いても、誰も必要としていないだろうということは、うすうす気づいています。でも自分はとても忘れっぽいので、どこかにキチンとまとめておき、いつでも見直せるようにしておかないと、不安なんです。なによりも自分のため、大事なことから順番に、わかりやすくGitをまとめてみたいと思います

0. 最初にやるべきこと
自分の名前とメルアドを設定する
git config --global user.name ""Isamu Suzuki""
git config --global user.email isamu@example.com


git configコマンドとは？
# 設定内容をリスト出力する（system, global, localの順）
git config --list

# 上のコマンドと同じ
git config --system --list
git config --global --list
git config --local --list
# --system ... gitコマンドをインストールしたディレクトリにある
#              gitconfigファイルの内容を表示
# --global ... HOMEディレクトリにある.giconfigファイルの内容を表示
# --local ... 対象リポジトリにある.git/configファイルの内容を表示


1. gitコマンドの基本操作

自分で新しいプロジェクトを始める場合
# リポジトリを初期化する
git init

# そこにあるすべてのファイルを追加する
git add .

# 最初のコミット
git commit -m ""Initial commit""

# リモートリポジトリを登録する
git remote add origin git@bitbucket.org:hoge/fuga.git

# リモートリポジトリに初めてプッシュする
git push -u origin master

# リモートリポジトリにプッシュする（2回目以降）
git push origin master


既存のプロジェクトに参加する場合
# リモートリポジトリからプルする
git clone git@bitbucket.org:hoge/fuga.git

# 自分が更新したファイルを追加する
git add hoge.txt

# コミット
git commit -m ""docs: add comment""

# リモートリポジトリにプッシュする
git push origin master


2. コミットメッセージには、プレフィックスをつける


feat: ... 新機能

fix: ... バグフィックス

refactor: ... 新機能でもバグフィックスでもないコード変更

perf: ... パフォーマンス向上

test: ... テストコードの追加・修正

style: ... コードの意味に影響しない変更（空白、フォーマット、セミコロン）

docs: ... ドキュメントだけの変更

chore: ... 雑用（ビルドプロセスの変更、ツールやライブラリの追加削除）


3. ローカルブランチの基本操作
# issue1というブランチを作成する
git branch issue1

# ブランチの一覧を表示する
git branch
#=> *master
#=>  issue1

# ブランチを切り替える
git checkout issue1

git branch
#=>  master
#=> *issue1

# (option) ブランチの作成とチェックアウトをまとめて行う
git checkout -b issue2

# issue1で行った変更をmasterに取り込ませる
git checkout master
git merge issue1

# ブランチを削除する
git branch -d issue1


git mergeコマンドとは？
HEAD（現在使用しているブランチの先頭）が指しているブランチに、指定したブランチを取り込む
# masterブランチをチェックアウトする
git checkout master

# masterブランチにissue1ブランチを取り込む
git merge issue1


git mergeコマンドを実行して、競合があった場合は？
git merge issue2
# CONFLICT (content): Merge conflit in myfile.txt
# Automatic merge failed: fix conflicts and then commit the result.

コンフリクト（競合）があったというメッセージが表示される。競合があった箇所に、Gitが差分を挿入しているので、これを修正する。競合箇所の修正が終わったら、改めてコミットする
git add myfile.txt
git commit -m 'issue2ブランチをマージ'


4. コミットに関してよく行う操作

コミットを修正する
git commit --amend -m ""new message""
git commit --amend --author ""Isamu Suzuki <isamu@example.com>""

# 変更を確認する
git log -1 --pretty=full

# リモートにまでプッシュしていた場合は、強制的に同調させる
git push -f origin master

git commitは、エディタが立ち上がるので、あらかじめ使い慣れたエディタを設定しておくと便利
# エディタをVimに設定する
git config --global core.editor 'vim -c ""set fenc=utf-8""'


コミットを取り消す
# 直前のコミットを取り消す（コミットのみ取り消す）
git reset --soft HEAD^

# 直前のコミットを取り消す（まったくなかったことにする）
# 新規ファイルは消えるし、編集内容も失われるので要注意
git reset --hard HEAD^

リモートにまでプッシュしていた場合は？
# 1. 強制的に同調させる
git push -f origin master

# 2. git resetは使わずに、取り消したいコミットを
#    打ち消すようなコミットを新しく作成する
git revert f5716bb --no-edit
#=> Revert ""<previous>"" というコメントでコミットされる
git push origin master


5. ブランチに関してよく行う操作

遡って（さかのぼって）ブランチ分けする
ローカルのmasterブランチに3回コミットをした後に、それらは新しいnew1ブランチにコミットすべきだったことに気づいたとする
# リモートのmasterブランチを追跡する、ローカルのnew1ブランチを作成する
git branch new1 origin/master

# 新しいブランチに切り替える
git checkout new1

# 新しいブランチに移したいコミットを1個づつチェリーピックする
git cherry-pick m0001
git cherry-pick m0002
git cherry-pick m0003

# masterに切り替える
git checkout master

# コミットを削除する
# （ヘッドから3個削除する場合のコマンド）
git reset --hard HEAD~3

# 強制的にリモートにプッシュする
git push -f origin master


6. リポジトリに関してよく行う操作

ローカルをリモートに強制一致させる
# リモートリポジトリの情報をすべてローカルに取得する
git fetch origin

# ローカル内で状態を動かす
git reset --hard origin/master


手元にないリモートブランチを取得する
# リモートからデータをすべて取得する
git fetch origin

# originのdevelopブランチを追跡するローカルのdevelopブランチを作成する
git branch develop origin/develop

# ローカルのブランチを確認する
git branch
#=>  develop
#=> *master

# developブランチをチェックアウトする
git checkout develop

### 以下は絶対にやってはいけない ###

git pull origin new-branch:new-branch
#=> new-branchの内容が、現在のローカルブランチにマージされてしまう。
#=> 例えば、masterブランチにいたならば、masterブランチにマージされる


リモートブランチを削除する
# リモートのhogeブランチを削除する
git push :hoge

### 以下解説 ###

# このコマンドには省略がある
git push origin hoge

# 省略なしバージョン
git push origin hoge:hoge

# ローカルのブランチをリモートのブランチにプッシュする
git push origin {ローカル}:{リモート}

# ローカルを空白にする => リモートのブランチを削除する
git push origin :{リモート}


リモートリポジトリを変更する
# 現在のリモートリポジトリを確認する
git remote -v
# origin  git@bitbucket.org:hoge/fuga.git (fetch)
# origin  git@bitbucket.org:hoge/fuga.git (push)

# リモートリポジトリを変更する
git remote set-url origin {新しいURL}


7. Gitのサブモジュールを使う
あるGitリポジトリを別のGitリポジトリのサブディレクトリとして扱うことができるようになる
git submodule add https://github.com/laradock/laradock.git
#=> (1) .gitmodulesファイルが出来上がる
#=> (2) ディレクトリだけが取り込まれる

.gitmodulesファイル
[submodule ""laradock""]
  path = laradock
  url = https://github.com/laradock/laradock.git

Visual Studio Codeのソース管理では、2つのソース管理プロバイダーがあるのが見える

最後に
間違いがあったら指摘してください。よく使うGitコマンドがよくまとまっていると思ったら、いいねボタンをクリックしてください。励みになります
",False,https://qiita.com//isamusuzuki/items/d3bb757b760f2bd834b3
"　　
　　
　　
この記事は
『プログラミング完全未経験からUnityでの開発現場に迎え入れてもらえた世界一の幸せ者』
の記事です。そのつもりでお読みください。
　　

シーンをスクリプトから操作する方法
名前空間が必要です。
using UnityEngine.SceneManagement;
 void JumpToScene() //シーンの切り替え
    {
        SceneManager.LoadScene(""OtherScene"");
    }

ただ、このまま JumpToScene();と呼び出すと前触れもなく
瞬間的にシーンが変わるのでワープの演出でもない限りナンセンスだと思います。
なのでInvoke(""JumpToScene"", 2.0f);みたいな感じで秒数指定して呼び出すといいかもしれません。
フェードアウトの機能を実装して""画面が真っ暗になったらシーンが切り替わる""
みたいなのもそれっぽい雰囲気出ていい感じです。
フェードイン、フェードアウトも今度まとめます。
",False,https://qiita.com//OKsaiyowa/items/5825299ae17faf89cd38
"先日、PHPとMySQLを用いて簡単なデータベース連携フォームを作成しました。
今回は、その時に作ったファイルをそれぞれ説明していきたいと思います。

使用ファイル
使用したファイルは全部で5つです。

definision.php
フォームに入力してもらう項目を定数として扱うように定義します。
このように、変更することが無い値やプログラムのどこからでも呼び出したい値は、定数とすると便利です。
このファイルは任意です。

index.php
最初のページです。
このページでは、フォームで登録したデータを全て閲覧できるようにしました。
そのために、MySQLのデータをweb上に取り出すためのSQLコードを書きました。
また、フォームに飛ぶためのリンクも設置しました。

input.php
情報を入力するフォームがあるページです。
確認ボタンを押すと、フォームに入力した情報をconfirm.phpに送ることができます。
このフォームで入力した情報を複数の画面で使いまわせるように、SESSIONを用いて、サーバー側に一時的にデータを保存できるようにしました。

confirm.php
先ほどフォームで入力した情報を閲覧する確認ページです。
戻るボタンと登録ボタンがあります。
また、フォームに未入力欄があった場合、フォームに戻るように仕向けるバリーデーション機能を実装しました。

complete.php
確認ページで登録ボタンを押すとこのページに遷移します。
フォームで入力した情報を表示して、登録完了したことをお知らせするページです。
しかし、システム上はこのページに遷移したと同時にデータベース上に情報を登録します。
そのために、MySQLに接続してデータをINSERTするためのSQLコードを書きました。
また、最初のindex.phpのページに飛ぶためのリンクも設置しました。
今回は、PHPとMySQLを用いたフォーム作成のために作ったファイルをご紹介しました。
",False,https://qiita.com//yuhei_umeda/items/053f44335fb285170e36
"基礎中の基礎ですが、せっかく実装したので。

深さ優先探索とはなんぞや
進めるところまで進んで、行き止まったら直前の分かれ道に戻り他の道を模索するという、探索アルゴリズムの初歩となります。

深さ優先探索は何に使えるのか
行ける場所全てを探索するという性質上、ある地点とある地点がつながっているかを判定することができます。
また、ゴールにたどり着かなかった場合は、取りうる全ての間を途中で試すという関係上、全状態の列挙にも使えたりします。
ただし、深さ優先探索によって見つかったルートが最短とは限りません(地図を見ずに行き当たりばったりで目的地に歩いていくルートが最短とは限らないのと同じです。)

深さ優先探索はどう実装するのか。
主に2つの方法があります。スタックを利用する方法と再帰関数を利用する方法です。このうち、再帰関数を使った実装は(なれれば)簡単にかけるので、再帰関数で簡単に実装できることが深さ優先探索の利点の一つとなっています。
※スタックとは
先に入れたものほど、あとに取り出されるデータ構造です。ものを積み上げておいて、取るときは上からとるイメージです。(stack自体に積み上げるの意味があります。)
*再帰関数とは
その関数の中で自分自身をよんで繰り返し処理をさせるような関数です。

実際の実装
以下のような道を探索していきます。このようにどの頂点がどこにつながっているかを表したリストを頂点隣接リストとよんだりします(今回上から下の道のみを考えています。)
std::vector<std::vector<int>> tree = {{1,2},{3,4},{4,10},{},{5,6},{7},{8,9},{},{},{},{};

//                 0
//               /   |
//              1    2
//             /  \ / |
//            3    4   10
//               /  |    |
//              5   6    |
//             /   / |  /
//            7   8   9
//


再帰関数
bool dfs_func(int state, int goal){
  if (state==goal){
    return true;
  }
  for (auto next:tree[state]){
    if (dfs_func(next,goal) == true){
      return true;
    };
  }
  return false;
}

自分のいる場所がゴールかどうか判断し、もし違った場合はつながってる道に対して判断することを繰り返して行きます。もし、途中でゴールに行き着いた場合、trueがどんどん連鎖的に返ってきて最終的に大本の関数がtrueになるような構造になっています。

stack
bool dfs_stack(int start, int goal){
  std::vector<int> stack;
  stack.push_back(start);
  bool is_goal=false; 
  while(!stack.empty()){
    int now = stack.back();
    stack.pop_back();
    for (int next:tree[now]){
      stack.push_back(next);
      if (next == goal){
        is_goal = true;
        break;
      }
    }
    if(is_goal){break;}
  }
  return is_goal;
}

stackでの実装です。つながっている頂点をstackに入れきって、stackの一番上を見てgoalか判定することを繰り返します。しかし、こちらは少々トリッキーな実装になっており、stackの一番上をみて隣接頂点をstackに積む前にstackから一番上を取り出してしまっています。これをしないと、どこまでいってもgoalにいきつかなかったときの判定が面倒になります。

再帰関数(通った道を表示)
bool dfs_func_root(int state, int goal){
    if (state==goal){
      std::cout << state;
    return true;
  }
  for (auto next:tree[state]){
    if (dfs_func_root(next,goal) == true){
      std::cout << state;
      return true;
    };
  }
  return false;
}

再帰関数で、通った道を表示するのは簡単で、trueを返すたびに今いる場所も一緒に捉えればよいです。
stackの実装で道順を表示するのは自分が思いついた限りでは結構面倒な方法しかなく、今回は諦めました。

上記のコードのテスト
int main (void){
  int s,g;
  std::cin >> s >> g;
  std::cout << ""stack ver"" << std::endl;
  show_res(dfs_stack(s,g));
  std::cout << ""func ver"" << std::endl;
  show_res(dfs_func(s,g));
  std::cout << ""root ver"" << std::endl;
  dfs_func_root(s,g);
  std::cout << std::endl;
}

0 9
stack ver
true
func ver
true
root ver
96410

0 20
stack ver
false
func ver
false
root ver

この通り、場合によっては遠回りをしてしまっていますし、つながっていない場合にはつながっていないことがわかります。

おわりに
今までずっとstackの方で実装してたのですが、再帰で実装したら思ったより楽だったので今度からこっちで実装しようと思っています。
",False,https://qiita.com//mosamosa/items/a3a3d0f45612658fefd8
"初めまして。Qiita初投稿になります。
フロントエンドエンジニア歴2ヶ月弱の初心者です。

背景
Ajaxで外部APIに通信する、という課題の為に駅データ.jpを利用しました。
駅データAPIの仕様が不親切(?)で躓いたため、学びを共有しようと思いました。
何か間違いがありましたら、ぜひコメントでご指摘ください。

駅データ.jpとは
すでに簡潔に説明されている記事があったので、引用させていただきます。
駅とか路線のマスターデータの入手方法 @kouさんより


無料（有料プランもあり）
事業者マスタ、路線マスタ、駅マスタ、接続駅マスタがCSVで提供されます
APIでの提供も用意されています
商用利用やデータの改変についても制約がなく使い勝手が良さそうです
有料契約すると取得できる情報がちょっと詳細になります


というものです。
こちらの都道府県APIを利用して、プルダウンから選択した都道府県の沿線情報取得を試みました。

完成予想（というか結果こうなる）


実装してみた（失敗例）
リクエストURLはXML形式とJSON形式のものが用意されていますが、
ここではJSON形式のデータを取得して画面に表示させようと以下のように書きました。
■ リクエストURL
XML形式 ： http://www.ekidata.jp/api/p/(都道府県コード).xml
JSON形式 ： http://www.ekidata.jp/api/p/(都道府県コード).json

site.js
//駅データ取得
$(function(){
    $('#train_search').click(function(){
      //クリック時に検索結果を初期化
      $('#train_result').html("""");
      //プルダウンから値を取得
      var element = $('#place').val();
      var trainUrl = 'https://www.ekidata.jp/api/p/' + element + '.json';
      $.ajax({
        url: trainUrl,
        dataType: 'jsonp',
        success: function(data, dataType) {
              xml.data.line.forEach(function(value){
              $('#train_result').append(value.line_cd + '：' +  value.line_name + '<br>')
            });
          },
        error: function(XMLHttpRequest, textStatus, errorThrown) {
              alert(""ng"");
              console.log(""ng"", XMLHttpRequest, textStatus, errorThrown);
          }
      });
    })
  })



結果：取得できない


何がいけなかったのか
結論から言うと、dataTypeが誤りです。JSON形式のデータのはずが、中身を見てみると...
愛知県の出力例：http://www.ekidata.jp/api/p/23.json

json.js
if(typeof(xml)=='undefined') xml = {};
xml.data = {""line"":[{""line_cd"":11411,""line_name"":""JR中央本線(名古屋～塩尻)""},{""line_cd"":11413,""line_name"":""JR飯田線(豊橋～天竜峡)""},{""line_cd"":11502,""line_name"":""JR東海道本線(浜松～岐阜)""},{""line_cd"":11506,""line_name"":""JR武豊線""},{""line_cd"":11508,""line_name"":""JR関西本線(名古屋～亀山)""},{""line_cd"":30001,""line_name"":""名鉄名古屋本線""},{""line_cd"":30002,""line_name"":""名鉄豊川線""},{""line_cd"":30003,""line_name"":""名鉄西尾線""},{""line_cd"":30004,""line_name"":""名鉄蒲郡線""},{""line_cd"":30005,""line_name"":""名鉄三河線""},{""line_cd"":30006,""line_name"":""名鉄豊田線""},{""line_cd"":30007,""line_name"":""名鉄空港線""},{""line_cd"":30008,""line_name"":""名鉄常滑線""},{""line_cd"":30009,""line_name"":""名鉄河和線""},{""line_cd"":30010,""line_name"":""名鉄知多新線""},{""line_cd"":30011,""line_name"":""名鉄築港線""},{""line_cd"":30012,""line_name"":""名鉄瀬戸線""},{""line_cd"":30013,""line_name"":""名鉄津島線""},{""line_cd"":30014,""line_name"":""名鉄尾西線""},{""line_cd"":30015,""line_name"":""名鉄犬山線""},{""line_cd"":30017,""line_name"":""名鉄広見線""},{""line_cd"":30018,""line_name"":""名鉄小牧線""},{""line_cd"":31027,""line_name"":""近鉄名古屋線""},{""line_cd"":99509,""line_name"":""あおなみ線""},{""line_cd"":99510,""line_name"":""東海交通事業城北線""},{""line_cd"":99511,""line_name"":""愛知環状鉄道線""},{""line_cd"":99512,""line_name"":""リニモ""},{""line_cd"":99513,""line_name"":""名古屋市営地下鉄東山線""},{""line_cd"":99514,""line_name"":""名古屋市営地下鉄名城線""},{""line_cd"":99515,""line_name"":""名古屋市営地下鉄名港線""},{""line_cd"":99516,""line_name"":""名古屋市営地下鉄鶴舞線""},{""line_cd"":99517,""line_name"":""名古屋市営地下鉄桜通線""},{""line_cd"":99518,""line_name"":""名古屋市営地下鉄上飯田線""},{""line_cd"":99520,""line_name"":""豊橋鉄道渥美線""},{""line_cd"":99521,""line_name"":""豊橋鉄道東田本線""},{""line_cd"":99523,""line_name"":""ゆとりーとライン""}]}
if(typeof(xml.onload)=='function') xml.onload(xml.data);


JSONファイルかと思いきや、中身がJavaScriptでした、というオチ。
しかもややこしいことに、if(typeof(xml)=='undefined') xml = {};のxml（変数名）の中身がxmlではなくJSONP...

話は逸れますが...
変数名は適切につけてほしい！
初心者が見ると「あれ、xml.dataの中にデータが入ってるけどこれxml持ってるの？？？」と思うこともなくはないはず...。というかそう思って混乱しました...。
xml.dataの中にちゃんとJSON形式で入ってますね。

再実装してみた（成功例）

site.js
//駅データ取得
$(function(){
    $('#train_search').click(function(){
      //クリック時に検索結果を初期化
      $('#train_result').html("""");
      var element = $('#place').val();
      var trainUrl = 'https://www.ekidata.jp/api/p/' + element + '.json';
      $.ajax({
        url: trainUrl,
        type:'GET',
        dataType: 'script',  //ここを修正
        timeout:1000,
        success: function(data, dataType) {
              xml.data.line.forEach(function(value){
              $('#train_result').append(value.line_cd + '：' +  value.line_name + '<br>')
            });
          },
        error: function(XMLHttpRequest, textStatus, errorThrown) {
              alert(""ng"");
              console.log(""ng"", XMLHttpRequest, textStatus, errorThrown);
          }
      });
    })
  })


これで期待通りに動きました。めでたしめでたし。

まとめ
APIに書いてあることが全てそのままであるとは限らないと学びました。
JSON形式って書いてあったらJSONデータそのものかと思うじゃないですか普通...。
本来の目的であったAjaxもエラーと格闘しながら学べたと思います。
ついでにJSONも。JSONPなんて存在も知らなかったです。
あと変数名大事ですね！！！！！！

参考
最後に参考にしたページを載せておきます。助かりました。
Ajaxで外部APIに通信する方法がうまくいかない
クロスドメインで使う XMLHttpRequest と JSONP のお話
",False,https://qiita.com//n_skij/items/03075222ba7ef2916f3e
"pandasの勉強を始めたばかりなのですが、下記を見た時にPyCon JP 2018 Tutorial の「Pythonを用いたデータ分析入門」で
「エクセルで出来ることは全部Pandasでできる」
と言われたのを思い出したのを機に作ってみました
自分がよく使うものを中心に基本的なものばかりです
SQL と Pandas の対応表
https://qiita.com/takaiyuk/items/5232442eaeb01299b265

準備

使用データ
下記のデータを使います
総務省統計局
https://www.stat.go.jp/data/nihon/02.html
第2章　人口・世帯
 2- 2　都道府県別人口と人口増減率（エクセル：34KB）
https://www.stat.go.jp/data/nihon/zuhyou/n180200200.xls

ちょっと加工
エクセルを開いて北海道～沖縄以外の行を消してcsvで保存します
(ファイル名はn180200200.csvとします)
※xlrdのインストールをしていません

データの説明
北海道から沖縄までの人口データのファイルです
項目は下記となっています



No
列
項目




0
A列
都道府県名


1
B列
平成22年人口(千人)


2
C列
平成27年人口(千人)


3
D列
人口集中地区


4
E列
人口密度


5
F列
人口増減率（平成22～27年）


6
G列
平成28年推計総人口


7
H列
平成28年推計人口性比（女性100に対する男性）


8
I列
平成28年推計人口増減率（対前年）（人口1,000につき）




使用環境
python3.5
jupyter notebook

基本的なもの

並べ替え

平成27年の人口の多い順に並べ替え


エクセルの場合
「n180200200.xls」の10行～56行を選択
→リボンの「データ」押下
→並べ替え押下
→優先されるキーで「列B」と「降順」を選択
→OKを押下



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"", encoding=""shift-jis"", header=None)
#カンマを削除
df[2]=df[2].str.replace(',', '')
#型変換しないと文字列でソートされます
df[2]=df[2].astype('int')
df.sort_values(2,ascending=False)


実行結果



-
0
1
2
3
4
5
6
7
8




12
東京
13,159
13515
13,295
6,168.7
2.7
13,624
97.2
8.0


13
神奈川
9,048
9126
8,616
3,777.7
0.9
9,145
99.7
2.0


26
大阪
8,865
8839
8,456
4,639.8
-0.3
8,833
92.7
-0.8


22
愛知
7,411
7483
5,802
1,446.7
1.0
7,507
100.1
3.2


10
埼玉
7,195
7267
5,828
1,913.4
1.0
7,289
99.7
3.2


11
千葉
6,216
6223
4,622
1,206.5
0.1
6,236
98.8
2.1


27
兵庫
5,588
5535
4,299
658.8
-1.0
5,520
91.2
-2.7


0
北海道
5,506
5382
4,047
a)68.6
-2.3
5,352
89.1
-5.6



・・・



フィルタ

人口密度が1000人以上を抽出


エクセルの場合
E9を選択
→「データ」リボンの「フィルター」押下
→フィルタで「数値フィルター」
→「指定の値以上」を選択
→1000を入力
→OKを押下



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
#カンマを削除
df[4]=df[4].str.replace(',','')
#先頭から数字以外で始まる文字を削除
df[4]=df[4].str.replace('^[^1-9]+','')
df[4]=df[4].astype('float')
df.loc[df[4] > 1000]


※エクセルでは文字列がユーザ書式になっていました
実行結果



-
0
1
2
3
4
5
6
7
8




10
埼玉
7,195
7,267
5,828
1913.4
1.0
7,289
99.7
3.2


11
千葉
6,216
6,223
4,622
1206.5
0.1
6,236
98.8
2.1


12
東京
13,159
13,515
13,295
6168.7
2.7
13,624
97.2
8.0


13
神奈川
9,048
9,126
8,616
3777.7
0.9
9,145
99.7
2.0


22
愛知
7,411
7,483
5,802
1446.7
1.0
7,507
100.1
3.2


26
大阪
8,865
8,839
8,456
4639.8
-0.3
8,833
92.7
-0.8


39
福岡
5,072
5,102
3,693
1023.1
0.6
5,104
89.6
0.6






関数

sum

B60に平成22年の都道府県の人口を合計


エクセルの場合
B60を選択する
→=sum(B10:B56)を入力



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
#カンマを削除、型変換
df[1]=df[1].str.replace(',','')
df[1]=df[1].astype('int')
df[1].sum()


実行結果
128054



average

B60に平成22年の都道府県の人口を平均する


エクセルの場合
B60を選択する
→=average(B10:B56)を入力



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
#カンマを削除、型変換
df[1]=df[1].str.replace(',','')
df[1]=df[1].astype('int')
df[1].mean()


実行結果

2724.553191489362


max

F60に人口増減率（平成22～27年）の最大


エクセルの場合
F60を選択する
→=max(F10:F56)を入力



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
df[5]=df[5].astype('float')
df[5].max()


実行結果

2.9


min

F60に人口増減率（平成22～27年）の最小


エクセルの場合
F60を選択する
→=min(F10:F56)を入力



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
df[5]=df[5].astype('float')
df[5].min()


実行結果

-5.8


countif

A60に山の字の入った都道府県の個数を数える


エクセルの場合
A60を選択する
→=COUNTIF(A10:A56,""*山*"")を入力



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
df[0].str.contains('山').sum()



実行結果

6


sumif

G60に山の字の入った都道府県の平成28年推計総人口の合計


エクセルの場合
G60を選択する
→=SUMIF(A10:A56,""*山*"",G10:G56)を入力




pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
df[6]=df[6].str.replace(',','')
df[6]=df[6].astype('int')
(df.loc[df[0].str.contains('山')])[6].sum()



実行結果

7267


if

22年と27年の人口を比べて27年が増えていたら、J列に""○""を表示する


エクセルの場合
J10を選択する
→=if(B10<C10,""○"","""")を入力
→J56までコピーする



pandasの場合
import pandas as pd
df=pd.read_csv(""n180200200.csv"",header=None,encoding='shift-jis')
df[1]=df[1].str.replace(',','')
df[1]=df[1].astype('int')
df[2]=df[2].str.replace(',','')
df[2]=df[2].astype('int')
df[9]=""""
df.loc[df[1]<df[2],9]=""○""
df



実行結果



-
0
1
2
3
4
5
6
7
8
9




0
北海道
5506
5382
4,047
a)68.6
-2.3
5,352
89.1
-5.6



1
青森
1373
1308
610
135.6
-4.7
1,293
88.6
-11.3



2
岩手
1330
1280
408
83.8
-3.8
1,268
92.8
-9.1



3
宮城
2348
2334
1,495
320.5
-0.6
2,330
95.6
-1.6



4
秋田
1086
1023
358
87.9
-5.8
1,010
88.5
-13.0



5
山形
1169
1124
491
120.5
-3.9
1,113
92.7
-9.6



6
福島
2029
1914
816
138.9
-5.7
1,901
97.9
-6.9



7
茨城
2970
2917
1,113
478.4
-1.8
2,905
99.4
-4.2



8
栃木
2008
1974
892
308.1
-1.7
1,966
98.9
-4.2



9
群馬
2008
1973
788
310.1
-1.7
1,967
97.5
-3.0



10
埼玉
7195
7267
5,828
1,913.4
1.0
7,289
99.7
3.2
○


11
千葉
6216
6223
4,622
1,206.5
0.1
6,236
98.8
2.1
○


12
東京
13159
13515
13,295
6,168.7
2.7
13,624
97.2
8.0
○


13
神奈川
9048
9126
8,616
3,777.7
0.9
9,145
99.7
2.0
○


14
新潟
2374
2304
1,121
183.1
-3.0
2,286
93.9
-8.0



15
富山
1093
1066
403
251.0
-2.5
1,061
93.7
-4.7



16
石川
1170
1154
594
275.7
-1.3
1,151
93.9
-2.7



17
福井
806
787
346
187.7
-2.4
782
94.2
-5.5



18
山梨
863
835
261
187.0
-3.3
830
95.8
-6.3



19
長野
2152
2099
719
154.8
-2.5
2,088
95.0
-5.1



20
岐阜
2081
2032
776
191.3
-2.3
2,022
93.9
-4.9



21
静岡
3765
3700
2,216
475.8
-1.7
3,688
96.9
-3.4



22
愛知
7411
7483
5,802
1,446.7
1.0
7,507
100.1
3.2
○


23
三重
1855
1816
789
314.5
-2.1
1,808
94.9
-4.2



24
滋賀
1411
1413
702
351.7
0.2
1,413
97.3
-0.1
○


25
京都
2636
2610
2,181
566.0
-1.0
2,605
91.7
-1.9



26
大阪
8865
8839
8,456
4,639.8
-0.3
8,833
92.7
-0.8



27
兵庫
5588
5535
4,299
658.8
-1.0
5,520
91.2
-2.7



28
奈良
1401
1364
884
369.6
-2.6
1,356
89.2
-5.9



29
和歌山
1002
964
359
203.9
-3.9
954
88.8
-9.9



30
鳥取
589
573
212
163.5
-2.6
570
91.3
-6.8



31
島根
717
694
168
103.5
-3.2
690
92.5
-6.4



32
岡山
1945
1922
897
270.1
-1.2
1,915
92.3
-3.6



33
広島
2861
2844
1,834
335.4
-0.6
2,837
94.0
-2.3



34
山口
1451
1405
691
229.8
-3.2
1,394
90.0
-7.4



35
徳島
785
756
247
182.3
-3.8
750
91.0
-7.4



36
香川
996
976
318
520.2
-2.0
972
93.8
-4.3



37
愛媛
1431
1385
733
244.1
-3.2
1,375
89.6
-7.5



38
高知
764
728
317
102.5
-4.7
721
88.9
-10.0



39
福岡
5072
5102
3,693
1,023.1
0.6
5,104
89.6
0.6
○


40
佐賀
850
833
262
341.2
-2.0
828
89.5
-5.4



41
長崎
1427
1377
661
333.3
-3.5
1,367
88.6
-7.5



42
熊本
1817
1786
854
241.1
-1.7
1,774
89.1
-6.7



43
大分
1197
1166
551
183.9
-2.5
1,160
89.9
-5.7



44
宮崎
1135
1104
509
142.7
-2.7
1,096
88.8
-7.2



45
鹿児島
1706
1648
663
179.4
-3.4
1,637
88.4
-6.6



46
沖縄
1393
1434
972
628.4
2.9
1,439
96.7
4.0
○






参考
pandasのDataFrameのデータ操作をよくわすれるので、よく使用する操作を自分のためにまとめた
https://qiita.com/kakiuchis/items/46ff158295686c0c71cf
pythonのdataframe カラム毎にまるごと型変換
https://qiita.com/nkam/items/e6e0feca70fda8fec2b1
pandasの文字列メソッドで置換や空白削除などの処理を行う
https://note.nkmk.me/python-pandas-str-replace-strip-etc/
Pandasで不要な文字を取り除いたり置換する方法まとめ
https://deepage.net/features/pandas-str-replace.html
Markdown記法 チートシート
https://qiita.com/Qiita/items/c686397e4a0f4f11683d
Python Pandasでのデータ操作の初歩まとめ − 後半：データ集計編
https://qiita.com/hik0107/items/0ae69131e5317b62c3b7
Pandasでデータ集計をする際に最低限覚えておきたいメソッド
https://qiita.com/kyo-bad/items/f5ddb7e4b8e7ad9103c5
",False,https://qiita.com//lunar_sword3/items/f68d9af0f426ccc49817
"タイトル通りのことをやったので，メモとして書いた記事です.
ただの忘備録なので，記事の新規性についての突っ込みは勘弁してください．

本記事は以下に該当する人向けです

AI? 機械学習? 楽しそう!
Python? 知らねえ!
TensorFlow? 知らねえ!
Virtual Environment? 強そう!!!!!!!!!

このぐらいの知識量の方向けの記事です．
「なんだか知らんけど動く！なんか楽しい！」を意識した記事です．
このあたりを触った経験がある方は，本記事から得られることがないのでブラウザバック推奨です．

この環境からスタートです
本記事はこんな環境からスタートします．

Windowsが入っている
インターネットにアクセスできる
SSDもしくはHDDに100GBぐらい空きがある
7zipとかの大容量ファイルを展開できるソフトがある


こんなことします
これからやることをざっくりと説明します．

Virtual Boxのインストール
isoファイルを入手
仮想Ubuntuの構築
Pythonのインストール
pipのインストール
TensorFlowのインストール
Sublime Textのインストール
TensorFlowで遊んでみる(テスト)

Pythonは3系を入れます．
2018年11月現在で，あえてPython2系を入れる必要があるケースは稀だと思いますし...

仮想環境ってなに?
テキトーに説明すると，「OSの中にOSを用意する」ことです．
もともとあったOSからメモリとHDDの容量を借りてきて，その資源を使ってOSの中で別のOSを動かすことができます．
そんなことして何がいいの? と思われるかもしれませんが...
よくわからなくなった時に容赦なくリセットできる
という利点があります．
特に，自分で初めて行う環境構築なんかでは「やってるうちに何がなんだかわからなくなった」ということがよくあります．
仮想環境上に大事なデータを入れていない限りは，また環境構築をすればいいだけなので容赦なくデータを吹き飛ばせるというわけです.
「でもいちいち環境を作りなおすの面倒じゃないの?」とお思いの貴方．
作って壊してを繰り返しているうち，環境構築に慣れて楽になってくるので大丈夫です

まずはVirtual Boxから
Virtual Boxは仮想環境を利用するために使うやつです．
ここに飛んで，「Virtual Box <バージョン> Platform Packages」の「Windows Hosts」をクリックしてダウンロードします．
ダウンロードしたexeファイルを起動すると，セットアップが始まります．
英語がたくさん出てきて「なんのこったよ」って感じになりますが，「Next」を押し続けて大丈夫です．
(デスクトップやスタートメニューを少しでも汚されたくない人は英語を頑張って読んでくださいね)

isoをもらってくる
OSをインストールするには，それ用のファイルが必要です．
Virtual Boxで仮想OSを作成するときも必要になるので，ダウンロードしましょう．
このへんからisoが入ったzipファイルをもらってきます．
余計なものがたくさん入ってますが，セットアップ手順が楽なやつです．
最新のバージョンをダウンロードします．
1GBもダウンロードするのでけっこうかかります．お茶飲んで待ちましょう．
ダウンロードできたらzipを展開します．
1GB超えたあたりのファイルはWindows標準のソフトやLhaplusでは解凍できなかったりするので，
それが可能なソフトで解凍しましょう．
(そのソフトはどうやってインストールするの?っていうのは本筋から離れすぎるので割愛します，ごめんなさい)

Ubuntuの設定
以下，重要そうなところは適宜スクショを載せていきます．
さっきインストールしたVirtual Boxを起動して，左上のほうにある「新規」を押します．

名前はなんでもいいです．
今回は「PythonとTensorFlowのTest」なので「py_tf_test」にしました．
タイプに「Linux」バージョンに「Ubuntu(64-bit)」を選択．

次に仮想OSへ割り当てるメモリサイズの選択です．
普通のPCで遊ぶならせいぜい1~2GBかと思いますが，
(私の環境では)画面酔いするレベルで処理落ちしたので，酔いやすい方はお気をつけて．
私は数秒で酔いました，
仮想OSをインストールした後でも変更できます．

次にストレージの割り当てを行います．「すでにある仮想ハードディスクを使用する」のところにあるフォルダをクリックして，先ほど展開したファイルの中にある「ubuntu-ja-<version>-desktop-amd64.vhd」を選択し，「作成」をクリックします．
これで設定が出来上がりました．Virtual Boxの左側に出てくるのでクリックして選択，
画面左上あたりにある「起動」をクリックします．

仮想環境を削除したい
「なんかうまくいかなくなった」「こいつに100GBも食われてるのやだ」とお思いの貴方．
Virtual Box上の仮想環境アイコンを右クリックして「除去」を選択すると削除できます．

Ubuntuのインストール
日本語で懇切丁寧に書かれているうえ，めんどくさい設定が全部省いてありました．
指示に従ってください!!!!!!以上!!!!!!!!!

前準備
Pythonをインストールする前に，おまじないをかけておきましょう．
左上にある「コンピュータとオンラインリソースを検索」をクリックして，「terminal」と打ちます．
文字だらけの不格好なウィンドウが出てきます．この後は大体こちらで操作します．
terminalで次のように打ち込みます．
$はterminalの入力待ちを示す記号なので，打つ必要はないです．
$ sudo apt-get update

非常にざっくりと説明すると，「大抵の人が使うようなソフトをインストールできる状態にしておく」コマンドです．

Pythonのインストール
お待たせしました．Pythonをインストールしましょう．
Pythonは可読性の高さ，扱いやすさ，拡張しやすさが特徴の言語です．
機械学習系の開発が盛んなので，今回はこれを使って楽に機械学習してみましょう．
一緒にpipも入れておきましょう．Pythonを楽に拡張するためのツールです．
terminalで次のように打ち込んでください．
$ sudo apt-get install python3
$ sudo apt install python3-pip


TensorFlow(keras)のインストール
次にTensorFlowをインストールします．
TensorFlowは，雑に言うと機械学習でよく使うやつです．
Kerasがあると楽に機械学習ができるので，そっちも入れてしまいましょう．
terminalで次のように打ち込みます．
$ pip3 install tensorflow
$ pip3 install keras


sublimetextのインストール
正直なところエディタはなんでもいいのですが，今回はSublimeTextをインストールします．
ここからダウンロードできるので，適当な位置に展開します．
展開したら起動して，次へ進みましょう．

TensorFlow(keras)で遊ぶ
MNISTという，手書き文字の判定コードを動かしてみましょう．
下のコードをSublimeTextにコピペして，ホームディレクトリにでも保存しましょう．
「コードに何が書いてあるの?」ってあたりは割愛します．

sample.py
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop

batch_size = 128
num_classes = 10
epochs = 20

# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(60000, 784) # 2次元配列を1次元に変換
x_test = x_test.reshape(10000, 784)
x_train = x_train.astype('float32')   # int型をfloat32型に変換
x_test = x_test.astype('float32')
x_train /= 255                        # [0-255]の値を[0.0-1.0]に変換
x_test /= 255
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy'])

history = model.fit(x_train, y_train,  # 画像とラベルデータ
                    batch_size=batch_size,
                    epochs=epochs,     # エポック数の指定
                    verbose=1,         # ログ出力の指定. 0だとログが出ない
                    validation_data=(x_test, y_test))

score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])



保存したら，terminalで次のように打ち込み，プログラムを起動しましょう．
$ cd ~/
$ python3 keras.py

実行後，データを読み込むため出力まで多少時間がかかります。
下のような見た目になれば成功です．
(値は実行ごとに多少の差があると思います)
Using TensorFlow backend.
60000 train samples
10000 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 512)               401920
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_3 (Dense)              (None, 10)                5130
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0
_________________________________________________________________
Train on 60000 samples, validate on 10000 samples

Epoch 18/20
60000/60000 [==============================] - 7s 117us/step - loss: 0.0182 - acc: 0.9948 - val_loss: 0.1091 - val_acc: 0.9829
Epoch 19/20
60000/60000 [==============================] - 7s 118us/step - loss: 0.0199 - acc: 0.9951 - val_loss: 0.1045 - val_acc: 0.9834
Epoch 20/20
60000/60000 [==============================] - 7s 119us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.1097 - val_acc: 0.9835
Test loss: 0.10974634569421897
Test accuracy: 0.9835


おわりに
正直なところ，本記事と同一内容の記事は大量にあるので，
この記事では「とりあえず動いて楽しくなれる」ことを意識した記事を書きました．
今回使用したUbuntuの日本語Remixですが，libre officeをはじめとして不要なものがごろごろ転がっているうえ，
妙に重くて使いにくいと感じたので，気分がのれば本家のisoでやり直したものに更新するかもしれません．
",False,https://qiita.com//ANNEX_IBS/items/59f8ff7200bf29e4e53a
"

始めに
どうも初めまして。Qiita初投稿のhashimotoと申します。
今年26才にして新卒として人材業界に入り、ひょんなことから会社内での異動でSEとして8月から働き始めました。
いつもQiitaでみなさんの投稿を楽しく読ませて頂き、更に業務中のわからない点を調べるのに、利用させていただいております。
最初の投稿ということで、SEとなるまでPCでニコニコ動画見たりyoutube見たりくらいのネットリテラシーしか持ち合わせていなかった私が、プログラミングを悪戦苦闘しながら覚えてきたこの3ヶ月で、どんなことが難しく感じ、どんなことを簡単だったと感じたかをだらだらと綴っていこうかなーと思います。プログラミング学習をほっぽって、楽しい楽しい初心者向け記事を読み漁った感想を述べていこうかなーと思います。

この記事の目的
　特にありません。というのは明らかにQiitaという技術記事投稿サイトに投稿する理由としてはそぐわないですね（そもそもポエムの時点でダメという前提は今回は目を瞑ってください...）
　この記事の目的は、

最近よく目にする、学ぶべき言語は？とか、初心者向けの記事とは？という素朴な疑問に対する自分なりの解釈を伝えたい
自分と同じ初心者を応援する
自分に興味を持って欲しい

の３つです。
とりあえず、目的がわかった状態で私の文章を読んでいただければ、乱文ですが、少しは私が伝えたいことがわかっていただけるのかなーと思いますので、どこか頭の片隅にでも置いて読んで見てください。

周囲の目って気になりません？
正直、私はキョロ充（古い）です。人の目が気になってしょうがない人です。
試験時間とか、作業する授業とかだと、周囲をチラチラみて「周りの人ってどうやってるのかな？」と意識してしまうような人間です。
これよく見たらただのカンニング野郎じゃないですか…夜中に文章を書いてはいけない(戒め)
　そんな私なので、プログラミングを始めたときには周囲のみんながどれだけ頑張っているかが気になってしょうがなかったです。
　特に、自分と同じ初心者がどれだけ頑張っているのかがわかれば頑張る指標となるし、ロールモデルとなれるかなと思いそんな記事ないかなーと検索していました。

ネットの記事は大体その言語の熟練者が書いている
　読めばわかりますが、記事は大体わかっている人が書いてくれています。なので、大体の記事は、熟練者の方が『あーーー、初心者の頃にこうやっておけばなーーー、もっと楽になったのになーーー』という自戒の念と、初心者への「だから、こうやってやるといいよ！」という善意１００％で書いてくださってます。
　でも、それは私にとってかなり罠だったと思います。成功事例を読むのも楽しいし、初心者への熟練者からのアドバイスは為になるものが多いです。そして全員がみんなに読んでほしいから記事を書いているわけなので、推敲を重ねて記事を書きますので、すごい面白いんですよね。
　でも島根県でフリープログラマーとして頑張っている女の子の話を読んでも、すごいなぁとは思うけれども、結局何を頑張ってたのかに関しては触れられていないし、「成功したよ！」とだけ言われても「過程をくれよ...」という感想しか浮かばない。
　そして、月給20万ちょっとの私が、やっとの思いで「はぁ、はぁ、よし、PHP、javascriptを使って研修用のFAQサイトを完成させることができたぞ！」と達成感に浸っていたときに、
全ての開発者が学ぶべき5つの言語なんて記事を読んで見たときの衝撃たるや。
　全然知らない言語がポンポン飛び出してきたかと思えば、「あれやれ、これやれ、これなら成長できるぞ」
といったガイドラインがあるのですよ。その中でPHPに関しては一切触れられてないし、でも会社的にもPHPをやらねばならないし。
　ものすごくひねくれた見方をすると、自分がやっていることに対して、否定されているとも取れてしまいました。
（もちろん、元記事の投稿者、及び翻訳してくださったpoly_softさんにそんな意図は絶対にないと思います。記事自体、とても興味深く、私も楽しく読ませていただきました。翻訳して頂きありがとうございました）

世の中は初心者に優しくしようとしすぎている
　こんな記事（この言語が次のトレンドだ！とか、この言語をやっておけば間違いない！といった記事など）は、ネットを少し周回すれば、様々な有識者が思う最善の記事として投稿されているものが、大量に見つかります。
それも、そのライターが熟練のライターだけあって、それぞれの記事が物凄い説得力と読ませる文章力を保持していた状態で。
初心者応援するのはいいですし、見てきた人にしかわからないものがあるのも事実です。　
しかし、こういった情報が氾濫すると、最初に『プログラミング学びたいなー』と思ってネットで調べる段階で、情報の濁流に呑まれて何から手をつけたらいいのかわからなくなってしまいそうです（実際自分は異動が決まったときに、最初に前提として学ぶべき言語とかないかなーと思って調べて、いろんな面白いブログ、記事を2時間かけて読んで、異動してから上司先輩に聞こ、という結論に達しました）。

解説記事や初心者向けの記事は駆逐されるべきなのか？
　今までの私の論調を見て、『初心者の為に記事書いても初心者を惑わすだけなら、書かない方がいいの？』
という結論に達してしまうかと思われてしまいそうですが、私の意見は、そんなことは全くないです。
善意で書いてくれて、とても楽しくなるように趣向が凝らされているこういった記事は、その言語を学びたい初心者が最初に読むべき記事だとすら思います。ガイドラインもしっかりしていて、理由も明白だからです。
絶対的に必要なのは、初心者の目的意識です。
データサイエンティストになりたいのならpythonやった方がいいとか、webアプリケーションを作りたいならPHPかJavaとか、iOSアプリを作成したいならswiftだとか、各個人が、何を作りたいのか、何を会社から求められているのかを明確にすることが一番大事なのです。
これを読んでいる初心者のあなたに問いたいです。
あなたはプログラミングを学んで、何をしたいですか？正直、とりあえず手に職つける為。今よりお金を稼ぐ為という人もいるかと思います。
ちなみに偉そうにいっていますが、私はまだわかっていません。やっていくうちに、やりたいことが見つかればと思っています。

　記事を読むのが好きな初心者のあなたへ
　こんな場末の記事を読むほど、活字ジャンキーなあなたへ送ります。記事を読むときに、目的意識を持つことが、非常に大事です。今までの論調から、初心者が読むべき記事を明記することは私は避けさせていただきます。ただ、各プログラミング言語が何に強いのか、そして何ができるのかくらいは見ておくべきかもしれません。
　
　あなたが自分の目的に合う言語を選択できることを心から祈っています。

最後に
ここまで読んでくださりありがとうございます。
　正直、私が上記で色々意見させていただいた、初心者向けに作成された時間泥棒な記事は、とっても大好きです。
面白くて、為になる。
　お金を稼げる言語だとかAIを学習するのにいい言語だとか、わかりやすい言語だとか、フルスタックになる為には知らないといけない言語だとか言語にも色々あることもしれたのはこういった記事のおかげです。
　しかし、一概にプログラミング言語といってもこのような多様性がある昨今、こういった記事を読むだけで無限に時間が取られてしまうことも事実だと思います。
　どうか、初心者の皆様が、無限に時間を取られることなく、自分の目的を達成することに、この記事が少しでもお役に立てれば嬉しいです。
　最後になりますが、私への誹謗、中傷はいくらでも受け付けますが、初心者向けの記事を書いてくださっている方々に対しての批判は無いようにお願いいたします。
　私が一番伝えたいのは、初心者は記事を読むときは目的意識を常に忘れずに読もうね！ということです。
　PHPの記事を読んでいるのに、気がついたらPythonの記事とか読んでいる私からの、自分への言葉でした。
もしよろしければ、コメント、いいねお願いします！
",False,https://qiita.com//charmston/items/df31a419a4e57ebe86ba
"お疲れ様です。
SNSなどをシェアしたりすると、サムネイルとして表示される画像のことをOGP画像と言います。
少し前まで、勝手に画像が付いてくるものだと思っていました.....
エンジニアになって、OGPの設定をすることがあったので、
忘れないように備忘録としてQiitaを書いておきたいと思います。
最後に便利なツールもまとめておきますので、よろしければご活用ください。
また、この記事を読んでアドバイス等ありましたら
是非コメントをよろしくお願いいたします。

そもそもOGP画像とは？
ちなみにOGPとは、”Open Graph Protocol”の略です。
プロコトルと付いているのでお約束事ってことですね！！
「OGP画像」とか「ogpイメージ」と呼ばれることが多いです。
WEBページをシェアしたときに表示される画像は、Webサイトであらかじめ設定する必要があります。
OGPという仕組みによってWebページとSNSが連携しているということなんですね。

OGP画像の重要性
×デメリット
・FaceBook側が自動的に説明文や画像を表示してしまう。
◯メリット
・どのようなページなのかを伝えることができ、ユーザーの訴求率が高まる
・より多くの人にWebページを見てもらえる。
サムネイル画像の表示される方がユーザーの目に留まりやすくなり、
Webサイトに飛んでもらえる確率がUPするので、とても大切ですね！

OGP設定方法
基本的にはmetaタグをHTMLのheader要素に記述することで設定ができます。
WEBサイト・FaceBook・Twitterの３つを設定する必要があります。
/* 共通 */
<meta property=""og:url"" content=""サイトのURL"" /> 
<meta property=""og:title"" content=""サイトのタイトル"" /> 
<meta property=""og:image"" content=""画像のURL"" /> 
<meta property=""og:description"" content=""サイトのディスクリプション"" /> 

/* twitter　　*/
<meta name=""twitter:card"" content=""カード種類"" /> 
<meta name=""twitter:site"" content=""@ユーザー名"" /> 

/* FaceBook */
<meta property=""fb:app_id"" content=""AppID"" />


POINT

■ カードの種類
summary　(小)　/  summary_large_image（大）　のどちらかを設定します。
サムネイルの大きさを選ぶことができます。

■ ＠ユーザー名
＠から始まるアカウント名を入れます。

■ AppID
Facebook上でOGP画像を表示させる場合は、必ずfb:app_idを記載します。
idの発行に付いては下記の記事をご覧下さい。
https://design-plus1.com/tcd-w/2018/01/facebook_app_id.html

使用する画像はどんなものが良いか？
①真ん中に被写体がきている画像
②文字を入れる場合は大きめに
デバイスによっては、横長の画像が正方形で表示される場合があります。
正方形の場合は左右が切り落とされる感じになるので、中心に被写体があるとGOODです。

画像
1200 × 630 (1：1.91)が推奨されています。
比率を守って入れば大丈夫ですが、スマホの表示の際に画質が悪くなる可能性があるので上記のサイズをオススメします。
画像はできる限り大きいほうが良いですね。なお、拡張子は.pngが推奨されています。
.jpgも問題なければ良いかと個人的に思っています。

シュミレーター
作成した画像を確認することができます。
http://ogimage.tsmallfield.com/

注意点
OGP画像を更新した際に、注意すべき点があります。
それは、キャッシュです。
WEBサイトの情報をあらかじめ取得して保存をしています。
あらかじめ保存（キャッシュ）することで、多くのユーザーにシェアされたときにその記事を繰り返し取得する必要がなくなります。
しかし、あらかじめ取得し保存したことで変更がされない場合があります。
そんな時は、OGP情報を更新しましょう。
下記のページにキャッシュをクリアしたいサイトのURLを入れるとクリアすることができます。

Twitter
https://cards-dev.twitter.com/validator

FaceBook
https://developers.facebook.com/tools/debug/og/object/

雑談
instagramやLINEは、手動でクリアすることができず三日ほどかかりました....とほほ

まとめ
OGP設定を行うことで、人の目にふれる機会が多くなります。
どんな画像や文章ならクリックしてもらえるのだろうか？興味を持ってもらえるのだろうか？
と考えながら正しく作成することが大切だと感じました。
",False,https://qiita.com//marietty123/items/532b11315fa10f4f23a7
"

はじめに
営業日のみ日報を送りたかったので作成しました。
上から土日判別、祝日判別を組んでいます。
条件に合致したら、returnで終了してます。
祝日はGoogle Calendarから持ってきています。

//営業日を判別するスクリプト
//土日祝日は”false”,平日は”true”を返す
//祝日はGoogleカレンダーから取得
//目的：土日祝日動いていないアカウントにはメールを送らない

function checkHoliday() {

  var moment = Moment.moment();
  var today = moment.format(""YYYY/MM/DD"");
  var Weekday = moment.format(""dddd"");

  //HACK:elseifのほうが自然?
  if(Weekday == ""Sunday"" || Weekday == ""Saturday""){
    return false;
  }

  var holiday = CalendarApp.getCalendarById(""ja.japanese#holiday@group.v.calendar.google.com"");//祝日カレンダー
  var date = moment.toDate();//getEventsForDayでDATE形式が必要なため
  var holidayEvent = holiday.getEventsForDay(date);

  if(holidayEvent.length > 0){//Googleカレンダーから祝日の有無を確認
    return false;
  }
  return true; 
}

Logger.log(checkHoliday());

",False,https://qiita.com//talaw-K/items/17ff8b7f9b80543dc021
"※だいたい実話です。師匠がた、ありがとうございました。

35歳無職ワイ
ワイ「今日もやる事ないわ〜」
ガチャッ。
師匠（26歳）「就職活動は進んどるんか」
ワイ「！？」
ワイ「はい！quizaという、プログラミングのテストを受けてそのランクを元に企業からスカウトが来るサイトで頑張ってます〜！」
ワイ「（酒飲まん日はな）」
師匠「ほんまか。ああいったサイトで問題を解くなら再帰を使えるようにしておくとすんなり解けることがあるで」
ワイ「はあ、そうでっか」
師匠「再帰は知っとるか」
ワイ「ワイがまた働き始めることでっか」
師匠「それは再起や」
ワイ「はい」
師匠「再帰いうんは、関数の中でその関数自身を使うことや」
師匠「正しくは再帰呼出しやな」
ワイ「そんな事したら無限ループになりまへんか」
ワイ「自分自身に手紙を書いて、その中に”前へ一歩進め。そのあとこの手紙を初めから読み直せ”って書いてあるようなもんやないですか」
ワイ「永遠に歩き続けなあきませんやん」
師匠「お前、意外と頭ええな」
師匠「せやから、終了条件を決めておかなあかん」
ワイ「はあ」

試しにやってみる
師匠「ほな試しに階乗を求める関数を書いてみい」
ワイ「階乗・・・5×4×3×2×1みたいなやつですか」
師匠「せや。言語はRubyでな」
def kaijo(n)
  #処理内容
end

puts kaijo(5)

師匠「こんな感じで使えるやつを書いてみるんや」
ワイ「ほなfor文でiを1ずつ増やすか減らすかしながら掛けていく感じですか」
師匠「それやと再帰呼出しをせえへんやろ」
師匠「再帰を使ってやってみるんや」
ワイ「ええと、kaijoっていう関数の中でkaijoを呼ぶと、そのkaijoの中でもkaijoが呼ばれるから・・・」
ワイ「アカン。夢の中の夢の中の夢の中の・・・みたいになって想像できまへん」
師匠「それが意外と簡単なんや」
師匠「5の階乗いうんは、5に対して4の階乗を掛ければ求まるんや」
ワイ「まあ、それが階乗ですから」
師匠「そしてその4の階乗いうんは、4に対して3の階乗を掛ければ求まるんや」
ワイ「はあ」
師匠「つまり階乗いうんは、元の数字に対して（元の数字-1）の階乗を掛けたもんなんや」
師匠「それが階乗の定義や」
ワイ「階乗を定義するのに階乗って言葉を使ってますやん」
ワイ「せやから、定義自体が再帰してますやんwww」
師匠「せや」
ワイ「えっ」
師匠「それが再帰のコツや！」
師匠「それをそのまま書くだけや」
def kaijo(n)
  return n * kaijo(n - 1)
end

puts kaijo(5)

師匠「基本はこれだけや」
師匠「5の階乗を求めるために、関数kaijoはさっきの定義どおり5 * (4の階乗)を返すんや」
ワイ「kaijoが完成してないのに、kaijoの中でkaijoを使うんでっか」
師匠「せや、自分自身を使う事でkaijoは完成するんや」
師匠「せやけど、さっきも言うた通り終了条件を決めておかんと無限ループになってまう」
師匠「階乗の終了条件は何やと思う？」
ワイ「5×4×3×2×1やから、1が来た時が終了ポイントですわ」
師匠「そうや」
師匠「せやから、こうや」
def kaijo(n)
  if n == 1
    # nが1の時は、再帰せずそのまま1を返す
    return n
  end
  # それ以外は再帰呼出しをする
  return n * kaijo(n - 1)
end

puts kaijo(5)

ワイ「ちょっとスマホアプリのどこでもRubyで試してみますわ」
ワイ「どこでもRuby〜！」
ワイ「実行、と」
どこでもRuby「実行結果：120」
ワイ「ホンマや！合ってるわ！（知らんけど）」

まとめ
師匠「まず5の階乗を求めるのに、4の階乗を求める」
師匠「その4の階乗を求めるのに、3の階乗を求める」
師匠「その3の階乗を求めるのに、2の階乗を求める」
師匠「その2の階乗を求めるのに、1の階乗を求める」
師匠「1の階乗は1って決まっとるからそのまま1を返す」
return 5 * kaijo(4)
# ↓↓↓
return 5 * 4 * kaijo(3)
# ↓↓↓
return 5 * 4 * 3 * kaijo(2)
# ↓↓↓
return 5 * 4 * 3 * 2 * kaijo(1)
# ↓↓↓
return 5 * 4 * 3 * 2 * 1

師匠「こんな感じで、返す値の後ろのほうが段々と変身していく感じや」
ワイ「おお〜、再帰してますやん」
師匠「再帰を使うと一見複雑なゲームが一瞬で解けたりもするで」
師匠「なんてったって定義をそのまま関数に書くだけやからな」
師匠「合計の数を変数に入れて覚えておく必要すらあらへん」
ワイ「ホンマや。変数使ってへん」
師匠「あとは終了条件をif文で書くだけや」
師匠「（ホンマはもう少し別のパターンもあるけどな）」
ワイ「分かりました！」
ワイ「なんか感動しましたわ」
ワイ「再帰ってカッコエエ・・・再帰ハンパないわ・・・」
師匠「ええからおまえも早く再起せぇや」
",False,https://qiita.com//jzmstrjp/items/942dc4cefe0d8794ace3
"

背景
どうも、私がWindowsおじさんです。 
職場でマシンをWindowsからMacへ全面移行することになり、生まれて初めて本格的にMacを使うことになりました。 
自分へのメモを兼ねて、おじさんがMacを扱うための試行錯誤を記録します。

前提

Macのスペック

MacBook Pro 13-inch
macOS High Sierra 10.13.6
Core i7 2.5GHz
memory 8G


おじさんのスペック

Windows歴20年弱
Win10Pro
エディタはVSCode
ターミナルはTera Term
Macに対するリテラシー


触れたことは過去に数回
「使った」ことはない




おじさんの試行錯誤

キー配置がWindowsとめっちゃ違う
おじさんの解決策 

Windows用の日本語109キーボードを繋げる


トラックパッドが器用に操作できない
おじさんの解決策

Windows用マウスを繋げる


マウスのスクロールの方向がWindowsと逆
おじさんの解決策

ナチュラルじゃなくする
「システム環境設定」「マウス」で「スクロールの方向: ナチュラル」のチェックを外す


ファンクションキーがFn押してる時みたいな動きをする
おじさんの解決策

標準のファンクションキーとして使う設定にする
「システム環境設定」「キーボード」「キーボード」で「F1、F2などのキーを標準のファンクションキーとして使用」をチェックする


IMEの挙動がWindowsとめっちゃ違う
おじさんの解決策

Windows風の操作なるものに切り替える
「システム環境設定」「キーボード」「入力ソース」で「Windows風のキー操作」をチェックする


ショートカットがWindowsとめっちゃ違うし、そもそもキーが違う(物理)
おじさんの解決策

キーバインドを変更してWindows風にしてしまう
具体的には karabiner-Elements というアプリを導入する 
これは強力で、かなりWindows風の操作に寄せることができます。

おじさんのキーバインド変更例(Simple Modifications)


caps_lockをleft_controlにする

PCキーボードの無変換キーを英数キーにする

PCキーボードの変換キーをかなキーにする

left_controlをleft_commandにする

right_controlをright_commandにする

そのほか、karabinerには あらかじめ用意された設定 をインポートする便利な機能がついているので、これを利用します。 
以下はおじさんが導入した設定

5ボタンマウスの「戻る」「進む」を使えるようにする
Change button4,5 to back,forward (rev 1)

HomeキーとEndキーを使えるようにする
PC-Style Home/End

Winキー + Lでスクリーンロック出来るようにする
PC-Style Lock Screen

F5キーでリロード出来るようにする
PC-Style Reload(F5, Ctrl+R)

PrintScreenキーを使えるようにする(いくつかパターンがあります)
PC-Style Screenshot to File (PrintScreen to select) 
Altキー + F4キーでアプリケーションを終了できるようにする
PC-Style Quit Application (Alt+F4 to Command+Q)



標準のターミナルだとHomeとEndが効かない、それになんか見難い。。
おじさんの解決策

別途、ターミナルを入れる
おじさんは定番とされる iTerm2 を入れたらキーが効くようになった
brew cask install iterm2



ショートカットはどうやって作るの？
option + command を押しながら対象のファイル/フォルダをDrag&Drop

Alt + Tab みたいなのはどうやるの？
command + tab + ←/→

Ctrl + PageUp/PageDown みたいなのはどうやるの？
command + option + ←/→

参考記事
Macの操作をWindowsっぽくする

所感
Windows風のキー操作を設定してしまえば、おじさんでもそう違和感なくMacを使うことができました。 
エディタにVSCodeを使っていたことも地味に有益でした。Settings Syncで設定を共有できますし、使用感もほとんど変わらない点が非常に助かります。
",False,https://qiita.com//GEROMAX/items/52328379d726132c73f3
"

目的
とりあえず、すごいざっくりとした目標を。
最終目標はタイトル通りの
「AIを使用したリアルタイムの音声変換」
として、そこに至るまでの小目標として以下２点。
・Pythonである程度コーディングできるようになる
・AIとかある程度理解する
PythonとAIに関してほぼ前提知識がない状態なので、
自分自身の学習目的が結構メイン。
小目標がすごいおおざっぱなのは最終目標に至るまでの過程をほぼ考えてないから。

背景
Pythonとか勉強したいなって思っていたのと、最近はやりのvTuberをやってみようと思ったのでなんか同時にできないかなって考えた結果。
恋声とかのボイチェン使ってもよかったけれども、どうせならPythonの勉強がてらツールから作ってしまえばいいのでは？ということで。

その他
上でもちらっと書いたけどPythonの知識はほぼ皆無、そもそもインストールするところから始まる。
プログラミング全般まで広げればJava,C,JavaScriptとかなら少しはやったことある程度。
また、高卒のため機械学習の記事とか読んでると見る線形代数とかそのあたりの知識は無い。
とりあえず01はPythonのインストールとかから初めてこまめに記事を書いていければいいなと。
タイトルの番号が2桁になっているのは続けてやるぞという意思表示。
仕事の合間にちょこちょこ進めていくことになるのでどれくらいのペースで出来るかわからないがとにかく動く成果物を作っていくことを目標にやっていきます！
",False,https://qiita.com//Shito101839/items/26e1fe2ba976de27dc83
"この記事では配列オブジェクトとハッシュオブジェクトの違いってなに？？？
という方向けに解説していきたいと思います。
最初はなかなかわかりづらいこの２つのオブジェクトですが、頭の中でイメージができるようになると理解が早まると思います。

この記事が参考になりそうな人

配列オブエクトとハッシュオブジェクトの違いがイマイチわからない人
配列オブエクトとハッシュオブジェクトのイメージが掴めない人


配列オブジェクトのイメージ
まずハッシュオブジェクトから解説していきます。
配列オブエクトとは複数のオブジェクトに対して番号をふる形で格納するオブジェクトです。
CDアルバムを例えに説明してみます。
CDアルバムはタイトルがあって、その中に曲が入っていますよね？
下記のようなイメージです。

RIOT ON THE  GRILLというタイトルのCDのなかに１曲目RedHot 2曲目モンスター 3曲目Snake Fightingという風に曲が並んでいます。※ここではアーティスト名はおいておいてください。
これを配列オブジェクトで書くと

sample.rb
riot_on_the_grill = [""Red Hot"",  ""モンスター"", ""Snake Fighting""]


となります。
ただここで一つ注意点があります。
それは配列オブジェクトのインデックスは０からスタートするということです。
要するに左から順番にRed Hotには0 モンスターには1という風に番号がふられます。
これはコンピューターの数の数え方に準じていますのでそう覚えてしまいましょう。

配列オブジェクトから値を取り出してみる
配列オブジェクトから値を取り出すには配列名　 = [インデックス]という構文を使います。
例えばRed Hotを取り出すには以下のようになります。

sample.rb
riot_on_the_grill = [""Red Hot"",  ""モンスター"", ""Snake Fighting""]
puts riot_on_the_grill[0]
=> Red Hot # Red Hotが出力される



ハッシュオブジェクトのイメージ
続いてハッシュオブジェクトです。
ハッシュオブジェクトには配列オブジェクトのように１番目・２番目・・・という順番がありません。
その代わりオブジェクトと、それを取り出すためのキーがセットになります。
キー => ""オブジェクト""という形式で書きます。
先ほどのCDの例で説明します。
CDにはアルバム名とアーティスト名がありますよね。
アルバム名""RIOT ON THE GRILL""というオブジェクトにtitleというキーを指定、
アーティスト""ELLEGARDEN""というオブジェクトにartistというキーを指定します。
それをsongというハッシュ名で書くと以下のようになります。

sample.rb
song = {:title => ""RIOT ON THE GRILL"", :artist => ""ELLEGARDEN""}


:titleというキーにRIOT ON THE GRILLが入り、artistキーにELLEGARDENが入っていますね。
これでハッシュにキーを持たせてオブジェクトが格納できました。

ハッシュの省略記法
ちなみに省略形での記法もあり、こちらを使うのが一般的です。

sample.rb
song = {title: ""RIOT ON THE GRILL"", artist: ""ELLEGARDEN""}



ハッシュオブジェクトから値を取り出してみる
ハッシュから値を取り出すにはハッシュ名[キー]で取り出せます。

sample.rb
song = { title: ""RIOT ON THE GRILL"", artist: ""ELLEGARDEN"" }

puts song[:title]
=> RIOT ON THE GRILL #出力される
puts song[:artist]
=> ELLEGARDEN #出力される


それぞれの値が取り出せています。
",False,https://qiita.com//yossy_sa/items/a18db075eba952b4e631
"「Amazon Web Services 基礎からのネットワーク＆サーバー構築 改訂版」(日経BP社)を参考に業務でインフラ構築することになりました。
インフラ初心者には特にCHAPTER6の「6−4　踏み台サーバーを経由してSSHで接続する」が難しく、解説が理解できずハマったので自分用メモに残します。

そもそも踏み台サーバーって何？

踏み台サーバーとは、インターネットに直接繋がないサーバーをSSHで接続するために経由されるサーバーのことです。
この図ではプライベートサブネット内のインスタンスはプライベートIPアドレスしか持たないので、インターネットには接続できません。
そこで、このインスタンスにMySQLなどのデータベースソフトをインストールする際には
①パブリックサブネット内のパブリックIPアドレスを持つインスタンスへSSHでアクセス
②そのインスタンスからプライベートサブネット内のインスタンスへSSHでアクセス
③プライベートサブネット内からコマンドを叩き、パブリックサブネットを経由してインストール
という流れになります。
２つのサーバーは同じVPC内にあるので、プライベートIPアドレスしか持たないインスタンスへも②の方法でアクセスが可能です。
今回はローカル環境からプライベートサブネット内のインスタンスへSSH接続するまで②をまとめました。

踏み台サーバーを使った接続方法

パブリックサブネット内のインターネット接続可能なインスタンスを「インスタンス①」、プライベートサブネット内のインスタンスを「インスタンス②」とします。
また、インスタンス①の秘密鍵ファイルは""my-key.pem""、インスタンス②の秘密鍵ファイルは""my-key-private.pem""でどちらも現在はローカル環境のホームディレクトリに存在します。
ローカル環境からインスタンス①へのSSH接続は問題ありません。現在ローカル環境にあるインスタンス①の秘密鍵を使います。
しかし、インスタンス①からインスタンス②へアクセスしようとしても、インスタンス①はインスタンス②の秘密鍵を持っていないのでSSH接続ができません。そこで図中にあるように、インスタンス②の秘密鍵をローカル環境からインスタンス①へコピーする必要があるのです。

鍵のコピーでハマった
ではscpコマンドを使ってインスタンス②の秘密鍵をインスタンス①にコピーしてみます。
scpとはSSHプロトコルを用いて安全にファイルをコピーするためのコマンドです。
[ローカル側]
$ scp -i my-key.pem my-key.pem <ユーザー名>@ <インスタンス①のパブリックIPアドレス>:~/

cp: <ユーザー名>@: No such file or directory
<ユーザー名>@<インスタンス①のパブリックIPアドレス>:: Permission denied (publickey).

p148のコードをそのまま実行しましたがファイルが見つからないのと、Permission deniedのエラーが2つ出てしまいました。
ちなみにユーザー名は本に従って進めていれば""ec2-user""になっていると思います。

正しいコマンドを確認

$ scp -i <送信先の秘密鍵のパス=インスタンス①の秘密鍵のパス> <送信するファイルのパス=インスタンス②の秘密鍵のパス> <ユーザー名>@<送信先のパブリックIPアドレス=インスタンス①のパブリックIPアドレス>:<ファイルをどこに配置するかのパス>

間違っていた点は

送信するファイルはインスタンス②の秘密鍵なのに、送信するファイルのパスにインスタンス①の秘密鍵のパスを書いていた
ユーザー名と送信先のパブリックIPアドレスの間のスペースは不要　(本にはスペースが書いてあるけど不要です!)


でした。
インスタンス①の秘密鍵はmy-key.pemですが、インスタンス②の秘密鍵はmy-key-private.pemなので、送信するファイルのパスはmy-key-private.pemです。
上記2つを修正して、No such file or directoryが出ていたのでさらに秘密鍵や送信するファイルのパスを絶対パスで書きます。

$ scp -i ~/my-key.pem ~/my-key-private.pem <ユーザー名>@<インスタンス①のパブリックIPアドレス>:~/
<ユーザー名>@<インスタンス①のパブリックIPアドレス>:: Permission denied (publickey).

Permission deniedが消えません。
調べてみると、秘密鍵へのアクセス制限が無いのでファイルのコピーができないようです。

秘密鍵ファイルの権限を変更
$ chmod 777 ~/my-key-private.pem

chmod　数字　権限を変更したいファイル
で権限が変えられます。（アルファベットで権限を指定する方法もあります）
この数字が誰にどの権限を与えるかを意味しており、７７７はすべてのユーザーにすべての権限を与えることを意味します。
これでもう一度scpコマンドを叩くと
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Permissions 0644 for '/Users/<ユーザー名>/.my-key-private.pem' are too open.
It is required that your private key files are NOT accessible by others.
This private key will be ignored.
Load key ""/Users/<ユーザー名>/.my-key-private.pem"": bad permissions


秘密鍵を他のユーザーからもアクセスができる状態にしないでね！という内容のエラーが出ます。
自分だけがアクセスできるように権限を再度変更する必要があるようです。
$ chmod 700 ~/my-key-private.pem

700は所有者にすべての権限を与えることを意味します。
実際に権限が変わったかどうかを確認します。
lsはファイルをリスト表示するコマンドで、-lのオプションを付けると詳細も合わせて表示されます。
$ ls -l
total 24
<中略>
-rwx------@  1 <ユーザー名>  staff  1696 Nov  5 14:54 my-key-private.pem

""-rwx------"" の部分は左からファイル種別（ディレクトリならd）、所有者の読み取り(r)、書き込み(w)、実行権限(x)、グループの所有者の読み取り(r)、書き込み(w)、実行権限(x)、他者の読み取り(r)、書き込み(w)、実行権限(x)の有無を表します。
2,3,4番目以外はハイフンになっているので、今回は所有者のみに読み取り、書き込み、実行権限があることが分かります。
インスタンス①にSSH接続し、秘密鍵を受け取る側でも権限を変更します。
$ ssh -i ~/my-key.pem <ユーザー>@<インスタンス①のパブリックIPアドレス>

ここにユーザー名のフォルダ（ec2-user)があるのでこちらの権限を700に変更して先程と同様に変更してみます。
[<ユーザー名>@ip-10-0-1-10 ~]$ chmod 700 ~　
[<ユーザー名>@ip-10-0-1-10 ~]$ cd ..
[<ユーザー名>@ip-10-0-1-10 ~]$ ls -l
total 4
drwx------ 3 ec2-user ec2-user 4096 Nov  5 09:59 ec2-user

所有者に読み取り、書き込み、実行権限が与えられました。
ではローカル環境に戻って鍵ファイルのコピーを再度試します。
$ scp -i ~/my-key.pem ~/my-key-private.pem <ユーザー名>@<インスタンス①のパブリックIPアドレス>:~/

100% 1692   118.6KB/s   00:00

できた！
再度インスタンス①に入ってファイルの一覧を確認してみましょう。
[<ユーザー名>@ip-10-0-1-10 ~]$ ls -l
total 4
[<ユーザー名>@ip-10-0-1-10 ~]-rwx------ 1 ec2-user ec2-user 1692 Nov  6 02:34 my-key-private.pem


ちゃんとコピー出来ているようです。
ではこの先程ローカルからインスタンス①へSSH接続したときと同様にインスタンス②へ接続してみます。

[<ユーザー名>@ip-10-0-1-10 ~]$ ssh -i ~/my-key-private.pem <ユーザー>@<インスタンス②のプライベートIPアドレス>
The authenticity of host '<省略>' can't be established.
ECDSA key fingerprint is <省略>
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '<省略>' to the list of known hosts.

  __|  __|_  )
  _|  (     /   Amazon Linux AMI
 ___|\___|___|

[<ユーザー名>@ip-10-0-2-10 ~]$


この""10-0-2-10""の部分がインスタンス②のプライベートIPアドレスになっていますね。
これでパブリックサブネットを踏み台にしてプライベートサブネットに入れました！
この後は忘れずに鍵ファイルやフォルダの権限を400(所有者の読み込み権限のみ)に戻しておきましょう。
",False,https://qiita.com//Yuki_A/items/ab7d284566b5fd51d9ca
"プログラミングを学び始めたとき、「このプログラムはどうやってこのような動きを実現しているのか」と疑問に感じたことはないでしょうか。私は二進数に変換されているくらいの曖昧な認識しかなく、友人にこの疑問を投げかけられたとき何も応えられませんでした。
そこでプログラムが動く仕組みについて調べてみるとなるほど面白い。
プログラミングを学ぶ上で知っていて損はないと思える内容なので記事にしようと思います。

はじめに
まずなにより私がプログラミング初心者であることを念頭に記事を読んでほしいです。記事の内容に間違い等があれば是非指摘してください。もう一つ大事なのはこの記事はコンパイラ型言語、特にC言語について扱っていきます。

コンパイル
C言語の場合プログラムを実行前にコンパイルという作業があります。これはC言語が二進数で表される機械語を、人間が理解できる形にした高級言語だからです。なのでコンピュータが理解できる機械語に変換する必要があります。
ちなみに、コンピュータで直接処理できるように変換されたコードをオブジェクトコード、プログラミング言語で記述されたコードをソースコードといいます。そして、オブジェクトコードを解析しソースコードを作成することを逆コンパイルといいます。

１と０
機械語は二進数で表されています。二進数で十進数の数を表したり、文字コードにより文字を二進数で表現できることはご存知の方も多いでしょう。もし知らない方がいたら少し調べてみるといいです。
また、論理演算が大事なものとなります。AとBのどちらか一方でも１のとき１を返す論理和(OR)、両方が１のときに１を返す論理積(AND)、１なら０・０なら１を返す否定(NOT)などがあります。これら論理演算を組み合わせて１と０の値を変更し、目的の動作を実現させます。

少し長い機械の話

トランジスタ
機械ではどのように１と０を判断しているのでしょうか。それは電圧で判断しており、多くの場合＋５Vの直流電流が１を表し、０Vが０を表します。そしてこのスイッチの役割をしているのがトランジスタと呼ばれる半導体素子です。論理演算の話に戻ると、論理和は並列、論理積は直列に二つのスイッチを繋いだら良いことがわかりますね。
集積化は進み最新のマイクロプロセッサ－には１０億個を軽く超える数のトランジスターが集積されているようです。１０億個のスイッチがパソコンに内蔵されていると思うとすごいですね。

CPU(Central Processing Unit)
機械語となったプログラムの内容を解釈して実行する装置はCPUです。主に「レジスタ」「制御装置」「演算装置」「クロック」の４つに分かれており、相互に電気的に接続されています。レジスタは処理対象となる命令やデータを格納する領域で一種のメモリーのようなものです。制御装置はメモリー上の命令やデータをレジスタに読み出し、命令の実行結果に応じてコンピュータを制御します。演算装置は、メモリーからレジスタに読み出されたデータを演算します。クロックはCPUが動作するタイミングとなるクロック信号を発生させます。
特に大事なのがレジスタであり、演算を行うデータおよび演算後のデータを格納する「アキュムレータ」や、次に実行する命令が格納されたメモリーのアドレスを格納する「プログラム・カウンタ」などいくつかの種類があり、ひとつのCPUの内部に２０～１００個のレジスタがあります。

メインメモリー
メインメモリー命令とデータを格納するところで、読み書き可能なメモリー素子で構成されています。１バイト(８ビット)ずつにアドレスとよぶ番号がついています。CPUと制御用チップなどを介して繋がっており、CPUがアドレスを指定してメインメモリーに格納された命令やデータを読み出し・書き込みをおこないます。

命令と対象
これまでの説明でプログラムが実行される仕組みが何となくわかってきたでしょうか。ここまでくるとC言語で書かれたプログラムと大差ない気もしてきませんか。C言語は原則ソースコードの上から下に読んでいきます。CPUも同じでプログラムカウンタは命令を一つ実行し終えるごとにアドレスの値を一つずつ増やします。基本、命令とその対象のアドレスは離れています。
例えばAという値が100番地に入っていて、Bという値が101番地に入っているとき、1番地の命令が「レジスタXに100番地の値を読み込みなさい」、2番地の命令が「レジスタXの値と101番地の値の和を102番地に入れなさい」とすれば、A＋Bの値が102番地に格納されるという具合です。
C言語のアドレスやポインタと同じ考えですね。
ちなみに命令部分をオペコード、命令対象をオペラントということも覚えておくとよいでしょう。

おわりに
以上で今回の説明は終わりです。図もなくわかりにくい説明になっていると思いますが、初投稿なのでどうかお許しください......。他の方の記事なども参考にしてくれたらと思います。
また、機械語に近いアセンブリ言語を学ぶと、より理解が深まるかもしれませんので、興味のある方は是非調べてみてください(私も学ぼうと思っています)。

余談
・コンピュータでの処理に適した形式のコード全般をオブジェクトコードといいますが、C言語コンパイラが生成するコードは、特定の機種のみで実行可能なので特にネイティブコードといいます。これはJavaなど機種に依存せず実行できるものと区別するためらしいです。
・逆コンパイルにより得られるソースコードが自分の意図したものであるとは限りません。これはある動作をさせるプログラムの書き方が何通りもあることを考えれば理解できると思います。
・トランジスタのオンオフ切り替えのタイミングは同時であり、１秒間あたりの切り替え回数をクロック周波数という。切り替えがはやいほどコンピュータの性能は高いです。

参考文献
・図解雑学-コンピュータのしくみ　著:山田宏尚
・プログラムはなぜ動くのか　著:矢沢久雄
(twitter → @Programshi )
",False,https://qiita.com//Programshi/items/3011126a0410c0b7d4f9
"今回はgoに備え付けられているコマンドをご紹介します。

コマンド一覧

go help
困った時には、何はともあれヘルプを見てみましょう。
$ go help
Go is a tool for managing Go source code.

Usage:

        go command [arguments]

The commands are:

        build       compile packages and dependencies
        clean       remove object files and cached files
        doc         show documentation for package or symbol
        env         print Go environment information
        bug         start a bug report
        fix         update packages to use new APIs
        fmt         gofmt (reformat) package sources
        generate    generate Go files by processing source
        get         download and install packages and dependencies
        install     compile and install packages and dependencies
        list        list packages
        run         compile and run Go program
        test        test packages
        tool        run specified go tool
        version     print Go version
        vet         report likely mistakes in packages

Use ""go help [command]"" for more information about a command.

Additional help topics:

        c           calling between Go and C
        buildmode   build modes
        cache       build and test caching
        filetype    file types
        gopath      GOPATH environment variable
        environment environment variables
        importpath  import path syntax
        packages    package lists
        testflag    testing flags
        testfunc    testing functions

Use ""go help [topic]"" for more information about that topic.


go build
Goのプログラムをビルドするためのコマンドである。
オプションなどが多々あり少し混乱することがあるので、
下記のようなディレクトリがあるとして話を進める。
app
┣ config.go // package main
┗ main.go // package main

この状況でappディレクトリ直下でgo buildを実行すると、appという名前の実行ファイルが生成される。
build実行時に特定のファイルやパッケージを指定しない場合には、
カレントディレクトリの*.goファイルの全てに対してコンパイルし、
そのビルド結果をカレントディレクトリの名前をもつ実行ファイルとして生成する。
またimportで外部のパッケージに依存している場合でも、依存先のパッケージも合わせてビルドしてくれるため、
開発者は依存するパッケージの場所などをファイルに記載する必要はない。
app
┣ app // 実行ファイルが追加される
┣ config.go // package main
┗ main.go // package main

-xオプションをつけることで、ビルド実行時のログを表示することができる。
ビルド時にエラーが出る際には、このようにログを出してみると何かヒントが得られるかもしれない。
$ go build -x
WORK=/var/folders/rt/659tpm_95t73f1_2dq5zjwpm0000gn/T/go-build175319691
mkdir -p $WORK/b001/
cat >$WORK/b001/importcfg << 'EOF' # internal
# import config
packagefile fmt=/usr/local/Cellar/go/1.10.2/libexec/pkg/darwin_amd64/fmt.a
packagefile math/rand=/usr/local/Cellar/go/1.10.2/libexec/pkg/darwin_amd64/math/rand.a
packagefile runtime=/usr/local/Cellar/go/1.10.2/libexec/pkg/darwin_amd64/runtime.a
EOF
// まだまだ続く...


go install
パッケージや実行ファイルをビルドした結果を、環境変数GOPATH内の特定の場所にインストールする。
go installは実行の過程go buildによるビルド処理も行っている。
go installを実行することで、$GOPATH/src配下に置かれたパッケージのソースコードをビルドし、
そのビルド結果が実行ファイルであれば$GOPATH/binへ、
それ以外であれば$GOPATH/pkgへインストールされる。

go get
外部パッケージのダウンロードとインストールをまとめて実行するコマンドである。
具体的な動作としては、指定したパスからパッケージのソースコードをダウンロードし、対象のパッケージに対してgo installを実行している。
引数は主に次のものがある



引数
意味




-u
対象のパッケージの更新と依存パッケージの更新を検出し、パッケージの再ダウンロードとインストールを実行する


-d
対象パッケージのダウンロードのみを行い、その後停止する


-t
対象パッケージに付属するテストが依存するパッケージも合わせてダウンロードする




go run
ビルドとプログラムの実行を一緒にやってくれる。
簡易的に動作確認したい場合などに有効である。

go env
Goのビルドシステムに関連する環境変数の内容を確認できる。
あの環境変数の設定ってどうだっけ？となったらひとまず叩いてみるといいかもしれない。
$ go env
GOARCH=""amd64""
GOBIN=""""
GOCACHE=""/Users/croco/Library/Caches/go-build""
GOEXE=""""
GOHOSTARCH=""amd64""
GOHOSTOS=""darwin""
GOOS=""darwin""
GOPATH=""/Users/croco/go""
GORACE=""""
GOROOT=""/usr/local/Cellar/go/1.10.2/libexec""
GOTMPDIR=""""
GOTOOLDIR=""/usr/local/Cellar/go/1.10.2/libexec/pkg/tool/darwin_amd64""
GCCGO=""gccgo""
CC=""clang""
CXX=""clang++""
CGO_ENABLED=""1""
CGO_CFLAGS=""-g -O2""
CGO_CPPFLAGS=""""
CGO_CXXFLAGS=""-g -O2""
CGO_FFLAGS=""-g -O2""
CGO_LDFLAGS=""-g -O2""
PKG_CONFIG=""pkg-config""
GOGCCFLAGS=""-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=/var/folders/rt/659tpm_95t73f1_2dq5zjwpm0000gn/T/go-build892773146=/tmp/go-build -gno-record-gcc-switches -fno-common""

たまに確認しそうは環境変数は下記かなと。



環境変数
内容




GOARCH
コンパイラが対象とするCPUアーキテクチャ


GOBIN
go installによってインストールされるコマンドの格納ディレクトリ。デフォルトでは$GOPATH/binとなる


GOOS
コンパイラが対象とするOS環境


GOPATH
パッケージのソースコードとオブジェクトファイル、実行ファイルなどが格納されるディレクトリ


GOROOT
Go本体のインストール元




go fmt
Goのソースコードを推奨される形式へ自動的に整形するためのコマンドである。
go fmtは文法エラーのチェック、インデントや空行の適正化、不要な空白などの削除などを行う。
コマンドはgo fmt [-n] [-x] [packages]という感じである。
例えば下記のようなコードがあったとして、 go fmtと叩く

main.go
package main

import(
""math/rand""
""fmt""
)

func main() {
i := rand.Intn(10)

        fmt.Println(i)
}


すると下記のようにインデントなどを綺麗に整えてくれる。

main.go
package main

import ( // インデントだけでなく、アルファベット順に並び替えられている！
    ""fmt""
    ""math/rand""
)

func main() {
    i := rand.Intn(10)

    fmt.Println(i)
}


ちなみに、
-nオプションは実行されるコマンドの表示のみを行いたい場合につける（実行してもファイルは書き換えられない）
-xオプションは実行されるコマンドを表示する
$ go fmt -n
/usr/local/Cellar/go/1.10.2/libexec/bin/gofmt -l -w main.go


go doc
Goのパッケージのドキュメントを参照するためのコマンドである。
このパッケージの使い方ってなんだっけ、となった際に叩いてみるといいかもしれない
例えばfmtパッケージの場合は下記のような情報が返される
$ go doc fmt
package fmt // import ""fmt""

Package fmt implements formatted I/O with functions analogous to C's printf
and scanf. The format 'verbs' are derived from C's but are simpler.


Printing

The verbs:

General:

    %v  the value in a default format
        when printing structs, the plus flag (%+v) adds field names
    %#v a Go-syntax representation of the value
    %T  a Go-syntax representation of the type of the value
    %%  a literal percent sign; consumes no value

Boolean:

    %t  the word true or false

// まだまだ続く...

また、パッケージの後にピリオドで区切るとパッケージの特定の関数などを指定することもできる。
fmt.Println関数のドキュメントを参照する場合は下記のような情報が返される。
$ go doc fmt.Println
func Println(a ...interface{}) (n int, err error)
    Println formats using the default formats for its operands and writes to
    standard output. Spaces are always added between operands and a newline is
    appended. It returns the number of bytes written and any write error
    encountered.


go test
goのパッケージに付属しているテストを実行するためのコマンドである。
-coverオプションをつけることでテストのカバレッジ率を計測することもできる。

合わせて読みたい

今日から始めるGolang【言語の特徴と基本文法】


参考

スターティングGo言語
go installコマンドについて

",False,https://qiita.com//yu-croco/items/8f1f8e10246e6f058636
"

はじめに
フロントエンド自体が初心者なので正直この辺りについては全然理解が追い付いてないです。
今回は簡単なサンプルを作りながら自分なり理解していこうと思います。

メソッドハンドラ
Vue.jsでは「v-on」ディレクティブを用いることで要素にイベントリスナをバインドします。
<button v-on:click=""hoge"">Click me!</button>


イベント一覧
※すべてではないです 。

click, dblclick
クリック、ダブルクリック時に発火するイベントです。
See the Pen yQaMpY by b1san (@b1san1) on CodePen.


keyup, keydown
キーアップ、キーダウン時に発火するイベントです。

キー修飾子
キー修飾子を指定することで任意のキーのみでイベントを発火させることができます。
キーコードで指定も可能ですが、以下のように用意されているものもあります。

enter
tab
delete
esc
space
meta
up
down
left
right

See the Pen NERRwZ by b1san (@b1san1) on CodePen.


focus, blur
focusはフォーカスされたとき、blurはフォーカスが外れたときにイベントが発火します。
See the Pen vQXXrz by b1san (@b1san1) on CodePen.


change
フォーカスされてから外れるまでの間に値が変化している場合にイベントが発火します。
See the Pen VVKmMO by b1san (@b1san1) on CodePen.


mouseover, mouseout
mouseoverはマウスカーソルを当てたとき、mouseoutはマウスカーソルが外れたときにイベントが発火します。
See the Pen jQMMdW by b1san (@b1san1) on CodePen.


mouseenter, mouseleave
mouseover, mouseoutと同じくマウスカーソルを当てたとき、外れたときにイベントが発火しますが、子要素にイベントを伝搬させないという違いがあります。
サンプルを動かしてもらうと両者のイベントの発火数の違いがわかると思います。
mouseover, mouseoutは、子要素であるpタグやdivタグ、さらにその子要素のpタグにカーソルを当てたり外した場合でも親側のイベントが発火しています。
一方、mouseenter, mouseleaveは、自身の要素のみにおいてイベントが発火しています。
See the Pen YRGpzd by b1san (@b1san1) on CodePen.


mouseup, mousedown
mousedownはマウスのボタンを押したとき、mouseupはマウスのボタンを離したときに発火するイベントです。
マウスのすべてのボタンに反応します。

キー修飾子
キーの場合と同じでイベント発火を任意のボタンに絞ることができます。

left
right
middle

See the Pen aQmBWj by b1san (@b1san1) on CodePen.


scroll
スクロール時に発火するイベントです。
See the Pen wQzgLE by b1san (@b1san1) on CodePen.


submit
submiされたときに発火するイベントです。
See the Pen zMKoaJ by b1san (@b1san1) on CodePen.


イベント修飾子
イベントを制御するために次のような修飾子が用意されています。

stop
イベントの伝搬を防ぎます。(stopPropagation()）
例では子要素（背景緑）側にstop修飾子を設定することで親要素（背景黒）のclickイベントの伝搬を止めています。
See the Pen dQpNOG by b1san (@b1san1) on CodePen.


prevent
要素のデフォルトアクションを抑止します。（preventDefault()）
例ではaタグのhref要素にalert()を仕込んでいますが、preventを設定しているほうでは発火していないことがわかります。
See the Pen qQaRxb by b1san (@b1san1) on CodePen.


capture
addEventListener() optionsのcaptureに該当します。
例を動かすとわかる通り子要素（背景緑）をクリックした場合のイベント発生順序が異なります。
See the Pen RqGKJg by b1san (@b1san1) on CodePen.


self
イベントが設定された要素自身にアクションした場合のみイベントが発火します。
例だとselfを設定していない場合は、親要素内のどの要素をクリックしてもイベントが発火しています。
selfを設定した場合だと、親要素内の要素をクリックしてもイベントは発火せず、自身の要素をクリックした場合のみイベントが発火しています。
See the Pen jQMydd by b1san (@b1san1) on CodePen.


once
addEventListener() optionsのonceに該当します。
イベントの発火を一回に制限します。
See the Pen WYGRBL by b1san (@b1san1) on CodePen.


passive
addEventListener() optionsのpassiveに該当します。
preventDefault()の処理に依存せず処理を実行できるため、Scroll Jankの解決策として利用されます。

おわりに
フロントエンドのイベント理解するの難しい( ;∀;)
",False,https://qiita.com//b1san/items/6bc0be17cd6ed687520c
"　　
　　
　　
この記事は
『プログラミング完全未経験からUnityでの開発現場に迎え入れてもらえた世界一の幸せ者』
の記事です。そのつもりでお読みください。
　　
　　

ゲームオブジェクトの色をスクリプトから変更する方法
ゲームオブジェクトの色を変更とありますが
正確には""ゲームオブジェクトのmaterialのプロパティを変更する""？ですね。
なのでまずはmaterialのプロパティを確認しましょう。
　　
ゲームオブジェクトのInspector画面で下の方までスクロールすると、
画像のようにmaterialの設定を見れる？ようなやつがあるので
右上の歯車っぽいやつを押してみてください。

そうするとSelect Shaderというのがあるのでそこに飛びましょう。

そして、Propertiesの_Colorと書いてあるところを確認してください。
ここが_TintColorとなっている場合があります。
どちらになっているかで若干コードが変わります。


①_Colorの場合
GetComponent<Renderer>().material.color = Color.red;


②_TintColorの場合
GetComponent<Renderer>().material.SetColor(""_TintColor"", Color.red);

正直、プロパティの_Colorとか_TintColorとかはよくわかりません。
もしわかる人いたら教えてください。
あと、色の変え方調べてたら
this.renderer.material.color = Color.red;

というやり方がけっこう出てくるんですが、旧形式の書き方なので注意してください。

""情弱乙ｗｗｗ""と煽られます。
",False,https://qiita.com//OKsaiyowa/items/ea2bd4898dc9ad36fe70
"世間の求人募集を見ての所感ですが、web業界のバックエンドはスクリプト言語はRuby、コンパイラ言語はGoがブームですね。
Ruby, node.jsなどのスクリプト言語ばかりに手をつけていた私も、最近ようやくGoを勉強してみようかなと思い始めました。
今回はGoの特徴と基本文法をまとめました。
これからGo触ってみようかなーといった方の参考になれば幸いです。  
記載内容に誤りやツッコミどころがありましたら、マサカリ飛ばさずに優しくコメント頂ければ幸いです。　 

Goの概要
Goは2009年頃にGoogleの内部プロジェクトとして開発がスタートした静的型付け言語です。
開発の発端はGoogleですが、オープンソースプロジェクトです。
Goは当初、Googleのエンジニアが既存の言語（恐らくですが、Googleで特に使われているPython, C++）の良いところ取りをした言語を作るという試みで始まったようです。
構想の主なポイントは下記です（wikiより）。

JavaやC++のように、静的に型付けされ、巨大なシステムでもスケールする
RubyやPythonなどの動的な言語のように生産性が高く、リーダブルであり、過度なボイラープレートが必要ない
IDEが必須ではない。ただし、十分にサポートする
ネットワークおよびマルチプロセッシングをサポートする

超絶ざっくりまとめると、
静的型付けだけど動的型付けみたいに読み書きがしやすい言語
を目指した言語です。
特徴としては、

シンプルな言語仕様(継承・Genericsなどがない)


HaskellやScalaなどの他の静的型付け言語に比べて学習コストが低い


パフォーマンスが良い


C++に迫る勢いだとか


コンパイルが早い
GCとメモリ安全性
平列処理が書きやすい


goroutineとchannelの活用


Googleの後押しがある
Dockerなどの有名なプロジェクトで採用されている実績がある

余談ですが、体感としてはGoよりGolangの方がググラビリティが良いです（特に英語の記事）。

基本文法

ザックリ仕様

ファイルの拡張子は.go

変数や関数など、プログラムを構成する要素はすべて何らかのパッケージに属している
一つのファイルに記述できるのは単一のパッケージについてのみである（一つのファイルに複数のパッケージに関する処理は定義できない）
ファイルの内部で使用するプログラムを外部から参照する場合にはimportを使用する
エントリーポイント（実行が開始される場所）はmainパッケージの中に記載されている処理

一つのディレクトリには一つのパッケージに関する定義のみ記載できる


パッケージのスコープ
Goのスコープの単位は大きい単位順に、パッケージ、ファイル、関数、ブロック、制御構造文がある。
パッケージに定義されている定数、変数、関数などが、他のパッケージから参照可能かどうかは、識別子の一文字目によって決まる。
一文字目が大文字の場合には他のパッケージから参照可能であり、一文字目が小文字の場合には他のパッケージから参照不可能である。
package fruit

const (
  A = ""banana"", // 外部から参照可能
  b = ""apple"" // 外部から参照不可能
)

var (
  C = ""water melon"",  // 外部から参照可能
  d = ""pineapple"" // 外部から参照不可能
)

// 外部から参照可能
func Harvest() {
 ...
}

// 外部から参照不可能
func cultivate() {
 ...
}

あるパッケージから別のパッケージの変数や処理を参照する場合にはimportを使う。
例えば、mainパッケージから上記のfruitパッケージを参照したい場合には下記のように記載する。
package main

import ""fruit"" // fruitパッケージをimportする

fruit.A // ""banana""を参照
fruit.Harvest() // Harvest()を参照
fruit.cultivate() // fruit内のスコープでのみ使えるので、コンパイルエラーとなる


変数
明示的に宣言する場合にはvarを使う。
型推論で変数を定義する場合には変数名 := 値と記載する。
基本的にはvar宣言しなくても型推論で記載できるので、可読性の観点からも型推論で書く方が良さそう。
// 明示的に宣言する場合
var name string
var x, y int // 複数の変数を一度に定義できる
var ( // ()で記載すると複数の型の変数の定義ができる
 x, y int
 z string
)

// 型推論の場合
x := 1
bol := false


定数
constを用いて定義する。
const hoge = ""hoge""
const ( // ()で記載すると複数の定数を一度に記載できる
  fug = ""fuga""
  piyo = ""piyo""
)

GoにはJavaの列挙型(enum)のような機能は無いが、識別子iotaと定数宣言を用いることで列挙型に近い表現ができる。
const (  // iotaは0から始まり、定義されるたびに1ずつ増える
  a = iota // a == 0
  b = iota // a == 1
  c = iota // a == 2
)

const (  // 2つ目以降の宣言を省略することも可能
  d = iota // a == 0
  e        // a == 1
  f        // a == 2
)


基本型

文字列型
文字列はstring型として定義する。
文字列は""""で囲む。
hoge := ""ほげ""

//複数行の文字列はバックスラッシュで書くと便利
fuga := `
Hello
World
`


論理値型
論理値はbool型として定義する。
trueまたはfalseの値を持つ。
// varで記載する場合
var boolean bool
boolean = true
// 型推論で記載する場合
bool := false


数値型
数値はint型として定義する。
実装依存を防ぐためにint64やint32と明確に定義することも可能である。
// varで記載する場合
var n int
n = 1
// 型推論で記載する場合
i := 1

数値の型変換をする場合には、明示的に変換後の型で定義する必要がある。
n := 1
b := byte(n) // byte型へ変換
i64 := int64(n) // int64型へ変換
u32 := uint32(n) // uint32型へ変換


浮動小数点
サイズの異なる2つの浮動小数点であるfloat32とfloat64がある。
float32はJavaでいうところのfloatであり、float64はdoubleである。
f64 := 1.0 // float明示的に型を指定しない場合にはfloat64となる
f32 := float32(1.0) // float32を定義する場合には明示的に宣言する必要がある


配列型
配列型を定義する場合には、配列内部に入っている値の型を宣言する必要がある。
明示的に初期値を与えない場合のデフォルトは、文字列は""""、整数値は0、真偽値はfalseとなる。
arr := [5]int{1, 2, 3, 4, 5} // [n]intの宣言で、配列に格納する要素数をn個で配列を作る

arr2 := [5]int // == [0, 0, 0, 0, 0] 変数作成時に初期値を指定する必要はない

また、配列内の要素数を明示的に記載しない方法もある。
その場合、初期値で与えられた要素数が、配列の要素数となる。
arr := [...]int{1, 2, 3, 4, 5}


参照型

make
参照型の生成には、組み込み関数のmakeを使う（使わなくても定義できる）。



呼び出し方
型T
意味




make(T, n)
スライス
要素数と容量がnであるT型のスライスを生成


make(T, n, m)
スライス
要素数がnで容量がmであるT型のスライスを生成


make(T)
マップ
T型のマップを生成


make(T, n)
マップ
T型のマップを要素数nをヒントにして生成


make(T)
チャネル
バッファのないT型のチャネルを生成


make(T, n)
チャネル
バッファサイズnのT型のチャネルを生成




スライス
いわゆる可変長配列のことである。
スライスは生成時に配列に格納する値の要素数と容量を確保する。
生成時に確保した要素数よりも大きい容量が確保されていると、スライスの要素数を拡張して行く際にメモリ上に新しい領域の確保が不要になる。
その一方で、元の容量よりも大きい数の要素数を入れると、Goは元のスライスが格納していたデータを丸ごと、新しいより大きなメモリ領域へコピーする。
メモリ上の別領域へデータをコピーする処理はコストが高いため、スライスを生成する際にあらかじめ格納される要素数に合わせて容量を確保する方が良い。
var s []int // int型の要素を持つ配列sを生成
s2 := make([]int, 5) // 要素数と容量が5である配列s2を生成

fmt.Println(s2)
// => [0, 0, 0, 0, 0] 初期値を指定していない場合は配列と同様に0が入る

len(s2) // 現在の要素数はlen()を使って調べられる
// => 5

cap(s2) // 現在の容量(capability)はcap()を使って調べられる
// => 5


マップ
いわゆる連想配列のことである。
生成はmap[キーの型]要素の型で定義する。
m := make(map[int]string)
m[1] = ""Banana""
m[10] = ""Apple""

fmt.Println(m)
// => map[1:Banana 10:Apple]

m2 := map[int]string{1: ""Banana"", 2: ""Apple""} // マップ生成時に初期値を与える場合



チャネル
チャネルはキューの性質を備えるデータ構造である。
チャネルにはキューを格納する領域であるバッファを持っている。
チャネルはキューの性質を使い、複数のゴールーチン間で安全にデータを共有するための仕組みである。
生成は make(chan データ型, バッファサイズ) または chan [データ型]で定義する。
<-chanを使用すると、そのチャネルは受信専用チャネルを意味する。
chan<-を使用すると、そのチャネルは送信専用チャネルを意味する。
指定がない場合には、受信も送信も可能なチャネルとなる。
var ch1 <-chan int // 受信専用チャネル
var ch2 chan<- int // 受信専用チャネル
var ch3 chan int // 送受信どちらでもできるチャネル

ch3 <- 5 // チャネルに整数5を送信
i := <-ch3 // チャネルから整数値を受信

具体的なお話については、下記の方々の記事が参考になると思います。
- GoのChannelを使いこなせるようになるための手引
- Go言語のchannelって一体何よ ~基礎編~【golang】

関数
関数の定義にはfuncを使い、func 関数名(引数 引数の型) 戻り値の型 {}で宣言する。
戻り値を持つ場合には、戻り値を返す場所でreturnをつける。
func plus(x, y int) int { // 引数x,yはint型であり、戻り値もint型
  return x + y
}

複数の戻り値を返すこともできる。
func div(a, b int) (int, int) { // 引数a,bはint型、戻り値はint型の値が2つ返される
  q := a / b
  r := a % b
  return q, r
}

func main() {
  q, r := div(19, 7)
  fmt.Println(""商=%d 剰余=%d\n"", q, r)
}

戻り値を破棄する場合には_を使う。
上記の関数divの例を使うとこん感じで書ける。
q, _ := div(19, 7) // 戻り値の2つ目は破棄される

エラーハンドリングは下記のようにするのが一般的っぽい。
result, err := doSomething()
if (err != nil) {
  // エラー処理
}


制御文

if
他言語とほぼ同じであるが、{}は省略不可である。
if x == 1 {
  // x==1の場合に実行する処理
} else if x == 2 {
  // x==2の場合に実行する処理
} else {
  // xが1でも2でもない場合に実行する処理
}


for
条件なしのforは無限ループとなる。
for {
  fmt.Println(""I am in infinite loop"")
}

典型的なforはこんな感じ
for i := 0; i < 100; i++ {
  fmt.Prinltn(i)
  i++
}

rangeと絡めて使うことも多いとか
fruit := [3]string{""Apple"", ""Banana"", ""Melon""}
for i, s := range fruit { // i=配列のインデックス、s=配列内の要素
  fmt.Printf(""fruit[%d]=%s\n"", i, s)
}

// => fruit[0]=Apple
// => fruit[1]=Banana
// => fruit[2]=Melon


switch
n := 3
switch n {
case 1, 2:
  fmt.Println(""1 or 2"")
case 3:
  fmt.Println(""3"")
default:
  fmt.Println(""others"")
}

// => 3


defer
関数の終了時に実行される式を登録できる。
一つの関数内での登録は幾つでもできるが、実行される順番はあとで登録されたものから順番に実行される。
func main() {
  defer fmt.Println(""Hello GO!"")
  defer fmt.Println(""Hello World!"")
  fmt.Println(""done"")
}

// => done
// => Hello World!
// => Hello GO!


panic/recover
Goのランタイムを強制的に停止させる機能を持つ。
panicはプログラムにおいて、これ以上処理を継続できない状態を意味するので、他言語で言う所の例外処理と同じように多用すべきではない。
panicを実行するとランタイムパニック（run-time panic）が発生し、実行中の関数は中断される。
ただし、中断時までに登録されたdeferは全て実行される
package main
import ""fmt""

func main() {
  defer fmt.Println(""Hello go!"") // panic発生時も実行される
  panic(""runtime error!!!"")  // ここで処理が終了する
  fmt.Println(""Hello world"") // これは実行されない
}

panicで上がってきたrun-time errorによるプログラム中断を回復するのがrecoverである。
panicが発生した際にrecoverを実行するためには、recoverをdeferと一緒に使う。
recoverはinterface{}型を戻り値とし、その値がnilではない場合にpanicが実行されたと判断する。
func main() {
  defer func() {
    if x := recover(); x != nil { // panicが発生した場合、 x != nilはtrueとなる
      fmt.Println(x) // 変数xは、panicに渡されたinterface{}
    }
  }()
  panic(""Panic occured!!"")
  fmt.Println(""Hello Go!"") // これは実行されない
}

// => Panic occured!!


go
並行処理を司る機能である。
ゴールーチン（goroutine）と呼ばれる、スレッドよりも小さい処理単位で並行して動作する。
deferと同様に、関数呼び出し形式の式を受け取る。
package main

import ""fmt""
func sub() {
    for {
        fmt.Println(""sub loop"")
    }
}

func main() {
    go sub() //ゴールーチン開始
    for {
        fmt.Println(""main loop"")
    }
}

/*
sub loop
sub loop
sub loop
main loop
main loop
sub loop
...
*/


構造体とインターフェース
Goにはクラスという概念はなく、代わりに構造体（struct）とインターフェース（interface）を用いる。

ポインタ
GoにはC言語でも使われているポインタの概念がある。
ポインタとは、あるデータ構造のメモリ上のアドレスと型の情報である。
ポインタ型は*intのように、ポインタ（*）を操作・参照する対象の型の前に置くことで定義する。
演算子&(アドレス演算子)を用いて、任意の型からそのポインタ型を生成することができる。
var i int
p := &i // iのポインタ型を生成
fmt.Printf(""%T\n"", p) // => ""*int""

ポインタからの変数から値を参照するには、演算子*をポインタ型の変数の前に置くことで、ポインタ型が指し示すデータのデリファレンス（ポインタ型が保持するメモリ上のアドレスを経由して、データ本体を参照する）することが可能である。
var i int
p := &i // iのポインタ型
i = 5
fmt.Println(*p)
// => 5
*p = 10
fmt.Println(i) // iと同じメモリ上の値を書き換えたのでiの参照するあたいも5 -> 10に変わる
// => 10

ポインタについては、こちらの方々の記事がわかりやすいと思います。
- 【Go言語入門】構造体とポインタについて
- Goで学ぶポインタとアドレス
- Goのポインタ

構造体
オブジェクト指向言語で言う所のクラスに相当し、複数の任意の型の値を一つにまとめたものである。
構造体の定義はtype 構造体の名前 struct {フィールド}として宣言できる。
type Point struct {
  x int
  y int
}

type Point2 struct {
  x, y int // 同じ型のフィールドは一括で宣言できる
}


構造体とポインタ
構造体は値型であるため、関数の引数に構造体を渡した場合には構造体のコピーが生成される。そのため元の構造体に対して影響を与えることができない。
type Point struct {
  X, Y int
}

func swap(p Point) { // Pointのstruct型を持つpを引数に持つ
  x, y := p.Y, p.X
  p.X = x
  p.Y = y
}

func main() {
  p := Point{X:1, Y: 2}
  swap(p) // 値渡しで処理される
  fmt.Println(p.X) // 1
  fmt.Println(p.Y) // 2
}

元の構造体に対して影響を与えるためには（参照渡しで処理を実行するためには）、呼び出す関数側の引数を構造体型へのポインタを受け取るようにする必要がある。
type Point struct {
  X, Y int
}

func swap(p *Point) { // Point型のポインタを受け取るようにする
  x, y := p.Y, p.X
  p.X = x
  p.Y = y
}

func main() {
  p := Point{X:1, Y: 2}
  swap(&p) // Point型のポインタを渡す
  fmt.Println(p.X) // 2
  fmt.Println(p.Y) // 1
}

指定した型のポインタ型を生成するには関数newがある。
type Person struct {
  Name string
  Age int
}

p := new(Person) // pは*Person型


メソッド
Goのメソッドは、任意の型に特化した関数を定義することで実現できる。
メソッドの定義の際、funcの直後にレシーバーの型とその変数名が必要になる。
定義したメソッドはレシーバー.メソッドで呼び出すことができる。
type Person struct {
  Name string
  Age int
}

func (p *Person) SayHello() { // *Person型のメソッドSayHello
  fmt.Println(""Hello World!"")
}

func main() {
  p := &Person{Name: ""山田太郎"", Age: 30}
  p.SayHello()
  // => Hello World!
}


インターフェース
インターフェースは型の一つであり、任意の型がどのようなメソッドを実装すべきかを定義するものである。
インターフェースの宣言はtype インターフェースの名前 interface{}と記述する。
インターフェースを用いることで、異なる型に対して共通の性質を付与することができる。
これにより、汎用性の高い関数やメソッドを定義することができる。
type Stringify interface {
  ToString() string
}

// Person型
type Person struct {
  Name string
  Age int
}

func (p *Person) ToString() string {
  return fmt.Println(""%s(%d)"", p.Name, p.Age)
}

// Car型
type Car struct {
  Number string
  Model string
}

func (c *Car) ToString() string {
  return fmt.Println(""%s(%d)"", c.Number, c.Model)
}

// 異なる型を共通のインターフェース型にまとめる
vs := []Stringify {
  &Person{Name: ""山田太郎"", Age: 30},
  &Car{Number: ""ぬ-100-001"", Model: ""ZR103""},
}

foo _, v := range vs {
  fmt.Println(v.ToString())
}

// => 山田太郎(30)
// => ぬ-100-001(ZR103)


学習に役立ちそうなサイト/書籍


A Tour of Go

何はともあれチュートリアル



スターティングGo言語

基本的な文法や概念理解に



Goプログラミング実践入門　標準ライブラリでゼロからWebアプリを作る

基本的な文法などを理解した後にちょっと手を動かしたい場合には、こちらがちょうどいいかなと思います




まとめ(所感)
Goは静的型付け言語を扱った経験が乏しいエンジニア（私）にも優しい文法になっていて、学習コストが低そうだなという印象です。
速くて書きやすいという点で、さすがGoogleのエンジニアが生みの親なだけはあるなという感じ。
その一方で、Go特有の概念（ポインタ、panicなど）はちゃんと抑えないとね。

合わせて読みたい

今日から始めるGolang【コマンド集】


参考

The Go Programming Language
golang.jpプログラミング言語Goの情報サイト
スターティングGo言語
なぜGo言語 (golang) はよい言語なのか・Goでプログラムを書くべき理由
他言語プログラマがgolangの基本を押さえる為のまとめ
Go言語(Golang) はまりどころと解決策
動的言語だけやってた僕が、38日間Go言語を書いて学んだこと
なぜ私達は Python から Go に移行したのか
仕事の言語に飽きてきた人はGoを使ってみてほしい――『スターティングGo言語』著者が語るGoのパワー
なぜGo言語は成功を収めることができたのか - Googleの人が語る（翻訳）

",False,https://qiita.com//yu-croco/items/08666f8c50cbfaaadd2b
"

まずはじめに
つい先日、はじめてjenkins pipelineのためのJenkinsfileを作成し、PHPアプリケーションのワンクリックデプロイを実現しました。今回は振り返りの意味も込めて、その際に事前に知っておくと良かった点をまとめていきます。これから、Pipelineを構築したい方は参考にしてみてください。
環境は以下の通りです。
CentOS: 7.3.1611
Jenkins Version: 2.73.2
私の場合は、Pipelineを作成する前にRubyで書かれたCapistranoというデプロイツールでリリース用のスクリプトをすでに構築・運用済みでした。慣れてしまえば十分な環境でしたが、毎回コマンドを叩く手間と増加するチームメンバーへの共有コストを考えるとよりシンプルな手順が必要だと感じてきたため、Jenkinsへの移行を決断しました。
移行プロセスとして、ゼロからCapistranoのタスクを全てPipeline上の各ステージで書き直すことも考えましたが、Capistrano側の修正でそのまま再利用可能であると判断したためPipeline上でCapistranoの各タスクを実行する方針で進めました。
私のケースと同様にすでに別の方法である程度リリースを自動化されているのであれば、Pipelineの構築はそれほど難しい作業ではないので、ぜひ試してみることをオススメします。

ポイント
それでは早速ポイントに入っていきましょう。

1. Declarative vs Scripted 2種類のSyntaxの違い
(公式)Declarative versus Scripted Pipeline syntax
こちらのドキュメントが参考になります。
要約すると、Jenkinsfileの記法はDeclarative記法とScripted記法の2種類あり、Declarative記法が後発です。Declarative記法の方がより柔軟でかつ読み書きしやすい記法です。
Jenkins Pipelineの投稿記事をググる記事によって記法が様々ですので、まずはじめにこの点を押さえて記法の違いを認識しておくことが大事です。
簡単な見分け方は開始タグを確認することです。



開始タグ
記法




pipeline
Declarative


node
Scripted



ちなみに私はDeclarative記法で記述したので、次のポイントからはその点にご注意ください。

2. 環境変数の設定方法
(公式)Environment
こちらの公式ドキュメントに記載の通り、環境変数はpipeline直下またはstageディレクティブ内でのみ定義が可能です。#8. Script実行結果をコマンドに渡す方法 で紹介しますが、実行コマンドの結果を保存する際にも活用可能です。
pipeline {
    agent any
    environment {
        work_dir='/home/jenkins/work'
        bundle='/home/jenkins/bin/bundle'
        deploy_dir='/home/jenkins/deploy'
    }
    stages { // 私の場合、特にステージごとの環境変数は必要なかったです。
        stage('Check Environment') {
            environment { 
                LOCAL_VAR='/home/jenkins/target_dir' 
            }
            steps {
                sh 'printenv'
            }
        }
    }
}


3. ビルド時のユーザー入力
(公式)parameters
こちらのparametersを設定すると、Jenkinsの「Build Now」が「Build with Parameters」へと変化し、ビルド開始時にユーザー入力を受け付けるようになります。私の場合、リリース用のgit情報とデプロイ対象サーバーを選択できるようにしました。
pipeline {
    agent any
    environment {
        work_dir='/home/jenkins/work'
        bundle='/home/jenkins/bin/bundle'
        deploy_dir='/home/jenkins/deploy'
    }
    parameters {
        // 公式ドキュメントではchoiceの場合、choices: ['one', 'two', 'three']
        // のようにかけるそうですが、なぜか私の環境ではsyntax errorがでてしまったため、
        // 以下のように\n改行コードを入れることでセレクトボックス入力が可能になりました。
        choice(name: 'BRANCH_OR_TAG', choices: 'Branch\nTag\n', description: 'Select Checkout Type')
        string(name: 'CHECKOUT_POINT', defaultValue: 'develop / v1.0.0', description: 'Input Branch / Tag Name')
        choice(name: 'SERVERS', choices: 'hogehoge.com\nfugafuga.com\n', description: 'Select Deploying Servers')
    }
    stages {
        stage('Check Environment') {
            environment { 
                LOCAL_VAR='/home/jenkins/target_dir' 
            }
            steps {
                // 公式ドキュメントより
                // このように記述することで各ステージの途中で入力を受け付けるようになる
                input {
                    message ""Should we continue?""
                    ok ""Yes, we should.""
                    submitter ""alice,bob""
                    parameters {
                        string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?')
                    }
                }
                // 以下のようにsteps内からparametersの値へアクセス可能
                // 変数展開のためにダブルクオート
                sh """"""
                    printenv
                    echo ${params.BRANCH_OR_TAG}
                    echo ${params.CHECKOUT_POINT}
                    echo ${params.SERVERS}
                """"""
            }
        }
    }
}


4. 各Section/Directiveの階層関係
(公式)Pipeline Syntax
書き進めていく際にSection/Directiveの階層構造を認識していくことが大事です。Declarative記法の場合、割と厳密な階層構造が指定されているので雰囲気で記述していくと頻繁にsyntax errorが発生します。

Section定義位置・子要素早見表




Section
top-level
stage直下
指定可能な子section
指定可能な子directive




agent
○
○




stages
○
○
stage



post
○
○




steps

○






Directive定義位置・子要素早見表




Directive
top-level
stage直下
stages直下
steps直下
指定可能な子section
指定可能な子directive




environment
○
○






options
○
○指定可能なオプションは限定






parameters
○
○






triggers
○
○






stage


○

stepsstages
parallel


tools
○
○






input
○







when
○







parallel

○



stage


script



○

stage




5. ディレクトリ移動してコマンドを実行する方法
(公式)Pipeline: Basic Steps
#4で紹介した基本的なDeclarative記法のDirectiveの機能に加え、pluginという形で様々機能が提供されています。その中の一つdir()を使うことで、ディレクトリを移動してからコマンドを叩くことが可能になります。dir()などのプラグインはstepsセクション内でのみ使用可能です。
pipeline {
    agent any
    environment {
        work_dir='/home/jenkins/work'
        bundle='/home/jenkins/bin/bundle'
        deploy_dir='/home/jenkins/deploy'
    }
    stage('Change Directory And Echo') {
        steps {
            dir(work_dir) {
                 sh ""echo ${deploy_dir}""
            }
        }
    }
}


6. 並列処理の方法
(公式) Parallel
pipeline {
    agent any
    stages {
        stage('Parallel Stage') {
            parallel {
                stage('Branch A') {
                    agent {
                        label ""for-branch-a""
                    }
                    steps {
                        echo ""On Branch A""
                    }
                }
                stage('Branch B') {
                    agent {
                        label ""for-branch-b""
                    }
                    steps {
                        echo ""On Branch B""
                    }
                }
                stage('Branch C') {
                    agent {
                        label ""for-branch-c""
                    }
                    stages {
                        stage('Nested 1') {
                            steps {
                                echo ""In stage Nested 1 within Branch C""
                            }
                        }
                        stage('Nested 2') {
                            steps {
                                echo ""In stage Nested 2 within Branch C""
                            }
                        }
                    }
                }
            }
        }
    }
}

上の例は公式ドキュメントからの引用です。#4で紹介した通り、parallelはstage直下にのみ指定可能です。私は感覚としてstages直下にそのままparallelを指定し複数のstageを実行しようと試みたのですが、うまくいきませんでした。
平行処理をする場合は、一度平行処理全体のstageを作成し、その配下にparallel+複数のstageを指定しなければいけません。

7. Bashでコマンド実行する方法
こちらのポイントに関しては、私自身もベストプラクティスを模索中です。
私の場合、すでにCapistranoでデプロイしていたため、サーバ上の.bashrcに環境変数がある程度指定してあり、かつCapistranoタスクがbashに依存していました。
理想的にはJenkinsfile一つで必要な環境変数の準備が完結することが望ましいですが、今回はbashの環境変数を使用するために以下のようにしました。
pipeline {
    agent any
    environment {
        work_dir='/home/jenkins/work'
        git_dir='/home/jenkins/repository'
        cap_dir='/home/jenkins/capistrano'
        bundle='/home/jenkins/bin/bundle'
        deploy_dir='/home/jenkins/deploy'
    }
    stage('Change Directory And Echo') {
        steps {
            dir(work_dir) {
                 sh """"""
                     echo 'source ~/.bashrc' > composer.sh
                     echo 'cd ${cap_dir}' >> composer.sh
                     echo 'GIT_DIR=${git_dir} ${bundle} exec cap production composer:install' >> composer.sh
                     bash ./composer.sh
                     rm ./composer.sh
                 """"""
            }
        }
    }
}


8. Script実行結果をコマンドに渡す方法
最後に簡単なロジックを挟み、その結果をコマンド実行時に利用する方法です。
ポイントは以下の3点です。


environmentディレクティブ内に変数を定義(top-level or stage内)

stepsセクション直下のscriptディレクティブ内にgroovyで処理を記述
変数展開して環境変数をsh内で展開して使用

以下の例は、ビルド開始時にユーザー入力からCapistranoへ渡すrolesの指定を行う場面です。
pipeline {
    agent any
    environment {
        work_dir='/home/jenkins/work'
        git_dir='/home/jenkins/repository'
        cap_dir='/home/jenkins/capistrano'
        bundle='/home/jenkins/bin/bundle'
        deploy_dir='/home/jenkins/deploy'
        // ポイント1
        role=''
    }
    parameters {
        choice(name: 'ROLE', choices: 'first\nsecondthird\n', description: 'Server Cluster')
    }
    stage('Change Directory And Echo') {
        steps {
            // ポイント2
            script {
                if (params.ROLE =~ /first/) {
                    role = ""first""
                } else if(params.ROLE =~ /second/) {
                    role = ""second""
                } else if(params.ROLE =~ /third/) {
                    role = ""third""
                }
            }
            dir(work_dir) {
                 // ポイント3
                 sh """"""
                     echo 'source ~/.bashrc' > composer.sh
                     echo 'cd ${cap_dir}' >> composer.sh
                     echo 'GIT_DIR=${git_dir} ${bundle} exec cap production --roles=${role} composer:install' >> composer.sh
                     bash ./composer.sh
                     rm ./composer.sh
                 """"""
            }
        }
    }
}

",False,https://qiita.com//zizu21105/items/38831adf42e3cfeed66a
"　　
　　
　　
この記事は
『プログラミング完全未経験からUnityでの開発現場に迎え入れてもらえた世界一の幸せ者』
の記事です。そのつもりでお読みください。
　　
　　

Unity(C#)でサウンドを再生、停止する方法

方法①publicで変数を宣言する。
public AudioSource audioSource;

　　
そうしたらInspector側に　　""ここに音源を入れてくれや！""
みたいなのが表示されるのでそこに音源をぶち込みましょう。


音を再生する方法
audioSource.Play();    

　　

音を停止する方法
audioSource.Stop();      



方法②ゲームオブジェクトに直接AudioSourceをAdd Componentする。
まずは変数宣言しときましょう。
AudioSource audioSource;

　　
次に、Inspector側でAudio SourceコンポーネントをAdd Componentします。

この場合はAudioClipってとこに音源をぶち込みましょう。

音を再生する方法
audioSource = this.GetComponent<AudioSource>();
audioSource.Play();    

　　

音を停止する方法
audioSource = this.GetComponent<AudioSource>();
audioSource.Stop();      

めちゃくちゃ簡単ですね。
”ボールを取ったら音を鳴らしたい！”　みたいなときは
もしコライダーと接触したら...という処理の場所に呼び出せばいいだけです。
もしBGMとか設定するつもりの人はAudio SourceのInspector画面いじって
""Play on Awake""と""Loop""にチェック入れてみてください。
コード一行も書かずにできます。
",False,https://qiita.com//OKsaiyowa/items/d1a131f6b3e356c19388
"この記事は
『プログラミング完全未経験からUnityでの開発現場に迎え入れてもらえた世界一の幸せ者』
の記事です。そのつもりでお読みください。
以下のスクリプトを重力を制御したいゲームオブジェクトにくっつけて任意のタイミングで呼び出せばOKです。
Rigidbody rd;

rd = this.GetComponent<Rigidbody>();
rd.useGravity = true;


""true""でON、""false""でOFFです。
",False,https://qiita.com//OKsaiyowa/items/86f8e5a49da0f2d0d96a
"プログラミングの勉強を初めてちょっと時間も立ったので、今まで自分が触ってきた(注:触っただけで全くマスターしていない)言語たちについて触れて、その痕跡を残しておこうかなと思った次第です。
あくまで感想、所感程度なのと、深く言語をやっているわけでもないので、ツッコミどころ等多々あるかもしれませんが、逆に超初心者の方には初心者の視点からの感想のほうが有用かなぁなんて思ったりします。
強い人達はこんなこともできるよみたいなの教えてくれると嬉しいなぁ…
基本触った順に行きますが、一部初回あまりにわからなすぎたのは二回目以降触れたときで書きます。

1.Python

良い点

とにかく書きやすい。セミコロンすらないし、変数の型も気にしなくてよい。
同じく読みやすい。インデントせざるを得ない仕様のおかげで、初心者でもある程度読めるコードになる。
モジュール、パッケージが多彩かつ、最近流行りの機械学習とかが強い。
科学技術計算もできる。


悪い点

パッケージを使わないと遅い。
変数宣言がないことの裏返しでなんかいろいろふわふわしている(気がする。)
イテレータとかジェネレータとかは初心者には厳しい。


その他
なんかローカルサーバ周りとか結構いろいろなことを簡単にかけるパッケージが存在するらしく、ふとした時に便利。
GUIを書くのは結構しんどいのでやめたほうがいいかもしれない。

2.C

良い点

早い。
ポインタとかで圧倒的成長できる


悪い点

書きにくい。動的配列がないのが個人的には結構辛い。
segmentation fault
用途はまだまだあるがどっちかと言うとニッチな領域(だと思っている。)


その他
教育目的ではやっぱりよく使われるので、そういう面では無駄になりにくいかもしれない。

3.C++

良い点

早い。
boostとか便利
Cに比べると安全だし諸々が書きやすい（標準入出力とか）
簡単に使うだけならオブジェクト指向がよくわかってなくても結構なんとかなる


悪い点

流石にPythonよりは書きにくいし、学習コストも重め。


その他
学習コスト以外は特に難点がない気もする。

4.Java

良い点

結構早い(Cとかとそんなに変わらないらしい)
強制的にオブジェクト指向を学べる
Write once, run anywhere
コンパイラ？がかなりうるさいので謎挙動とかはあんまりない(気がする)
アクセスレベルとかが厳しいので安全なコードをかく練習ができる(多分)


悪い点

学習コストが個人的にはかなり高い(C++より厳しかった)
どうしても付きまとうオブジェクト指向周りの横文字が覚えにくい
開発環境の準備とか実行とかが初心者にはかなり厳しい。(まだ良くわからない。)


その他
Eclipseが重い。
わりとなんでもできる言語のイメージ。またJavaに文法が似ている言語も多いらしいのでそれも利点かもしれない。

5.fortran

　良い点

早い。特に科学技術計算とかはめっちゃ早い(イメージ)
BASICほどではないが、文法は簡単だと思う。


悪い点

レガシーコードの存在(implicit none,固定形式…)
配列のインデックスが1から始める
文字列の扱いに弱い。
情報がネットには少なく、正直ちょっと学びにくい。


その他
シミュレーションとかなら最強(だと思う)

6.Bash

良い点

ファイルとかの簡単な編集がサクッとかける。
エラーをファイルに吐かせたり、他言語を使うときにサポートとして便利。
とにかくサポートとしてとても優秀。


悪い点

計算は苦手。
文法は結構独特で個人的にはちょっと覚えにくい


その他
サーバーとか触るときもシェルの知識は役立つ気がするのでその点も良い点かもしれません。

7.HTML

良い点

ウェブに必須
ものすんごい簡単


悪い点

そんなに面白いことはできない(と思っている)


8.JavaScript

良い点

変数宣言とかがゆるふわで結構適当に書いても通る
ブラウザで確認できるので開発環境の準備が超簡単
すぐに見た目に出せるので、その点楽しい。


悪い点

ふわふわかける言語の常で、気を使わないとすぐスパゲッティコードになる
最近はこれを直でかくことはまずないらしい。
流行りのラッパー？がよく変わるイメージ
ウェブ界隈は仕様変更も多く、ついていくのが大変なイメージ


その他
趣味レベルで書くなら、デメリットの多くを無視できるので良い。

9.CSS

良い点

ウェブページを豪華にできる
簡単
CSS芸とか言われるおもしろいこともできる


悪い点

特にないきがする


10.C＃

良い点

Unity
書きやすい気がする(まだあまりさわれていない)
Javaと似てるといえば似てるので、覚えやすい。


悪い点

Windows依存が激しい(それ用の言語なので仕方ないが…)
似ているのはいいが頭の中で混ざる(なれていないだけですが…)


おわりに
以上は全て個人の感想です。
やった言語が増えたら増えるかもしれません。
",False,https://qiita.com//mosamosa/items/21d907c4c533a3061c2c
"　　
　　
この記事は
『プログラミング完全未経験からUnityでの開発現場に迎え入れてもらえた世界一の幸せ者』
の記事です。そのつもりでお読みください。
　　

コンポーネントのON、OFF(enable)
enable はインスペクターにチェックボックスがあるタイプのコンポーネントなら、ON,OFFできる。(たぶん)

Renderer _renderer;
GameObject cube;

 _renderer = cube.GetComponent<Renderer>();
 _renderer.enabled = false;


SetActiveはゲームオブジェクトの表示をオン、オフに切り替える。
GameObject cube;

cube.SetActive(false);

　　
　　
enableに出会うまでは　なんでもかんでもSetActiveかDestroy使ってました。。。
",False,https://qiita.com//OKsaiyowa/items/9579ac348ac860cd522e
"

趣旨
VirtualBoxとVagrantを利用してLinux環境構築をしたいときに
せっかくならhomebrew使ってやりたい！
ということで、やってみました。
自分の備忘録として、初心者向けの記事として、丁寧に記録を残したいと思います。

環境
homebrewはインストール済みの状態

homebrew
macOS向けのパッケージマネージャー

brew-cask
homebrewの拡張機能
MacのGUIアプリケーションもコマンド一発で管理できるように

virtualboxのダウンロード
$ brew search virtualbox
2018-11-10 12:17:02.595 defaults[32749:1604438] 
The domain/default pair of (kCFPreferencesAnyApplication, AppleLanguages) does not exist
==> Casks
virtualbox
virtualbox-extension-pack
homebrew/cask-versions/virtualbox-beta
homebrew/cask-versions/virtualbox-extension-pack-beta
AkienoMacBook-Pro:~ akie$ brew install virtualbox

VirtualBoxがhomebrew-caskでインストールできることが確認できました
$ brew cask install virtualbox
==> Satisfying dependencies
==> Downloading https://download.virtualbox.org/virtualbox/5.2.20/VirtualBox-5.2.20-125813-OSX.dmg
######################################################################## 100.0%
==> Verifying SHA-256 checksum for Cask 'virtualbox'.
==> Installing Cask virtualbox
==> Running installer for virtualbox; your password may be necessary.
==> Package installers may write to any location; options such as --appdir are ignored.
Password:
installer: Package name is Oracle VM VirtualBox
installer: Installing at base path /
installer: The install was successful.
🍺  virtualbox was successfully installed!

インストールできました

確認
$ brew cask list
virtualbox

一覧にVirtual Boxが表示されました

Vagrantのインストール
VirtualBoxの操作を簡単にしてくれます
$ brew search vagrant
2018-11-10 12:37:31.730 defaults[34303:1623081] 
The domain/default pair of (kCFPreferencesAnyApplication, AppleLanguages) does not exist
==> Formulae
vagrant-completion

==> Casks
vagrant                             vagrant-manager                     vagrant-vmware-utility
$ brew cask install vagrant
==> Satisfying dependencies
==> Downloading https://releases.hashicorp.com/vagrant/2.2.0/vagrant_2.2.0_x86_64.dmg
######################################################################## 100.0%
==> Verifying SHA-256 checksum for Cask 'vagrant'.
==> Installing Cask vagrant
==> Running installer for vagrant; your password may be necessary.
==> Package installers may write to any location; options such as --appdir are ignored.
Password:
installer: Package name is Vagrant
installer: Upgrading at base path /
installer: The upgrade was successful.
🍺  vagrant was successfully installed!
$ brew cask list
vagrant                             virtualbox

インストールされました

まとめ
コマンドだけでGUIアプリも管理できて、便利です！
VirtualBoxを使って、開発・勉強を進めていきたいと思います

TODO
The domain/default pair of (kCFPreferencesAnyApplication, AppleLanguages) does not exist

が出てくるのがよくわからないです。一旦保留。
",False,https://qiita.com//kinako3/items/b4433918931fd790a8b8
"モチベーションも大事だと思うので覚書として使用します。
八週目の感想今週はinterfaceとコード作りました。
一度作ればその定義でいくらでも共通化や変更できるので大変便利だと思いました。

また、ご指導頂いた事もあり、実際の計算に理解がない学習は、効率悪いし
テスト勉強のような学習になってしまっていたので注意が必要と思いました。
一応終わらせたいので一気にしてから、毎週自力で作りたいです。
できればスマホで動く所まで。パーツ以外は自動化？できるんじゃないかと思いました。
とにかく、10週まで一気に終わらせて、メインで自作練習していこうと思いました。
今週、前半は今後の方針を考えて後半はコードを考えてました。単純に自分で考えるのは楽しい。
その際、調べすぎるとコピペ上手くなるだけになりそうなので注意したい。
恥を忍んで初心者全開で始める方が、背伸びしてコピペするより学習になるはず。
最初に自分で作った型表、仕様を確認する位で基本考えながら作りたいなと。
(そして色々ご指摘頂けたらいいなぁと…でも何も反応ない記事も多々ありその方が恐怖)
あとスマホの実装が具体的に気になって、ｸﾗｲｱﾝﾄやﾊﾞｯｸｴﾝﾄﾞ等の設計手順とか
DevOps調べてたらはまって若干焦りました。
この調子で2年でリリース可能なのか…まずやる事を整理しました。
複数選択肢作ったけど、全部並行して進めようと思います。
1.一気に学習終わらせる
2.ついでに学習項目使って自作
3.終わらせたら今後メインで自作しまくる(鍛える)
4.並行でXamarin/DevOps(モバイルアプリ設計手順)学習
5.並行でモック・プロトタイプ・クローンアプリ?(説明用)
6.並行で事業計画書等の資料製作(説明用)


　　　　　　　　　　　　　　

自作してみた(interface使った何か)

XamarinのOS間共有コード作ってみたかったけど、
API実装はまだ分からないので出来る範囲で作って画面に出しました。
数個のクラスでコンソールに表示しただけです。
具体的にはユーザの入力に対し☆印で出力するようなもので、恐らくcaseで書けるかなと。
イメージはできたけど書き方忘れて前記事見ながら、動けばいいという心境になり暗中模索…

InterReview
using System;



public interface IReview
{
    void Review(int r);
}




class IOS1 : IReview
{
    public void Review(int r)
    {
        var IReview = new star();
    }
}



class star
{
    static void Main()
    {
        Console.WriteLine(""評価を入力してください(5段階)"");
        int star = int.Parse(Console.ReadLine());

        switch (star)
        {
            case 0:
                Console.WriteLine(""☆☆☆☆☆"");
                break;
            case 1:
                Console.WriteLine(""★☆☆☆☆"");
                break;
            case 2:
                Console.WriteLine(""★★☆☆☆"");
                break;
            case 3:
                Console.WriteLine(""★★★☆☆"");
                break;
            case 4:
                Console.WriteLine(""★★★★☆"");
                break;
            case 5:
                Console.WriteLine(""★★★★★"");
                break;
            default:
                Console.WriteLine(""error"");
                break;

        }
    }
}





━━━━━━━━━━━━━━━━━以上。以下interface仕様━━━━━━━━━━━━━━━━━


インターフェイス interface

interfaceは、異なる言語や機能間の共通コードを作成し共通化できます。
使用にはpublic interface 名前{抽象メンバ}とします。
基本的なコード例をここに示します
namespace Phoneword
{
    public interface IDialer//インターフェイス定義
    {
        bool Dial(string number);//インターフェイス定義
    }
}




※使用する場合、習慣的に先頭にIがつきます。(IDialer…etc)
実装には、実装したいクラスの横に: interface名とします。また、複数実装可能です。
class class名: interface1, interface2, ...
例えば前の学習で、Xamarinの共通プロジェクトIDialer.csとAndroid・iOS両プロジェクト(PhoneDialer.cs)間の共通コードを作成しました。interface用にCSファイル作成して定義しているのでプロジェクト間の共通コードといえると思います。

Phoneword.Android.IDialer.cs
namespace Phoneword//共通なので上階層
{
    public interface IDialer//インターフェイス定義
    {
        bool Dial(string number);//インターフェイス定義
    }
}


定義したinterfaceを、Phoneword.Android.PhoneDialer.cs Phoneword.iOS.PhoneDialer.csにそれぞれ実装していきます。
実装するthrow new～のような文は、
ちなみにこのアプリはテキストから電話番号に変換するアプリなので、interfaceの定義bool型 Dialは
null(ありえない番号)でFalse、TrueでMONOというAndroidのAPIに移項し処理されます。
一方iOSはTrue、False共にUIKitというiOSのAPIに移項し処理されます。

Phoneword.Android.PhoneDialer.cs
using Android.Content;
using Android.Telephony;
using Phoneword.Droid;
using System.Linq;
using Xamarin.Forms;
using Uri = Android.Net.Uri;

[assembly: Dependency(typeof(PhoneDialer))]
namespace Phoneword.Droid
{
    public class PhoneDialer : IDialer// interfaceでIDialer実装(習慣的に先頭Iで始まる)
    {
        public bool Dial(string number)// interface(抽象メンバ)なのでそれぞれ定義した
        {
            var context = MainActivity.Instance;
            if (context == null)
                return false;

            var intent = new Intent(Intent.ActionDial);
            intent.SetData(Uri.Parse(""tel:"" + number));

            if (IsIntentAvailable(context, intent))
            {
                context.StartActivity(intent);
                return true;
            }

            return false;
        }

        public static bool IsIntentAvailable(Context context, Intent intent)
        {
            var packageManager = context.PackageManager;

            var list = packageManager.QueryIntentServices(intent, 0)
                .Union(packageManager.QueryIntentActivities(intent, 0));

            if (list.Any())
                return true;

            var manager = TelephonyManager.FromContext(context);
            return manager.PhoneType != PhoneType.None;
        }
    }
}




Phoneword.iOS.IDialer.cs
using Foundation;
using Phoneword.iOS;
using UIKit;
using Xamarin.Forms;

[assembly: Dependency(typeof(PhoneDialer))]
namespace Phoneword.iOS
{
    public class PhoneDialer : IDialer// interfaceでIDialer実装(習慣的に先頭Iで始まる)
    {
        public bool Dial(string number)// interface(抽象メンバ)なのでそれぞれ定義した
        {
            return UIApplication.SharedApplication.OpenUrl(new NSUrl(""tel:"" + umber));
        }
    }
}


このように、newインスタンス作る事で実装されまています(右クリの自動実装はとりあえずの空の実装 throw new NotImplementedException(); )

memo:
共通定義できるインターフェイス(定義)(抽象メンバのみ)
多重定義できるオーバーロード(読み込み)(引数違う)
再定義できるオーバーライド(上書き)(引数同じメソッド)

インターフェイスの継承

interfaceの継承は、ｸﾗｽの継承と同じでｲﾝﾀｰﾌｪｲｽｸﾗｽを継承する事ができます。
使用には、interface 名前 : 継承したいインターフェイス名とします。
基本的なコード例をここに示します
// C# program to illustrate the interface 
using System; 

// interface declaration 
interface Vehicle { 

    // all are the abstract methods. 
    void changeGear(int a); 
    void speedUp(int a); 
    void applyBrakes(int a); 
} 

// class implements interface 
class Bicycle : Vehicle{ 

    int speed; 
    int gear; 

    // to change gear 
    public void changeGear(int newGear) 
    { 

        gear = newGear; 
    } 

    // to increase speed 
    public void speedUp(int increment) 
    { 

        speed = speed + increment; 
    } 

    // to decrease speed 
    public void applyBrakes(int decrement) 
    { 

        speed = speed - decrement; 
    } 

    public void printStates()  
    { 
        Console.WriteLine(""speed: "" + speed +  
                          "" gear: "" + gear); 
    } 
} 

// class implements interface 
class Bike : Vehicle { 

    int speed; 
    int gear; 

    // to change gear 
    public void changeGear(int newGear) 
    { 

        gear = newGear; 
    } 

    // to increase speed 
    public void speedUp(int increment) 
    { 

        speed = speed + increment; 
    } 

    // to decrease speed 
    public void applyBrakes(int decrement){ 

        speed = speed - decrement; 
    } 

    public void printStates()  
    { 
        Console.WriteLine(""speed: "" + speed +  
                          "" gear: "" + gear); 
    } 

} 

class GFG { 

    // Main Method 
    public static void Main(String []args)  
    { 

        // creating an instance of Bicycle  
        // doing some operations  
        Bicycle bicycle = new Bicycle(); 
        bicycle.changeGear(2); 
        bicycle.speedUp(3); 
        bicycle.applyBrakes(1); 

        Console.WriteLine(""Bicycle present state :""); 
        bicycle.printStates(); 

        // creating instance of bike. 
        Bike bike = new Bike(); 
        bike.changeGear(1); 
        bike.speedUp(4); 
        bike.applyBrakes(3); 

        Console.WriteLine(""Bike present state :""); 
        bike.printStates(); 
    } 
} 



Output:
Bicycle present state :
speed: 2 gear: 2
Bike present state :
speed: 1 gear: 1


このように、多重継承と抽象化を実現できました
",False,https://qiita.com//h_okabe/items/c7aeeb452bdcbc9c51fe
"

はじめに
returnの概念で沼にハマったので記載

以下は同じ

var number = ""1""
  if( number == ""1""){
    return true;
  }else{
    return false;
  }
//true

var number = ""1""
  if( number == ""1""){
    return true;
  }
  return false;
//true


理由
上はifを使った単純な条件分岐です。
今までしっくり来ていなかったのは下なのですが、
return が来るとその時点でScriptが終わってしまうためです。
下の場合でいうと、
数字が1なのでそのまま進みreturn　trueを発火させて終わりですが、
数字が1以外の場合は、return　trueをスキップするため、Scriptが終わらず
return　false　で終わることになります。

検証用

function demo() {

  var number = ""2""

  if( number == ""1""){
    return true;
  }
  return false;

}

Logger.log(demo());//上のfunction demoという関数の結果をログ出力する

",False,https://qiita.com//talaw-K/items/088c4e6286f7a6544b74
"本ワークショップに応募するとき「Qiitaに記事を上げる」と約束したので書きました。
結論から言うと、参加して本当に良かったです。

Django Girlsとは？
Django Girls 公式サイトによると、

Django Girls is a non-profit organization and a community that empowers and helps women to organize free, one-day programming workshops by providing tools, resources and support. 

一言でいうと1Dayワークショップ等を通してプログラミングの普及活動をしている非営利団体のようです。

私のスペック

業務でPython 2系3系を触った経験あり(≠webアプリ、本当に触っただけ)、直近1ヶ月ほど
自分のキャリアの模索のため、独学でRailsを2ヶ月ほどダラダラと(今は使ってない、Railsチュートリアル半分までやった)
前職は外資インフラ系プロジェクトマネージャ
前職までは技術未経験、前職を辞めた後ブランク約2年を経て今の会社に入社


参加動機
下記状況、思いが重なった感じです。

Rails を独学でやっていた頃は1人でしんどくて、仲間が欲しいと思った (結局Railsは続かなかった)
勉強のためにRailsを触っていた流れでDjangoにも興味はあった(が、触る余裕はなかった)
最近IT企業(≠web業界)に就職してPythonを触る機会はあった


達成したかったこと

初めて触るDjangoで何か作る
お互い頑張れるお友達ができたら嬉しいな☺️


カリキュラム

概要
インストールDayとワークショップDayの2日間に分かれて開催された。
インストールDayの参加は必須でなく、参加者数はワークショップDayの半分くらいだったかと。
Django Girls のチュートリアルに沿ってアプリ作成を進めました。

インストールDay


日時：　11月2日（金）19:00~21:00
場所： EventSpace Co-Creation Labo @ 東京都千代田区平河町

この日の目標はDjangoをインストールするまで進めること。Git, PythonAnywhereはスキップ。
時間が余っていたのでこれを機にAnacondaを卒業したいと思い、コーチに確認しAnacondaを無効にしてvirtualenvを使うことに。
具体的には、.bash_profileのAnacondaのエイリアスをコメントアウト。
2.ワークショップDay

日時：　2018年11月3日（土）10:00〜18:00
場所：　サイボウズ株式会社 東京オフィス @ 東京都中央区日本橋

現地で撮った写真は掲載OKとのことなのでバンバン載せます！
会場はこんな感じ！

途中休憩を挟みながら、8時間でチュートリアルをこなし、なんとかwebアプリ(ブログ)を作りきった。


私がつまずいたり疑問に思ったりしたところ
プログラミング初心者向けとのことだったので、普段なら聞けないような初歩的なこともとにかく聞きました！

出るわ出るわ、エラー画面！
エラーメッセージをちゃんと読み確認して、それに合った対処を。
エラー画面とかメッセージって、読みたくなくなりますよね…
例えば、python manage.py runserver を叩いたコンソールにて
File ""/hoge/djangogirls/blog/views.py"", line 17, in post_new
    form = PostForm()
NameError: name 'PostForm' is not defined

とか
File ""/hoge/djangogirls/blog/urls.py"", line 6, in <module>
    path('post/new', views.post_new, name='post_new'),
AttributeError: module 'blog.views' has no attribute 'post_new'

とか
まずはエラー画面の最後の3行くらいを読んで

どこのファイルの
どの箇所に
どんな症状のエラーが出てるか

が読むことができれば解決に近づきそう。
私のような初心者がやらかしがちなエラーの原因は、例えば

タイプミス
ファイル置く場所間違えた
コピペミス

とかかな？
私が出したエラーはほぼ上記3つに当てはまりました。。
ブラウザで出るエラーも同様で、
例えばviews.pyに実行したいメソッドが定義されて！とエラーが出ていたら
コピペ漏れか書き忘れなのでちゃんと定義しましょう、ということ。

MVCモデル…じゃないの？
Railsを少しかじっていたので、views.pyのファイル名が異質なものに見えた。
MVCモデルのcontrollerに当たるはずなのにviewsとはこれ如何に…🤔と。
(と言っても、私自身MVCモデルを説明できるレベルにはないのですが。。)
DjangoではMTVモデルと呼ぶらしいのです。
MTVとはModel Template Viewの略。
詳細はPython Django チュートリアル チートシート「全体像」参照

このドットは何だろう…
blog/admin.py
from django.contrib import admin
from .models import Post
​
admin.site.register(Post)

.modelsの"".""は 「カレントディレクトリ配下」という意味。
 .をつけないとmanage.pyがあるディレクトリから読んでしまう。
場所を指定しないと名前が衝突しImportエラーが起こるので要注意。

環境作ったはずなのに動かない！
Djangoを入れたインストールDay当日中は良かったのですが、ワークショップDayでちょっと焦りました。。
virtualenvに慣れておらず、djangogirls直下で下記コマンドを打つの忘れてたのが原因。
$ source myvenv/bin/activate
そもそも直近では会社Windowsでも私物MacでもAnacondaでしかPythonを使っていなかったためこれから慣れます、はい。

雰囲気
参加者30名、全員女性。
外国出身の方も数名。年代、業種は本当にバラバラ。バリバリ働いてるママさんもけっこういた。
生き方というかチャレンジ精神というか、皆さんすごくパワフルで尊敬できる方たちしかいなくて、この環境にいられただけでもすごく良い体験だった。
いい意味でめっちゃ焦る。
お友達もたくさんできました☺︎嬉しい〜〜

参加者に何名か外国の方がいたけど、教える側だけじゃなく参加者側も英語話せる人が多かったのびっくりした。グローバル…😳— まどかす (@madok_s) 


参加者の方々を見てると、とにかくみんな「強い意志」を持ってて前向き。好奇心だったり自分の働き方を考えたいとか色んな人と知り合いたいとか。私も頑張るぞ💪💪💪— まどかす (@madok_s) 

ワークショップは以下基準にグループ分けされ進行してたかと。
Windows/Mac
Django インストールした/してない
1グループ3〜4人に1人コーチがつく感じでした。
困ったタイミングですぐコーチに対応してもらえる環境で聞きやすかった。
ちなみにコーチは男女混合。

適度に休憩！
頭フル回転しっぱなしでお腹がすぐ空いてしまう。。

良い眺めとかわいいランチ🌈💗
そしてLTタイム！
nikkieさんのLTで、

わからないことはメモしてまた出てきたら調べる

というフレーズ？が印象的だった。
わからないことを調べるとまたわからないことが出てきて、わからないことで溢れてしまうし
それがよく使う知識なのか否かの優先度がわからないから
確かにー！となった💡
あと今村かずきさんのLTも刺さった。
Django Girlsは女性を優遇しているのではなく、女性に機会を与えたいんだよ、という話。
確かに勉強会とか私の職場も男性の参加者の方が女性よりかなり多い。難しいけど、向き合い続けなければならない問題だなと感じています。

おやつ！

めっちゃフォトジェニックなケーキでびっくり。思わずインスタにも上げちゃいました(笑)

懇親会まできっちり？楽しみました。
現役エンジニアのコーチの方々のお話から感じたのは
エンジニアへの門戸は開かれているけど、ちゃんと戦略考えないと生き残るのが難しそう
自分より優秀な人はいくらでもいる
ということ。
わかってはいたつもりだけど…
技術を磨きつつちゃんと考えなきゃ。
継続しなきゃ。

これから

まずはDjango Girls Tutorialの拡張版を今年中に終えたいな〜とゆるく考えてます。
それと同時に、友人のアプリ開発を手伝いながら、簡単なものから作っていこうかなと考えてます。
やっぱり大好きなことば、自然言語とITが重なる場所に身を置いて、人々のコミュニケーションを助けることを生業としたいので、その方向へ行けるようにしていきたいです。

まとめ

初めて触るDjangoで何か作る

-> 達成🙌

お互い頑張れるお友達できたら嬉しいな☺️

-> 達成🙌
助けを借りながらもDjangoでwebアプリを作り、色んな人と出会って話して情報が頭の中でひっちゃかめっちゃかだったけどとりあえず楽しかったです！！(語彙力
次回はスタッフとして(コーチは難しいと思うので)Django Girlsワークショップに参加したいと思いました！
最後まで読んでくださった方、ありがとうございましたm(_ _)m


参考リンク
https://djangogirls.org
https://djangogirlsjapan.gitbook.io/workshop_tutorialjp
https://blog.soracom.jp/blog/2018/04/18/howto-techblog/
https://qiita.com/maisuto/items/bcdb0fd6c63cf0c544d6
https://gitpitch.com/ftnext/2018_LTSlides/master?p=django_girls_Nov_for_beginners/#/
",False,https://qiita.com//ykeiko/items/156bb0cd3ff587600fdf
"

０．初めに
私はエンジニアではないただのドシロウトです。
自分が登録していないマストドンのインスタンスの直近のローカルタイムライン(LTL)を眺めようとして以下の記事を見つけました。

タイムラインを“のぞき見”　アカウント作成不要のマストドン用Webサービス登場　「Mastodon Timeline Peeping Tool」
http://www.itmedia.co.jp/news/articles/1704/20/news037.html

Mastodon Timeline Peeping Toolを利用すれば、確かにアカウントなしで他のインスタンスのローカル、連合タイムライン（FTL）をのぞき見できます。
私は複数インスタンスの直近の情報が見たかったのでこのツールの公開されているソースを調べました。

yukimochi/Mastodon-Timeline-Peeping-Tool
https://github.com/yukimochi/Mastodon-Timeline-Peeping-Tool

多分マストドン基礎知識なんでしょうがストリーミングAPIを利用して直近の情報を得ているとはじめて知りました。

tootsuite/documentation
https://github.com/tootsuite/documentation/blob/master/Using-the-API/Streaming-API.md

そこでMastodon Timeline Peeping Toolを参考にしてHTMLファイル一つで複数インスタンスのLTL、FTLをまとめて取得、簡易表示するツールを作ってみました。

１．作ったもの
以下の画像の通りです。

ボタンを押すと事前に指定済みの複数インスタンスのLTLまたはFTLに新着投稿があるとリアルタイムに表示します。
HTML表示は可能な限り簡素化しました。

２．コード
短いですが以下です。HTMLファイル一つでストリーミングできてます。

test.html
<!DOCTYPE html>
<html>
<head>
  <meta http-equiv=""content-type"" content=""text/html; charset=UTF-8"">
</head>
<body>
  <h1>マストドンストリーミングAPI</h1>
  <button class=""start-ws-ltl"">ローカルタイムライン</button>
  <button class=""start-ws-ftl"">連合タイムライン</button>
  <br><br>
  <div class=""activity-stream""></div>
<script>
var ws_connection = null; /* webソケット接続用 */
// インスタンスをカンマ区切りで指定
var instance = '（インスタンスを指定）';
var instance2 = instance.split(',');
var federate = null; /* ローカル、連合タイムライン判定用 */

window.onload = function () {
  event.preventDefault();
  var ws_ltl = document.getElementsByClassName(""start-ws-ltl"")[0];
  var ws_ftl = document.getElementsByClassName(""start-ws-ftl"")[0];
  // ローカルタイムラインボタン押下時
  ws_ltl.addEventListener('click', function (event) {
    event.preventDefault();
    federate = false;
    // インスタンス分だけWebSocket接続を開く
    for (var i=0;i < instance2.length;i++) {
      open_ws(instance2[i], federate);
      // メッセージが受信時のddEventListener
      ws_connection.addEventListener('message', function (event) {
        insert_toot(JSON.parse(event.data).payload);
      });
    };
  });

  // 連合タイムラインボタン押下時
  ws_ftl.addEventListener('click', function (event) {
    event.preventDefault();
    federate = true;

    for (var i=0;i < instance2.length;i++) {
      open_ws(instance2[i], federate);
      ws_connection.addEventListener('message', function (event) {
        insert_toot(JSON.parse(event.data).payload);
      });
    };
  });
}

// WebSocket接続関数
function open_ws(instance2, federate) {
  url = 'wss://' + instance2 + '/api/v1/streaming';
  if (federate) {
      url += '?stream=public';
  } else {
      url += '?stream=public:local';
  }
  ws_connection = new WebSocket(url);
}

// トゥート→HTML書出し関数
function insert_toot(text) {
  entrys = processing_entrys(text); /* トゥート取得 */
  if (entrys !== null) {
    entrys.forEach(entry => {
      parent = document.getElementsByClassName(""activity-stream"")[0];
      parent.innerHTML = entry+'<hr>'+ parent.innerHTML;
    });
  }
}

// トゥート編集関数
function processing_entrys(data) {
  statuses = [JSON.parse(data)];
  if (statuses.length > 0) {
    entrys = [];
    for (let idx = 0; idx < statuses.length; idx++) {
      const status = statuses[idx];
      try {
        account = status['account'];
        media_attachments = status['media_attachments'];
        status__header = html_status__header(status['url'], status['created_at'], account['url'], account['avatar'], account['display_name'], account['acct']);
        status__content = html_status__content(status['content']);

        md_att = [];
        for (let idx_md_att = 0; idx_md_att < media_attachments.length; idx_md_att++) {
            const media_attachment = media_attachments[idx_md_att];
            md_att.push(media_attachment['url']);
        }
        MediaGallery = null;
        if (media_attachments.length > 0) {
            MediaGallery = html_MediaGallery(md_att);
        }
        entrys.unshift(html_entry(status__header, status__content, MediaGallery));
      }
      catch (e) {

      }
    };
    return entrys;
  } else {
    return null;
  }
}

// トゥート結合関数
function html_entry(status__header, status__content, MediaGallery) {
  var entry;
  entry = status__header + '<BR>';
  entry += status__content + '<BR>';
  if (MediaGallery !== null) {
    entry += MediaGallery + '<BR>';
  }
  return entry;
}

// トゥートヘッダー編集関数
function html_status__header(status_url, status_time, author_url, author_avatar_url, author_name, author_id) {
  var status__header;
  status__header = author_name + ' ' + new Date(status_time).toLocaleString();

  var wkaa = author_url.replace('https://','');
  wkaa = wkaa.replace(('/@'+author_id),'');
  status__header += ' @' + author_id +'(' + wkaa +')';

  return status__header;
}

// トゥートテキスト部編集関数…特に何もしていない
function html_status__content(text) {
  var status__content;
  status__content = text;
  return status__content;
}

// トゥート画像部編集関数
function html_MediaGallery(img_urls) {
  var media_gallery;
  img_urls.forEach(img_url => {
    media_gallery = '<img width=\""100\"" src=\""';
    media_gallery += img_url;
    media_gallery += '\""><BR>';
  });
  return media_gallery;
}

</script>
</body>
</html>



３．気づいた事
試していて気づいた事は以下です。

流量の少ないインスタンスをまとめて眺めるのに便利
多分、ずっと動かすと重くなる（HTMLに書き出しつづけるので）
インスタンスによってはストリーミングに対応していない(Qiita丼など)
複数タブでこのHTMLを実行するとエラーになる
ローカルでも動く


４．まとめ
元のツールが素晴らしいのでたったこれだけでストリーミングが取得できました。
なかなか便利かなと思います。
以　上
",False,https://qiita.com//basictomonokai/items/a445c9a8029e94ad25f4
"久しぶりの投稿になります！
来年から新横浜で仕事をすることになったので、横浜エリアの賃貸情報を効率よく比較できたらなーと思い、スクレイピングにてデータを取得することにしました。
参考にしたサイトは、こちらです。
機械学習を使って東京23区のお買い得賃貸物件を探してみた　〜スクレイピング編〜

やりたいこと
Suumoさんのデータから、新横浜近くである港北区および神奈川区にある物件を以下のようなデータ構造で取得したい。
* 写真をいれる
SUMMOさんの利用規約を確認したところ、以下のようになっています。

ユーザーは、本サイトを通じて提供されるすべてのコンテンツについて、当社の事前の承諾なく著作権法で定めるユーザー個人の私的利用の範囲を超える使用をしてはならないものとします。

今回の場合は、あくまで個人的に参照するためにデータを集めるということなので、大丈夫でしょう。

環境
OS：Windows 10
使用する言語：Python
ライブラリ：BeautifulSoup
自分はwindows10上でAnacondaを使っているので、以下のコマンドで仮想環境の構築を行いました。
conda create -n scraping python=3.6.0
pip install beautifulsoup4

とくに仮想環境にこだわらないのであれば、そのまま「pip install beautifulsoup4」として頂いても問題ないと思います。 

コードの説明
完成したコード全体は以下のようになりました。
順番に説明したいと思います。
from bs4 import BeautifulSoup
import urllib3
import re
import time
import pandas as pd
from pandas import Series, DataFrame

## ステップ1
http = urllib3.PoolManager()
url = ""https://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&pc=30&smk=&po1=25&po2=99&shkr1=03&shkr2=03&shkr3=03&shkr4=03&sc=14102&sc=14109&ta=14&cb=0.0&ct=7.5&md=02&md=03&md=04&md=05&et=9999999&mb=0&mt=35&cn=9999999&fw2=""
response = http.request('GET', url)
soup = BeautifulSoup(response.data, ""html.parser"")

## ステップ2
pages = soup.find_all('ol', class_='pagination-parts')
pages = str(pages)[::-1]
m = re.search(r'\<\d\d\d\>',pages)
max_page_number = int(m.group(0).replace(""<"", """").replace("">"", """")[::-1])
urls = []
urls.append(url)

## ステップ3
for i in range(max_page_number - 1):
    page_num = str(i + 2)
    url_page = url + '&pn=' + page_num
    urls.append(url_page)

names = [] #マンション名
addresses = [] #住所
locations0 = [] #立地1つ目（最寄駅/徒歩~分）
locations1 = [] #立地2つ目（最寄駅/徒歩~分）
locations2 = [] #立地3つ目（最寄駅/徒歩~分）
ages = [] #築年数
heights = [] #建物高さ
floors = [] #階
rent = [] #賃料
admin = [] #管理費
others = [] #敷/礼/保証/敷引,償却
floor_plans = [] #間取り
areas = [] #専有面積
detail_urls = [] # 詳細URL

## ステップ4
for url in urls:
    response = http.request('GET', url)
    soup = BeautifulSoup(response.data, ""html.parser"")

    apartments = soup.find_all('div', class_='cassetteitem')

## ステップ5
    for apartment in apartments:

        room_number = len(apartment.find_all('tbody'))

        name = apartment.find('div', class_='cassetteitem_content-title').text
        address = apartment.find('li', class_='cassetteitem_detail-col1').text

        for i in range(room_number):
            names.append(name)
            addresses.append(address)

## ステップ6
        sublocation = apartment.find('li', class_='cassetteitem_detail-col2')
        cols = sublocation.find_all('div')
        for i in range(len(cols)):
            text = cols[i].find(text=True)
            for j in range(room_number):
                if i == 0:
                    locations0.append(text)
                elif i == 1:
                    locations1.append(text)
                elif i == 2:
                    locations2.append(text)

## ステップ7
        age_and_height = apartment.find('li', class_='cassetteitem_detail-col3')
        age = age_and_height('div')[0].text
        height = age_and_height('div')[1].text

        for i in range(room_number):
            ages.append(age)
            heights.append(height)

## ステップ8
        table = apartment.find('table')
        rows = []
        rows.append(table.find_all('tr'))

        data = []
        for row in rows:
            for tr in row:
                cols = tr.find_all('td')
                if len(cols) != 0:
                    _floor = cols[2].text
                    _floor = re.sub('[\r\n\t]', '', _floor)

                    _rent_cell = cols[3].find('ul').find_all('li')
                    _rent = _rent_cell[0].find('span').text
                    _admin = _rent_cell[1].find('span').text

                    _deposit_cell = cols[4].find('ul').find_all('li')
                    _deposit = _deposit_cell[0].find('span').text
                    _reikin = _deposit_cell[1].find('span').text
                    _others = _deposit + '/' + _reikin

                    _floor_cell = cols[5].find('ul').find_all('li')
                    _floor_plan = _floor_cell[0].find('span').text
                    _area = _floor_cell[1].find('span').text

                    _detail_url = cols[8].find('a')['href']
                    _detail_url = 'https://suumo.jp' + _detail_url

                    text = [_floor, _rent, _admin, _others, _floor_plan, _area, _detail_url]
                    data.append(text)

        for row in data:
            floors.append(row[0])
            rent.append(row[1])
            admin.append(row[2])
            others.append(row[3])
            floor_plans.append(row[4])
            areas.append(row[5])
            detail_urls.append(row[6])


        time.sleep(10)

## ステップ9
#各リストをシリーズ化
names = Series(names)
addresses = Series(addresses)
locations0 = Series(locations0)
locations1 = Series(locations1)
locations2 = Series(locations2)
ages = Series(ages)
heights = Series(heights)
floors = Series(floors)
rent = Series(rent)
admin = Series(admin)
others = Series(others)
floor_plans = Series(floor_plans)
areas = Series(areas)
detail_urls = Series(detail_urls)

suumo_df = pd.concat([names, addresses, locations0, locations1, locations2, ages, heights, floors, rent, admin, others, floor_plans, areas, detail_urls], axis=1)

suumo_df.columns=['マンション名','住所','立地1','立地2','立地3','築年数','建物の高さ','階層','賃料料','管理費', '敷/礼/保証/敷引,償却','間取り','専有面積', '詳細URL']

suumo_df.to_csv('suumo_shinyoko.csv', sep = '\t',encoding='utf-16')


ステップ1
BeautifulSoupにGETしたデータを格納してパースしています。
リクエストを送るために利用するurllib3ですが、
以前のurllib2にあったopenurl関数が使えなくなっているので、
以下のようにPoolManager()を呼び出したあと、request('GET', url)としています。
## ステップ1
http = urllib3.PoolManager()
url = ""https://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&pc=30&smk=&po1=25&po2=99&shkr1=03&shkr2=03&shkr3=03&shkr4=03&sc=14102&sc=14109&ta=14&cb=0.0&ct=7.5&md=02&md=03&md=04&md=05&et=9999999&mb=0&mt=35&cn=9999999&fw2=""
response = http.request('GET', url)
soup = BeautifulSoup(response.data, ""html.parser"")


ステップ2
こっちは少し強引ですが、ページネーションの最終ページを取得している部分です。

ページネーションHTMLの文字列を取得
1で取得した文字列を[::-1]で反転させる
re.searchとm.groupにて、ページ数の最大値を取得。同時にreplaceで<>を削除
最後に反転した文字列を復元

あまり良いやり方ではないと思っているので、もう少しスマートにできるやり方が見つかり次第修正するつもりです。
## ステップ2
pages = soup.find_all('ol', class_='pagination-parts')
pages = str(pages)[::-1]
m = re.search(r'\<\d\d\d\>',pages)
max_page_number = int(m.group(0).replace(""<"", """").replace("">"", """")[::-1])
urls = []
urls.append(url)


ステップ3
ここでは、取得したい情報があるページのURLリストを作成しています。
調べたところ、URLには以下のような法則性がありました。
１ページ目：ttps://suumo.jp/jj/...&fw=2
２ページ目以降：ttps://suumo.jp/jj/...&fw=2&pn=ページ数
ですので、１ページ目は初期設定に使ったURLをそのまま使用。
２ページ目以降は「１ページ目のURL + &pn=ページ数」としたものを順にリストurlsへ追加しています。
## ステップ3
for i in range(max_page_number - 1):
    page_num = str(i + 2)
    url_page = url + '&pn=' + page_num
    urls.append(url_page)

ステップ３の直後に、空リストの集団が出てきますが、ここに取得したデータをどんどん入れていきます。
このリストを最後にpandasで結合して、一つのcsvにして出力します。
names = [] #マンション名
addresses = [] #住所
locations0 = [] #立地1つ目（最寄駅/徒歩~分）
locations1 = [] #立地2つ目（最寄駅/徒歩~分）
locations2 = [] #立地3つ目（最寄駅/徒歩~分）
ages = [] #築年数
heights = [] #建物高さ
floors = [] #階
rent = [] #賃料
admin = [] #管理費
others = [] #敷/礼/保証/敷引,償却
floor_plans = [] #間取り
areas = [] #専有面積
detail_urls = [] # 詳細URL


ステップ4
ここでは、ステップ3で作成したURLを順に処理するようにしています。
また、apartmentsという、マンションの情報がまとまったブロックを30個分取得しています。
このあと、1ブロックずつデータを取り出す必要があるので、あらかじめマンション単位で分割したデータを用意しています。
↓このブロックが1ページあたり30個分あるので、これを取得

## ステップ4
for url in urls:
    response = http.request('GET', url)
    soup = BeautifulSoup(response.data, ""html.parser"")

    apartments = soup.find_all('div', class_='cassetteitem')


ステップ5
ここから、マンション１つに対してのデータ取得処理になります。
マンション１つに対して複数の部屋の貸し出し情報が掲載されているため、あからじめroom_number = len(apartment.find_all('tbody'))で部屋の情報を掲載しているテーブルから部屋の数を取得しています。
nameとaddressは、いったんデータを取得したのち、部屋数の分だけデータをコピーして下に挙げたリストに追加しています。

names = [] #マンション名
addresses = [] #住所

## ステップ5
    for apartment in apartments:

        room_number = len(apartment.find_all('tbody'))

        name = apartment.find('div', class_='cassetteitem_content-title').text
        address = apartment.find('li', class_='cassetteitem_detail-col1').text

        for i in range(room_number):
            names.append(name)
            addresses.append(address)


ステップ6
ステップ6では、立地のデータを取得しています。

locations0 = [] #立地1つ目（最寄駅/徒歩~分）
locations1 = [] #立地2つ目（最寄駅/徒歩~分）
locations2 = [] #立地3つ目（最寄駅/徒歩~分）

今回は、HTMLの構造が下に挙げたようにliの下にdivの子が3つある状態になっています。
<li cassetteitem_detail-col2>
 <div>text1</div>
 <div>text2</div>
</li> 

ですので、まずはfind_all('div')で子要素のdivをすべて取得してcolsに代入します。
その後、for i in range(len(cols))の中にて
　cols[i]からcols[0].find(text=True)、cols[1]find(text=True)、cols[2]・・・
といった形で要素を取得して、上で挙げたlocations0~2のリストに追加しています。
## ステップ6
        sublocation = apartment.find('li', class_='cassetteitem_detail-col2')
        cols = sublocation.find_all('div')
        for i in range(len(cols)):
            text = cols[i].find(text=True)
            for j in range(room_number):
                if i == 0:
                    locations0.append(text)
                elif i == 1:
                    locations1.append(text)
                elif i == 2:
                    locations2.append(text)


ステップ7
以下のリストにデータを追加する個所です。

ages = [] #築年数
heights = [] #建物高さ

ここもステップ6で説明したHTMLと同じ構造をしているのですが、すこし違う手法でデータを取得しています。
age_and_height('div')[0].textとありますが、以下のようなイメージになります。
<li cassetteitem_detail-col3>

 <!-- age_and_height('div')[0]に該当 -->
 <div>text1</div>

 <!-- age_and_height('div')[1]に該当 -->
 <div>text2</div>

</li> 

## ステップ7
        age_and_height = apartment.find('li', class_='cassetteitem_detail-col3')
        age = age_and_height('div')[0].text
        height = age_and_height('div')[1].text

        for i in range(room_number):
            ages.append(age)
            heights.append(height)


ステップ8
ステップ8では、以下のリストを取得しています。

floors = [] #階
rent = [] #賃料
admin = [] #管理費
others = [] #敷/礼/保証/敷引,償却
floor_plans = [] #間取り
areas = [] #専有面積
detail_urls = [] # 詳細URL

自分はここで結構ハマりました。
tableをパースしてtr、tdタグ以下の構造を取得したまでは良かったのですが、td以降のタグ構造がカラムごとにバラバラだったので。
最終的に、各々のtd以下のliタグをfind_allで取得して、そこから各spanに保持しているテキストを取得することにしました。
ここでのHTMLの構造はこんな感じです。
<ul>
 <li><span>text1</span></li>
 <li><span>text2</span></li>
</ul> 

上で挙げたHTMLの例に対応したpythonのコードはこちら。
# ここでul > li以下のHTMLをリストで取得
_rent_cell = cols[3].find('ul').find_all('li')   

# 1つ目のspanが保持するテキストを取得
_rent = _rent_cell[0].find('span').text

# 2つ目のspanが保持するテキストを取得
_admin = _rent_cell[1].find('span').text

## ステップ8
        table = apartment.find('table')
        rows = []
        rows.append(table.find_all('tr'))

        data = []
        for row in rows:
            for tr in row:
                cols = tr.find_all('td')
                if len(cols) != 0:
                    _floor = cols[2].text
                    _floor = re.sub('[\r\n\t]', '', _floor)

                    _rent_cell = cols[3].find('ul').find_all('li')
                    _rent = _rent_cell[0].find('span').text
                    _admin = _rent_cell[1].find('span').text

                    _deposit_cell = cols[4].find('ul').find_all('li')
                    _deposit = _deposit_cell[0].find('span').text
                    _reikin = _deposit_cell[1].find('span').text
                    _others = _deposit + '/' + _reikin

                    _floor_cell = cols[5].find('ul').find_all('li')
                    _floor_plan = _floor_cell[0].find('span').text
                    _area = _floor_cell[1].find('span').text

                    _detail_url = cols[8].find('a')['href']
                    _detail_url = 'https://suumo.jp' + _detail_url

                    text = [_floor, _rent, _admin, _others, _floor_plan, _area, _detail_url]
                    data.append(text)

        for row in data:
            floors.append(row[0])
            rent.append(row[1])
            admin.append(row[2])
            others.append(row[3])
            floor_plans.append(row[4])
            areas.append(row[5])
            detail_urls.append(row[6])

        time.sleep(10)

取得したデータは、textにリスト形式で代入し、最後にリストのdataに追加しています。
最後にできるdataの構造は以下のようになります。
[
 [_floor1, _rent1, _admin1, _others1, _floor_plan1, _area1, _detail_url1],
[_floor2, _rent2, _admin2, _others2, _floor_plan2, _area2, _detail_url2],
...
]


ステップ9
最後は、いままで取得してきたデータをpandasを使って結合しています。

Seriesでシリーズ化
1のデータをpandas.concatで結合
suumo_df.columnsで、各カラムにタイトルを追加
suumo_df.to_csvで、結合したデータをCSV形式にして出力

## ステップ9

#Seriesでシリーズ化
names = Series(names)
addresses = Series(addresses)
locations0 = Series(locations0)
locations1 = Series(locations1)
locations2 = Series(locations2)
ages = Series(ages)
heights = Series(heights)
floors = Series(floors)
rent = Series(rent)
admin = Series(admin)
others = Series(others)
floor_plans = Series(floor_plans)
areas = Series(areas)
detail_urls = Series(detail_urls)

# 1のデータをpandas.concatで結合
suumo_df = pd.concat([names, addresses, locations0, locations1, locations2, ages, heights, floors, rent, admin, others, floor_plans, areas, detail_urls], axis=1)

# 各カラムにタイトルを追加
suumo_df.columns=['マンション名','住所','立地1','立地2','立地3','築年数','建物の高さ','階層','賃料料','管理費', '敷/礼/保証/敷引,償却','間取り','専有面積', '詳細URL']

# 結合したデータをCSV形式にして出力
suumo_df.to_csv('suumo_shinyoko.csv', sep = '\t',encoding='utf-16')

長い説明となりましたが、以上になります！
",False,https://qiita.com//bottusan1073/items/2093b76ff7734d733879
"先日phpMyAdminを用いてMySQLのいろはを勉強しました。
まずは、データベースに登録したデータをウェブ上に取り出すことができたので、備忘録を兼ねてアウトプットしていきたいと思います。
※コード間違いがある可能性があります。
その際はご指摘くださるとありがたいです。

手順

１.データベースへの接続

index.php
<?php
    $db = new mysqli('アドレス', 'ユーザー名', 'パスワード', 'データベース名');
    if ($db->connect_error) {
        echo $db->connect_error;
        exit();
    } else {
        $db->set_charset(""utf8"");
    }

    echo ""<table>"";




２.データベース（form）内の4つのカラム（number,name,age,ex）内のデータを取り出す。

index.php
  $sql = ""SELECT number,name,age,ex  FROM form"";
    if ($result = $db->query($sql)) {

        while ($row = $result->fetch_assoc()) {
            echo ""<tr><td>"" . $row[""number""] . ""</td><td>"" . $row[""name""] . $row[""age""] . $row[""ex""] . ""</td></tr>"";
        }




３．結果を閉じて、データベースを切断

index.php
      $result->close();
    }
    echo ""</table>"";

    //データベース切断
    $db->close();
?>


今回は、データベース内にあるデータを取り出すコードを記載しました。
次は、データベースにデータを登録できるように勉強していきたいと思います。
",False,https://qiita.com//yuhei_umeda/items/6e64146dc502cc5aee9d
"

背景
現在新米エンジニアとして派遣先企業に出向しています。そちらでExcelを使用した設計書を作成することになりました。
これが…噂に聞いてたExcel方眼紙設計書か…などと思いながら作成しています。その中で、効率よく作業するために行った工夫をいくつかしたので紹介したいと思います。

環境

Windows7
Excel2010


実際にやっていること

①ウィンドウ表示は最大にしない
これはExcel2010までの問題でウィンドウというのは、アプリケーション自体のウィンドウのことではなくファイルごとのブックに対するウィンドウです。Excel2010までは複数ウィンドウでエクセルを開くことができません。Excelファイルをいくつ開こうとも、一つのウィンドウ内に収まってしまうのです。MicrosoftのDeveloperにも記事がありました。
同時に扱うファイルが多くなれば見にくくなり、ウィンドウを最大表示にしていると左右で違うファイルを扱うのが困難になってきます。そのためウィンドウは最大化しないほうが扱いやすくなります。

Excel2010まではこのようにウィンドウを自由に並べることはできない。

クイックアクセスツールバーの活用
自分自身も初めて活用したのがクイックアクセスツールバーです。ウィンドウの左上にある小さいボタン群のことです。
これの良いところは、登録するとAlt+数字キーのショートカットが使用できるということです。
「Excelのショートカットキーってたくさんあるからいらないのでは？」と思っていました。ですが、罫線の枠を削除する。や外枠だけ使用する。などのかゆいところに手が届くを実現してくれるのがクイックアクセスツールバーです。わたしは以下のように登録しています。

ウィンドウ整列
ウィンドウの切り替え
外枠太罫線
罫線なし
その他の罫線(セルの書式設定を開く)
格子


写真はExcel2013
①ウィンドウ表示は最大にしない　とウィンドウの整列は非常に相性がよく、Alt+1のショートカットキーで簡単に並び替えができるので重宝しています。比較したいファイル以外は一旦最小化しておいて並び替えするととてもスムーズに作業ができます。これを言いたいがためにこの記事を書いたといっても過言では無いです。

印刷設定を早めにしておく
今回作成しているのが設計書ということもあって、印刷して提出するということも想定しています。また、内容のチェックを行う際にディスプレイで確認するのと、手元に紙を置いて赤ペンでチェックをつけていくのとでは後者の方がやりやすいように思います。そのため、印刷することを意識して設計書を作成しています。
しかし、いくら印刷を意識していたとしても印刷設定を疎かにしてしまうとあとからレイアウトが崩れてしまいます。そのため、早めに印刷設定を行っておくのがよいかと思います。
おすすめは、拡大縮小の項目を『全ての列を1ページに印刷』にしておくことです。これで横幅めいっぱい使用した印刷ができます。

最後に
Excelで設計書を作成している企業はたくさんあると思います。なにかと便利なExcelですが、そもそもこのアプリケーションは表計算ソフトです。Excelが便利なのは百も承知ですが本質は忘れないようにしたいと思います。最後に一番伝えたいことを一言お伝えして終了したいと思います。

Excelの設計書なんて滅んでしまえ！！
",False,https://qiita.com//right1121/items/eb245c0f13fb9ccac52f
"

はじめに
　PDCAのC(Check:振り返り)がやりたかった．自分が触れてきた言語やツールの感想とよかったこと，まずかったことの3つにまとめて書いていく．「プログラミングやりたいけど全然わかんない!」という人でも，1年半でここまで進めるという実例にもなると思う．また，あまりにも駄文なので流し読みを推奨する．(ごめんなさい!!!)

触った言語・ツール
極力，時系列を意識して書いていく．年月はおおよそ．

Html:　2017年4月
大学に入って初めてパソコンを触ったわけだが，なにかものを作るという行為をはじめてしたのがこれ．ブラインドタッチも当然できず，あらゆる物事がちんぷんかんぷんだった．

C:　2017年9月
はじめてのプログラミング言語．ポインタが理解できなかった．パソコンの知識が足りないせいだったと確信している．セグメンテーションエラーにめちゃくちゃ苦しんだ．だんだんググる力が身についてくる．

Python:　2017年10月
ｃ言語よりも簡単で感動するも，クラスを理解することができなかった．scikit-learnをつかって機械学習をやるが，数学が大事だと気づく．年明けぐらいから，ただコードを書けるだけではいけないと感じ始める．

Haskell:　2017，2018年(わすれた)
関数型言語の存在を知る．素直にすげーと思った言語だが，文法も今まで見たものとは全く異なるもので理解に苦しんだ．またここで紹介する言語の中で，AOJの問題をとき始めるまでに最も時間がかかった言語でもある．

Rust:　2018年8月
オライリーのRustの本が日本語訳になったのをきっかけにはじめた．Haskellをやっていたおかげもあり，traitへのイメージなどがつかめた．Box, AsRef, マクロ, 並列処理などが理解できなかった．これはプログラミングそのものに対する，知識不足によるものだと考える．

Go: 2018年10月
難しい言語をやってきてかなり疲れてきたので，簡単だと評判のgolangを始める．実際覚えることは少なくすぐにAOJの問題をとき始めることもできた．また，はじめてwebサーバを立てることにも成功した言語．これによりweb周りがどの様になっているのかを理解することができたので，感謝の極みである．

SQL:　2018年10月
Goと並行して進め，postgresを使った．最初，データの置き場所はpostgresを作った企業のサーバにあるという盛大な勘違いをしていた．

Nim:　2018年11月
Goがあまりに快適で新しい言語をやるのはやめようと思っていたが，ジェネリクスがある言語を使いたくて始めた．今の所不満はないが，強いて言うなら日本語の文献が少ないのが残念．

Docker
コマンドは理解したが，うまく活かせていないのが現状．Dockerfileの書き方やdocker.compose(?)についての知識がほしい．

Vim
最初の頃は一生懸命.vimrcを編集していたが，.vimrcいじってもプログラミングはできないと思い，結局他のエディタにvimの拡張を入れている．移動とかコピペとかいろいろ便利．

Git
Version管理をするためのもので，なんとなくgit commitとかgit pushとかしているが，大規模なコードを書いていないので，うまく活かせていない．

Windows
初期の頃はWindowsでプログラミングしていたが，意味のわからんerrorが多発するので，Windowsでプログラミングすることはもうないと思う．

Linux
Goでwebサーバを立てたときにCentOsを使ったが，よくわからないままコマンドを打っていた．コマンドの意味などを理解するのには時間がかかりそう．(systemctlなど)

Mac
今メインで使っているOS．非常に高かったが，買う価値はあったと思う．

AOJ
プログラミングの問題をいっぱい用意してくれているところ．一通り文法をやったら，これで腕試しするのが毎回の流れ．

Atcoder
プログラミングのコンテストを開催しているところ．コンテストと聞くとハードルが高そうだが，全然そんなことはない．またコンテストでの自分の順位や他人のコードを見れたりするので，モチベも上がるし，勉強にもなる．

Udemy
学習コンテンツを動画で出しているところ．セールをやっているので，下手に本買うよりコスパが良かったりする．

よかったこと

　無知の知

　関数型言語に会えたこと

　快適な環境

　一旦時間をおいてから，取り組む

　ググる力の向上

　インプット・アウトプット

　圧倒的成長

まずかったこと

　焦りからくる短期間での詰め込み

　自分への投資の出し渋り

　コードを書かずに読むだけ

　基礎から始めない

　寝ない

　手汗

これから

Nimを頑張って，なんか作る．
ウルトラスーパーギガンティックエンジニアになる．
手汗を止める


さいごに
さいごまで読んでくれてありがとう!!!
リンクを貼るといい記事になるそうですが，貼りません．気になったものがあったらググってください!
",False,https://qiita.com//kf_hmd/items/a77c47a3c0135a4ad648
"先日、ひょんなことから人生初の OSS 貢献を果たしました。
色々試行錯誤しながら取り組み、学ぶことが多くありました。貴重な経験ができたと思うので、体験記としてまとめます。
僕のように、初めて OSS 貢献を始めようと思っている方の参考になれば幸いです。

作成した Pull Request
Update user_create action changeable and update meeting_create endpoint by gotchane · Pull Request #2 · hintmedia/zoom_rb · GitHub
ビデオ会議サービスの Zoom の API が V1 -> V2 にバージョンアップすることに伴い、Zoom の Ruby クライアントのメソッドを修正した Pull Request です。

背景

2018/11/1 で Zoom API V1 が depreciated になり、Zoom API V2 の利用がメインになる
社内で開発している Web アプリで、Zoom API クライアント の gem であるzoomus を使っていた
しかし zoomus が V1 の対応しかしておらず、Issue はあるが対応メドはたっていない状況だった
depreciated まであと約1ヶ月。割と詰んでいる、どうしよう。。


対応方針検討
最初は zoomus 自体に修正は入れず、Web アプリ側で、zoomus のメソッドをオーバーライドして対応できないかと考えていました。
そんな中会社の先輩 @aki77 さんから、「別のプロジェクトでも同様に zoomus を使ってるので、直接 PR 送ればいいのでは？」というアドバイスが。
とはいえ、zoomus は V1 の対応のみ。API エンドポイントや認証方法の修正なども対応範囲が広く、大変そうでした。
どうすればよいかあれこれ調べていたところ、会社の先輩 @mat_aki さんからの助け舟が。
zoomus の fork リポジトリ群から、API V2 の対応を進めていそうなリポジトリを見つけてくださいました…！
https://github.com/hintmedia/zoom_rb
コミットログも活発で、ガンガン更新していそうな感じ。

また gem もリリースしているようでした。これは有力。
https://rubygems.org/gems/zoom_rb
こちらを使えると、利用しているメソッド修正のみの対応に範囲が絞られ、見通しが立ちそうです。このリポジトリを fork して、今回の Web アプリで利用するメソッドを書き換える対応を進めていきました。

Pull Request を送るまで
こちらを参考に PR 作成を進めました。
https://github.com/hintmedia/zoom_rb#contributing
GitHub Help にCreating a pull request from a fork というガイドがあるので、こちらも参考になります。
fork して Pull Request を送るのも初めての経験でした。。

1. Fork it
まずリポジトリを fork します。fork してできたリポジトリは以下の通り。
https://github.com/SonicGarden/zoom_rb

2. Create your feature branch
ローカルにリポジトリを clone し、開発ブランチを作成します。
$ git clone https://github.com/SonicGarden/zoom_rb
$ cd zoom_rb
$ git checkout -b my-new-feature


3. Commit your changes
ここからは修正対応を進めていきました。簡単に対応について書いていきます。

依存 gem インストール
今回は gem の開発なので、依存している gem をインストールします。
$ bundle


テスト実行
gem で準備されているテストを実行し、テストがすべて通ることを確認します。今回は特に修正の必要はありませんでした。
$ bundle exec rspec


開発開始
Zoom API ドキュメントをそれぞれのバージョンで確認して、レスポンスの違いを確認しながら修正を進めました。

V1: https://zoom.github.io/api-v1

V2: https://zoom.github.io/api


社内アプリで利用しているのは user_create と meeting_create というメソッド。それぞれ以下のような差分がありました。



メソッド
API V1
API V2




user_create
作成オプションが create のみ
作成オプションが create,custCreate, autoCreate, ssoCreateの4パターンに


meeting_create
URLが/meeting/create

URLが/users/{userId}/meetings




リクエストパラメータは Zoom API の Playground を参考に確認しました。
https://developer.zoom.us/playground/
修正はこんな形で。URLや、パラメータの渡し方を修正する感じです。

修正ができたら、実際にローカルのコンソールで API 実行できるか確認し、
$ bundle exec irb

関連する spec も追加し、All Green になることを確認しました。


4.Push to the branch
開発が完了したら リモートリポジトリにプッシュします。社内ではこの時点で、fork したブランチを基に、社内の Web アプリを先に修正して使えるようにしました。これも fork の利点だなと思います。
$ git push origin my-new-feature

あとは Pull Request を作成し、マージされるのを待つのみ…！

5. Create new Pull Request
いよいよ Pull Request を作成します。
fork 先で Pull Request を作成すると、fork 元の master ブランチが base になった状態で差分確認画面に移ります。便利ですね…！

慣れない英文、初めての OSS への PR だったので、社内の人に書いた description を確認してもらってから出しました。できる限り丁寧に書いたつもりです。

関連して、CI が spec と関係ないところで失敗していたので、併せてコメントを残しておきました。


待望のコメント
そして PR を出してから 20日ほど経ち、ついに PR にコメントが来ました！

概ね OK だけど、 Hash の Key と Value を入れ替えてくれ、というリクエストでした。
Public Beta 版のSuggested Change 機能を使ってコメントくださいました。見やすい…！


そしてマージへ
修正完了の連絡後、ついにマージされました！！
gem のバージョンも上げてくださりました。このときの達成感は尋常ではありませんでした…！


今回の OSS 貢献を通して学んだこと

Pull Request を出すという目的を持つと、必然的にコードを理解しようとして読むようになる
仕事で使っているライブラリを修正する必要が出てきたときは、まず fork して修正して使える状態にしておき、本家リポジトリに PR を出しておくと良い
fork 先に必要な修正をしているリポジトリがあるかもしれない
海外のプログラマと直接英語でやり取りするのは非常に刺激的。日本にいても英語を使ういい機会になる
今回は偶然 API クライアントの修正だったが、分量的にも、API クライアントはとっかかりとしては良い題材だと思う
修正していて気づいたことは Pull Request にコメントを入れておくと、メンテナーが対応してくれるかも


おわりに
今回の OSS 貢献を通して、OSS 貢献の魅力を体感することができました。これからも OSS 貢献できたら記事にまとめようと思います！
",False,https://qiita.com//gotchane/items/82552cd4fe48219272a7
"

はじめに
本記事は、オブジェクト指向をざっくり理解している人向けに書きます。

オブジェクト？
そもそも、オブジェクトの捉え方ですが…
それは圏論の言葉で言うと「圏」であり…

それ難しいから（笑）
そもそもから話そうではないか我が友よ（笑）
そもそも数学嫌いが多いじゃないか
重要なのは対話じゃないか

アレルギーの方へ…
例えば、1+1=2　という数式で説明します。
実は、証明できます。
しかし、そんな事をしなくても自明の理（当たり前）です。
この、ある値がとり得る当たり前の図式 を結論だけ言って整理すると…
（間はめちゃくちゃ長いです）
1に1を足すと2になるというメソッドで切り出せるということになります。
（すごく乱暴です。）
こういう、当たり前の図式　の事を、自然変換という名前で読んでいるだけです。
で、それを、分かりやすい言い方に変えるとメソッドになります。
分かりづらいなら、
x+y=z
xにyを足すと z になる
という動詞だということに着目してください。
これを普段のオブジェクト指向から説明すると、
オブジェクトAをupdateするとAの各プロパティは B になる
つまりは、メソッドはオブジェクトが持つ自明の理を実装しているに過ぎません。

メソッドが自然変換だと考えたら出来ること
ここもゴニョっと言えますが…
乱暴に片付けてしまうと、オブジェクトが持っているプロパティが、ある値になる。
それは、～～～～～～の理由で…
という風に記述しますよね。
つまり、あまりにも長いメソッドは、リファクタリングをします。
細かい記述に書き直せるというわけです。
このリファクタリングされたメソッドは、もしかすると
オブジェクトが持っている値を直接書き換えない場合が多いのではないでしょうか？
ところが、リファクタリングしても、切り出したメソッドの再利用価値が低いなどということも考えられます。

メソッドの分解 = 自然変換の分解
あるメソッドは、リストをイテレート（巡回）した結果を返すとします。
イテレートの際は、必ず幾つかの約束を果たさねばなりません。
よくある設計ですよね？
そこで、意気揚々とリファクタリングします。
実は、リファクタリングは、自然変換の分解です。
分解すると、より小さなメソッド（つまり関数）に切り出せます。
そういった概念を、「圏論」の言葉でいうと「関手」、「射」と読んでいるだけです。
つまり、関数型プログラミングの最初の理解に大切なことは。

メソッド内だけで、関数型プログラミングをする
本当にミクロな書き方が出来ると思います。
そういうところで便利さが分かってくると面白くなってきますので、
怖がらずにまずは
ラムダ（無名関数）
を使った表現に変えてみることから、初めてみてください。
",False,https://qiita.com//johnny-shaman/items/90002ac38a8cae06e538
"

結論

""英語""が今の時代で1番重要な自然言語

はじめに
こんにちは、がっちょです。
今回は""自然言語""について話そうと思います。

自然言語とは
Wikipediaによると

自然言語（しぜんげんご、英: natural language）とは、人間によって日常の意思疎通のために用いられる、文化的背景を持って自然に発展してきた言語である。
(Wikipedia - 自然言語)

つまり、簡潔に言うと私たちが会話をするために話している言語のことです。
英語・日本語・中国語・スペイン語・アラビア語・ラテン語・シルボ語……などなど
これらの言語を自然言語と呼びます。
エスペラント語のように人工的に作られた言語を人工言語と呼びます。

英語
英語は重要な自然言語です。
開発者にとって絶対に欠けることが出来ない自然言語だと思います。
では、具体例を見てみましょう。
以下のソースコードはC言語で文字列を出力するものです。

Source.c
#include <stdio.h>
int main(void) {
    printf(""Hello, World!\n"");
}


皆さんお気づきでしょうか？
実はこのソースコード、英語が使われています。

英語
include //含める
main //主要な
void //空(から)の
print //表示される
Hello, World! //こんにちは、(このプログラミング言語の)世界！


このように多くのプログラミング言語は英単語が多く登場します。
多くのプログラミング言語の入門書に書かれているHello, World!も
英語ですので、英語の重要さがわかると思います。

日本語
この記事を見ているほとんどの方は、日本語が母語の方がほとんどだと思います。
母語が日本語で、日本語に自信がないという方はまず日本語を勉強すると良いと思います。
私は日本語が苦手です……英語も苦手です……
皆さんはどうですか？
日本語がある程度できないと英語力は身に付きません。
例えば、
英語：dog
日本語：犬

dogという英単語があります。
その英単語は、日本語では犬という意味らしいです。
しかし、日本語の犬という単語を知らないと
dogという英単語の意味はわかりません。
このように日本語の語句を知らないことによって理解できない英単語が出てきます。
逆に考えると日本語の語句をたくさん知っている人のほうが、英単語をたくさん覚えられます。
まずは日本語を勉強すると良いです。

最後に
自然言語を学んで、開発がもっと楽しいものになれば良いですね😊

読んでみると良い記事(この記事いいね👍)
全ての開発者が学ぶべき5つの言語
重要なプログラミング言語が知りたかったらこちらの記事を見ると良いかもです。
5 Programming Languages Every Master Developer Should Learnの翻訳記事みたいです。
全ての開発者が学ぶべき1つの言語
D言語について詳しく解説している記事です。
全ての開発者が学ぶべき1つのNim
Nimについて詳しく解説している記事です。
はじめてNimの存在を知りました。
追記
超天元突破ギガドリル並みにタイトルの主語が大きかったので、主語を消滅させました。
",False,https://qiita.com//Gaccho/items/e12e998b0a1788d55f5f
"

はじめに
今更感はありますが、まとめてみようと思いました。

グローバルスコープが保持しているオブジェクトの違い
node.js: Object
browser: window
worker : workerGrobalScope
node.jsは、シンプルにオブジェクトです。
browserは、開いているウインドウを意味するインスタンスになっています。
workerのグローバルスコープは意識することが無いと思います。

必要不可欠な記述の差

node.js: サーバーを記述するという書き方。
ライブラリの読み込み
const server = require(""http"").createServer()

何かが起こったら何かをするという考えでイベントハンドラーを記述する。
server.on(""get"", (req, res) => {
  res.writeHead(200, {'Content-Type' : 'text/html'});
  res.end(""hello world"");
})

サーバーなのでtcp udp のポート番号とIPアドレスの指定をして、
通信を待ち受ける。
server.listen(8080)

ここまでがnode.js上での主要な書き方です。

browser: ビルトインオブジェクトが多い
HTMLElementの扱いとイベント処理を記述することがメインです。最近では端末のデバイスにアクセスできる方向もあります。
エレメント
const myArt = document.createElement(""article"");
myArt.id = ""today"";
myArt.class = ""main article"";
myArt.appendChild(new Text(""hello world""));
document.body.appendChild(myArt);

イベント処理
myArt.addEventListener(""drop"", ((e) => e.style.backgroundColor = ""#880088""));

以上がbrowserでの主要な書き方です。

worker: browser側でインスタンスを作成
インスタンスからのメッセージを受け取れるように、addEventListenerを記述します。
//browserから、workerを作成。
const worker = new Worker('worker.js');
worker.addEventListener(""message"", (e) => alert(e.data));
worker.postMessage('Hello World!');

self.addEventListener('message', (e) => {
  console.log(e.data + ""from View"");
  self.postMessage(e.data + ""from worker"");
});

worker内では、基本的にすごく遅い計算結果やpromiseの結果を返却したりします。
serviceWorkerの場合は、利用方法が違うようです。
よくまとまっている記事がありましたので、　こちら　をどうぞ…
以上、javascriptの言語としての違いというより、実行環境の違いをまとめてみました。
",False,https://qiita.com//johnny-shaman/items/9ab9c67aa2535c58ad0a
"この記事は前回書いた記事【挫折する前に】Ruby 引数がイマイチわからない人への発展編です。
前回の記事では初歩的な解説をしましたので、前回の記事→本記事
の順で読むと理解が深まるのではないかと思います。

本記事の内容

前回の記事【挫折する前に】Ruby 引数がイマイチわからない人へで省略した部分の解説をしていきます。


この記事が参考になりそうな人

前回の記事を読んでいただいた方
「引数がわからなくて挫折しそう！」という初心者の方
「引数がイマイチわからない」という方


前回のおさらい
前回の記事ではイメージを掴んでいただくことを優先して、用語やコードの動きを極力省いて解説しましたが、今回はそれの補足をしていきたいと思います。
前回書いたコードは以下のようなものでした。

sample.rb
def tashizan(hikisu, hikisu2)
hikisu + hikisu2
end

puts tashizan(10, 20)
=>30


「tashizan」メソッドに引数(hikisu)と(hikisu2)を渡して、そこに10と20という数字を入れた結果、30という値を得ることができました。
この部分に関して、前回の記事を読んでいただいた方の中には、もしかしたら「おおよそのイメージはできたけどコードがどういう動きかイマイチ理解できない」
という方もいたと思うので解説します。
この部分を理解するには戻り値(返り値)というものを理解する必要があります。

戻り値(返り値)とは
元々Rubyにはメソッドの値を返すという特性があります。
もし値が空だった場合でもnilという値を返してくれます。
では上記のコード(sample.rb)でどこに値が戻っているかというと、２行目の部分に戻っています。
実はここには下記のように「return」が省略されています。

sample.rb
def tashizan(hikisu, hikisu2)
return hikisu + hikisu2
end

puts tashizan(10, 20)
=>30


このreturnは省略することができるので書いていませんでしたが、これによって、実際は４行目のメソッドが呼ばれた時に1行目の引数に値が渡され、２行目の値に戻っていたことがわかるかと思います。
ちなみにメソッド複数の値がある場合、そのメソッドの中で最後に得られる値が戻り値となります。
文章だけではイメージしにくいかと思うのでコードをみてみましょう。
先ほどの足し算の文の下に掛け算の文を追加してみます。

sample.rb

def tashizan(hikisu, hikisu2)
hikisu + hikisu2
hikisu * hikisu2
end

puts tashizan(10, 20)
=>200


3行目の掛け算の方の値が結果として返ってきていますね。
これが先ほど説明した、そのメソッドの中で最後に得られる値が戻り値となる。ということです。

仮引数と本引数
return文がわかったところで話を引数に戻します。
引数を理解する上で、まず２種類の引数があることを理解する必要があります。
１.仮引数
２.本引数(実引数)
です。
仮引数はメソッドを定義する際に使う引数のことで、
本引数はそのメソッドを実際に呼び出す際に使う引数のことです。
実際のコードで説明すると、１行目が仮引数、５行目が本引数となります。

sample.rb
def tashizan(hikisu, hikisu2)
return hikisu + hikisu2
end

puts tashizan(10, 20)
=>30


先ほどのreturnの流れを思い出してみてください。
１行目で""定義した""tashizanメソッドの引数が、5行目で""呼び出されて""値が戻っていましたよね。
つまり、「仮引数はメソッドを定義する際の引数」で本引数は「メソッドを呼び出す際の引数」ということになります。
ここで大事なポイントがあります。
それは、仮引数の名前は本引数と同じでなくてOKということです。
ここで「どういうこと？？？」となる人が多いと思うので事項で解説していきます。

仮引数をもう少し深掘る
「仮引数」という名前のとおり、仮なのでどんな名前にしても値は渡ります。以下のコードをみてください。
※今回はイメージを掴んでもらうことを優先しています。本来は、英語かつ他の人がみてもわかる引数名にするのがベストです。

sample.rb
def tashizan(hikisu) 
puts hikisu + hikisu  # =>20が出力される
end

hikisu = 10

tashizan(hikisu)


例によってtashizanメソッドを定義しました。
簡単にコードの流れを説明します。
※大前提として、プログラミングのコードは上から順に読まれていくことを理解しておいてください。
1~3行目はメソッドを定義していますが、この時点ではメソッドが呼び出されていないため、実行されず、次の行に処理が移ります。
５行目で数値10を左の""hikisu""に代入しています。
その""hikisu""を８行目の本引数に渡し、先ほど同様、1行目の仮引数に値が渡って出力されます。
では、先ほど説明した本引数と仮引数の名前は同じでなくてもOKというのはどういうことか？コードでみていきます。

sample.rb
def tashizan(abc) 
puts abc + abc  # =>20が出力される
end

hikisu = 10

tashizan(hikisu)


仮引数の値が本引数と異なっていても値が渡るか調べるため、1行目の引数名を""hikisu""から""abc""に変更してみました。
無事、先ほどと同じように20が出力されていますね。
このことから、呼び出す際のメソッド名(tashizan)が合っていれば、仮引数と本引数の名前は必ずしも同じでなくても値は渡るということがわかったかと思います。

まとめ
上記の例で仮引数の名前を変更しても値が渡ることがわかりました。
これはつまり、本引数(hikisu)は数値１０を渡す式であり、その値が仮引数(abc)にコピーされているイメージです。
よって、仮引数の名前を変更しても本引数には反映されないということを表しています。
今回は引数のイメージにフォーカスして、なるべく伝わりやすく書いたつもりです。
引数を理解できると様々なことに応用できるようになるので、慣れるまで復習し続けることが大切だと思います。
",False,https://qiita.com//yossy_sa/items/536913fd58f90d6811f7
"

はじめに

10/31(水) Spiring Fest 2018に参加したので、その感想を書きます。


新人勉強会でSpringを使って学習して行きアプリ開発をするので、どんなことができるか考えるためと、自分の勉強のモチベーションアップになればいいなと思い参加しました。




Spring Fest概要
公式サイト ：http://springfest2018.springframework.jp/


日本Springユーザ会（JSUG）は、日本におけるSpring Frameworkの情報交換を目的とし、2006年に設立されたユーザコミュニティです。
Spring Frameworkのさらなる知名度向上や普及促進のため、東京を中心に定期的な交流を行っておりますが、さらなるSpring Frameworkの認知度の向上、普及促進を図るため、本カンファレンスを開催いたします。
2018-10-31（水）09:30 - 20:30
KFCHall & Rooms
東京都墨田区横網1-6-1 KFCHall & Rooms 

（メールの文章より引用）
一般参加無料です！

会場の様子

こちらが会場エントランスです。
受付で、メールで送られていたQRコードを読み取ってもらい、参加できました。
ノベルティやチラシが入ったトートバッグを配っていたので貰いました。
ペンがとてもありがたいです

Room11（初心者向けのセッション）の部屋です。
結構な人が集まっていて、ほぼ毎回立ち見の方がいました。私も11時からのこの回は立ち見で参加しました。
立ち見でも見えやすいような文字の大きいスライドだったのでとても助かりました。
午後は人も落ち着いてきて、座って参加できました。午前中の参加者の方が多いのかな？という印象でした。（初心者向けだけかもしれないです）

ホールの様子です。講演会ってかんじで身が引き締まりました（内容は少し難しかったのでいつか分かるようになりたいです。）

各セッションにはTwitterのハッシュタグがあり、参加した人の感想やメモなどがすぐに共有されているのも面白かったです。
後で見直せるようにスライドはスライドシェアなどに共有されている方が多かったです。


参加したセッション

11:00～11:45「これからSpringを使う開発者が知っておくべきこと」 #sf_11

14:15～15:00「Spring Bootで作るRestful Web Service」#sf_13

15:30～16:15「Pivotal認定講師が解説！基礎からのOAth2.0とSpring Security5.1による実装」 #sh-h4

16:30～17:15「Spring BootでHello Worldのその先へ～」#sf_15



１．「これからSpringを使う開発者が知っておくべきこと」／土岐 孝平 氏
スライドはこちら


@Service、@Autowired ってなんだろう、なんか書いてると動くみたい、といった感覚しかなかったので、詳しく分かりやすく知ることができてすごく満足感が高かったです。

 @Service ：Springにオブジェクトを作ってもらってDIコンテナで管理された状態になる
 @Autowired　：他のBeanをインジェクションしてほしい、と宣言するアノテーション


インジェクジション→依存するオブジェクトを自分で作ったりせず、DIコンテナに渡してもらう


@Transactional　：このメソッドでDBトランザクションを起こしてほしい、と宣言するアノテーション
 「スレッドセーフ」な書き方をする
トランザクションの入れ子をしたいときはクラスを分割する
トレースログはAOPを使う、Proxyを自動で作れる（個別のクラスファイルで出力しない）


初心者向け(for beginner)で間違えやすいところをピックアップしていて、4月からの研修でなんとなく聞いたことある程度だった知識が肉付けされて、おまじない感が薄れたことが一番良かったなと思いました。
今度の新人勉強会でのアプリ開発の際にまた見返したいです。


２．「Spring Bootで作るRestful Web Service」／大野 渉 氏

環境構築からテストまで開発の流れを説明があり分かりやすかったです。
Spring Boot とは、Restfulとは何か、から始まり細かく聞くことができたのですんなり入ることができました。自分でそれぞれ調べてもボンヤリとしか分からないところが多かったのでまとめて聞くとすっきりしたのが良かったです。


Spring Boot：Spring Frameworkベースのアプリケーションを手軽に作成することができるフレームワークである。
Restful：Webアーキテクチャのひとつ　Restの法則にのっとっている
環境構築がとても楽

Dependenciesをボタンでぽちぽち選ぶだけ




Spring Bootに内蔵されている機能をフルに使っていけば簡単に色々作れるよ、というお話でした。よく覚えて活用したいなと思います。　

HTTPメソッド


GET →サーバーからの状態の読み取り
POST →サーバ上で状態の作成
PUT →サーバ上で状態の更新
DELETE →サーバ上で状態の削除


Spring MVC Test Framework：サーブレット使用しないでRest Conntrollerのテストが可能




３．「Pivotal認定講師が解説！基礎からのOAuth2.0とSpring Security5.1による実装」／多田 真敏 氏
スライドはこちら

ホールに行ってみたかったので参加しました。
内容は中級者向けとのことですごく難しかったのですが、最新の技術について少し触れることができて良かったです。


OAuth2.0 ：オーオース、認可の流れを規定したプロトコル
①リソースオーナー、②リソースサーバー、③クライアント、④認可サーバー


認可サーバーからちゃんと発行されたアクセストークンかどうかをリソースサーバーでチェックする必要があるが、特にやり方はちゃんと決まってない






４．「Spring BootでHello Worldのその先へ～ウェブDBプレスのSpringBoot特集で伝えたかったこと＆伝えきれなかったこと～」／藤野 真聡 氏
スライドはこちら
 - Web+DB PRESS vol106に掲載されていたアプリ（Qiitaをクロールして表示するもの）のコードの解説などでした
   - ミドルウェアとDBを連携したWebアプリ開発
   - MongoDBは使い勝手がいい
   - mavenコマンド　依存関係を見つつ進める
 - いまいちピンときていないので、スライドが公開されたら雑誌自体を読むのも含めてよく復習したいです。
 - デモをしていただいて、コードを見たり、実際にアプリが動いているところを見るのはとてもワクワクしました。

感想

こういったカンファレンス等に行ったことがなかったので、全員が前のめりで参加している雰囲気も含めて自分の刺激になりました。
勉強会、セミナーといった堅苦しいかんじもなく、かといってゆるい訳でもないので超初心者な私にとっては居心地よかったなと思いました。
時間が1コマ45分というのも、一気にグッと集中して聞けるのでとても助かりました。
会場ごとで階が違って、私が行きたいところは全部11階でした(beginner向けは全部ここでした)。他の場所の様子も知りたいなと思い、ホールに足を運んでみたらまた違った緊張感を感じることができました。（内容は中級者～上級者向けでしたので半分も理解できなくて残念でした。。）
知り合いの方にお会いしたりできたので、懇親会にも参加できればもっと交流できてコネクションの場にもなりそうでした。LT大会も行われると聞いたので、次の機会にはぜひ参加してみたいです。

",False,https://qiita.com//mlfmy/items/da3d3976fbae1c412e9a
"

データ型

文字


String型


文字列を扱う
「""""」で囲う



char型


1つだけの文字を扱う
「''」で囲う




整数


byte型


1byte



short型


2byte



int型


4byte



long型


8byte
数値の最後に「L」をつける




浮動小数


float型


数値の最後に「F」をつける



double型


論理値


boolean型


値はtrueかfalse





型変換(キャスト)
データの前に変換したい型を()で囲う
double d = 1234.56;
int i = (int)d;
",False,https://qiita.com//KokomamobuN/items/7355c62055c5c844d6e1
"

はじめに
質問、してますか？
エンジニア諸氏は日々分からないこと、疑問点にぶつかっていることと思います。
そういう時の対処法としては人に聞くのが手っ取り早いですよね。
場合によってはGoogle先生に聞く方が早いかもしれませんが、「担当している処理独特の関数」「設計書の提出先」なんかはGoogle先生でも分かりません。
先輩、上司、お客さんに質問する時に気を付けておくといいポイントを3つ紹介します。

前提
紹介する前に。
良い質問と悪い質問の違いってなんだか分かりますか？
私は「自分の得たいと思ってる回答に辿り着く時間」だと思ってます。
質問やその回答にかかった時間は「相手に消費させてる時間」でもあります。
ただ、よく「先輩、上司も暇ではないのでその時間を使っているという自覚を持った方がいいでしょう。」なんて言いますが正直あまり気にしなくていいです。
分からないまま進められて後で大幅な手戻りが発生するより逐一「ほうれんそう」してくれた方が結果的にリスクが少なくなるからです。
かといって全く気にしないのも問題ですのでこの記事がちょっとした助けになると幸いです。
前置き長くてすいません…
釣りタイトルとか言わないで

ポイント1. 質問内容は明確か？
これの悪い例としては(あってはならないことですが)「分からない所が分からない」というのが極致でしょう。
曖昧な質問をするな、ということですね。
自分では曖昧じゃないと思ってても相手からしたら「なぜその考えに至ったのか」も気になるところです。
なぜそういう質問・疑問が出てきたのか、そこに至るまでのロジック・意図を伝えてあげるとより明確に伝わると思います。

ポイント2. できれば Yes or No で答えが来るようにする
これ以上ないほど簡潔な答えですよね。
作業が特に止まることもなく進めることができます。
質問というより確認、お伺いを立てるといった感じです。
とはいえこの世には「はいといいえ」だけでは答えることが出来ない事も多くあります。
そういう時は選択肢、こちらで考えた案をいくつか用意してあげることがポイントです。

例「こちらで考える対応と致しましては①,②,③の内どれかになると思われますがいかがでしょうか。」

こうすれば相手は選ぶだけですし、もしも適切でない場合は「こうしてください」と要望が飛んでくるでしょうから「自分の得たいと思ってる回答に辿り着く時間」の削減につながります。

ポイント3. 自分は何が適正だと思っているか
ポイント2にも関連することですが、選択肢をいくつか出すときは自分がこうだと思っている、これが一番適正(めんどくさくない対応)だと思った択を提案することになります。
その為には担当している処理・機能についての理解や他の質問・課題と関連してないかを把握するスキルも必要になりますので初めの内は難しいと思います。
私も「これって規約に違反してるんじゃないか？」とか「これ設計書によっては表記がバラけてるけど、どっちに揃えればいいの？」といった疑問が既に他の人が出した課題によって解決されていることがままありました。
こういった「自明の理」であることをいちいち聞くのはかえって失礼となってしまうので注意が必要です。
ちょっとめんどくさい話ですがやはり礼節・マナーは大事です。

まとめ
こういうことは学校でも研修でもあまり教えてくれませんが、大事なことだと思います。
どんどん質問してコミュニケーション取る人は伸びます。たぶん。
例え明確な質問が出来なくても、自分一人で悩んでても時間の浪費をするだけですので思い切って丸投げするのも大事です。
じゃあこの話はなんだったんだ！ってことになりますが笑
ちなみに私は選択肢でいくつか提案する時に、適正であり・工数がかからず・楽に対応できて・最もめんどくさくない提案を一番最初に置いています。
ちょっとしたコツですので活用できるときは活用してみてください笑
以上です。
",False,https://qiita.com//soboro1207/items/4660dcedcf623c1ab8fb
"

はじめに
新聞や雑誌と同じく、定期的に届くメルマガは、最新であることにアイデンティティを持っていますよね。
つまり、最新号以外のメルマガのメッセージなど不要ということになります。
しかし、プライベートのメールをすべてGmailにまとめている私の場合、「1つ前のメルマガ」は既に受信トレイの遥か彼方。ムダにGoogleアカウントの容量を食われてしまいますし、何よりも受信トレイがゴチャゴチャです。
というわけで、寝ている間に勝手に削除してくれるスクリプトを書いてみました。

参考にした記事


Gmailの受信トレイ内のメールを自動削除するやり方 - Qiita
やろうとしている方向はこちらと同じ。


ワンポイント

定期的に削除したいメルマガは増減がありそうなので、Googleスプレッドシートに対象のアドレスなどを入力、参照するようにした。
重要なアドレスは対象にしないが、たまに「こんなにも自動削除しているのかー」と眺めたいので処理結果をログとして残す。


実装

script.js
function mailMagazineDeleteBatch() {
  // アドレスなどを入力してあるスプレッドシートファイルを参照
  var spreadSheet = SpreadsheetApp.openById(/*ファイルID*/);
  var sourceSheet = spreadSheet.getSheets()[0];
  // 送信元アドレスと削除しない期間を配列として取得
  var fromList = sourceSheet.getRange(2, 1,sourceSheet.getLastRow(), 2).getValues();
  var deleteCount = 0; //ログ用削除件数カウンタ
  for (var i=0; i<fromList.length; i++){//アドレスの数だけ繰り返し
    //アドレス毎に指定日前かつスターが付いていない受信トレイ内のスレッドを検索
    var deleteThreads = GmailApp.search('from:'+fromList[i][0]+' older_than:'+fromList[i][1]+'d -is:starred in:inbox');
    for (var j=0; j<deleteThreads.length; j++) {//スレッドの数だけ繰り返し
      deleteThreads[j].moveToTrash(); //1件ずつ削除
      deleteCount++; //削除カウンタ加算
    }
  }
  //　2つめのシートにログ出力
  var logSheet = spreadSheet.getSheets()[1];
  var insertRow = logSheet.getLastRow()+1;
  logSheet.getRange(insertRow, 1).setValue(new Date()); //処理完了日時
  logSheet.getRange(insertRow, 2).setValue(deleteCount); //削除件数
}



 基本的な文法はJavaScriptに従う1

 ロジックはコメントの通りですね。


用意したスプレッドシード


ログ出力結果


おわりに
初めてQiitaに投稿してみました。
記事の編集画面がサクサクと書けて幸せだったのでたまに投稿できたらなと思います。




シンタックスハイライト用に.jsとしたが実際には.gsである ↩



",False,https://qiita.com//nakajima_1995/items/cd03f04368e3d0a2b252
"プログラミング初心者がまず初めにつまづくのが引数ではないでしょうか。
「引数とはメソッドに渡す情報のことで…」なんて言われても最初はピンとこない人も多いと思います。
今回はそんな引数をわかりやすく記事にまとめてみました。
かなり色々なことを省略していますが、引数の概念がわかってもらえたらなと思います。

この記事が参考になりそうな人

「引数がわからなくて挫折しそう！」という初心者の方
「わかりそうだけどイマイチわからない」という方


引数とは
一応あらためて教科書的に説明をしておくと、引数とは
メソッドの値をメソッドの外に渡す仕組みのことです。
ん〜、やはりこの説明では？？？なので解説していきます。

そもそも、なぜ引数を使う必要があるのか？
一言でいうと、なんども同じプログラムを書く手間が減るからです。
もし引数を使わないと、同じメソッドを何度も定義したりしなくてはいけないので、手間や時間もかかりますし、コードの可読性(見易さ)が悪くなります。
この記事では難しい説明は極力省いた方がいいと思うので、今の所はひとまず
「引数を使う事でプログラミングが楽になる」
というイメージでいいんじゃないかと思います。

実際に引数を使ってみる
では、実際に引数がどのような役割で、どのように楽になるかをみていきます。
例えば以下のような足し算をするメソッド「tashizan」を作ってみます。

sample.rb
def tashizan
10 + 10
end
puts tashizan
=>20


10 + 10という計算をするメソッドを定義し、20という値を出力できました。
これで完結するプログラムであれば上記のままでも問題ないのですが、例えば同じ要領で20 + 20 , 30 + 30・・・という計算をしたい場合、また１からメソッドを作って書くのは面倒ですよね？
そこで引数の登場です。
「hikisu」という名前で引数を使ってみたいと思います。
※今回はメソッド名・引数名を理解しやすいようにローマ字表記にしていますが、通常は英語かつ、他の人がみてもわかりやすい名前にするのが一般的です。

sample.rb
def tashizan(hikisu)
hikisu + hikisu
end
puts tashizan(hikisu)


さて、メソッドと引数が書けました。
しかし、これでは数字がどこにも入っていないため計算ができませんよね？
そこで一番下に記述したputs tashizan(hikisu)の(hikisu)のなかに数値１０を入れてみます。

sample.rb
def tashizan(hikisu)
hikisu + hikisu
end
puts tashizan(10)
=>20


無事計算できてますね。先ほどと同様、20という結果が得られました。
さて、ここからが引数を使うメリットです。
別の数値を引数として渡してあげると複数の出力を得ることができます。

sample.rb
def tashizan(hikisu)
hikisu + hikisu
end
puts tashizan(10)
puts tashizan(20)
puts tashizan(30)

=>20
=>40
=>60


メソッドを新しく定義することなく、引数の数値を変えるだけで10+10,20+20,30+30の結果が出力できました。
かなり楽になったと思いませんか？もっと複雑なメソッドであればより便利さを実感できます。

少し応用してみる
前項の例では同じ数字の足し算しかできませんでした。
では、違う数字の足し算をするにはどうすればいいか？
簡単です。その場合は引数を増やせば対応できます。

sample.rb
def tashizan(hikisu, hikisu2)
hikisu + hikisu2
end
puts tashizan(10, 20)
=>30


「hikisu2」という引数を追加し、値を20にしました。
10＋20で30という結果が得られています。
このように引数が複数ある場合、左から第一引数・第二引数と数えます。

まとめ

引数とは・・・メソッドの値をメソッドの外に渡す仕組みのこと。
メリット・・・なんども同じプログラムを書く手間が減る。
複数の値を扱うには引数を増やす。

いかがでしたでしょうか？
引数を使う意味や便利さが少しはわかっていただけたでしょうか？
今回はわかりやすさを重視したので、色々な部分を省きました。
実際は今日やった部分に対して、実引数、仮引数、戻り値などという名前が付いますが、それはまた別の記事で書いていますので、興味のある方は【挫折する前に】Ruby 引数がイマイチわからない人へ 発展編 をお読みください。
",False,https://qiita.com//yossy_sa/items/e625918df8a0f2dae59a
"

はじめに
「効率的な開発手法の提案」の実践編です。
今回は、1～10の足し算の結果を出力するプログラム(Windowsのコンソールアプリーション)をCで作成します。

開発環境
「Visual Studio 2017」をダウンロードしてインストールします。
今回はコマンドラインでコンパイルを行います。
参考: チュートリアル: コマンド ラインでのネイティブ C++ プログラムのコンパイル

最低限のワンパス
まずは最低限の処理を書いたプログラムを作成します。

test.c
#include <stdio.h>
int main()
{
    printf(""test"");
    return 0;
}


Windowsメニューにある開発者コマンド プロントを開き、プログラムを保存しているフォルダに移動します。

開発者コマンド プロンプトで「cl test.c」を入力してビルドします。
C:\test>cl test.c
Microsoft(R) C/C++ Optimizing Compiler Version 19.15.26730 for x86
Copyright (C) Microsoft Corporation.  All rights reserved.

test.c
Microsoft (R) Incremental Linker Version 14.15.26730.0
Copyright (C) Microsoft Corporation.  All rights reserved.

/out:test.exe
test.obj

「test.exe」ができるので実行した結果が以下になります。
C:\test>test.exe
test

以上が最低限のワンパスで、ここまで動けば環境が原因で動かないというのを排除できます。

処理の追加
処理の追加時は逐一「ビルド＋実行」します。
まずは10回ループしてループカウンタを出力します。

test.c
#include <stdio.h>
int main()
{
    int i;
    for (i = 0; i < 10; i++) {
        printf(""%d\n"", i);
    }
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
0
1
2
3
4
5
6
7
8
9



上記を1～10の足した結果にするためにはそれぞれ+1する必要がありそうなのでループの条件を変更します。

test.c
#include <stdio.h>
int main()
{
    int i;
    for (i = 1; i <= 10; i++) {
        printf(""%d\n"", i);
    }
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
1
2
3
4
5
6
7
8
9
10



ここまでで、ループ処理は正しく動いている事が確認済みとなります。
あとはループカウンタを足して出力するようにして完成です。

test.c
#include <stdio.h>
int main()
{
    int result;
    int i;
    for (i = 1; i <= 10; i++) {
        result += i;
    }
    printf(""result=%d\n"", result);
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
result=14771018



上記はバグがあり正しい結果が出ていません。
ビルド時には以下の警告が出ています。
c:\test\test.c(7) : warning C4700: 初期化されていないローカル変数 'result' が使用されます

上記を修正した結果が以下です。

test.c
#include <stdio.h>
int main()
{
    int result = 0; /* ここを修正 */
    int i;
    for (i = 1; i <= 10; i++) {
        result += i;
    }
    printf(""result=%d\n"", result);
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
result=55



以上で完成です。

まとめ
デバッグや結果の出力処理が入るため1行単位で動かせてはいないですが、大体3行程度でビルド＋実行を繰り返しています。
これによりresultの初期化を忘れたとしてもループ処理は正しく動いていることは確認済のため、結果を蓄積するところにバグがあると切り分けやすくなっています。（今回はコンパイラが教えてくれていますが。。。）

おまけ：アンチパターン
例えば一気にプログラムを作ってしまった事を想定していくつかバグを仕込んだプログラムを用意しました。

test.c
int main()
{
    int result;
    int i;
    for (i = 0; i < 10; i++) {
        result = i;
    }
    printf(""result=%d\n"", result);
    return 0;
}



ビルドした結果
C:\test>cl test.c
Microsoft(R) C/C++ Optimizing Compiler Version 19.15.26730 for x86
Copyright (C) Microsoft Corporation.  All rights reserved.

test.c
Microsoft (R) Incremental Linker Version 14.15.26730.0
Copyright (C) Microsoft Corporation.  All rights reserved.

/out:test.exe
test.obj
test.obj : error LNK2019: 未解決の外部シンボル _printf が関数 _main で参照されました。
test.exe : fatal error LNK1120: 1 件の未解決の外部参照



ビルドした結果_printfが見つからないとエラーが出ています。
ある程度Cに慣れていれば簡単に解決できますが、初心者であれば何が問題かがわかりにくいエラーメッセージとなっています。
例えば「#include <stdio.h>」が無いと言ってくれれば分かりやすいですが、初心者であればmain()の中の処理に問題があるのかと思ってしまいそうです。
同じ状況で「最低限のワンパス」の場合は他に処理が無いので問題箇所を絞りやすいです。
上記を修正したプログラムが以下です。

test.c
#include <stdio.h> /* ここを修正 */
int main()
{
    int result;
    int i;
    for (i = 0; i < 10; i++) {
        result = i;
    }
    printf(""result=%d\n"", result);
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
result=9



実行すると意図した結果となっていません。
結果の数字が意図した結果より小さいためうまく足せていないと気づけそうです。
上記を修正したプログラムが以下です。

test.c
#include <stdio.h>
int main()
{
    int result;
    int i;
    for (i = 0; i < 10; i++) {
        result += i; /* ここを修正 */
    }
    printf(""result=%d\n"", result);
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
result=811840



今度は意図した結果よりも大きな数字が出ています。
変数が初期化されていないと気づけそうです。
上記を修正したプログラムが以下です。

test.c
#include <stdio.h>
int main()
{
    int result = 0; /* ここを修正 */
    int i;
    for (i = 0; i < 10; i++) {
        result += i;
    }
    printf(""result=%d\n"", result);
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
result=45



おしい結果が出ています。
意図した数字が足されていないと気づけそうです。
上記を修正したプログラムが以下です。

test.c
#include <stdio.h>
int main()
{
    int result = 0;
    int i;
    for (i = 1; i <= 10; i++) { /* ここを修正 */
        result += i;
    }
    printf(""result=%d\n"", result);
    return 0;
}



ビルドして実行した結果
C:\test>test.exe
result=55



以上で完成です。
簡単なプログラムなのに複雑な問題解決をしているのがわかります。
うまく問題に気づくことができればスムーズに問題解決できますが、ひとつ修正しただけでは意図した結果とならないため修正自体を疑ってしまい次の問題解決へスムーズに移れない可能性が高いです。
また複数の問題を同時に対応しない方が良いです。対応した問題が解決しているかが判断し難くなるためです。
",False,https://qiita.com//nakka_/items/a695bd3151133c090d5c
"自分がハマりがちなポイントを中心にメモ。
Getting Started on Heroku with Ruby　に記載されていない点を覚書き。
環境
Ruby: 2.5.1
Rails: 5.2.1
Heroku: 7.18.3 darwin-x64 node-v10.12.0

デプロイの手順

本番環境のDBをPostgreSQLに変更する　（←忘れない！）
heroku login
heroku create
git add .
git commit -m ""Deploy to heroku!""
git push heroku master

heroku run rails db:migrate　（← これ忘れがち！）


ハマりがちなポイント２点

本番環境のデータベースをPostgreSQLに変更していない
デプロイ後にDBをマイグレーションしていない


本番環境のデータベースをPostgreSQLに変更していない

RailsのデフォルトDBはsqlite3なのでHerokuにデプロイする前にPostgreSQLに変更する


DBをPostgreSQLに変更する手順

Gemfileにてsqlite3をdevelopement, testブロックの中に移動させる
pgをproductionブロックの中に追記する


Gemfile
group :development, :test do
  gem 'sqlite3'
end

group :production do
  gem 'pg'
end

...省略...


3 . bundle installを実行して変更完了！

デプロイ後にDBをマイグレーションしていない


git push heroku masterでデプロイした後、heroku run rails db:migrateでマイグレーションする！

",False,https://qiita.com//t10kiyotaka/items/2735ea88ca28181f43ae
"

目的

プログラミング初心者向けにC/C++を勉強する方法を書籍を中心に説明
まだ中身がすっからかんなので、もう少し充実させる予定


方法

具体的な目的となる開発対象を設定すべき（超重要）


ユニークじゃなくていい、写真ビューワーとかテキストエディタとかでもいい
何か作る前提で、参考書をざっと読む、ただしなるべく手を動かしてコードを入力する
手を動かさずに参考書を読んだ場合、


さぁなんか作るか、ん、最初はどうやるんだっけ？で結局身についていないことに気づく
良書においても、完全な解説というのは存在しなく、何らかの内容を省略している、実際に作業しながら省略されたところを補間しながら理解していないと、どこかで内容がわからなくなる


手を動かしたり、悩みながら、同じ内容を何回も入力して、結局はある程度時間をかけないと身につかないということ


手順：


１．まずはC++を全体的に説明している本を読む、その次にポインタ解説が詳しい本を読む 
２．このタイミングで、自分で作りたいアプリを書き始める、わからないところはwebや読んだ参考書で調べて、なるべくメモを残しておく
３．ある程度アプリが完成したところで、中級者向けの本を読む、読みながらリファクタリングしていく




参考書籍

初心者向け


C++全体把握用


http://amzn.asia/d/7aJBOQD


ポインタ勉強用


http://amzn.asia/d/6uz1uD1




中級者向け


Effective本（C++のベストプラクティス集）


http://amzn.asia/d/0ZiQuJK
http://amzn.asia/d/1RxfogC
http://amzn.asia/d/dTjY3lB


作法


http://amzn.asia/d/gwFDcey


オブジェクト志向


http://amzn.asia/d/00rcanh


ソフトウェアエンジニアの心得


http://amzn.asia/d/6F1VokD
http://amzn.asia/d/fwljFFy
http://amzn.asia/d/39SingB




上級者


高速化


http://amzn.asia/d/6D6x0D2


開発技法


http://amzn.asia/d/dNSkwoo





",False,https://qiita.com//imasaaki/items/9803029225ef17137d4f
"

1.起きたこと
php artisan db:seed

を実行するとClass ArticlesTableSeeder does not existが発生する。

２.原因
PHPのオートロードされるクラスが定義されてないのでクラスが見つからない、違う以前の問題だった。

3.解決策


composer dump-autoloadを実行する。


結果：composer: command not found　→ composerがないので入れる。



composerをいれる。下記をターミナルに貼り付ける。



Composer successfully installed... が出ていたら(๑>◡<๑)b
ちゃんと入ってるか確認したい場合はcomposer -v




php -r ""copy('https://getcomposer.org/installer', 'composer-setup.php');""
php -r ""if (hash_file('SHA384', 'composer-setup.php') === '93b54496392c062774670ac18b134c3b3a95e5a5e5c8f1a9f115f203b75bf9a129d5daa8ba6a13e2cc8a1da0806388a8') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;""
php composer-setup.php
php -r ""unlink('composer-setup.php');""



composer dump-autoloadを実行する。



Package manifest generated successfully.が出ていたら(๑>◡<๑)b



php artisan db:seedを実行する。



Database seeding completed successfully.が出ていたら(๑>◡<๑)b



php artisan tinkerのあとApp\Article::all()->toArray();で確認する。
おわりっ！


4.参考

[PHP] laravelでseederを利用
composer

※コード挟むとリストが1に戻る現象の回避方法教えてください
",False,https://qiita.com//mayobimu/items/60249fe463aef08e4995
"　　
　　
　　
この記事は
『プログラミング完全未経験からUnityでの開発現場に迎え入れてもらえた世界一の幸せ者』
の記事です。そのつもりでお読みください。
　　
　　

Unity(C#)で別のスクリプトのメソッドを簡単に呼び出す方法
まずヒエラルキー上を確認しましょう。
呼び出したいメソッドがAdd Componentされた状態のオブジェクトがありますか？
あれば準備完了です。
ヒエラルキー上に無い場合はスクリプトをAdd ComponentしたらOKです。
　　
　　
　　

メソッドを呼び出す方法
呼ぶ出される側のクラスの名前は適当に""YobidashitaiClass""ってことにしましょう。
呼び出したいメソッドの名前は適当に""YobidashitaiMethod""ってことにしましょう。
FindObjectOfType<YobidashitaiClass>().YobidashitaiMethod();

　　
　　
めちゃくちゃ簡単ですね。
ただし、処理速度とか難しい話は知りません。
",False,https://qiita.com//OKsaiyowa/items/99379662422cbafd9ca7
"

概要
Vue.jsを初めて触る人に向けたサンプルと解説です。
React.jsと比較する形で書いています。

Vue.jsとは
原作者であるEvan You氏が中心となって開発中の、Javascriptフレームワークです。
日本語の公式サイトがあるため、初心者にもかなり分かりやすいです。
https://jp.vuejs.org/

環境構築
vue-cliというコマンドラインツールが主流です。
https://cli.vuejs.org/

単一ファイルコンポーネント
VueはReactと同じく、パーツ単位でHTMLをまとめるコンポーネント指向ですが、.vue拡張子の単一ファイルコンポーネントによって、その「パーツを構成するために必要な HTML・CSS・JSをひとまとめに」して独立した1つのファイルで扱うことができます。
それぞれ<template>、<script>、<style>が担当します。

<template>
  <div class=""say-hello"">Hello!!<div>
</template>

<script>
  export default {
    name: 'Hello'
  }
</script>

<style scoped>
  .say-hello {
    font-weight: bold;
  }
</style>


コンポーネントオプション

name
コンポーネント名を表現します。

components
他のコンポーネントをimportして使いたい場合はここに記述しておく必要があります。

props
親コンポーネントから渡されたpropsを使いたい場合はここに記述しておく必要があります。
なお、propsの型をここで指定することができます。書き方は以下を参照してください。
プロパティのバリデーション

data
Reactでいうstateです。以下のように定義することで、template内で{{ }}で囲うことで出力できるようになります。


data: function () {
    return {
      msg: 'Hello World!'
    }
  }


<p>{{ msg }}</p>


methods
Reactでいうハンドラです。以下のように定義します。

methods: {
    greet: function (event) {
      alert(event.target.tagName)
    }
  }

次にgreetメソッドにclickイベントリスナをバインドします。
これによってボタンが押されたときにBUTTONというアラートが出ます。

<button v-on:click=""greet"">Greet</button>


インラインメソッドハンドラ
メソッド名を直接指定する代わりに、インラインJavaScript式でメソッドを指定することもできます。

methods: {
    say: function (msg) {
      alert(msg)
    }
  }


<button v-on:click=""say('hi')"">Say hi</button>


computed
算出プロパティといいます。返り値が必須です。通常のプロパティと同様に、template内でデータバインドすることができます。
またある算出プロパティに依存されているプロパティが変更されると、この算出プロパティに依存する全てのバインディングを更新します。
算出プロパティの代わりに、同じような関数をメソッドとして定義することも可能です。最終的には、2つのアプローチは完全に同じ結果になります。
算出プロパティの場合

computed: {
    reversedMessage: function () {
      return this.msg.split('').reverse().join('')
    }
  }


<p>{{ reversedMessage }}</p>

メソッドの場合

methods: {
  reverseMessage: function () {
    return this.msg.split('').reverse().join('')
  }
}


<p>{{ reversedMessage() }}</p>

違いは、「算出プロパティは依存関係にもとづきキャッシュされる」ということです。
算出プロパティは、それが依存するものが更新されたときにだけ再評価されるのに対し、メソッド呼び出しは、再描画が起きると常に関数を実行します。
キャッシングする必要の有無によって使い分けてください。

ディレクティブ
Reactではjsxファイル内で普通にjavascriptで条件分岐やループ処理を書きますが、Vueにはディレクティブという特殊な書き方があり、よりシンプルに書けます。

v-bind
プロパティを渡します。

<input type=""text"" v-bind:value=""msg"" />

なお、以下のように省略して記述できます。

 <input type=""text"" :value=""msg"" />


v-on
イベントを受け取ります。

<input type=""text"" v-on:input=""msg"" />

なお、以下のように省略して記述できます。

<input type=""text"" @input=""msg = $event.target.value"" />


v-model
Reactは「データ」→「画面（HTML）」という方向でしかデータが流れない、一方向データバインディングですが、
Vueでは双方向データバインディングを書くことができます。
データと表示をバインドします。

 <p>{{msg}}</p>
 <input type=""text"" v-model=""msg"" />

これは以下と同じ意味です。

<p>{{msg}}</p>
<input type=""text"" :value=""msg"" @input=""msg = $event.target.value"" />

一方向にしたい場合は:か@の片方だけにすればいいです。

v-if
条件分岐です。
下記はtodoのeditingプロパティがtrueなら入力フォームを、falseならテキストを表示する例です。

<input v-if=""todo.editing"">
<span v-else>{{ todo.text }}</span>


v-for
ループ処理です。
keyにはユニークな値をセットする必要があります。ただし、:keyというように独特の記述の仕方があるので注意してください。

<div v-for=""item in greetings"" :key=""item.id"">{{ item.text }}</div>


data: function () {
      return {
        greetings: [
          {
            id: 1,
            text: 'おはよう！'
          },
          {
            id: 2,
            text: 'こんにちは！'
          },
        ]
      }
    }


親コンポーネントから子コンポーネントにpropを渡す
静的な値（文字列など）は以下の書き方で親コンポーネントから子コンポーネントにpropとして渡せます。
<blog-post title=""My journey with Vue""></blog-post>

動的な値を渡す場合は変数の値をバインディングする必要があります。
なお、静的な値を以下の書き方で渡すこともできます。
<blog-post :title=""post.title""></blog-post>


ライフサイクル

created
インスタンスが作成された後に呼ばれます。

mounted
インスタンスがマウントされた後に呼ばれます。
ReactでいうcomponentDidMount()のようなものです。

参考記事

Vue.jsをはじめるにあたって押さえておくべきこと
Vue.js概要？
ReactにするかVue.jsにするか、jQueryだけ触っていたエンジニアがサンプル作って比較してみた
Vue.js を vue-cli を使ってシンプルにはじめてみる

",False,https://qiita.com//gcsungwoo/items/15941c1ae0762fb566e6
"

FizzBuzz問題とは
プログラマなら大抵のかたは聞いたことがあるかと思いますが元は単純なパーティゲームで
– 2人以上のプレイヤーが1から順番に数字を発言していく
– 3で割り切れるときは「Fizz」を発言
– 5で割り切れるときは「Buzz」を発言
– 両方で割り切れるときは「FizzBuzz」を発言
– 間違えた人から脱落
細かな違いはあるでしょうが大体上記のようなルールのゲームです。

FizzBuzzとは？
このFizzBuzzをプログラムとして画面に出力するコードを書くことが
FizzBuzzとはプログラマがプログラムを書くことができるか調べるクイズのようなものです。
当然ながらどのプログラム言語を選んでも解くことはできるのでプログラムの勉強を始めた方は一度解いてみてください。

正解発表

Python.py
#fizzbazz問題
cnt = 1 #カウンタ変数

while cnt < 101:
    if cnt % 3 == 0 and cnt % 5 == 0:
        print('FizzBuzz')
    elif cnt % 3 == 0:
        print('Fizz')
    elif cnt % 5 == 0:
        print('Buzz')
    else:
        print(cnt)
    cnt = cnt + 1 #自分のような初心者はcntのインクリメントを忘れて
                  #無限ループに突入してしまいがちなので注意！


",False,https://qiita.com//kk31108424/items/b554c01672d990493c86
"※最初に投稿背景を載せてます。興味の無い方は次項から読み進めて下さい。
本記事の投稿主ですが、これまでインプット一辺倒であまりアウトプットしてこなかったため
その練習も兼ねてと、Qiitaに記事を投稿させて頂く事になりました。
これまでのQiitaとの関わり方は、ネットで情報収集してる際に検索ヒットする
何やら緑色の背景で黒い部分にコマンドが載ってるデザインのページだなという印象でした。
(まさか自身が投稿する側になるとは何処で縁があるか分からないものです。)
この度利用を始めさせて頂くにあたり、意外にもユーザ登録方法について簡単に書かれた記事が
投稿されていないように見えたので「まだコードは書けないけどこれなら・・・！」
との思いで書いたのが今回の記事になります。

はじめに
ここではQiitaに興味を持ち、使い始めるにはどうすれば良いか？ 
との疑問を解消するための説明を載せています。 
まず、Qiitaは記事を読むだけであればユーザの登録等は不要でそのまま利用出来ます。
しかし、ユーザ登録すると興味のある新着投稿記事を自動で纏めて表示する 
タグフィードや、気に入った記事を纏めて見返す際に役立つストック
投稿された記事へのコメントや自身でも記事の投稿が可能と、個人利用の範囲だけで無く
他ユーザとの交流、アウトプット面での機能も拡充されますので、気軽に試して頂ければと思います。 
本記事が読んで頂いた方の助けになれば幸いです。

ユーザ登録のやり方

ユーザ登録
まずは、ユーザ登録に進みます。
QiitaのTopページ右上のユーザ登録ボタンを押下して下さい。 

するとユーザ登録画面が表示されます。 

Qittaで利用するアカウント登録には以下4種類の方法がありますので 
お好みの方法で、画面の指示に従い登録を進めていきましょう。

GitHubアカウントを連携して登録
Twitterアカウントを連携して登録
Googleアカウントを連携して登録
Qiita用に新しいアカウントを作成して登録(※要メールアドレス)


用語解説(GitHub)
GitHubとはソフトウェア開発プロジェクト向けにWeb上で利用出来るソースコード管理サービスを指します。 
ソースコードを介し、日々開発者同士での交流がワールドワイドに行われていますので、こちらにも興味があればアカウントを作成しQiitaと連携して登録しておくと、自身で作成したソースコードの公開にも役立ちます。
GitHub公式

タグをフォロー
ここでは自分の興味がある分野(ワード)についてタグという形で登録(フォロー)します。 
Qiitaでは投稿主が記事を投稿する際に最低でも1つのタグが付けられるため
ここでタグをフォローする事でフォローしたタグが付いた記事=自分が興味を持ちそうな 
記事を探すのに役立ちます。 
また、フォローしたタグが付けられた記事は纏めてタグフィードから閲覧出来ます。　
タグは後からでも登録出来るので、フォローしたいタグが無い場合は無視して進んでも問題ありません。
タグの登録が終わったらQiitaの利用を開始するを選択してください。


最後に
ここまで読んだ方ならQiitaを利用し始めてみたものの、使い方はどうすれば？ 
との疑問も沸いてくるかと思います。 
Qiitaの使い方としてはザックリと分けて、記事の閲覧・投稿の2つになりますが 
そちらについてはQiitaさんのサポートページに載っていますので参照頂けますと幸いです。
Qiitaの使い方
",False,https://qiita.com//pukupukuaaa/items/33edfae2deb8b376f35f
"プログラミング教室ITWORKSのayakaです。
あまりにメジャーになりすぎて「いまさら聞けないぞ…」という自分のために簡単な構成図を交えて、まとめてみたいと思います

Git = ソースコードを管理するシステム
Gitのできること＝ソースコードを管理する事です。
整理整頓が苦手な人が個人開発し修正をしまくっていると、どのソースコードがいつ時点の物なのか？さらにはどれがリリース版なのか？わからなくなってきすこれは大問題ですよね。。しかしGitのシステムコマンドを使えばコマンドを打った時点のソースコードをファイルの改定順に保管できます。(バージョン管理)

他にもある!
補足ですが他にもソースコードを管理するシステムは存在します。

CVS
Subversion


違いは""分散型""と""集中型""
ざっくり図のような違いがあります。

分散型(Git)
各々ローカルにリポジトリを持ち、そこへコミットできます。まずはローカルへコミットするため、開発者がソースコードを改定する毎にコミットできます。

集中型(CVS/Subversion)



GitとGitを利用するサービス
GitとGitを使用したサービスを混同すると設置時に混乱します
各サービスは個人の用途により選択しましょう。


Git
commitやpush等のコマンドが利用できるソースコード管理システム
GitHub
Gitを扱うためのWebUIが用意されたオンラインサービス
Gitblit
Gitを扱うためのWebUIが用意されたローカル用サービス

",False,https://qiita.com//ayaka_Itworks/items/d8c1b6736278c7d38a98
"

Vimとは？
viから派生した高機能なテキストエディタです。
viはLinuxに標準搭載されているため、使ったことがある人は多いと思います。
大体同じような操作感ですが、viと違うところもあります。

vimの特徴

ストレスの少ないマルチプラットフォーム
とにかく軽くて速い
基本的にキーボードだけで操作が可能


インストール方法

Mac/Ubuntu


MacとUbuntuには最初からインストールされています。


Windows 



http://www.kaoriya.net/software/vim/ からダウンロードできます。


ローカル開発環境



sudo yum install -y vim インストールが完了すれば、 vim コマンドを利用できるようになります。




vimのモードについて



key
内容




ノーマルモード
カーソル移動やテキストの削除、コピー、ペーストなどの簡単な指示を行う。


ビジュアルモード
テキストを選択するだけのモード。


挿入モード
実際にテキストを入力するモード。


コマンドラインモード
ファイルを開いたり、検索・置換などの様々な指示を行う。




モード切り替え



key
内容




i
挿入モードへ


o
新しい行を追加し挿入モードへ


R
上書きモードへ


v
ビジュアルモードへ


V (Shift + v)
行選択のビジュアルモード


Ctrl + v
矩形選択のビジュアルモードへ


ESC
コマンドモードに戻る


ctrl + [
コマンドモードに戻る


ctrl + z
vim を一時停止する


ciw
カーソル上の単語を削除してインサートモードへ




Vimの保存と終了



key
内容




:w [ファイル名]
指定されたファイル名で保存する。ファイル名を省略した場合には、現在のファイルに上書きする


:w! [ファイル名]
書き込みを強行する


ZZ
Vimを終了する。ファイルが変更されている場合には、変更を保存する


:wq
ファイルを保存してVimを終了する


:q
Vimを終了する。ファイルに変更がある場合には警告が標示される


:q!
Vimを強制的に終了する。ファイルに変更がある場合は、変更分は無視




カーソル位置を移動



key
内容




h(または←)
左に1文字分移動する


j(または↓)
下に1文字分移動する


k(または↑)
上に1文字分移動する


l(または→)
右に1文字分移動する


Ctrl + d
半画面分下に移動する


Ctrl + u
半画面分上に移動する


gg
ファイルの先頭へ移動する


G
ファイルの末尾へ移動する


O
行頭へ移動する


$
行末へ移動する


w
単語単位で右へ移動する


b
単語単位で左へ移動する


Ctrl + f
1画面分下に移動する


Ctrl + b
1画面分上に移動する


zz
カーソルが画面中央になるようにスクロール


Ctrl + o
古いカーソル位置に戻る。 (Old)


Ctrl + i
新しいカーソル位置に進む。




コピー&ペースト&削除



key
内容




yy
今いる行をコピー (yank)


p
カーソルの場所に、ペースト


yy5p
現在の行をコピーし、下に5行追加する


gv
直前の選択範囲を再選択


x
カーソルの位置にある文字を削除する。キーボードの[Del]と同じ挙動です


X
カーソルの位置の左の文字を削除する。キーボードの[BS]と同じ挙動です


dd
カーソルのある行を1行削除する


D
カーソルのある位置から行末まで削除する




検索と置換



key
内容




*
カーソル下の単語を検索


#
カーソル下の単語を検索 (上方向に検索)


:%s/hage/hoge/g
単語の置換(hageをhogeへ置換)。% はファイル全体を表す。




編集



key
内容




.
直前の変更を繰り返す


u
Undo


Ctrl + r
Redo




特殊文字



key
内容




Ctrl-v return
改行文字の入力


Ctrl-v tab
Tab 文字の入力




ウィンドウ



key
内容




:vsplit
画面を左右に分割する


:sp filename
新しいバッファと分割ウィンドウでファイルを開く


:e filename
今いるWindowにファイルを開く (Edit)


:e test*.cc
ワイルドカードを指定してファイルを開く


Ctrl +w h
左のWindowへ移動


Ctrl + w l
右のWindowへ移動




バッファ



key
内容




:buffers
編集中のバッファ一覧を表示


:bn
次のバッファに移動


:bd[elete]
バッファを削除




単語補完



key
内容




Ctrl + p
単語補完(前方検索)


Ctrl + n
単語補完(後方検索)




マクロ



key
内容




q<letter>
マクロの記録開始


q
マクロの記録停止


@<letter>
マクロの実行




レジスタ



key
機能




:reg
レジスタの一覧表示


Ctrl + r ""
ヤンクした文字列をペースト




コマンドの意味



key
意味




g
繰り返し


c
1回毎に確認




その他



key
内容




:!command
外部コマンドの実行


:
範囲選択中に : を入力すると、選択領域の範囲指定( ’<,’> )が自動で挿入された状態でコマンドモードに入る。これを置換に利用すると便利。 (例: :'<,'>s/old/new/g)


Ctrl + a
数値のインクリメント


Ctrl + x
数値のデクリメント



Interactive Vim tutorial
Learning VIM while playing a game
",False,https://qiita.com//kyawphyonaing/items/b31378ccde7957bc541d
"

Opening
今回はバージョン管理ツールについて
個人で開発をしている分にはあったら嬉しい程度のツールですが、複数人で開発をする場合は必須（のはず…いまだに共有ドライブで開発しているのは自殺行為だと思います）のツールなので、コマンドを覚えるだけでなく、どういう運用をすべきか考えられるレベルを目指しましょう。
バージョン管理システムは色々ありますが、当記事ではGitとSVNについて説明します。

Appendices
そのうち書き足していくつもりです。

【新人向け】これだけは知っておいて欲しい基礎知識
【新人向け】これだけは知っておいて欲しい基礎知識　Linux編
【新人向け】これだけは知っておいて欲しい基礎知識　Shell編
【新人向け】これだけは知っておいて欲しい基礎知識　DB編
【新人向け】これだけは知っておいて欲しい基礎知識　アプリケーション編
【新人向け】これだけは知っておいて欲しい基礎知識　プログラミング編
【新人向け】これだけは知っておいて欲しい基礎知識　Agile編


Body

バージョン管理システムとは
まずは、バージョン管理システムの概要について

バージョン管理システムとは？管理方式の違いと履歴を管理する利点 - 発注ラウンジ


バージョン管理は、特定のファイルに対して加えられていく変更を保存し、後からその当時の状態に戻れるように変更履歴を残します。ここでいう変更履歴とは5W1Hのような情報を指していて、「いつ」「誰が」「どのように」ファイルを変更したかを記録していくことが大切です。特にシステム開発など、複数人が同一のファイルを頻繁に更新するような状況で、ファイルをバージョン管理する必要がある場合に活用されます。
対象となるファイルの種類に制限はなく、ソースコードに限らずさまざまなフォーマットのドキュメントや画像・動画ファイル、音楽ファイルなども管理できます。変更履歴を逐一記録しておけば、内容を過去の状態に戻すこともできるようになり、ファイルの扱いやすさが飛躍的に向上します。


Git
続いて、現在デファクトスタンダードなバージョン管理システムであるGitについて

Gitによるソフトウェア開発をはじめよう - OSDN



Gitとは？
Gitは分散型のソースコード管理システムです。ソースコードに対して加えられた変更点を記録し、任意の時点のコードを復元するバージョン管理機能や、ほかの開発者がソースコードに加えた変更を自分のソースコードに取り込むマージ機能、ほかの開発者が公開しているソースコードを自分の作業環境にコピーするクローン機能といった、ソフトウェア開発において有用な多数の機能を備えています。

細かな使い方については、公式ドキュメントを読むのが一番だと思います。
（他にも良記事はいっぱいありますが、細部が間違っていて苦労した経験があるので…）
バージョン管理の用語についてもリンクを載せたかったのですが、だいたいセットで間違った内容がくっついていたので、公式ドキュメントを読みながら勉強しましょう。

Pro Git


ホスティングサービス
Gitではホスティングサービスを利用してリモートリポジトリを建てるのが一般的なので、どんなサービスがあるのか押さえておきましょう。
個人的には、エンタープライズだとGitLabが使いやすいんじゃないかなぁと思っています。
ただ、GitHubほどネット上に良記事は転がっていないので、マージリクエストなどの運用方法について記事を書きたいところ。

Gitホスティングサービス5選【比較表付き】 - Find Job!



GitHub
ソーシャルコーディングの草分け的存在となった、プロジェクトホスティング＋SNSのサービスです。
法人利用（Organization plans）は個人でプライベートリポジトリを持つよりも高いですが、ビジネス利用に最適化されている（公式サイト）とのこと。
セキュリティ上の理由でGitHubにソースコードが置けない場合には、社内ネットワーク上において使うGitHub Enterpriseという選択肢もあります。

GitLab
GitLabはGitHubのクローンプロジェクト。MITライセンスで公開されており、無料で使えます。
GitHubクローンとしてかなり忠実に作られているため、GitHubに慣れたユーザでも違和感なく使うことができます。
ユーザごとにサーバを用意してインストールするのが特徴で、GitHub Enterpriseほどの費用がかからないのはうれしいところ（メンテナンスコストは別途見積もりましょう）


ブランチモデル
Gitにはブランチモデルが複数あります。
「必ずどれかに従ってブランチを切らなければいけない」というわけではありませんが、各々のプロジェクトに適したブランチの切り方を考えるためにも、以下の３つのプラクティスについて勉強しておきましょう。
（普段「GitLab flowのような何か」を採用しているのですが、使っているうちにGit-flowとの違いがわからなくなってきてしまったので、もう少し勉強して記事にまとめておきたいと思います。）

GitLab flowから学ぶワークフローの実践 - POSTD



Git-flow
Git-flowはGitブランチを活用するために最初に提案されたフローの1つで、大変注目されました。masterブランチとは別にdevelopブランチがあり、その他にfeature、release、hotfixというブランチがあります。developブランチでの開発作業を進めた後に、releaseブランチを作成し、成果は最終的にmasterブランチへマージされます。Git-flowはよくできた標準モデルですが、複雑なために2つの問題があります。1つは開発者がmasterブランチではなく、developブランチを利用しなくてはいけないことです。2つ目の問題は、hotfixブランチとreleaseブランチがもたらす複雑さです。

GitHub flow
Git-flowをよりシンプルにしたのがGitHub flowです。GitHub flowではfeatureブランチとmasterブランチしか使いません。シンプルでわかりやすいフローで、多くの開発チームが採用し成功を収めています。Atlassianブログでも類似した戦略が勧められていますが、それはfeatureブランチをリベースするやり方です。全てをmasterブランチにマージし、こまめにデプロイすることで、デプロイ待ちのコードの量を最小限にできます。これは無駄のない継続的デリバリのベストプラクティスにも則っています。しかしGitHub flowにも、デプロイ、環境、リリース、インテグレーションについて問題があります。

GitLab flow
GitHub flowでは、featureブランチをマージするたびに本番環境へデプロイ出来ることを想定します。しかしそれはSaaSアプリケーションでは可能ですが、大抵の場合はできません。リリース時間をコントロールできない例の1つはiOSアプリケーションで、AppStoreのバリデーションをパスする必要があります。また別の例としては、デプロイの時間枠（平日午前10時～午後4時、オペレーションチームがフル稼働の場合）があるのに、その時間外にコードをマージする場合です。そういう場合には、デプロイしたコードを反映させたproductionブランチを作成します。masterブランチからproductionブランチにマージして新しいバージョンをデプロイできます。本番環境のコードを知るためにはcheckoutでproductionブランチに切り替えればいいわけです。


SVN
お次は、Subversionについて
Subversion自体は嫌いではないですが、Subversionに固執するメンバーに嫌な思いをさせられているので記事のチョイスはてきとーです。
「ブランチ」や「タグ」、「チェックアウト」など、用語の意味や動作がGitと異なるものがあるので注意が必要です。

Apache Subversion - Wikipedia


歴史的には広く使われているバージョン管理システムの一つにCVSがあった。CVSにはディレクトリの移動の管理やネットワーク対応の点、不可分な更新などの点で難があった。これらCVSの問題点を解決すべく開発されたのがSubversionである。
Subversionは集中型（クライアント・サーバ型）であるが、その後、GitやMercurialやBazaarなどの分散型のバージョン管理システムが登場するようになった。例えば、Linuxカーネルの管理にはGit、Mozilla Firefoxの管理にはMercurial、MySQLの管理にはBazaarが使われている。


ブランチモデル
正確にはブランチモデルではありませんが、Gitのブランチモデルに対応するような、SVNのフォルダ構成の考え方について
SVNのクライアントツールは下記のフォルダ構成を前提にしていることが多いので、特別な理由がない限りは標準のフォルダ構成にしておきましょう。
また、branch-es、tag-sとsが付くように、それぞれのフォルダはbranch、tagをグルーピングしたフォルダになります。
branchesの下でbranchを分けず、それより下の階層でbranchを分けるのは標準から外れるのでやめましょう。
（ツールによってはブランチ作成やマージにひと手間掛かるようになるので辛さが増します。）

Subversion の「trunk」「branches」「tags」の使い方 - tracpath



trunk
trunk（トランク）は、開発の中心（メインライン）となるディレクトリで、ほとんどのファイルをここに格納します。通常は開発中の最新バージョンを格納しておきます。Gitでいう、masterブランチのようなものですね。後述するbranchesのひとつと考えることもできます。最低限トランクさえあれば開発を進めていくことができますので、初心者の方はまずここからはじめてみましょう。

branches
branches（ブランチ）は、トランクから分岐した別のツリーです。ブランチ（枝）という名前は、トランク（幹）から枝分かれすることからきています。トランクとは別に開発を進めたいときに、トランクからブランチを作ります。ブランチを作るということは、トランクディレクトリをブランチとして、「branches」ディレクトリ内にコピーすることです。別々のディレクトリとして存在しますので、もちろん履歴も別々のリビジョンを持つことになります。

tags
tags（タグ）は、ある特定時点のトランクに名前をつけたものです。Subversionのリポジトリは履歴を保持しているため、いつでも過去のファイルを自由に取り出すことができます。しかしリビジョン番号では、取り出したい履歴がいくつのリビジョンなのかすぐにはわかりません。そこでタグとして名前をつけておくことで、必要なときにタグを参照してすぐに取り出せるようにします。タグの実態は、tagsディレクトリ内にコピーされたトランクディレクトリです。通常、一度作成したタグを変更することはありません。デフォルトでは、ファイルは読み取り専用になっていませんので、誤って変更してしまわないように注意しましょう。


Git vs. SVN
「結局GitとSVNってどっちが良いの？」という質問に対しては、基本的にGitを採用しておけば間違いないと思っています。
集中管理方式と分散管理方式の違いや学習コストなどで比較されることが多いと思いますが、そもそも特別な理由がない限りはデファクトスタンダードな技術・ツールを採用する方が辛さは少ないと思っています。
SVNであれば特定フォルダだけローカルに落として作業することが簡単にできたりと、SVNなりの良さはあります。が、Qiita内の記事を読んでいれば分かるように、環境構築の話ではGitを採用することを前提としている記事が多かったり、不具合やエラーの対応方法が簡単に検索に引っかかったりと、利用しているユーザーが多い技術・ツールほど良質なドキュメントが多く存在します。（その分間違った内容の記事を目にする機会も多いですが）
参考：
Git 特定のフォルダのみcloneする - Qiita
以下、蛇足ですがGitとSVNでよく比較される内容について個人的な意見を書いていきます。

学習コスト
GitはSVNより学習コストが掛かると説明している記事は多いですが、個人的な意見としてはむしろ今からSVNを学ぶ方が学習コストは高いと思っています。
Gitと比較してSVNの方が学習コストが掛からないと言われるのは、SVNでは複雑な運用をすることが少ないという文化に起因していると思っています。SVNでチケット毎にブランチを作成して、コードレビューを設けてマージをし、CIまで回そうと思うと参考になるドキュメントを探すのも一苦労です。
既にSVNを利用しており事情があってGitに移行できない等でもない限り、Gitを学べば良いと思っています。

ローカルコミット
SVNと比較してGitの便利な点としてよく挙げられているローカルコミットですが、個人的には手間でしかないと思っています。
開発途中のソースをコミットできるという利点は、ブランチを細かく作成する運用をしていればSVNでも同じことができます。また、複数人で開発している場合、（当たり前ですが）ローカルコミットはローカルリポジトリにしか反映されません。開発メンバーが事故や病気で急に数週間不在になったり、端末が故障したりした場合、ローカルコミットの内容を拾うのは難しくなります。チームでバージョン管理システムを使うのであれば、リモートリポジトリに反映させなければ意味がありません。基本的には、ブランチを細かく作成し、日々リモートに反映させるのが理想だと思います。ただ、そうした運用をするとコミットログが荒れてしまうので、rebaseで後からコミットログを分かり易く編集できるGitの方が優れていると思います。

Closing
以上です。
",False,https://qiita.com//tomPlain/items/540e73e5fb6cb2dcb1f6
"Qiitaに自分も何か記事を投稿してみようと思い、これまでの学習の話を記事にすることにした。

自分の障がいとプログラミングを学ぶきっかけ
　私は、生まれつき脳性麻痺という障がいを煩っている。脳に障がいがあるために、手足と言語が不自由ある。実際にパソコンも鉛筆を加工した棒を一本握ってキーボードをたたいている。もちろん人よりも入力は遅い。言語に関しても自分の発する言葉はほとんど聞き取れにくく、基本筆談で用を済ませている。
　そんな自分が、ある雇用型の障がい者施設で働いていたところにそこの代表から｢もっとスキルアップしなさい｣と強く勧められ、その代表の知り合いの方が経営してるあるIT企業がプログラミング教育にも力を入れていることを教えていただき、プログラミングの学習が始まった。
　最初は、そこの会社が自分の働いてるところよりも１時間くらい電車で行くところでしたので、できるだけ自宅で学習し、分からないところは最初はご担当の方にSNSのメッセージでやりとりをしながら学習を進めていった。月に１度か２度はそこの会社を訪問することで、進捗や疑問点などをFaceToFaceで見ていただき、今後のアドバイスをいただく形を取っていた。

プログラミング学習のために時間を作る
　当時私は、朝８時から事務の仕事をしていて家に戻れるのは、夕方から夜だった。土日も他の用事もこなす必要が生じたので学習時間を確保することには、苦労したと思う。でも心に決めていたこととして｢一日一行以上codeを打つ｣という目標を掲げて、１時間から２時間、時には３０分もできないときもあったけど、毎日学習することを目標に取り組んでいた。

学習内容は下記の通り
・技術書や教本を読む
　これは通勤の時間や昼休みも活用していた
・ドットインストール
　映像自体のテンポは、早いものの繰り返し見られるところが自分には合っていたと思う
・実際にアプリを作る

プログラミング学習で役立ったこと
　上記に書いたような学習方法を取り組んでいく一方で、早い段階で、プログラミング学習でお世話になっている会社の方々が中心となって行われていた｢コミュニティー｣に参加できたことは大きな助けとなった。そのコミュニティーが主催する勉強会に毎回のように参加させて頂いたり、中でも一番の助けになったのは、、｢Slack｣に入れて頂いたことだと思う。言語に不自由がある私にとってSlackは大きな助けになった。口頭で伝えられない分、Slackにガンガン書き、codeを貼って教えてもらう日々を今でも続いている。このおかげで私の学習は、だいぶスムーズになったと思う。

Slackとは・・・
　IT企業にお勤めの方なら、よくご存知だと思いますが、チャットアプリの一つです。

時には、自分を追い込む
　プログラミング学習の最初の頃は、CoronaSDKと言うプログラミング学習には、ちょうどよい言語で色々勉強したり、作っていたりしたが、それに慣れてからjavaを使ったAndroid開発へ移行した際に、自分一人で学習していくのに限界を感じ、一時期、毎週のように往復２時間を掛けてお世話になった会社に通っていた時期がありました。確かに体力的にきつかったですが｢人生の勝負｣だと思って頑張ってみました。

自分が障がいを抱えている故に
　日本人が、今まで来た環境として、多くの方が障がい者になれていないのが現状です。その中において自分が理解していること、信頼してもらえるようになるために、また時には頑張っていることを示すために行ったことをまとめてみました。

学習の週報
　この学習プロジェクトが始まった際に、新着を週報でお世話になっている会社のご担当者の方と週報で確認し合うことにしました。内容としては淡々とした内容でしたが｢必ず送る｣事を心掛けました。学習の新着が鈍くてもできなかった理由をきちんと書くようにしました。片手で収まるくらいはお休みをいただいたこともありますが、基本毎週お送りしました。

イベントに積極的に参加する
　お世話になっている会社やコミュニティが、何らかのイベントや勉強会を開く際は、できる限り行けるよう自分の予定を調整しました。当時は、自分の年次休暇はそのためにあるんだ位の勢いで、予定を調整し、イベントや勉強会に参加させて頂きました。当時私は、土曜が仕事だったときもありまして、その時は朝に自分の業務を最低の範囲ですませて、往復２時間電車に揺られて、イベントに行き、帰りも自分の業務に支障が出ていないか電話等で確認しつつ、家路に戻ることもよくありました。

感謝を忘れない
　当然のことですが、どんな時も感謝する気持ちをもってこの学習を行いました。私は、特殊学級というクラスを出たもので、守られてはいたものの勉強は限られていた部分もあり、こうして専門的な事柄を学べることは大きな喜びでした。だからこそ、どんなことでもきちんと感謝しました。

学習を約２年ほど行って

できた事柄
　この期間中にAndroidのアプリを２つリリースすることができた。これらに加えて、プログラミングに必要な調査する方法も少し身につけることができたと思います。まだまだ分からなくて、教えてもらうこともありますが、プログラミングで詰まったときに自己解決が、少しですができるようになりました。

ステップアップ
　こうした事柄を約２年ほど行いました。私もさらにステップアップしたいと考え、プログラミング学習でお世話になっている会社の代表に就職相談をさせて頂きました。「どこか紹介して頂けないでしょうか」この問いに代表は｢うちに来たら｣と仰ってくださいました。
現在私は、プログラミング学習でお世話になった会社で働いています。
業務内容は、学習していたときよりと異なる部分もありますが、私はこの学習によってプログラミングの基礎を固めることができました。よく友人から違う業務内容で大丈夫なの？？と聞かれることもありますが、基礎ができていればどんなプログラム言語も勉強し、扱っていくことはわりとできると思います。と言いつつもまだまだ勉強の身です。最高の環境が与えられたからこそ、さらに自分をパワーアップしていきたいです。

最後に
ここに自分の例を書きましたが、偶然の良さが重なった部分が多いことは、認めざるを得ません。しかし、努力しようと励むときにその道が開けてくると私は思います。「私には無理。」と決めつける方によくお目にかかります。特に障がいを抱えておられると、自分では何もできないように感じるときもあります。私もそういう気持ちになるときもありました。しかし大切なのは努力を辞めないいことです。これからプログラミングの学習を始める人に向けてこの記事を書きました。社会人であれば色々大変だと思います。
でもぜひ、一歩一歩学習を進ませてください。
",False,https://qiita.com//yows1031/items/6d1ecfa6e98c35c41f45
"

はじめに
社会に出始めた若い世代の人達が今後IT業界で活躍するために意識することをアウトプットします。また、学生の方やIT以外の分野の方にも読んでいただきたい記事です。

本文
ある程度、キャリアを積むと必然的に使ってる技術が陳腐化してくるときがくると思います。一度、身につけたスキルでずっと飯を食べれるほど甘い世界ではなくなってきています。
身につけたスキルが陳腐化してしまったときにやることがあります。それは、新しい技術をキャッチアップすることです。いわゆる学び直しをいうものをしなければなりません。
ここで学び直しの期間のために、若い世代の人達が意識するべきことが３つあると私は考えています。
１つ目は、学び方を学ぶことがあげられます。社会人になると必然的に自分の時間というものが少なくなります。そのときに必要なスキルとして、短期で集中して学び、やり方を掴むことだと思います。
２つ目は、根幹となる本質的な知識を身につけることがあげられます。
いつの時代も流行の技術があると思います。流行の技術を追うことも必要な場合もありますが、特に若い世代に必要なのは、腰を据えて根幹となる本質的な部分を身につけることだと考えます。
３つ目は、エンジニアとして成長したいなら人として成長することが大事だということです。人として成長すると、自分の技術レベルがあまり高くなくても、周りの人が技術を教えてくれます。反対に技術レベルが高くても、人として未熟なままだと誰にも相手にしてもらえなくなります。

おわりに
上記の３つは、当たり前だと思う人もいると思いますが、この当たり前のことが当たり前以上にできる人材が活躍し続けることが出来る人材だと思います。また、時間や体力が比較的ある学生や20代のうちに意識すると、かなり有利に働くと思います。
皆さんの参考になれば幸いです。
",False,https://qiita.com//ifuriito09/items/3473c37fbee2d7b6dd48
"webシステムを開発していく上で、フロントエンドやバックエンドなどのアプリケーションの実装に対する知見が必要なことは当然ですが、それと同様に知っておくべきことの中にデータベースの設計があります。
データベースはビジネスにおいて重要なデータを保存（永続化）する役割をもつので、webシステムの根幹を司る要素であり非常に重要です。
今回はそんなデータベースを設計する際のテクニックの一つである正規化についてまとめました。
あまり厳密な説明ではないかもしれませんが、雰囲気が伝われば幸いです。

正規化とは
wiki大先生によると

関係データベース (リレーショナル・データベース) において、正規形と呼ばれる形式に関係（リレーション）を準拠させることにより、データの一貫性の維持と効率的なデータアクセスを可能にする関係設計を導くための方法である。

また、

正規形には様々なものが存在するが、いずれにせよ、正規化を行うことにより、データの冗長性と不整合が起きる機会を減らすことができる。

とのことです。
つまり正規化とは、データの異常や不整合、冗長性の排除を実現するための手法です。

正規化の目的
素早く正規形を見抜く実践テクニックによると、

1事実1カ所（1 fact in 1 place）を目指して、テーブルの整合性を保ったまま、テーブルの冗長性を排除して、データを効率的に管理できるようにすること

だそうです。
つまり、管理しやすいテーブル設計にすることを目的としています。
余談ですが、正規化に失敗したDB（およびそれを支えるwebシステム）は、冗長なカラムやテーブルが散財しており、データの生合成が取れていないことが多々あります。
どのテーブルのどの値が現実の事項を正しく反映しているか分からないのに、それを土台にしてアプリケーションを構築するのは非常に苦痛なものです（実際はアプリケーション側も信頼できない可能性が高いですが...）。
また、その上で機能拡張や保存するデータの種類が増えると悪夢は絶えません。。
というように、webシステムに蓄積していく大切なデータを、効率的に管理するために正規化を行います。

正規化のステップ
正規化をしていくには主に6つのステップを経ていきます。
ただ、実際の業務では主に第1正規形~第3正規形までが行われますので、今回は第3正規形までについてご紹介します。
- 第1正規形（1NF）
- 第2正規形（2NF）
- 第3正規形（3NF）
- ボイスコッド正規形（BCNF）
- 第4正規形（4NF）
- 第5正規形（5NF／PJNF）

今回は下記のような八百屋さんの商品管理台帳(といっても厳密な台帳ではありませんが..)を例に、正規化を行なっていきます。


第1正規形
第1正規形を満たすためには、「リレーションであること」が求められます。
リレーションとは主に下記を満たすことです。
1. 行が上下で順序づけされていない
  - 行の位置に依存したテーブルであってはならない
2. 列が左右で順序づけされていない
  - カラムの位置に依存したテーブルであってはならない
3. 重複する行は存在しない
4. カラムの値は、そのカラムを構成するドメインに属するデータを一つ持っている
  - 繰り返しグループがあってはならない
5. 全ての列の値は定義されたものだけであり、かつそれぞれの業において常に存在する

この中でもよく例に挙げられる事項としては、繰り返しグループがないことです。
上に挙げた商品台帳の場合、一つの行の中に（といってもセルを結合させていますが..）複数の商品情報が詰め込まれています。
第1正規形を満たすためには下記のように、複数レコードにデータを分けます。
また、在庫数は入出荷の数から計算できるので削除してしまいます（今回は簡単に計算できると仮定）。
既存のデータを元に導出（計算）できるものも削除するのがミソです。
これによりひとまず、同じ行の中での繰り返しグループは無くなりました。

商品台帳



仕入先
住所
代表者
代表者連絡先
商品名
単価
入荷数
入荷日




山田農園
鹿児島県
山田太郎
080-1111-2222
さつまいも
150
20
2018/10/15


山田農園
鹿児島県
山田太郎
080-1111-2222
ごぼう
200
20
2018/10/15


磯部農園
北海道
磯部五郎
090-1111-2222
玉ねぎ
100
50
2018/10/01


磯部農園
北海道
磯部五郎
090-1111-2222
かぼちゃ
130
10
2018/10/01




第2正規形
第1正規形を経て行内の重複は取り除けましたが、まだデータを管理するには不便な事項があります。
例えば、代表者の連絡先が変更になった場合には、テーブル内で該当する全ての連絡先を変更する必要があります。
また、新しい契約先農家が増えたとしても、全ての情報が揃うまではテーブルに記載できません。
これらの原因は、一つのテーブルに色々な要素を詰め込みすぎていることによります。
プログラムで言うところの、単一責任ではない、といったニュアンスです。
商品台帳としてレコードを一意に決めるためには、[仕入先、商品名、入荷日]あたりがあれば十分です。
そのためそれ以外の要素については、それぞれの要素ごとに別のテーブルに分けます。

入荷情報



仕入先
商品名
入荷数
入荷日




山田農園
さつまいも
20
2018/10/15


山田農園
ごぼう
20
2018/10/15


磯部農園
玉ねぎ
50
2018/10/01


磯部農園
かぼちゃ
10
2018/10/01




仕入先情報



仕入先
住所
代表者
代表者連絡先




山田農園
鹿児島県
山田太郎
080-1111-2222


磯部農園
北海道
磯部五郎
090-1111-2222




商品情報



商品名
単価




さつまいも
150


ごぼう
200


玉ねぎ
100


かぼちゃ
130



このように、分解したリレーションの情報を使って元のリレーションを再構築できることを無損失分解と言います。

第3正規形
以上により、テーブルの冗長性は随分と減りましたが、まだ改善の余地はあります。
例えば、山田農園を営んでいる山田太郎さんが他にも農園を持っていた場合です。

仕入先情報



仕入先
住所
代表者
代表者連絡先




山田農園
鹿児島県
山田太郎
080-1111-2222


山田ほのぼのファーム
鹿児島県
山田太郎
080-1111-2222


磯部農園
北海道
磯部五郎
090-1111-2222



この場合、山田さんの連絡先が変わってしまうと仕入先情報を管理しているテーブルで複数のカラムに対して更新をする必要が出てきます。
この場合、代表者をキーとして、代表者の情報を管理するためのテーブルを別途作ることにします。

代表者情報



代表者
連絡先




山田太郎
080-1111-2222


磯部五郎
090-1111-2222




入荷情報



仕入先
商品名
入荷数
入荷日




山田農園
さつまいも
20
2018/10/15


山田農園
ごぼう
20
2018/10/15


磯部農園
玉ねぎ
50
2018/10/01


磯部農園
かぼちゃ
10
2018/10/01




仕入先情報



仕入先
住所
代表者




山田農園
鹿児島県
山田太郎


磯部農園
北海道
磯部五郎




商品情報



商品名
単価




さつまいも
150


ごぼう
200


玉ねぎ
100


かぼちゃ
130




まとめ

リレーショナルデータベースは今やどんなwebシステムにも利用されており、かつサービスを展開する上で重要な情報を管理する役割を持っているので、それに対する知見というのもアプリケーション側の知見と同様に重要である。
リレーショナルデータベースを設計する上で鍵となる手法の一つに正規化があり、だいたい第1~第3正規形までをやるのが一般的。
正規化の目的は、データから冗長性を減らして管理しやすいようにすることを目的としている。


参考

理論から学ぶデータベース実践入門 ~リレーショナルモデルによる効率的なSQL (WEB+DB PRESS plus)
OSS-DB 第4回　データベースの正規化
素早く正規形を見抜く実践テクニック
第1章 SQL概説
SQLアンチパターン

",False,https://qiita.com//yu-croco/items/69db34cc0f3b1f20976b
"

はじめに
GoFのデザインパターンを紹介している『増補改訂版 Java言語で学ぶデザインパターン入門』を読んで、学んだ内容についてまとめます。

Prototypeパターン

Prototypeとは
日本語に訳すと「原型」を意味します。
この原型となるインスタンスを用いて、他のクラスをコピー(複製)し、新しいインスタンスを作成するパターンのことをPrototypeパターンと言います。
通常はインスタンスを生成する際にはnewを用いますが、Prototypeパターンではnewではなく、全てのクラスのスーパークラスであるjava.lang.Objectクラスで定義されているcloneメソッドを用いてインスタンスの作成を行います。

登場人物
Prototypeパターン使用するのは以下のクラス図に登場するクラスです。


抽象クラス


Prototype
createCloneメソッドを定義するクラスです。
実装はサブクラスのConcreatePrototypeクラスで行います。


実装クラス

ConcreatePrototype
スーパークラスのPrototypeクラスで定義されたcreateCloneメソッドの実装を行います。
Client
createCloneメソッドを使用するクラスです。


具体例
具体例として、以下のクラスをもとに説明します。


インタフェース


Productインタフェース



Product.java
package framework;

import java.lang.Cloneable;

// 1. java.lang.Cloneableを継承
public interface Product extends Cloneable {
    public abstract void display(String s);

//2. createCloneメソッドを宣言
    public abstract Product createClone();
}


ポイントは以下の2点です。
1.cloneメソッドでコピーできるようにjava.lang.Cloneableインタフェースを継承している。
2.createCloneメソッドを宣言している。
補足の説明を行います。
cloneメソッドは全てのクラスのスーパークラスであるjava.lang.Objectクラスで定義されています。
このcloneメソッドを実行する場合、コピー元のクラスはjava.lang.Cloneableインタフェースを実装している必要があります。
java.lang.Cloneableインタフェースを実装していない場合、CloneNotSupportedExceptionの例外が発生します。
また、createCloneメソッドに関してはcloneメソッドを使用するためのメソッドですが、実装はサブクラスのSurroundクラスとUnderLineクラスで行います。

実装クラス

Clientクラス


Client.java
package framework;

import java.util.*;

public class Client {
    private HashMap<String, Product> stringName = new HashMap<>();

    // 1. HashMapに登録
    public void register(String name, Product pro) {
        stringName.put(name, pro);
    }

    // 2. createCloneメソッドを使用
    public Product create(String proname) {
        Product p = (Product) stringName.get(proname);
        return p.createClone();
    }
}


Clientクラスはコピーのメソッドを使用するためのクラスです。
ポイントは以下の2点です。
1.registerメソッドでHashMapにProductインタフェースを実装したインスタンスを登録している。
2.createメソッド内でcreateClone()メソッドを使用している。


Surroundクラス



Surround.java
import framework.*;

public class Surround implements Product {
    private char srchar;

    public Surround(char srchar) {
        this.srchar = srchar;
    }

    // 1. displayメソッドの実装
    @Override
    public void display(String s) {
        int length = s.getBytes().length;
        for (int i = 0; i < length + 4; i++) {
            System.out.print(srchar);
        }
        System.out.println("""");
        System.out.println(srchar + "" "" + s + "" "" + srchar);
        for (int i = 0; i < length + 4; i++) {
            System.out.print(srchar);
        }
        System.out.println(""\n"");
    }

    // 2. createCloneメソッドの実装
    @Override
    public Product createClone() {
        Product p = null;
        try {
            p = (Product) clone();
        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
        return p;
    }
}


Surroundクラスはコピーのメソッドの実装を行うためのクラスです。
ポイントは以下の2点です。
1.コンストラクタで受け取った文字で、displayメソッドの呼び出し時に受け取った文字列を囲んで表示するdisplayメソッドの実装を行っている。
2.createCloneメソッドの実装を行っている。
2.に関して補足の説明を行います。ポイントは2つです。
1つ目はtry-catch文で囲まれている点です。
cloneメソッドはProductインタフェースでも説明したようにjava.lang.Cloneableインタフェースを実装している必要があります。
実装していない場合、CloneNotSupportedExceptionの例外が発生するので、例外が発生した場合のことを考慮してtry-catch文で囲んでいます。
2つ目はcreateCloneメソッド内でcloneメソッドを呼び出している点です。
cloneメソッドを呼び出せることができるのは自分のクラス(またはサブクラス)からしか呼び出すことができません。
そのため、Clientクラスからcloneメソッドを呼び出すためにはcreateCloneメソッドのような別のメソッドを一旦経由して呼び出しを行う必要があります。


Underlineクラス



Underline.java
import framework.*;

public class Underline implements Product {
    private char ulchar;

    public Underline(char ulchar) {
        this.ulchar = ulchar;
    }

    // 1. displayメソッドの実装
    @Override
    public void display(String s) {
        int length = s.getBytes().length;
        System.out.println(""\"""" + s + ""\"""");
        System.out.print("" "");
        for (int i = 0; i < length; i++) {
            System.out.print(ulchar);
        }
        System.out.println(""\n"");
    }

    // 2. createCloneメソッドの実装
    @Override
    public Product createClone() {
        Product p = null;
        try {
            p = (Product) clone();
        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
        return p;
    }
}


UnderlineクラスはSurroundクラスと同様にコピーのメソッドの実装を行うためのクラスです。
ポイントは以下の2点です。
1.コンストラクタで受け取った文字で、displayメソッドの呼び出し時に受け取った文字列の下にアンダーラインのように表示するdisplayメソッドの実装を行っている。
2.createCloneメソッドの実装を行っている。
補足の説明はSurroundクラスと同様のため、割愛します。

実行クラス


Mainクラス



Main.java
import framework.*;

public class Main {
    public static void main(String[] args) {
        // 準備
        Client manager = new Client();
        Underline ul1 = new Underline('-');
        Underline ul2 = new Underline('~');
        Surround sr1 = new Surround('#');
        Surround sr2 = new Surround('@');
        manager.register(""underLine text1"", ul1);
        manager.register(""underLine text2"", ul2);
        manager.register(""surround text1"", sr1);
        manager.register(""surround text2"", sr2);

        // 生成
        Product p1 = manager.create(""underLine text1"");
        p1.display(""Hello, world."");
        Product p2 = manager.create(""underLine text2"");
        p2.display(""Hello, world."");
        Product p3 = manager.create(""surround text1"");
        p3.display(""Hello, world."");
        Product p4 = manager.create(""surround text2"");
        p4.display(""Hello, world."");
    }
}


準備においてregisterメソッドでUnderlineとSurroundインスタンスの登録を行っています。
生成においてcreateメソッドを呼び出してインスタンスのコピーを作成し、displayメソッドを呼び出しています。

実行結果
Main.javaを実行した結果は以下になります。
それぞれ与えられた文字で下線表示、または囲んで表示ができていることが確認できます。

実行結果
""Hello, world.""
 -------------

""Hello, world.""
 ~~~~~~~~~~~~~　

#################
# Hello, world. #
#################

@@@@@@@@@@@@@@@@@
@ Hello, world. @
@@@@@@@@@@@@@@@@@


※Markdownで~だけだとコードとして認識されないようで全角スペースを追記しています。
エスケープできる方法があれば教えていただければ幸いです...
※追記　2018/11/8
links_2_3_4さんに末尾に全角スペースを追記する方法を教えていただきました。
ありがとうございます。

メリット
以下の3点があります。
1.類似のクラスのインスタンスが複数存在する際に、クラスを分ける必要がなく管理が容易である。
2.クラスからのインスタンス生成が難しい場合に、インスタンスの作成が容易にできる。
3.フレームワークと生成するインスタンスを切り分けることができる。

まとめ
原型となるインスタンスを用いて、他のクラスをコピーし、新しいインスタンスを作成するPrototypeパターンに関して学びました。
以下でサンプルコードをアップしていますのでよろしければ参考にどうぞ。

Prototypeサンプルコード

また、他のデザインパターンに関しては以下でまとめていますので、こちらも参考にどうぞ。

[随時更新]Javaでデザインパターンまとめ


参考文献

増補改訂版 Java言語で学ぶデザインパターン入門

",False,https://qiita.com//mk777/items/d745f3b22eac12989819
"

はじめに

自己紹介
初めまして、はなまと申します。
本業(?)は京都大学理学部で化学を勉強しています。
プログラミング初心者ながらアウトプットしていこうと思っております。拙いところがあれば是非ご指摘よろしくお願いします。
Qiitaでの初投稿として、最近始めたswiftの入門書の書評を書いていきたいと思います。

動機と本の紹介
今回swiftの勉強をしようと思ったきっかけは大学の友人が一緒にお酒についてのアプリを作らないかという声をかけてきたからです。
お酒もプログラミングも好きだったのでいいきっかけとしてswiftの勉強をすることにしました。
さて、swiftをどうやって勉強しようかとAppleの公式サイトを見ているとApple自身で学習ツールを提供してくれているということが書いてありました。そしてまず手始めにそこで見つけた『Swiftによるアプリケーション開発:入門編』を使って勉強を始めてみようと決意しました。
(上のリンクからiBookで無料で入手出来るので初めてみたい方は是非！)

書評

環境
macOS Mojave ver 10.14
Xcode ver 10.1
Swift 4
Swiftによるアプリケーション開発:入門編 p.5からダウンロードしたXcode9用のプロジェクトファイル
(2018/11/4現在　Xcode10用のプロジェクトファイルは更新されていませんが、画面レイアウトが少し異なる点以外はXcode9用でも十分学習できました。)
を使って学習しました。

目次と構成

Playgroundの基本
命名と識別子
文字列(Strings)
Hello, world!
最初のアプリケーション
関数(Functions)
BoogieBot
定数と変数(Constants and Variables)
型(Types)
パラメータと結果(Parameters and Results)
意思決定を行う
インスタンス、メソッド、プロパティ
QuestionBot
配列とループ(Arrays and Loops)
構造体を定義する(Defining Structures)
QuestionBot 2
アクションとアウトレット
アダプティブユーザーインターフェイス
列挙型とswitch文(Enumerations and Switch)
最終プロジェクト
アプリケーションデザイン
次のステップ
その他のリソース

以上の構成になっており、実際の作業はダウンロードしたプロジェクトファイルを使ってPlayground上で実際に手を動かしながら学べます。
iBookにはイントロダクションとまとめ、次のステップへのヒント、理解度チェックというそのレッスンの確認テストが掲載されています。
各理解度チェックは選択式のものでそのレッスンに出てきた新しい言葉の意味やコードのテンプレートの正誤を判定するもので手軽に出来る様に工夫されています。
練習問題の解答などについては友人のこちらのブログにまとまっているのでぜひ活用すると良いと思います。

感想
プログラミングに初めて触れる人でもレッスンを順にこなしていくことで無理なくスキルを身につけることができる様になっていると感じました。後半のレッスンでは少し長いコードを書くことも要求されますが、事前にしっかり解説や準備があるのでコーディングに対する苦手意識も生まれにくいと思います。
中でも僕が特筆したいと思うのはレッスン2の変数の命名とレッスン10の関数の命名です。
僕が初めてプログラミングに触れたのは約1年前でしたが今までコードを書いてきた上で使ってきた変数名や関数名は可読性や保守性の面であまりよくないものだということを実感させられました。

例
#Bad Naming
hA:Double //図形Aの高さ
vB:Double //図形Bの体積
func greeting(people: String) //挨拶文を出力する関数

#Good Naming
heightOfA:Double //図形Aの高さ
volumeOfB:Double //図形Bの体積
func printGreeting(to name: String) //挨拶文を出力する関数



恥ずかしながら1年前から書いてきたコードたちには上の様な名前が散見されていました^^;
上の例でもわかる様に下の変数は連結して一つのものを明確に表しているし、関数は文の形になっていて関数内部での引数の扱いも明示できています。Xcodeでコードを書くとオートコンプリート機能が働いて名前の続きは自動的に補填されるので多少長くてもわかりやすい命名をすることが大切だと感じました。(あまりにも冗長なものはもちろん逆効果ですが、、、)
この他にも後々重要になってくることや他の言語で開発するときにも重宝する様な情報も散りばめられていました。

作ってみた
学習のまとめとしてこの参考書を使って学んだことを使ってその日の状況に応じてオススメの夕食を提案するアプリを作ってみました。ソースコードはこちらです。
練習として構造体の定義や様々なアウトレットの配置をおこなってみました。
シンプルながら下の様なレイアウトのアプリの作成ができました。


まとめとこれから
親切にswiftに入門できる良書であり、これからアプリを作ってみたいという方にもお勧めできると思います。
以後は発展編である『App Development with Swift』(英語版のみ)を使って本格的なアプリ作成法を勉強したり、他のリソースを探してアプリケーションデザインの勉強をしたりしてアプリらしいアプリを作成できる様になりたいと思います。
駄文ながら閲覧していただきありがとうございました。
",False,https://qiita.com//hanama_chem/items/6e2a6f81bd5e8aca5b4e
"新卒としてweb企業に就職しそろそろ若手ではなくなってきたので、
自分への戒めも込めて書きたいと思います。
(エンジニアリングの話は、ほとんど出てこないです。)

信頼貯金を意識する
信頼が仕事をするうえで一番大事だと思います。
信頼がない状態で自分の意見を言っても聞いてもらえないですし、
信頼がない状態で仕事を任せてもらえるようにはならないと思います。
信頼はすぐに得られるものではなく、
日々の行いを通して少しずつ得られるものであり、
貯金のようなものだと思っています。
悲しいことに信頼は貯めるのは難しく時間がかかりますが、
無くなるときは、一瞬で無くなります。
自分の信頼貯金がいくらほどあるかをちゃんと把握して、
信頼貯金に応じた行動をすることが大切だと思います。
ただ信頼を得ることだけに意識を向けて人の顔を見て仕事をしてしまうと、
社内政治が始まるので、それは避けるべきだと思います。

オープンな存在になる
仕事は一人では行えないので他の人と関わりますが、
自分がどういう存在で何を考えているか・何に重きを置いているかなどを、
発信してオープンな存在にすることが大事だと思います。
オープンな存在の方が周りが自分を理解する部分が多くなるため、
自分のキャリアプランニングに沿った経験をさせてもらうこともできますし、
自分が働きやすい環境になると思います。
また教えてもらう立場の時には教える方も、
何がわからいのか・どこで詰まっているのかを知りたいので、
自分は何がわからないのか・何を教えてほしいかを明確に示すことも大事です。
あと、人は忙しいときやミスとしたときはクローズな状態になりがちですが、
そういった状態でもオープンな状態を維持して、
一人で抱え込まないことを意識すべきだと思います。

相手を尊敬、尊重する
エンジニアはプロダクトの開発に責任を負っているプロフェッショナルですが、
企画やデザイナーは同じプロダクトの別の部分の責任を負っているプロフェッショナルです。
一つのプロダクトにおいて別の責任を負った人がいるので、
必ず意見が食い違う時がありますが、否定的にとらえてはいけないと思います。
しっかりとなぜ相手はその意見になるか・相手は何を考えているかを考え、
相手の意見を尊重した上でシステムに落とし込むべきです。
またエンジニア同士でも、お互いの得意分野や設計に対する思想が違うので、
相手のコードや設計を否定せずに、どのアプローチがプロダクトに一番合っているかを考慮し、
コードレビューなどを行うべきだと思います。

自分の論を持つ
若手は教えてもらうことが多いのであまり意識しないかもですが、
何事にも自分の論を持つことが大事です。
自分の論がない状態で他の方に質問や意見を求めて回答が返ってきて、
それをそのまま取り入れても何も自分の成長には繋がらないと思います。
自分ではできない仕事でもいったん自分で出せる考えを出したうえで、
他の方に自分の意見をぶつけてみることが大事だと思います。
違っていたらそのズレから自分がどれだけ足りない部分があるかを、
知ることができるかと思います。
また自分の論を作るときにはしっかりと自信を持つことが大事です。
自信がない付け焼刃の論を持つぐらいなら、持たない方がましだと思います。

コストを意識する
コストといっているのは、時間的なコストの話です。
社会人は時間に対してお金をもらっているのでできるだけ無駄な時間を減らすことが
コスト面では大事だと思っています。
コミュニケーション部分でのコストは他人と自分の時間のコストがかかるので、
コーディングといった自分だけの時間のコストよりも意識する必要があると思います。
例えば他の人に質問をするときには事前に分からない内容をまとめておくことや、
会議に議題を持ち込むときには何を話すかを明確にしておく、などです。
仕事で話すときにちゃんと会話のゴールを決めて、
無駄な時間を使う手探りの会話を避けることが大事だと思います。
ただ仕事外の雑談は大いにすべきだと思います。

振り返りをちゃんとする
定期的に自分の振り返りを行うことが大事です。
普段の業務に対する振り返りもちろんですが、
自分のキャリアの棚押しも振り返りに含めて行うべきだと思います。
振り返りで意識すべきことは振り返りをしたうえで、
ちゃんと次の行動を明確にすることです。
振り返りをただ漠然として、
次に行う行動を決めないと意味がないので注意です。
またチーム内での振り返りもチーム力の強化になるのですべきだと思います。

チームを意識する
チームとして働くうえでは自分の仕事はもちろんですが、
チーム全体の問題点を自ら考え改善していく動きを自主的に行うことが、
チームにとっては大事だと思います。
チームに貢献しようとすると今までの自分視点ではなく、
チーム全体視点で物事を見る必要があるのですが、
これがすごく大事だと思います。
いかに自己組織化ができるようになるかが、チーム力につながるかと思います。

できるだけプロダクトの話をする
エンジニアは案件を遂行するのが仕事ですが、それだけがすべてではないと思います。
案件の裏にはKPIであったりサイトとしての目指している姿があるわけで、
それが単にシステムタスクとして落とし込まれたのが、案件です。
なので案件を行うときには最低でも一回は企画の方などと、
なぜ案件を行うのか・この案件終了後どうしていきたいのかといった、
プロダクト大枠にかかわる部分をちゃんと話すべきだと思います。
そうすることでシステムとしてのモチベーションも上がりますし、
何より話すことでより仕様書には書かれていないような企画の意図などを知ることができ、
より効率的なアプローチなどを考えることができるようになり、
案件をより良いものにすることができます。

若手のメリットを生かす
あまり意識していない人が多い気がしますが、
若手ってすごく貴重な時間だと思います。
メンターの方に質問をしたら親身になって教えてくれるし、
少しミスをしてもチームがカバーしてくれるし、
手を上げたら大体のことをやらせてもらえるし、
何よりミスをしたら、ちゃんと怒ってもらえることができます。
中途やベテランになるとそうはいかなくて一人の社会人になるので、
教えてもらう機会や注意される機会も少なくなって、
シビアに評価されてしまいます。
なので若手というメリットを最大限使って、
恐れずにできるだけ手を挙げていろいろ経験するのが本当に大事だと思います。

まとめ
エンジニアはエンジニアリングスキルも大事ですが、
エンジニアリングスキル以外にもいろいろ大事な部分があるかと思います。
(もちろん一定のスキルレベルはマストだと思いますが。)
コメントやツッコミいただけると嬉しいです。
",False,https://qiita.com//kojikaa/items/23867c6caa10821af7c5
"

【jQueryとは？】
jQuery：JavaScriptのライブラリでJavaScriptで行える全てのことをJavaScriptよりも簡単にできる。
→簡単に言うと、「JavaScriptで書かれた様々なお役立ち機能を、後に再利用しやすい形でまとめあげたパッケージ」
※JavaScript：HTML/CSSを操作するための言語
＜前提（処理のタイミング）＞
JavaScriptはHTMLのhead要素内に入っているので、書き換えたいbody要素を読み込む前に処理が終了している状態になっている。だから下記のような記述をして、HTML要素が全て読み込み終わったタイミングで処理を実行するように記述する。リセットCSS並に必須事項。

sample.js
$(function( ) {

});



【jQueryの基本ルール】
jQueryの基本ルールは極めてシンプルで2つだけ。
①操作したいHTML要素を取得
②取得したHTMLに操作（命令）を付与
→操作（命令）が起こるキッカケを設定してあげないといない。これを「イベント」と呼ぶ。
＜イベント＞
→HTMLの要素に対して行われた処理要求。「ユーザーがブラウザ上のボタンをクリックした」「テキストフィールドでキー入力をした」「要素の上にマウスカーソルを乗せた」などいずれもイベント。分かりやすくいうと、Railsで言うリクエストのイメージ。
ーーーーーーーーーーーーーーーーー
①操作したいHTML要素を取得
＜取得方法＞
■基本形
$(""セレクタ"")
■IDセレクタ
$(""#Selector"")
■クラスセレクタ
$("".Selector"")
■要素セレクタ
$(""h1"")
②取得したHTMLに操作（命令）を付与
＜操作（命令）方法＞
$(""セレクタ"").jQueryの命令 (  ) ;
$(""セレクタ"").jQueryの命令 (引数) ;

【メソッド】
■val( )メソッド
→フォームに入力された値を取得したいときに使う。
実際に使うときは、取得した値を変数に代入して使う。
＜例＞

sample.js
var input = $(""#keyword"").val( );
/*keywordというidセレクタに入力されたフォームの値を取得し、input変数に代入する。/*


■text ( )メソッド
→テキストを書き換えたいときに使う

sample.js
$(""#title"").text (""変更したタイトル名"") ;


■html ( )メソッド
→HTML要素のテキストをHTMLを含むテキストに書き換えたいときに使う

sample.js
$(""p"").html(""<strong>変更されたコンテンツ</strong>"");
/*これはHTML要素のテキストをstrongタグで強調された（太字）の「変更されたコンテンツ」というHTMLに書き換えている/*



★on( )メソッド
→どんなイベントが起きたら、何が実行されるかを設定したいときに使う

sample.js
$(function() {
    $(""セレクタ"").on(""イベント名"",function( ) {
        /*イベントが起きたときに行う処理/*
    });
});


これを基本ルールに則って解釈すると、
①操作したいHTML要素を取得
→$( "" セレクタ "" )
②取得したHTMLに操作（命令）を付与 ＆ 操作・命令が起きるキッカケ（イベント）を設定
→.on( "" イベント名 "", function( ) {
＜イベント名一覧＞
その他イベント名については、下記URLをご参照ください
https://javascript.programmer-reference.com/jquery-list-event/
■append ( )メソッド
→引数で指定したHTML要素を追加したいときに使う

sample.js
$ (""#lists"").append('<li class = ""list"">追加されたリスト</li>') ;


■remove ( )メソッド
→指定したHTML要素を削除したいときに使う

sample.js
$ ("".list"").remove( ) ;
/*今回は上記のappendメソッドで追加したlistクラスのセレクタ（"".list""）を指定して、削除しています/*


■attr( )メソッド
→HTML要素の属性値を取得・変更したいときに使う。

sample.js
$(""セレクタ"").attr(""取得したい属性名"") ;
$(""セレクタ"").attr(""変更したい属性名"", ""変更したい属性値 "")



sample.js
$(""changeButton"").on(""click"",function( ) {
    $(""img"").attr(""src"", ""http:// ~~~ "") ;
});


＜コードの説明＞
「changeButton」要素を「click」したときに、「img」要素の「src」属性の属性値を「http:// ~~~」に変更する。
※要素タグは、画像を表示する際に使用し、src属性は記述の際には必須の属性で、src属性の属性値を記述することで画像ファイルのURLを指定する。つまり上記の記述は実際のブラウザ上では、既に指定されていた「src」属性の属性値が「http:// ~~~」という別の属性値に変更されるので、表示される画像が「http:// ~~~」の画像に変化するという操作（命令）を付与したことになる。
→HTML5タグリファレンス　http://www.htmq.com/html5/img.shtml
■addClass ( )メソッド
→指定した要素に引数で設定したクラスを追加したいときに使う

sample.js
$(""p"").addClass(""red"");


■removeClass( )メソッド
→指定した要素に引数で設定したクラスを削除したいときに使う

sample.js
$(""p"").removeClass(""red"");


＜その他のメソッド＞
■hide ( )メソッド
→要素を非表示にしたいときに使う
■show ( )メソッド
→非表示の要素を表示にしたいときに使う
■fadeOut ( )メソッド
→指定したHTML要素をフェードアウトさせたいときに使う。
引数にはフェードアウトにかかる時間を記入。1000＝1秒
■fadeIn ( )メソッド
→指定したHTML要素をフェードインさせたいときに使う。
引数にはフェードインにかかる時間を記入。1000＝1秒
",False,https://qiita.com//shohei0718/items/8b5c51f9b0b077613cac
"ときどき「きれいですね」って言われることがあリます。
Qiitaなので、もちろん容姿のことではなく、ソースコードのことです。
お世辞でもそう言ってもらえるのは嬉しいので、普段どんなことを気をつけているか書いておく事にしました。なお、Code Completeとかに書いてあるような一般的なことは、書いても面白くないので省略してます :-P

そもそも書かない
自分が使っている言語やフレームワークの機能をしっかりと理解して、車輪の再発明をしないように心がけています。例えば、Pythonでループ処理でループのインデックス値も使いたい場合。例えばこんなソースコードを書いてみます。

ループのインデックス値を独自実装で使う
names = ['Alice', 'Bob', 'Charlie']
index = 0
for name in names:
    print (index, name)
    index += 1


でも、実はPythonにはループ時に要素だけでなくカウンタも取得できるenumerateがあります。

ループのインデックス値をPythonの機能で使う
names = ['Alice', 'Bob', 'Charlie']
for index, name in enumerate(names):
    print (index, name)


この例のように、自分が使っている言語やフレームワークにどんな機能があるのかを、常に好奇心を持って探すようにしています。

改行やスペースに注意を払う
インデントを揃えるのはもちろんなのですが、スペースや改行にも注意を払います。意味のない２文字以上のスペースや２行以上の改行は使わないようにしています。余分なスペースや改行は、ソースコードフォーマッターも積極的にはきれいにしてくれません。

こういう事にならないよう気をつけている
public class Person  {  /* ← 中括弧の前が意味なく２文字スペース */
    private String name;

    public void setName(String name) {
        this.name = name;
    }

    public String getName() {
        return this.name;
    }
        /* ← 意味なく２行以上の改行 */

}



画面は小さめ、フォントは大きめ
ときどき若い人で、巨大なモニターに小さな文字でコーディングをしている人を見ます。そんな見晴らしのいい環境だと、ちょっとやそっとのサイズではメソッドを分割しようというモチベーションがわかないので、ノートパソコンオンリーで作業しています。

名前を可能な限り短くする
変な略語にならないレベルで、変数名やメソッド名を短くする。特に意味の重複などには気をつける。

重複あり
public class Player {
    private int playerId

    public void setPlayerId(int playerId) {
        this.playerId = playerId;
    }

    public int getPlayerId() {
        return playerId
    }
}


このクラスだと、player用のものであることは明白なので、その辺を短くして見ます。
でも、Playerクラスの中に入っているので、idだけで意味は通じますよね、多分。

重複を削除
public class Player {
    private int id

    public void setId(int id) {
        this.id = id;
    }

    public int getId() {
        return this.id;
    }
}



ゴミ掃除をする
使われなくなったライブラリー、呼ばれなくなったメソッド、参照されなくなった変数などは、気がついたら消すようにしています。IDEのサポートもありますが、メソッド呼び出される可能性があるかどうかについて調べるには、そのメソッドをコメントアウトしてコンパイルしてみるとわかります（ただしリフレクションで呼ばれることを除く）。

神は細部に宿る
最後ですが、神は細部に宿ります。こう言った日々の活動の積み重ねが、コードをきれいにするコツかなと思いました。まる。
",False,https://qiita.com//nakaken0629/items/2009c87c23f7d2ad30ca
"

はじめに
このドリルはプログラミング学習社に向けた特訓ドリルになります。
初心者向けの教材は一通り勉強したけど、特に作りたいアプリがあるわけじゃない。
そんなWEBエンジニアの見習いのためにこのドリルを作成しました。
スキルアップに後輩の課題に活用してください。
プログラミングのスキルを上げるためにはコードを書くしかありません！
このマインドを大切にして、学習や仕事に取り組んでみてください。

ドリル本編

認証編

メールアドレス/パスワード認証

基本的なメール/パスワードでの新規登録/ログイン機能 ★☆☆

メールアドレスとパスワードで新規登録できる
登録したメールアドレスとパスワードでログインできる

定番中の定番ですね。ライブラリを使ってサクッと実装しましょう！

メールで確認認証　★★☆

新規登録後に登録アドレス当てにメールを送信
送信されたメールのリンクを押したら登録完了

登録したメールアドレスが本人のものか確認する機能ですね。
多機能なライブラリなら簡単に実装できるかも知れません。

パスワード変更 ★★☆

新しいパスワードを設定できる
ただし、現在のパスワードを入力も必須

乗っ取り防止のため現在のパスワードの入力の必須に挑戦しましょう！

パスワード忘れた時の再設定 ★★☆

登録したメールアドレスを入力
パスワード更新ページのリンクをメールアドレス当てに送信
新しいパスワードを設定できる

こちらも定番の機能ですね！

SNSログイン

SNSログイン ★☆☆

Facebook/Google/Twitter/Lineで新規登録/ログインできる


メール/パスワード認証との共存 ★★☆

SNSまたはメールアドレスで新規登録/ログイン

どんなDB設計にするかは色々な工夫ができそうですね

電話番号認証

電話番号認証 ★★☆

電話番号を入力
SMSで認証コードを送信
認証コードを登録したら、登録完了

WEBというよりかはアプリによくある認証方法ですね。
外部APIを使う流れになりそうですね

SNS編

ツイート機能

ツイート投稿 ★☆☆

文字制限をつける(140とか)
改行ができる
絵文字が入力できる
URLがリンクになる

こだわると意外と難しいツイート機能、余裕があったらOGタグ表示などにも挑戦してみよう

ハッシュタグ ★★☆

ハッシュに続いて単語を投稿するとタグとして登録できる
ハッシュタグはリンクになる
リンクに飛ぶとそのハッシュタグを含むツイート一覧が表示できる

twitterやinstaで定番のこちらの機能、初めての正規表現に最適！

いいね機能 ★☆☆

ツイートに対していいねできる
誰がいいねしたかわかる
ツイートのモデルに総いいね数を管理する

パフォーマンスを意識して、ツイートのモデルにいいね数を管理しよう！

画像添付

1つだけver　★☆☆

ツイートと一緒に表示
小中大など複数にサイズで保存する
画像をクリックしたら、拡大してプレビューできる


複数枚ver　★★☆

4つまで1つのツイートに画像を添付できる


フォロー

フォロー機能　★★☆

他のユーザをフォロー・解除できる
タイムラインにフォローしているユーザだけのツイートを表示する
フォロー一覧を見れる
フォロワー一覧を見れる
ユーザモデルにフォロー数、フォロワー数を管理する

ここで多対多のリレーションをマスターしよう！

ブロック

ブロック機能

他のユーザをブロック・解除できる
ツイート一覧にブロックしているユーザを表示しない
ブロックしたユーザは自分のプロフィール画面を表示できない

多対多のリレーションの復習！

DM機能

1対1のメッセージ★★☆

メッセージを送信できる
既読機能
画像を添付できる
ファイルを添付/ダウンロードできる


グループでのメッセージ　★★☆

グループを作って、複数人で上記のメッセージ機能ができる


リアルタイム化　★★★

リアルタイムにメッセージを確認できる

ここまできたら、中級者！

ECサイト編

ユーザ/管理ユーザ　★★☆

ユーザ、管理ユーザの複数ユーザのユーザ管理

ユーザ、管理ユーザを同一モデルで管理するか、別モデルで管理するか
コントローラをどのように管理するか、色々なやり方を試行錯誤してみましょう！

管理ユーザ向け

商品出品　★☆☆

商品名、画像、値段、在庫を設定して商品を出品できる

既存サービスの項目などを参考に自分なりに項目を追加すると勉強になると思います！

　カテゴリ機能 ★★☆

カテゴリを1~3階層で設定できる
商品にカテゴリを設定できる

少し特殊なリレーション関係になるこの機能、是非挑戦してみましょう！

ユーザ向け

商品検索　★☆☆

商品名で検索できる
カテゴリで絞り込みができる

初めての検索機能、初めての方はライブラリで、慣れてきたら自前で実装するのもありでしょう！

ショッピングカート機能　★★☆

カートに入れるで欲しい商品を管理できる
カートから商品を削除できる

こちらも定番の機能ですね。
そろそろ実装に慣れてきたのでは？

決済機能　★★☆

ショッピングカードの商品をクレジットカードで決済できる
購買履歴を確認できる

こちらも外部サービスを活用して実装してみましょう！
有名なとこだとStripeやPayjpなどがあります。

シェアリングエコノミー編
スペース貸し出しのシェアリングエコノミーを題材にしましょう

ホスト向け

　身分証確認　★★☆

身分を確認できる書類をアップロード出来る
管理画面で承認できる

C2Cサービスの定番ですね。身分情報を自身のDBで管理する場合、慎重にならないといけないですね。
外部サービスで管理するのも手かもしれません。

　銀行講座登録　★★☆

銀行口座を登録できる

余裕があったら、サジェスト機能や正しい銀行コードか確認するバリデーションも実装してみましょう！

　スペース登録　★☆☆

名前、複数画像、住所で登録できる


　ユーザ向け

　スペース検索 ★★☆

住所から絞り込みができる
値段で絞り込みができる


決済機能 ★★★

スペース予約時に決済できる
スペースのレンタル料に加えてサービス手数料(10%)を追加する
レンタル料はホストに振り込まれる
サービス手数料はサービス運営に支払われる

こちらも外部サービスを活用して実装してみましょう！
有名なとこだとStripeやPayjpなどがあります。

レビュー ★☆☆

レンタル後にスペースにレビューできる
星五段階とコメントを追加できる

こちらもC2Cでは定番の機能ですね。
サクッと実装してみましょう！

追記
随時、いい課題が見つかったら追加していこうと思います。
また、これを追加して欲しいなどの要望も大歓迎です！
要件が不十分なところもあるので、気長に管理していきたいと思います。
楽しいプログラミング学習になることを願っております！
",False,https://qiita.com//van_622/items/7ab41d97cbb0b3687bf4
"
Vueで最初に作ったものがなぜかデジタル時計だったお話

こちらの続きです。
デジタル時計作ったならやっぱりアナログ時計も作っておかないと…。
というわけで完成品がコチラ。
今回はアニメーションは使っておりません。
各針は角度をミリ秒単位で計算して動かしています。


構成図

針（Hand）と文字盤（Dials)の2つでアナログ時計（AnalogClock）を構成していきます。
非常にシンプルですね。

Hand

Hand.vue
<template>
    <div class=""hand"" :class=""type"" :style=""{'transform': 'rotate(' + rotate + 'deg)'}"" />
</template>

<script>
export default {
    props: {
        rotate: 0,
        type: ''
    },
}
</script>

<style scoped>
.hand {
    background-color: rgba(255,255,255,0.8);
}

.seconds {
    width: 1px;
    height: 150px;
    transform-origin: 0px 130px;
}

.minutes {
    width: 3px;
    height :150px;
    transform-origin: 1.5px 130px;
}

.hours {
    width: 5px;
    height: 100px;
    transform-origin: 2.5px 80px;
}
</style>


typeで針の種類、秒針（seconds)、分針（minutes）、時針（hours）を指定します。
rotateに針の角度を指定し、styleバインディングで動的に回していきます。

Dials

Dials.vue
<template>
    <div class=""dials"">
        <div 
            v-for=""n in 60"" 
            :key=""n""
            :style=""{
                'top':top(n) + 'px',
                'left': left(n) + 'px',
                'transform': 'rotate(' + rotate(n) + 'deg)'}""
            :class=""['scale', {'fifth': n % 5 == 0}]"">
            <p v-if=""n % 5 == 0"" 
               :style=""{'transform': 'rotate(' + -rotate(n) + 'deg)'}"">
                {{ n / 5 }}
            </p>
        </div>
    </div>
</template>

<script>
export default {
    methods: {
        top(val) {
            return 150 - Math.cos(Math.PI / 30 * val) * 150
        },

        left(val) {
            return 150 + Math.sin(Math.PI / 30 * val) * 150
        },

        rotate(val) {
            return 6 * val
        }
    }
}
</script>

<style scoped>
.dials {
    background-color: #333;
    border-radius: 300px;

    position: relative;

    height: 300px;
    width: 300px;
}

.scale {
    background-color: white;

    position: absolute;

    width: 1px;
    height: 10px;

    transform-origin: left top;

}

.fifth {
    width: 5px;
    height: 20px;
}

.fifth p {
    margin-top: 20px;
    margin-left: -15px;

    color: white;
    font-size: 28px;
    font-weight: bold;
    text-align: center;

    width: 35px;
}
</style>


60個の目盛をループで位置（left, top）と向き（rotate）を計算しながら設定しています。
位置に関しては以下のように高校数学で学んだことを活用します。（懐かしい…）

あとは5の倍数で目盛を太くして文字を表示しているだけです。

AnalogClock

AnalogClock.vue
<template>
    <div class=""clock"">
        <dials />
        <hand class=""hand seconds"" type=""seconds"" :rotate=""seconds""/>
        <hand class=""hand minutes"" type=""minutes"" :rotate=""minutes"" />
        <hand class=""hand hours""   type=""hours""   :rotate=""hours""/>
    </div>
</template>

<script>
import Hand from './Hand'
import Dials from './Dials'
import moment from 'moment'

export default {
    components: {
        Hand,
        Dials
    },

    data() {
        return {
            intervalId: undefined,
            time: undefined
        }
    },

    computed: {
        seconds() {
            let ss = moment(this.time).seconds()
            let nn = moment(this.time).milliseconds()
            return 6 * (ss + nn / 1000) 
        },

        minutes() {
            let mm = moment(this.time).minutes()
            return 6 * (mm + this.seconds / 360)
        },

        hours() {
            let hh = moment(this.time).hours()
            return 30 * (hh + this.minutes / 360)
        }
    },

    methods: {
        setTime() {
            this.intervalId = setInterval(() => {
                this.time = new Date()
            }, 10)
        }
    },

    mounted() {
        this.setTime()
    },

    beforeDestroy() {
        clearInterval(this.intervalId)
    },
}
</script>

<style scoped>
.clock {
    position: relative;
}

.hand {
    position: absolute;
}

.seconds {
    left: 150px;
    top: 20px;
}

.minutes {
    left: 149px;
    top: 20px;
}

.hours {
    left: 148px;
    top: 70px;
}
</style>


時刻の取得についてはデジタル時計編を参照してください。
針の動きを滑らかにするためにミリ秒を含めて角度の計算をしています。
簡単に解説をすると、例えば秒針の場合、秒の値だけだと0°→6°→12°→… というように角度が飛んでしまいます。
それをミリ秒を含めることで補完しています。
transitionでも滑らかに動かせますが、360°になるときに考慮が必要になります。

おわりに
さすがにもう時計シリーズはいいかな…。
",False,https://qiita.com//b1san/items/335fbd275ee56761faa1
"

はじめに
プログラミング学習を開始して69日が経過しました。
今後、Ruby / Ruby on Railsを本格的に学んでいくにあたり、開発環境の準備が必要そうだったため、このタイミングでアマゾンウェブサービス（AWS）の登録・初期設定をしてみましたが、初トライということもあり、非常に時間が掛かってしまいました。
そこで、次回設定時は効率よくできるように、備忘録として今回のトライで学んだことを残しておくこととしました。

1. アカウント作成
AWS アカウント作成の流れに従って実施。
ここまでは問題ありませんでした。

2. 初期設定時に必要な用語の理解
アカウントの作成後、初期設定をしようにも知らない用語のオンパレードだったため、まずはAWSのユーザーガイドをコツコツと読み進め、下記リンクにポイントをまとめていきました。

【AWS】初期設定、用語まとめ（ルートアカウント、IAM、IAMユーザー、IAMグループ、ポリシー）


3. 登録後すぐにやるべきこと
前章で用語をまとめたことにより、ようやく下記リンクの内容が分かるようになりました。

AWSアカウントを取得したら速攻でやっておくべき初期設定まとめ

これを見ると、登録後すぐにやるべきことは大きく分けて以下2点でした。
(1) セキュリティ設定
- MFA（多要素認証）の導入によるルートアカウントの保護
- 管理用のIAMアカウントの作成
- IAMパスワードポリシーの適用
- セキュリティステータスの確認
- git-secretsの設定
(2) 利用料金の監視・アラートの設定
- IAMユーザへの請求情報のアクセス許可
- AWSアカウントの設定
- 通知設定
- IAMユーザーへの切り替えとリージョン設定
- CloudTrailの設定
(1)はルートアカウントが悪用され、不正利用・高額請求などがないように、(2)は使用状況を把握し、高額な請求を予防するために必要な設定とのことです。
上記の設定ミスによる失敗談も見つけ、気を付けなければと思いました。
・初心者がAWSでミスって不正利用されて$6,000請求、泣きそうになったお話。
・AWS無料枠の落とし穴

3. 具体的な初期設定の方法
下記2つのサイトを参考にしながら、設定を進めました。
所々デザインが違った部分もありましたが、設定の流れを非常に分かりやすくまとめてくださっているのでこれらの通りに進めて行けばできます。

AWSアカウントを取得したら速攻でやっておくべき初期設定まとめ
AWS・IAM(Identity and Access Management)ユーザの作成


まとめ

AWSの登録・初期設定を行った。
初期設定時に出てくる用語をまとめた。(参考：【AWS】初期設定、用語まとめ（ルートアカウント、IAM、IAMユーザー、IAMグループ、ポリシー）)
登録後すぐにやるべきことは大きく分けて(1) セキュリティ設定、(2) 利用料金の監視・アラートの設定の2つであることを学んだ。

",False,https://qiita.com//fuku_tech/items/68275daf7c49323d66a1
"自分の手である程度動くコードが書けるようになってきた際、次のステップとし待ち構えているのが例外処理ではないでしょうか。
実際、業務などでプログラムを書く際には、プログラムやアプリケーションがクラッシュしないように適切に例外処理/エラーハンドリングをすべきシーンが多々あります。
今回はRubyを例にして例外処理についてまとめました。

例外処理とは
wiki大先生によると例外処理とは、

プログラムの上位の処理から呼び出されている下位の処理で継続不能、または継続すれば支障をきたす異常事態に陥ったとき、制御を呼び出し元の上位の処理に返し安全な状態になるよう回復処理をすること。その際に発生した異常のことを例外と呼ぶ。
だそうです。

また、継続不能や継続すると問題になる様な状態としては下記が挙げられています。
- ハードウェアの故障
- オペレーティングシステム等、システムの設定ミス
- ライブラリの欠損
- ユーザーの入力間違い
  - 数値入力を要求している場合での、英単語の入力
  - 存在しないファイルの指定
- 許されない演算（0での除算や実数演算で解が虚数になる演算など）
- 割り当てられていない記憶領域へのアクセス
  - 不正な値が与えられたポインタで参照する、或いは機械語レベルで不正な値が与えられたインデックスレジスタ等を用いてメモリ参照することとなった場合
  - ページフォールト
- プログラミング言語において、何も参照していないハンドルやポインター（Nullポインタ）を参照して操作しようとした場合（例としてJavaにおけるNullPointerExceptionなど）。

超ざっくりいうと、ユーザー側の不備（登録フォームの入力漏れなど）というよりは、システム側の不備（仕様の考慮漏れ、バグ、外部連携先のサービスがダウンしてるなど）によって発生した不具合からシステムが安全に回復するための処理のことを言います。

例外処理を実装する意義

プログラム/アプリケーションをクラッシュさせない
重大な例外が発生した際に例外処理を実装していないと、プログラムやアプリケーションが異常終了につながる可能性があります。
そうなると例外発生後にユーザーがシステムを正常に使用できないことはもちろん、システムを提供する企業にとっては甚大なる損害になります。

エラーの早期発見とシステム改修につなげる
例外発生を適切に捕捉してログに残したりslackなどに通知する仕組みを作っておくことで、不具合の早期発見や修正につなげることができます。

Rubyにおける例外処理

書き方
下記のようにbegin ~ rescue ~ endで処理をサンドイッチするのが基本です。
rescue節で捕捉したいエラーのクラスを指定します。
指定がない場合にはStandardErrorが捕捉されます。
begin
  # 例外が起こりそうな処理
rescue StandardError => error # 捕捉したい例外のクラスを記載する。記載しない場合にはStandardErrorが使われる
  # 例外が起こった際の処理(ココで上手いことエラーから復帰するようにする)
  # 変数'error'の中にエラーに関する情報が入っているのでこれを使う
end

メソッドに直接rescueを付けることもできます。
def devide(num1, num2)
  num1 / num2 #
rescue => error
  puts ""error is occured! #{error}""
end

例外の発生有無に関わらず共通処理をしたい場合には下記のように書きます。
begin
  # 例外が起こりそうな処理
rescue => error
  # 例外が起こった際の処理(ココで上手いことエラーから復帰するようにする)
ensure
  # 例外の有無にかかわらずその後実行したい処理を書く
end


注意点

rescue内部でエラーの情報を活用する(例外を握り潰さない)
例外処理は「begin ~ rescue ~ endで囲めばそれで良い」というものではありません。
例外処理の中で適切に例外を捕捉し、それをログに残したり通知する必要があります。

捕捉する例外は絞る
Rubyの場合はExceptionクラスが例外の大元のクラスですが、このクラスには膨大なサブクラスが付いているのでrescue Exception => errorなどとすると捕捉する必要のないエラーまで取得してしまいます。
そのため、例外処理を実装する上では、どのような例外を捕捉したいのかを考えた上で、その例外に対して捕捉するようにするべきです。

Rubyの例外クラス：Ruby Exceptions



何でもかんでも例外処理を挟もうとしない
例外処理の大切さがわかったからといって、ありとあらゆる処理に例外処理を挟むのは、パフォーマンスや可読性の観点で現実的ではありません。
例外の発生度合い、発生した際のクリティカル度合いなどを鑑みた上で、処理を入れるのが良いのではないかと思います。

参考

wikipedia 例外処理
Ruby 2.5.0 例外処理 begin
【Ruby】例外処理
Rubyの例外処理(begin - rescueとraise)をもう一度きちんと確認してみた。

",False,https://qiita.com//yu-croco/items/f8eb41ffa75bd3dac36e
"

背景
11月分のクレジットカードの明細を見てみると、AWSから472円ほど請求されていました。
9月末の2,3日しか設定いじっていない割に高い！と思ったのが、ことの始まりです。

最初に結論
かなり前にAmazon Linux 2でmysql-serverがインストールできないときの対処方法で、NATゲートウェイを設定していました。
そして、MySQLなどの色々なツールをインストールしたので、EC2インスタンスの停止させ、NATゲートウェイは放置していました。
これが諸悪の根源でした。

NATゲートウェイとは？
NATゲートウェイとは、インスタンスから直接インターネットに接続するのではなく、プライベートサブネットからインターネットへの片方向通信を実現する機能です。
図にすると、下のようなイメージのインフラ構成を実現していました。

こうすることで、プライベートサブネットにあるDBサーバに、ソフトウェアをネットワーク経由でインストールしたり、アップデートができます。

NATゲートウェイの金額は？
注意すべき点としては、NATゲートウェイに無料利用枠がないということです。
時間あたりの従量課金制となります。
また、ゲートウェイで処理したデータ量、データ転送量で金額が変動します。
詳しくは下記をチェックしてください。
料金 - Amazon VPC | AWS
東京リージョンの場合、1時間あたり0.062ドルかかります。

何が原因だったのか調査
EC2インスタンスもわざわざ無料枠であるAmazon Linux 2を選択して、お金がかからないものだと思っていました。
最初は、勉強がてら設定したElastic IPアドレスかと思いました。

Elastic IPアドレスとは？
Elastic IPアドレスとは、固定IPアドレスを割り当てるためのものです。
インスタンス1つにつき、Elastic IPアドレスは1つ無料で利用することができます。
ただし、実行中のインスタンスに限ります。
実行中のインスタンスに紐付いていない場合、少しばかり課金が発生するものです。

Elastic IPアドレスはいくら？
Elastic IPアドレスで、400円 = $4も取られているのか？？と思って調べてみました。
EIPで料金発生するパターンとしないパターン #AWS ｜ DevelopersIOより抜粋
・$0.00 : 実行中のインスタンスと関連付けられている Elastic IP アドレス 1 つ
・$0.005 : 実行中のインスタンスと関連付けられている追加の Elastic IP アドレスあたり/1 時間（比例> 計算）
・$0.005 : 実行中のインスタンスと関連付けられていない Elastic IP アドレスあたり/1 時間（比例計算）
・$0.00 : Elastic IP アドレスのリマップ 1 回あたり（1 か月間で 100 リマップまで）
・$0.10 : Elastic IP アドレスのリマップ 1 回あたり（1 か月間で 100 リマップを超える追加分）

Elastic IPアドレスの課金方式を知らず、EC2は停止していました。
そのため、$1いかないくらいは請求されていると考えました。
しかし、数日で$4も請求されないな…となりました。

請求画面を見て、犯人を特定する
そもそもAWSの請求を見れば分かるのでは？？と思い、請求画面を見ることにしました。
下記のURLにアクセスして、
https://console.aws.amazon.com/billing/home#/
Billsを選択すれば、請求明細を見ることができます。
10月の中旬～下旬あたりに気がついたので、10月の請求が恐ろしいことになっています。
NATゲートウェイで、20.03ドルとられています。

EC2は起動しっぱなしではなく、使わないときは停止させていたので、Elastic IPアドレス使用料も1.24ドルとられています。

NATゲートウェイ合計請求：$3.60 + $20.03
9月のNATゲートウェイ費用
0.062/h * 323h = $3.60
10月のNATゲートウェイ費用
0.062/h * 323h = $20.03
参考書が１冊購入できるくらいのお金になってしまいました。

反省①：請求額をモニタリングしよう！
無料だと思わず、たまには請求額をチェックしよう。
めんどくさいな〜と思ったら、CloudWatchで請求額をモニタリングし、一定金額に達したら、事前に設定していたメールアドレスに連絡がくるように設定するとよいです。
具体的な手法は、AWS公式が紹介しているので、参考にすると良いかと思います。
予想 AWS 請求額をモニタリングする請求アラームの作成 - Amazon CloudWatch

反省②：使わなくなったNATゲートウェイは削除しよう！
設定しているだけでお金がかかります。
自分みたいなAWS初心者は、プライベートサブネットが通信する必要があるときだけ設定し、終わったらすぐに削除するように心がけましょう。

反省③：Elastic IPアドレスを設定しているときは、EC2を停止しない！ ※ 時と場合による
EC2インスタンスを起動しているときは、Elastic IPアドレスを設定していてもお金はかかりません。（インスタンス1つにつき、Elastic IPアドレスは1つの場合）
停止するとお金がかかります。
自分みたいなAWS初心者で、EC2を無料利用枠でおさめようとしている場合は、起動しっぱなしで問題ないと思います。
お金がかかる状態でEC2を運用する人は、EC2を起動しっぱなしのほうが安いのか、起動していないほうが安いのか、計算する必要があるかもしません。

参考文献
EIPで料金発生するパターンとしないパターン #AWS ｜ DevelopersIO
月額料金の表示 - AWS 請求情報とコスト管理
予想 AWS 請求額をモニタリングする請求アラームの作成 - Amazon CloudWatch
",False,https://qiita.com//riekure/items/7b266ce291d780f1768b
"

はじめに
GoFのデザインパターンを紹介している『増補改訂版 Java言語で学ぶデザインパターン入門』を読んで、学んだ内容についてまとめます。

Singletonパターン

Singletonとは
日本語に訳すとトランプの1枚札のことで、全体の集合の中で1つしか存在しないものを意味します。
このことからシステム全体でインスタンスが1つしか存在しないことを保証するパターンのことをSingletonパターンと言います。

登場人物
Singletonパターンで使用するのは以下のクラス図に登場するクラスです。
と言ってもSingletonクラスの1つしかありません。


具体例
具体例として、singletonクラスと実行クラスをもとに説明します。

実装クラス


Singletonクラス



Singleton.java
package singleton;

public class Singleton {
    // 1.staticフィールド(クラス変数)としてsingletonを定義
    private static Singleton singleton = new Singleton();

    // 2.privateなコンストラクタ
    private Singleton() {
        System.out.println(""インスタンスの生成に成功しました"");
    }

    // 3.singletonインスタンスの返却
    public static Singleton getInstance() {
        return singleton;
    }
}


ポイントは以下の3点です。
1.privateなクラス変数としてsingletonが定義されている。
2.コンストラクタがprivateになっており、他のクラスから呼び出すことができない。(newできない)
3.singletonインスタンスを返却するgetInstance()メソッドを定義している。
補足の説明を行います。
まず、外部のクラスからpublicなgetInstance()メソッドが呼ばれます。
その際に初めて呼ばれる場合は、Singletonクラスの初期化が行われ、その時に一度だけSingletonクラスのインスタンスが生成されます。
以降、getInstance()メソッドが呼ばれる場合は既に生成したSingletonクラスのインスタンスが返されます。
コンストラクタ内ではインスタンスの生成が成功したことが分かるように出力を行っていますが、通常は不要です。

実行クラス


Mainクラス
getInstance()メソッドを2回呼び出し、同じインスタンスを取得していることを確認します。 


Main.java
package singleton;

public class Main {
    public static void main(String[] args) {
        System.out.println(""***実行クラス開始***"");
        Singleton obj1 = Singleton.getInstance();
        Singleton obj2 = Singleton.getInstance();
        if (obj1 == obj2) {
            System.out.println(""obj1 is obj2"");
        } else {
            System.out.println(""obj1 is not obj2"");
        }
        System.out.println(""***実行クラス終了***"");
    }
}



実行結果
Main.javaを実行した結果は以下になります。
obj1 is obj2が出力されており、同一のインスタンスを取得できていることが確認できます。

実行結果
***実行クラス開始***
インスタンスの生成に成功しました
obj1 is obj2
***実行クラス終了***



メリット
Singletonパターンを利用することで、システム全体でインスタンスが1つしか存在しないことが保証できます。
このようにすることで、複数のインスタンスが絡み合い、思いがけないバグが発生してしまうことを未然に防ぐことができます。

まとめ
インスタンスが1つしか存在しないことを保証するSingletonパターンに関して学びました。
以下でサンプルコードをアップしていますのでよろしければ参考にどうぞ。

Singletonサンプルコード

また、他のデザインパターンに関しては以下でまとめていますので、こちらも参考にどうぞ。

[随時更新]Javaでデザインパターンまとめ


参考文献

増補改訂版 Java言語で学ぶデザインパターン入門

",False,https://qiita.com//mk777/items/08b4a1c51d1833514f87
"

一つの要素をタプルとしてみなすために必要なこと

タプルとは
要素の変更ができないシーケンスのこと。

まずはタプルを定義する

Python
month_names = ('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December')

month_names[1]
#February



タプルは要素の削除ができない

Python
del month_names[2]
month_names

#TypeError                                 Traceback (most recent call last)
#<ipython-input-36-171798863721> in <module>()
#----> 1 del month_names[2]
#      2 month_names

#TypeError: 'tuple' object doesn't support item deletion




タプル同士の結合はできる

Python
#タプル同士の結合はできる
japan_month = ('睦月', '卯月')

all_months = month_names + japan_month
all_months

#('January','February''March','April','May','June''July','August','September','October',
#'November','December','睦月','卯月')



そのままタプルを変数に格納するとエラー

Python
japanese_month = ('如月' )

alls = all_months + japanese_month
alls

#TypeError                                 Traceback (most recent call last)
#<ipython-input-42-dba68d38ed91> in <module>()
#      1 japanese_month = ('如月' )
#      2 
#----> 3 alls = all_months + japanese_month
#      4 alls

#TypeError: can only concatenate tuple (not ""str"") to tuple



タプルの末尾にカンマをつけるとうまくいく

Python
japanese_month = ('如月', )

alls = all_months + japanese_month
alls


",False,https://qiita.com//kk31108424/items/e1ae2a085d67b3960f86
"@kazuki_sumi さんの『「プログラミング言語」って何？』の記事にコメントしたのですが、せっかくなので記事にさせていただきます。

プログラム
「プログラム」の語源は、program=pro-gram。pro=前=先、gram=文=書物。前もって書かれたもの。実行/実演する前に書いたもの、これから実行/実演する内容。
運動会でこれから子供たちが実行することが書かれているプログラム。
演奏会でこれから実演する内容が書かれているプログラム。
コンピュータにこれから実行させることが書かれているプログラム。

コンピュータ
コンピュータの動作原理は、電気を流すか流さないかで電球を点灯・消灯させるようなもの。

電球をディスプレイのドットにして、ドットに赤緑青(RGB)の色を付けて無数に並べ、オンオフで明るさを調整して、フルカラー動画を見せることができる。
スピーカーにつなげて音を鳴らしたり、ネットワークに信号を流して通信したり、すべて電気のオンオフで仕事してる。

バイナリ
電気の オン か オフ、それを数値の 1 と 0 で表現。
1 か 0 を並べた数値は「2進数=バイナリ=binary」。bi=2、2輪＝バイク、バイシクル。
1 か 0 かの情報1個を「ビット」。4個で「ニブル」、8個で「オクテット」「バイト」。
「ニブル」で表現できる2進数は 0000 ～ 1111。10進数にすると 0 ～ 15、16進数にすると 0 ～ F (0～9とA～Fの16個を使用)。
2進数で表現すると長くなるから、2進数8桁を16進数2桁に短くした表現が使われる。
16進数は「ヘキサデシマル=hexadecimal」。「hexa」は6、「deca」は10だけど、「hex」だけで16進数という意味に使われる。
hexagon＝6角形、deciliter＝10ml、decade=10年=一昔。december=旧10月=現12月。

機械語
コンピュータに仕事をさせるために0と1の組合せで作った「命令」が「機械語」。
8ビットCPUであれば、8ビットの組合せで様々な命令が作られている。64ビットCPUであれば1命令64ビット。
機械語はバイナリ(0と1の羅列)なので、機械語命令を保存した実行ファイルを「バイナリファイル」と呼んだりもする。

プログラミング言語
機械語は人間が理解しにくいので、機械語と1対1で文字にしたのが「アセンブリ命令」「ニーモニック」。
機械語/アセンブリ命令は単純な仕事をさせる命令。単純な命令を組み合わせて複雑な仕事をさせる。
複雑な仕事を機械語/アセンブリ命令でプログラムすると膨大になり、人間が理解するのが難しい。
複雑な仕事を人間が理解しやすいように書ける多種多様なプログラミング言語が存在する。
",False,https://qiita.com//shiracamus/items/8a47d969243810d57428
"


ちょっと遅すぎだわ・・・

Summary
外付けHDDにデーターを置いていろいろ作業してた
コピーする作業がものすごく遅い
パソコンで作業するなら内臓HDDの方がお得

Summary2
BashScriptは外付けHDDに関しての作業が苦手？
F#の方がBashScriptより早い気がする

Environment

~/tmp
(ins)$ sw_vers 
ProductName:    Mac OS X
ProductVersion: 10.14
BuildVersion:   18A391

~/tmp
(ins)$ uname -a
Darwin callmekoheis-MacBook-Air.local 18.0.0 Darwin Kernel Version 18.0.0: Wed Aug 22 20:13:40 PDT 2018; root:xnu-4903.201.2~1/RELEASE_X86_64 x86_64

~/tmp
(ins)$ bash --version
GNU bash, version 4.4.23(1)-release (x86_64-apple-darwin17.5.0)
Copyright (C) 2016 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

~/tmp
(ins)$ echo 'printfn ""%A"" <| System.Reflection.Assembly.GetAssembly( typeof<list<int>> ).FullName' | fsharpi --readline-

Microsoft (R) F# Interactive version 4.1
Copyright (c) Microsoft Corporation. All Rights Reserved.

For help type #help;;

> ""FSharp.Core, Version=4.4.1.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a""
val it : unit = ()


いらいらポイント
外付けHDDから内臓HDDにコピーする作業

作業はこんな感じ
外付けHDDにあるデーター（主に写真データー）から必要な写真データーをフォルダ構成を変えて内臓HDDにコピーする（それだけ！）

外付けHDDにあるデータの様子


かかかる時間
F#の方が１分ほどbash scriptより早いが、かなり遅いっ
bash script
4m22

F# script
3m39





内臓HDDでの作業
もしかして？とおもって、パソコンに内臓されているHDDに外付けのHDDにあるデーターをコピーしてみて、作業したら早くなった！
だいたい１分



書いてみたコード
ばばば、、、と書いたのであれかもしれない。。。
というか、そんなにコードとか書くの得意じゃないのであれかもしれない。。。
bash scriptのコード
# SrcHDD_='/Volumes/kohei120GB'
SrcHDD_='/Users/callmekohei/Desktop/mytest'
DstHDD_='/Users/callmekohei/tmp4'


function createDstFilePath () {

  echo $1 | sed \
    -e 's/\.JPG/\.jpg/' \
    -e 's/色補正後のデータ\///' \
    -e 's#'$SrcHDD_'/DVD[1-9]/バラ写真#'$DstHDD_'/BA#' \
    -e 's#'$SrcHDD_'/DVD[1-9]/フイルム#'$DstHDD_'/SL#' \
    -e 's#'$SrcHDD_'/DVD[1-9]/ポケットフイルム#'$DstHDD_'/PF#'

}

function copyFiles () {

    local srcFilePath
    local dstFilePath

    while read -r srcFilePath
    do

      dstFilePath=$( createDstFilePath $srcFilePath)
      dstFolder=$( dirname $dstFilePath )

      if [ ! -e $dstFolder ] ; then
        mkdir -p $dstFolder
      fi

      cp -f $srcFilePath $dstFilePath

    done

}

find $SrcHDD_/* -type f \
  | grep '後' \
  | copyFiles

F# のコード
open System.IO
open System.Text.RegularExpressions

// let SrcHDD_ = ""/Volumes/kohei120GB""
let SrcHDD_ = ""/Users/callmekohei/Desktop/mytest""
let DstHDD_ = ""/Users/callmekohei/tmp4""


// コピーしたい写真データーのリストを作成する
let srcJPGs () =

    [""DVD1"";""DVD2"";""DVD3"";""DVD4""]
    |> List.map ( fun s ->
        Directory.GetFiles( SrcHDD_ + ""/"" + s ,""*"", SearchOption.AllDirectories)
        |> Array.filter ( fun (s:string) -> s.Contains(""色補正後のデータ"") )
    )

// コピー元のファイルパスとコピー先のファイルパスを組み（タプル）にしたリストを作成する
let dstJPGs () =

    srcJPGs ()
    |> List.map( fun arr ->
        arr
        |> Array.map( fun srcPath ->

            let destinateFilePath =
                Regex.Replace( srcPath, @""色補正後のデータ/"" , """" )
                |> fun s -> Regex.Replace( s, SrcHDD_ + @""/DVD\d/バラ写真""         , DstHDD_ + ""/BA"" )
                |> fun s -> Regex.Replace( s, SrcHDD_ + @""/DVD\d/フイルム""         , DstHDD_ + ""/SL"" )
                |> fun s -> Regex.Replace( s, SrcHDD_ + @""/DVD\d/ポケットフイルム"" , DstHDD_ + ""/PF"" )
                |> fun s -> Regex.Replace( s, "".JPG"", "".jpg"" )

            srcPath, destinateFilePath

        )
    )

// もしコピー先のフォルダがなければ作成する
let createFolderIfnotExists () =

    dstJPGs ()
    |> List.map ( fun arr ->
        arr
        |> Array.map( fun tpl -> System.IO.Path.GetDirectoryName( tpl |> snd ) )
        |> Array.distinct )
    |> List.iter( fun arr ->
        arr
        |> Array.iter( fun folderPath ->
            if not ( Directory.Exists( folderPath ) ) then
                Directory.CreateDirectory( folderPath ) |> ignore
        )
    )


let main () =

    createFolderIfnotExists ()

    dstJPGs ()
    |> List.map ( fun arr ->
        arr |> Array.iter ( fun (src,dst) -> File.Copy(src, dst) ) )

main ()


追記 2018.11.5 mon 00:34
copyメソッドをパラレルしてみました！
外付けHDDから内臓HDDにコピーする際パラレルすると大幅に遅くなります！それ以外（同一HD）だと早くなります！ふむ〜。なぜに？？



parallel
src hdd
dst hdd
time




x
outer
inner
3m39.562s


o
outer
inner
8m41.194s


x
inner
inner
1m04.923s


o
inner
inner
44.122s


x
outer
outer
9m27.624s


o
outer
outer
8m29.336s




コード
let main () =

    createFolderIfnotExists ()

    dstJPGs ()
    |> Array.concat
    |> Array.map ( fun (src,dst) -> async{ File.Copy(src, dst) } )
    |> Async.Parallel
    |> Async.RunSynchronously

",False,https://qiita.com//callmekohei/items/5d3c211039331ac14b40
"

まず始めに
この記事はこれからプログラミングを始めようと考えている思っている方向けの記事で私自身都内のスクールのでプログラミングを学んだ身なのでその視点から話します。

結論
これからプログラミングを始めたくて最短で技術をものにしたければスクールに通うべきです！

スクールのメリット
例えばこれから野球を始めてやるときにどうやってボール投げればいいかなんて教えてくれる人コーチみたいな人がいますよね？
どうやってボール投げるか教えてもらった方がすぐ出来るようになりますよね？
それと同じでスクールに通うメリットはその人が長い時間かけて習得したノウハウを短期間でものにできることにあると思います。

スクールのデメリット
費用がかかる
本当に知りたいことが学べるか受けてみないとわからない

スクールに通うべき人
独学で挫折した
近くに教えてくれるような人がいない
どうやって勉強したら良いかわからない
手っ取り早く覚えたい

スクール通わなくても良い人
自分で目的を持ってプログラミングに触れることができる
費用をかけたくない
一緒に学べて教えてくれるメンター的存在がいる
これら以上のことがスクールに通う通わないの基準になるのかなと思います。
簡単ではありましたが参考にしていただければと思います。
もしもう少し深掘りな話を聞きたければツイッターのlinkを貼りますのでメッセージをいただければと思います。
https://twitter.com/jade91509386
",False,https://qiita.com//joe225/items/cf93589775de4b66a672
"

1.コンピュータに指示する言葉
コンピュータに動作を指示するにはどう動いてほしいかを順に命令する必要があります。その命令を出すのに使われるのが「プログラミング言語」であり、プログラミング言語で示された命令の列を「プログラム」といいます。
例：Ruby、PHP、C言語、Swift、C++、C#、Java、JavaScript、SQL等

2.コンピュータは数値しか理解していない！？
上記で示したプログラミング言語は英単語等を使いプログラムを書きます。
しかしコンピュータは本来「数値」しか理解できません。（より正確にはメモリに流れる電圧のあるなしを数値の０と１で表現している。）その数値の並びで表現されたプログラミング言語のことを「機械語」といいます。
人がコンピュータに何かさせるには機械語で語りかける必要がありました。ですが人が機械語を理解するのはとても手間がかかり、コンピュータに数値をプログラムしていくのにも途方もない時間が必要でした。そのためより簡単にかつ効率的にプログラムを書く必要性がありました。

3.高水準言語の登場
機械語に代わりより効率的にプログラムを書くために登場したのが高水準言語です。（1の例を参照）
これらの言語はそれぞれの文法にのっとり書かれたプログラムを翻訳プログラムにかけ、機械語に翻訳しなおすことでコンピュータを動かしています。
この人間よりのプログラミン言語を高水準言語といい、コンピュータに近い言語を低水準言語といいます。
",False,https://qiita.com//sumin/items/aa6e401ac3bab0cac618
"Vueに触れ始めて少し経ったある日、
「Vueも少しずつ慣れてきたし、何か一つ自分で作ってみようかな。
でもそんなに難しいのはまだ無理だし…。
とりあえず時計作るか！！」
というわけでこんな感じのものができました。

少々ネタではありますが、学んだことを織り交ぜながら紹介していこうと思います。

構成図

図のようにコンポーネントを作って組み合わせていきます。

LightStick
オンで光ってオフで消える「光る棒！！」を作ります。
縦棒と横棒を選べるようにしておきます。

LightStick.vue
<template>
    <div class=""stick"" :class=""[type, {light: light}]""  />
</template>

<script>
export default {
    props: {
        type: {
            typs: String,
            default: 'vertical'
        }
    },

    data() {
        return {
            light: false
        }
    },

    methods: {
        on() {
            this.light = true
        },

        off() {
            this.light = false
        }
    }
}
</script>

<style scoped>
.stick {
    background-color: rgba(200,255,200,0.2);
    border-radius: 10px;
}

.vertical {
    height: 50px;
    width: 10px;
}

.horizontal {
    height: 10px;
    width: 50px;
}

.light {
    background-color: yellowgreen;
}
</style>



NumberPanel
光る棒を組み合わせて数字の形を作ります。

あとは数字に合わせて「光棒何号」を光らせるかを決めるだけです。
「1」なら「いけ！！3号！！6号！！」といった具合です。

NumberPanel.vue
<template>
    <div class=""number-panel"">
        <light-stick type=""horizontal"" class=""stick stick1"" ref=""stick1"" />
        <light-stick type=""vertical""   class=""stick stick2"" ref=""stick2"" />
        <light-stick type=""vertical""   class=""stick stick3"" ref=""stick3"" />
        <light-stick type=""horizontal"" class=""stick stick4"" ref=""stick4"" />
        <light-stick type=""vertical""   class=""stick stick5"" ref=""stick5"" />
        <light-stick type=""vertical""   class=""stick stick6"" ref=""stick6"" />
        <light-stick type=""horizontal"" class=""stick stick7"" ref=""stick7"" />
    </div>
</template>

<script>
import LightStick from ""./LightStick""

export default {
    components: {
        LightStick
    },

    props: {
        number: 0
    }, 

    watch: {  
        number() {   //numberの変更とともに表示
            this.display()
        }
    },

    mounted() {   //初期表示設定
        this.display()
    },

    methods: {
        display() {
            switch(this.number) {
                case 0: 
                    this.zero()
                    break
                case 1:
                    this.one()
                    break
                case 2:
                    this.two()
                    break
                case 3:
                    this.three()
                    break
                case 4:
                    this.four()
                    break
                case 5:
                    this.five()
                    break
                case 6:
                    this.six()
                    break
                case 7:
                    this.seven()
                    break
                case 8:
                    this.eight()
                    break
                case 9:
                    this.nine()
                    break
            }
        },

        clear() {
            this.$refs.stick1.off()  //子コンポーネントのメソッド呼び出しにはref属性を使用
            this.$refs.stick2.off()
            this.$refs.stick3.off()
            this.$refs.stick4.off()
            this.$refs.stick5.off()
            this.$refs.stick6.off()
            this.$refs.stick7.off()
        },

        one() {
            this.clear()
            this.$refs.stick3.on()
            this.$refs.stick6.on()
        },

        two() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick3.on()
            this.$refs.stick4.on()
            this.$refs.stick5.on()
            this.$refs.stick7.on()
        },

        three() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick3.on()
            this.$refs.stick4.on()
            this.$refs.stick6.on()
            this.$refs.stick7.on()
        },

        four() {
            this.clear()
            this.$refs.stick2.on()
            this.$refs.stick3.on()
            this.$refs.stick4.on()
            this.$refs.stick6.on()
        },

        five() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick2.on()
            this.$refs.stick4.on()
            this.$refs.stick6.on()
            this.$refs.stick7.on()
        },

        six() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick2.on()
            this.$refs.stick4.on()
            this.$refs.stick5.on()
            this.$refs.stick6.on()
            this.$refs.stick7.on()
        },

        seven() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick3.on()
            this.$refs.stick6.on()
        },

        eight() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick2.on()
            this.$refs.stick3.on()
            this.$refs.stick4.on()
            this.$refs.stick5.on()
            this.$refs.stick6.on()
            this.$refs.stick7.on()
        },

        nine() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick2.on()
            this.$refs.stick3.on()
            this.$refs.stick4.on()
            this.$refs.stick6.on()
            this.$refs.stick7.on()
        },

        zero() {
            this.clear()
            this.$refs.stick1.on()
            this.$refs.stick2.on()
            this.$refs.stick3.on()
            this.$refs.stick5.on()
            this.$refs.stick6.on()
            this.$refs.stick7.on()
        }

    }
}
</script>

<style scoped>
.number-panel {
    background-color: #222;
    position: relative;
    height: 130px;
    width: 70px;
}

.stick {
    position: absolute;
}

.stick1 {
    top: 0;
    left: 10px;
}

.stick2 {
    top: 10px;
    left: 0;
}

.stick3 {
    top: 10px;
    left: 60px;
}

.stick4 {
    top: 60px;
    left: 10px;
}

.stick5 {
    top: 70px;
    left: 0;
}

.stick6 {
    top: 70px;
    left: 60px;
}

.stick7 {
    top: 120px;
    left: 10px;
}
</style>



Separation
ただの区切りです。ハイ。

Separation.vue
<template>
    <div class=""colon"">
        <div class=""circle"" />
        <div class=""circle"" />
    </div>
</template>

<style scoped>
.circle {
    background-color: yellowgreen;
    border-radius: 5px;
    width: 10px;
    height: 10px;
}

.colon {
    height: 130px;
    display: flex;
    flex-direction: column;
    justify-content: space-around;
}
</style>




DigitalClock
NumberPanelとSeparationを並べて時計の形を作ります。
あとは時刻の各値をNumberPanelに割り当てればOKです。
時刻の取得については別途解説します。

DigitalClock.vue
<template>
    <div class=""clock"">
        <div class=""display"">
            <number-panel :number=""hours2"" />
            <number-panel :number=""hours1"" />
            <separation />
            <number-panel :number=""minutes2"" />
            <number-panel :number=""minutes1"" />
            <separation />
            <number-panel :number=""seconds2"" />
            <number-panel :number=""seconds1"" />
        </div>
    </div>
</template>

<script>
import NumberPanel from ""./NumberPanel""
import Separation from ""./Separation""
import moment from ""moment""

export default {
    components: {
        NumberPanel,
        Separation
    },

    data() {
        return {
            time: undefined,
            intervalId: undefined
        }
    },

    methods: {
        setTime() {
            this.intervalId = setInterval(() => {
                this.time = new Date()
            }, 100)
        }
    },

    mounted() {
        this.setTime()
    },

    beforeDestroy() {
        clearInterval(this.intervalId)
    },

    computed: {
        hours1() {
            return moment(this.time).format(""HH"") % 10
        },

        hours2() {
            return Math.floor(moment(this.time).format(""HH"") / 10)
        },

        minutes1() {
            return moment(this.time).format(""mm"") % 10
        },

        minutes2() {
            return Math.floor(moment(this.time).format(""mm"") / 10)
        },

        seconds1() {
            return moment(this.time).format(""ss"") % 10
        },

        seconds2() {
            return Math.floor(moment(this.time).format(""ss"") / 10)
        }
    }
}
</script>

<style scoped>
.clock {
    background-color: #111;
    filter: drop-shadow(10px 10px 10px rgba(0,0,0,0.6));
    padding: 50px;
}

.display {
    background-color: #222;
    display: flex;
    justify-content: space-between;
    padding: 10px;
    width: 600px;
}    
</style>



setInterval
時計を実装するために一定間隔で時刻を取得する必要があります。
その時に使用するのがsetIntervalです。
data() {
    return {
        intervalId: undefined    //1. clearIntervalのためのIDを保持します
    }
},

methods: {
    do() {    //2. 一定間隔で処理を実行するためメソッドを用意します
        this.intervalId = setInterval(() => {  
            //処理内容
        }, 1000) //1秒間隔で処理
    }
},

mounted() {  //3. 上記で用意したメソッドをマウントのタイミングで呼び出します。
             //   これによりこのコンポーネントは一定間隔で処理を実行することになります。
    this.do()
},

beforeDestroy() {    //4. 使用後はしっかりとクリアする必要があります
    clearInterval(this.intervalId)
}


Moment.js

Moment.js

$npm install moment

javascriptで日付処理を容易にするためのライブラリです。
今回はフォーマットで使用しています。
これ以上はすみませんが割愛します。
vue-momentというのもあるみたいです。
詳しくは見ていません。ハイ。

おわりに
フロント学び始めたばかりだけど自由度が高くて楽しい！！でも難しい！！
今回のももっといい方法があるかもしれませんが、棒を光らせたい！！という思いでこんな感じになってしまいました。
次はもっといいものを作りたいですね。
",False,https://qiita.com//b1san/items/29a0d9b30f7ea72ac028
"

環境構築で悩んでいた
内科のクリニックを経営していて。
PCは電子カルテと情報発信のためのものとしか使ってなかった。
たまたま見たTwitter。
２ちゃんねるのひろゆき氏が「グーグルの機械学習の課題を終了」という投稿を目にする。
無料でやったとのこと。
え？？プログラムって無料で学べるの？
調べてみた。

プログラムって無料でできるの？
Paizaラーニングというサービスを発見。
おぉぉぉ。
ネットさえつながればどこでも学習ができるな～～～。
移動が多い自分としては本での学習は難しかったので助かった。
機械学習はpythonと書いてあるのでそれを学び始める。
課題も難しく。
う～～ん、わからん。
ネットで調べるとQiitaに出会う。
これが運命を変えた気がする。
詳しく読まずにブログのようなものかな～～って程度で記事をあげると。
manzyunさんからコメントが。
これ知っといた方がいいよ～って。
こんなフランクに教えてくれるの？

教えてもらえるなら
環境構築ができませんと言った瞬間。
shiracamusさんから
Google Colaboratoryでやってみたら？とアドバイスをもらう。
できた。
アップロードやダウンロードはよくわからないですが。
直接打ち込んでも動きました。
といっても
print(""テスト"")だけしかやってませんが。
これなら、環境構築とかしなくてもどこでもプログラムができそうです。
Qiitaの書き方もまだ理解できてないです。
ここにコードを書いたりもできるそうですがよくわからないのでこれから勉強します。
でも、このみなさんからのコメントで大きな一歩を進めました。
40歳ができるのかな～で始めたプログラミング。
こんな世界があるのかって知ることができたので。
やってよかった続けようって思えます。
ありがとうございます。
",False,https://qiita.com//junior/items/a64e01cab18cb77317bc
"モチベーションも大事だと思うので覚書として使用します。
七週目の感想七週目は、継承を学習。
学習書/ｶﾞｲﾄﾞﾗｲﾝとして参考にしている「猫でもわかるC#」にて序盤に軽く触れた継承が、
約30P分の量で再登場したので、じっくり臨みました。(書籍内でも主要な要素であろう事が伺えた･･･)
派生クラスやその多層化で、最初に比べればオブジェクト指向の片鱗位は理解できたと思います。



　　　　　　　　　　　　　　　

継承

継承は、OOP三原則(継承・ポリモーフィズム・カプセル化)の1つであり非常に重要な概念です。
変更に対して柔軟に対応するため、可能な限りこの三原則を用いて設計・保守していく事になります。
継承は、引き継ぐ事を意味するが、それは元となる規格であり機能ではない事に注意。
・参考: オブジェクト指向と10年戦ってわかったこと

継承は概念ですが、C#での具体例をあげるとすれば、
元のクラスを引き継いだまま新しいクラスを作成する事ができます(便利！)
元のクラスを基本クラス(base class)
新しく作成されたクラスを派生クラス(derived class)
といいます。
構文は以下のように書きます。
class 派生クラス名:基本クラス名
※基本クラスのpublicなメンバは派生クラスから呼び出す事ができます。
ただ全てpublicにしてしまうと、派生クラス以外に依存関係を作る恐れがあるため、
その場合protectedやprivate protectedを用いる事で基本クラスとその派生クラスにだけ影響を与えます。
基本的なコード例をここに示します
using System;

class MyBase// 基本クラス
{
    public int a = 10;
    public void BaseMethod()
    {
        Console.WriteLine(""ここは基本クラス"");
    }
}

class MyDerived : MyBase// 派生クラス作成
{
    public int b = 20;
    public void DerivedMethod()
    {
        Console.WriteLine(""ここは派生クラス"");
    }
}

class Inheritance01
{
    public static void Main()
    {
        MyDerived md = new MyDerived();// 派生クラスのインスタンス
        md.BaseMethod();// 基本クラスのpublicなメンバは派生クラスから呼び出せる
        md.DerivedMethod();
        Console.WriteLine($""md.a={md.a}"");
        Console.WriteLine($""md.b={md.b}"");
        MyBase mb = new MyBase();
        mb.BaseMethod();
        Console.WriteLine($""mb.a={mb.a}"");
    }
}




このように継承(inheritance)とは「引き継ぐ」ことを意味します。

他の継承に関連する機能を見ていきましょう。


名前の隠蔽

名前の隠蔽により、派生クラスでも基本クラスのメンバと同名のメンバを持つ事が可能になります。
使用には、newを頭につけるだけです(この時後ろの()は必要ありません)
以降のメンバ名は全てnewメンバが呼び出され元のメンバは隠されます。
※隠蔽された元のメンバを使いたい場合、base.メンバ名を用いる事で呼び出せます。
(但しstaticなメソッドでは扱えません)
基本的なコード例をここに示します
namespace Inpei
{
    class Base
    {
        public int x = 10;
        protected void BaseMethod()
        {
            Console.WriteLine(""Baseクラス"");
        }
    }

    class Derived : Base
    {
        new public int x = 20;// 名前の隠蔽
        new public void BaseMethod()// 名前の隠蔽
        {
            Console.WriteLine(""Derivedクラス"");
            Console.WriteLine($""base.x={base.x}, x={x}"");//base.メンバ名と更新されたメンバ
        }
    }

    class Inheritance
    {
        public static void Main()
        {
            Derived d = new Derived();// static method内はnewｲﾝｽﾀﾝｽ作成
            Console.WriteLine($""x={d.x}"");// 20

            //base はstaticでは使用できないnewｲﾝｽﾀﾝｽ作成しても無理
            //Base b = new Base();
            //Console.WriteLine($""base.x={base.x}, {x}"");

            d.BaseMethod();
        }
    }
}





このように、newメンバにより元のメンバが隠される事を、名前の隠蔽といいます。


オーバーライド

オーバーライドは、派生クラスで同名のメソッド等を作る事ができます。
引数の違うメソッドを多重定義できるオーバーロード(読み込む)(引数違う)とは違い、
オーバーライド(上書き)(引数同じ)は、再定義をする事ができます。
また、base.メンバ名と同じく、staticなメソッドでは定義できません。
メソッドの他、プロパティやインデクサでも使用可能です。
元のメソッドを仮想メソッド(virtual method)
新しいメソッドをオーバーライドメソッド(override method)
といいます。
使用には、アクセス修飾子の後ろにvirtualをつけ、
派生クラスでは、overrideと同じ引数で再定義します。
※引数は同じでなければなりません。
基本的なコード例をここに示します

※Mainメソッドで参照変数と代入する分のnewインスタンスを全て作成し、実行前どのクラスの参照を代入するかによってm.Nakigoe()の動作が異なっている事に注意。

namespace Overrider
{
    class Mamma1// Mamma 哺乳類
    {
        protected readonly int leg = 4;// readonly 読み込み専用にして変更不可
        protected string koe;// 規格

        public virtual string Nakigoe()// virtual koe返す
        {
            return koe;
        }
        public int Leg()// 全部4本足なのでoverride不必要 leg返す
        {
            return leg;
        }
    }

    class Cat : Mamma1// 派生クラス
    {
        public override string Nakigoe()// Override
        {
            koe = ""ニャー"";
            return koe;// 呼出元(仮想メソッド)にkoe返す
        }
    }

    class Dog : Mamma1// 派生クラス
    {
        public override string Nakigoe()// Override
        {
            koe = ""ワン"";
            return koe;// 呼出元(仮想メソッド)にkoe返す
        }
    }

    class Override01
    {
        public static void Main()
        {
            Mamma1 m; //参照変数mを宣言
            Cat cat = new Cat();// 代入用ｲﾝｽﾀﾝｽ作成
            Dog dog = new Dog();// 代入用ｲﾝｽﾀﾝｽ作成

            m = cat; // 参照変数mに代入
            Console.WriteLine($""猫の脚は{m.Leg()}本で鳴き声は{m.Nakigoe()}です"");// m(cat)の鳴き声

            m = dog;// どのｸﾗｽの参照を代入したかによってm.Nakigoe()の動作が異なる
            Console.WriteLine($""犬の脚は{m.Leg()}本で鳴き声は{m.Nakigoe()}です"");// m(dog)の鳴き声
        }
    }
}



このように、規格となるvirtual methodから各パーツであるoverride methodが作成されます。


クラスの多層階層

多層階層は、派生クラスを元に制限なく何層も派生クラスを作成できます。
使用には、新しい派生クラス:基となる派生クラスとします。
基本的なコード例をここに示します
namespace BaseDelived
{
    class MyBase// 基本クラス
    {
        protected int x = 10;
        public virtual void Show()
        {
            Console.WriteLine($""x={x}"");
        }
    }

    class Derived1 : MyBase// 派生クラス
    {
        protected int y = 20;// 派生クラスでy定義しただけ。意味なし
    }

    class Derived2 : MyBase// 上の派生クラスを継承した派生クラス
    {
        int z = 30;// x定義
        public override void Show()// overrideしてx=xをz=zに上書き
        {
            Console.WriteLine($""z={z}"");
        }
    }

    class inheritance
    {
        public static void Main()
        {
            MyBase mb;
            Derived1 d1 = new Derived1();
            Derived2 d2 = new Derived2();

            mb = d1;
            mb.Show();

            mb = d2;
            mb.Show();
        }
    }
}





このように、派生クラスを階層化しながらメソッドのoverrideが可能です。

クラスの継承とコンストラクタ

コンストラクタは、インスタンス生成時に呼び出されるメソッドで、
インスタンスが他クラスで使われる前に初期化処理してくれます。
この場合newで呼び出されたクラスのメソッドを指します。
使用上のルールとして、
・メソッド名(コンストラクタ名)はクラス名と同じ。
・戻り値は指定しない。
・呼び出されるタイミングはインスタンス生成時のみ。
基本的なコード例をここに示します
    public class Test
    {
        public string Tag;

        public Test() // こ
        {　　　　　　　 // の
            Tag = ""A"";// 部
        }　　　　　　　 // 分
    }
// この部分がコンストラクタになります。
// コンストラクタは戻り値voidを定義しません。
// さらに、メソッド名はクラス名と同じである必要があります。

    static void Main(string[] args)
    {
        Test test = new Test();
    }
// newにより呼び出されたメソッドがコンストラクタです。
// test.Tag には ”A"" という値がセットされる事になります。



では本題ですが、派生クラスのコンストラクタはそれぞれどのように実行されていくのでしょうか？
基本的なコード例をここに示します
namespace Inheritance
{
    class MyBase
    {
        protected int x;

        public MyBase()// MyBaseクラスのconstructor
        {
            Console.WriteLine(""ここはMyBase"");
            x = 10;
        }
    }

    class Derived1 : MyBase
    {
        public Derived1()// Derived1クラスのconstructor
        {
            Console.WriteLine(""ここはDerived1"");
            x = 20;
        }
    }

    class Derived2 : Derived1
    {
        public Derived2()// Derived2クラスのconstructor
        {
            Console.WriteLine(""ここはDerived2"");
            x = 30;
        }
        public void Show()
        {
            Console.WriteLine($""x={x}"");
        }
    }

    class Inheritance
    {
        public static void Main()
        {
            Derived2 d2 = new Derived2();// constructor呼出
            d2.Show();
            // 代入された値(d2)は、そのShowメソッドを呼び出す際、
            // xは遡って元の10、派生の20、その派生の30と基本クラスから順に呼び出される事になる。
            // 従ってShowメソッドは30となった。
        }
    }
}





このように、コンストラクタは基本クラスから順に呼び出される事になります。


抽象 abstract

抽象メソッド(abstract method)は、派生クラス毎に個別定義するメソッドになります。
以前の再定義するオーバーライドや多重定義するオーバーロードとは違います。
抽象メソッドは仕様(規格)であり中身は記述せず、実装クラスにて初めて記述します。
使用には、アクセス修飾子の後ろにabstractをつけ、クラス名の前にもabstractをつけます。
派生クラスでは、その仕様(規格)にそった個別定義ができます。
※基本クラスでの使用の場合、実装クラスで必ずoverrideにする。
基本的なコード例をここに示します
namespace Abstracting
{
    abstract class MyAbst// 抽象クラス&基本クラス
    {
        public abstract double Discri(double a, double b, double c);
    }

    class MyDscri : MyAbst// 派生クラス
    {
        public override double Discri(double a, double b, double c)// 必ずoverride
        {
            return Math.Pow(b, 2.0) - 4.0 * a * c;
        }
    }

    class Abst
    {
        public static void Main()
        {
            MyDscri md = new MyDscri();// 代入用インスタンス
            double d = md.Discri(1.0, 2.0, 3.0);// 代入
            Console.WriteLine(d);
        }
    }
}




このように、abstractしてれば異なる処理も個別に設定しリターンできる。


分割定義 partial


クラスの分割定義

クラスの分割定義は、クラスが肥大化した時に分割する事でパフォーマンスが向上します。
例えば、自動生成されるコードを処理したり何らかのバグでコードが変更された場合も分散により最小限の被害に抑えられます。
またクラスの分割定義は、他クラス、他ファイルをまたいで分散させる事もできます。
使用には、分割するクラスの頭にpartialをつけます。
基本的なコード例をここに示します
//File1.cs 内:
namespace PC
{
    partial class A
    {
        int num = 0;
        void MethodA() { }
        partial void MethodC();
    }
}

//File2.cs 宣言内:
namespace PC
{
    partial class A
    {
        void MethodB() { }
        partial void MethodC() { }
    }
}




メソッドの分割定義

メソッドも分割定義できます。但しpartial class内に限ります。
また、暗黙的にprivateになります。
使用には、分割するメソッドの頭にpartialをつけます。
基本的なコード例をここに示します
namespace PM
{
    partial class A
    {
        partial void Am(string s);
    }

    partial class A
    {
        partial void Am(String s)
        {
            Console.WriteLine($""Am: {s}"");
        }
    }
}



//--
partial class Earth : Planet, IRotate { }
partial class Earth : IRevolve { }
// これらは、次の宣言と等価です。

class Earth : Planet, IRotate, IRevolve { }



このように、コードをまたいで分割定義できます。

",False,https://qiita.com//h_okabe/items/bd193ed005b2ca190c0d
"※画像クリックでリンクが開きます

本書は「コードは理解しやすくなければいけない」という考えを元に、
「読みやすいコード」を書くことを目的としている。
「読みやすいコード」を指標として紹介している為、
開発に携わっている人は立場問わず、一読する価値があると思う。
複数のプログラミング言語が記載されているが、
簡単な例文しか扱っていない為、プログラミング初心者でも読み易い。
以下概要を記す。


第 I 部 表面上の改善

1章 理解しやすいコード
鍵となる考え
　・コードは理解しやすくなければいけない。
　・コードは他の人が最短時間で理解できるように書かなければいけない。

「理解する」とはコードに変更を加えたりバグを見つけたりできるという意味。
「他の人」とは自分のコードに見覚えのない未来の自分も含む。
コードは短くしたほうがいいが、「理解するまでにかかる時間」を短くするほうが大切。
「理解するまでにかかる時間」は、その他の目標(設計やテスト等のし易さ)とは競合しないし、寧ろそれらの達成に繋がることが多い。


2章 名前に情報を詰め込む
鍵となる考え
　・名前に情報を詰め込む。
　・気取った言い回しよりも明確で正確なほうがいい。

明確な単語を選ぶ
　⇒ Get ではなく、状況に応じて Fetch や Download などを使う。
汎用的な名前を避ける
　⇒ tmp や retval などの汎用的な名前を使うときは、それ相応の理由を用意する。
抽象的な名前よりも具体的な名前を使う
　⇒ ServerCanStart() よりも CanListenOnPort() のほうが明確だ。
接尾辞や接頭辞を使って情報を追加する
　⇒ ミリ秒を表す変数名の後ろに _ms をつける。
　⇒ エスケープが必要な変数名の前に raw_ をつける。
名前の長さを決める
　⇒ スコープの大きな変数には長い名前をつける。
　⇒ 短い名前はスコープが数行の変数につけるべき。
名前のフォーマットで情報を伝える
　⇒ クラスのメンバ変数にアンダースコアをつけて、ローカル変数と区別する。


3章 誤解されない名前
鍵となる考え
　・名前が「他の意味と間違えられることはないだろうか?」と何度も自問自答する。

filter length limit 等、プログラミングに使うには意味があいまいな単語は使用しない。
　⇒ 限界値を明確にするには、名前の前に max_ や min_ をつける。
　⇒ 範囲を指定するときは first と last を使う。
　⇒ 包含/排他的範囲には begin と end を使う。
　⇒ ブール値には is や has などを使い、disable のような否定形は避ける。
単語に対するユーザの期待にも注意する。
　⇒ get() や size() には軽量なメソッドが期待されている。


4章 美しさ
鍵となる考え
　・一貫性のあるスタイルは「正しい」スタイルよりも大切だ。
3つの原則
　・読み手が慣れているパターンと一貫性のあるレイアウトを使う。
　・似ているコードは似ているように見せる。
　・関連するコードをまとめてブロックにする。

5章 コメントすべきことを知る
鍵となる考え
　・コメントの目的は、書き手の意図を読み手に知らせることである。
　・コードからすぐにわかることをコメントに書かない。

コメントするべきでは「ない」ことを知る。
　⇒ コードからすぐに抽出できることはコメントしない。
　⇒ コードを補うコメントは、コメントを書くのではなくコードを修正する。
コードを書いているときの自分の考えを記録する。
　⇒ なぜ他のやり方と同じではないのかや、定数の値にまつわる「背景」をコメントする。
　⇒ コードの欠陥を TODO: や XXX: などの記法を使って示す。
読み手の立場になって何が必要になるかを考える。
　⇒ コードを読んだ人がハマりそうなところを予想してコメントをつける。
　⇒ ファイルやクラスには「全体像」のコメントを書く。
　⇒ 読み手が細部に捕らわれないように、コードブロックにコメントをつけて概要をまとめる。


6章 コメントは正確で簡潔に
鍵となる考え
　・コメントは領域に対する情報の比率が高くなければいけない。

複数のものを指す可能性がある「それ」や「これ」などの代名詞を避ける。
コメントに含める入出力の実例を慎重に選ぶ。
多くの意味が詰め込まれた言葉や表現を使って、コメントを簡潔に保つ。


第 Ⅱ 部 ループとロジックの単純化

7章 制御フローを読みやすくする
鍵となる考え
　・条件やループなどの制御フローはできるだけ「自然」にする。コードの読み手が立ち止まったり読み返したりしないように書く。
　・行数を短くするよりも、他の人が理解するのにかかる時間を短くする。
　・変更するときにはコードを新鮮な目で見る。一歩下がって全体を見る。

比較を書くときには、変化する値を左に、より安定した値を右に配置する。

if/else 文のブロックは、一般的には 肯定形 ・ 単純 ・ 目立つもの を先に処理する。

三項演算子 ・ do/while ・ goto を使うとコードが読みにくくなることが多いので、代替えとなるコードの記載と比較し、読みやすい方を採用する。
ネストしているとコードを追うのに集中力が必要になる為、 ガード節 等を用いてネストを浅くする。


8章 巨大な式を分割する
鍵となる考え
　・巨大な式は飲み込みやすい大きさに分割する。
　・「頭がいい」コードに気を付ける。あとで他の人がコードを読むときにわかりにくくなる

説明変数を用いて巨大な式を分割する。


修正前
if line.split(':')[0].strip() == ""root"":


↓

修正後
username = line.split(':')[0].strip()
if username == ""root"":



要約変数を用いて巨大な式を分割する。


修正前
if (request.user.id == document.owner_id) {
// ユーザはこの文書を編集できる
}
...
if (request.user.id != document.owner_id) {
// 文書は読み取り専用
}


↓

修正後
final boolean user_owns_document = (request.user.id == document.owner_id);

if (user_owns_document) {
// ユーザはこの文書を編集できる
}
...
if (!user_owns_document) {
// 文書は読み取り専用
}



ド・モルガンの法則を用いて巨大な式を分割する。


not (a or b or c) ⇔ (not a) and (not b) and (not c)
not (a and b and c) ⇔ (not a) or (not b) or (not c)


複雑な論理条件は小さな文に分割する。
問題を「否定」したり、反対のことを考えてみたりすることが必要になる。
巨大なコードブロックにも同じ技法が使える。


9章 変数と読みやすさ
鍵となる考え
　・変数のことが見えるコード行数をできるだけ減らす。
　・変数を操作する場所が増えると、現在値の判断が難しくなる。

邪魔な変数を削除する。
　⇒ 一時変数、中間結果を保持する変数、制御フロー変数は削除する。
変数のスコープをできるだけ小さくする。
一度だけ書き込む変数を使う。


第 III 部 コードの再構成

10章 無関係の下位問題を抽出する

目標に直接的に効果があるコードか、無関係の下位問題を解決しているコードかを切り分ける。
　⇒ 無関係の下位問題を解決しているコードが相当量あれば、それらを抽出して別の関数にする。
汎用コードをたくさん作る
　⇒ プロジェクト固有のコードから汎用コードを分離する。


11章 一度に一つのことを
鍵となる考え
　・コードは1つずつタスクを行うようにしなければいけない。

分割できるタスクは分割する


12章 コードに思いを込める

プログラムのことを簡単な言葉で説明する。
　⇒ 自分よりも知識が少ない人が理解できるような「簡単な言葉」で説明する能力が大切。
コードをより明確にする簡単な手順
　1.コードの動作を簡単な言葉で同僚にもわかるように説明する。
　2.その説明のなかで使っているキーワードやフレーズに注目する。
　3.その説明に合わせてコードを書く。


13章 短いコードを書く
鍵となる考え
　・最も読みやすいコードは、何も書かれていないコードだ。

不必要な機能をプロダクトから削除する。過剰な機能は持たせない。
最も簡単に問題を解決できるような要求を考える。
定期的にすべての API を読んで、標準ライブラリに慣れ親しんでおく。


第 IV 部　選抜テーマ

14章 テストと読みやすさ
鍵となる考え
　・他のプログラマが安心してテストの追加や変更ができるように、テストコードを読みやすくする。
　・コードを完全にテストする最も単純な入力値の組み合わせを選択しなければいけない。
　・テストには最もキレイで単純な値を選ぶ。

テストのトップレベルはできるだけ簡潔にする。入出力のテストはコード1行で記述できるといい。
テストが失敗したらバグの発見や修正がしやすいようなエラーメッセージを表示する。
テストに有効な最も単純な入力値を使う。
テスト関数に説明的な名前をつけて、何をテストしているのかを明らかにする。


15章 「分・時間カウンタ」を設計・実装する

これまでの技術を用いてコードを読みやすくする実践編。


付録


高品質のコードを書くための書籍

『Code Complete 第2版〈上〉〈下〉 完全なプログラミングを目指して』 スティーブ・マコネル 著
『リファクタリングプログラムの体質改善テクニック』 マーチン・ファウラー 著
『プログラミング作法』 ブライアン・カーニハン、ロブ・パイク 著
『達人プログラマー システム開発の職人から名匠への道』 アンドリュー・ハント、 デビッド・トーマス 著
Clean Code̶̶アジャイルソフトウェア達人の技』 ロバート・C・マーティン 著



プログラミングに関する書籍

『JavaScript: The Good Parts -「良いパーツ」によるベストプラクティス』 ダグラス・クロックフォード 著
『Effective Java 第2版』 ジョシュア・ブロック 著
『オブジェクト指向における再利用のためのデザインパターン』 エリック・ガンマ、 リチャード・ヘルム、ラルフ・ジョンソン、ジョン・ブリシディース 著
『珠玉のプログラミング 本質を見抜いたアルゴリズムとデータ構造』 ジョン・ベ ントリー 著
『ハイパフォーマンスWebサイト 高速サイトを実現する14のルール』 スティーブ・サウダーズ 著
『Joel on Software』 ジョエル・スポルスキー 著



歴史的記録

『ライティングソリッドコード バグのないプログラミングを目指して』 スティーブ・マグワイア 著
『ケント・ベックのSmalltalkベストプラクティス・パターン シンプル・デザインへの宝石集』 ケント・ベック 著
『プログラム書法』 ブライアン・カーニハン、P.J.プローガー 著
『文芸的プログラミング』 ドナルド・E.クヌース 著




解説
自然に読みやすいコードを書けるようになるための3つのステップ
　1.実際にやる
　　⇒ 他の人に読んでもらう。
　2.当たり前にする
　　⇒ 続けることが大事。
　3.コードで伝える
　　⇒ 添削コミットを行う。
「自分が書いたコードってどのくらい覚えているんですか?」
「ほとんど覚えていないですよ。」
「直すときどうするんですか?わからなくなってるじゃないですか。」
「忘れても見たら簡単にわかるように書いておくんですよ。」
",False,https://qiita.com//KGG/items/e01d492cea8530446221
"「プログラマーかっこいい！」と思って入った専門学校。
2年間勉強して就職してスキルを磨くぜ！と思ったものの、
専門学校の緩い雰囲気に流されて、いつの間にか11月。
これはヤバい。
と危機感を感じ、本腰いれて色々取り組んでいくことにしました。
その中の一つとして、Qiitaでその活動を記録していきます。

今後の予定

- プログラミング学習
まずは以下の二つを。


Progate　スライド形式のやつ。

ドットインストール動画形式のやつ。

元々どちらも取り組んでいたのですが、
カバー範囲が違ったりしているので両方やっていきたいです。

- Qiitaに学習内容を記録
ノート代わりとして、後で自分が見直せるようにまとめます。

- ブログ作成
Qiitaとは別で、資格や趣味、イベント等。
自分がどのような人物なのかを第三者が理解できるように。

- 作品制作
Webアプリや、簡単なゲーム等。学習の成果として。

- githubにコードをあげる
どんどん晒していきます。

- 資格取得
先日情報セキリュティマネジメントを受験しました。

IPA 独立行政法人 情報処理推進機構

多分これが一番大切だと思います。
専門学校に通った。だけでは、「学生の間、何をしていたの？」となってしまうのと、
会社に入ってから資格を取得しようとすると時間を作るのが中々大変だと思うので。
基本情報技術者等は春秋の年二回しかないので、しっかり勉強していきます。

最後に
ここまで読んで頂きありがとうございました。
会社に依存しないプログラマーを目指して頑張っていきたいと思います。
他にも「コレはやっとけ！」といったものや、指摘事項等あれば教えていただけると嬉しいです。
",False,https://qiita.com//KokomamobuN/items/7febb4b4b4df62dca6b9
"

はじめに

上の動画のように、Oculusのコントローラでモノを選択するという、 一見当たり前 のことをやろうとしたら
Unity初心者 & VR初心者ゆえなのか とんでもなく手こずりました 。 
手順を覚えられそうにないので、記事にしたためています。
今回の実装方法が果たして最善最楽の方法なのかよくわからないので、ご存知の方いらっしゃったら教えてください。
ちなみに、Easy Controller Selectionという記事にあるUnityパッケージを使わないで、 Oculus Integrationだけを使いました。(Easy Controller Selectionのパッケージを入れると、名前が同じスクリプトなどが発生して色々面倒だったり、Oculus Integrationだけを使っている人の記事を参考しにくいなどのデメリットが見えたので)

動作確認環境

HMD: Oculus Go
Unityバージョン: 2018.2.11f11
Oculus Integrationバージョン: 1.30.1 


主な参考資料

Use GearVrController with Selection Ray
Unity’s UI System in VR
Oculus Go 開発メモ - uGUI編


実装上必要になる知識

コントローラから何かを操作する(ための当たり判定をする)にはRaycasterを使う
Unity’s UI System in VRが参考になります。

UnityにおけるRaycasterとは


ある直線上にColliderを持つ物体が存在するかどうかを判定してくれる。
非VRゲームでは、例えばクリックされた対象を特定するために使われる。
VRでは


ユーザの視点を起点とする→視線操作の当たり判定に使う
コントローラを起点とする→コントローラ操作の当たり判定に使う




UnityのRaycasterには２種類ある



GraphicRaycaster: GUI用

PhysicsRaycaster: 3D物理コンポーネント用


Oculus Integrationでは以下のRaycasterが提供されている



OVRRaycaster: GraphicRaycasterに相当するもの

OVRPhysicsRaycaster: PhysicsRaycasterに相当するもの (今回は使わなかった)




Oculusアプリにするには特別なカメラや入力手段が必要

OVRCameraRigをカメラ・コントローラトラッキングに使う


OVRCameraRigというプレハブが頭や手の動きをトラッキングしてくれる


OVRInputModuleを持つEventSystemがコントローラ入力を処理する


EventSystemは入力やRaycastを処理して、イベントを送る役割を持つ。シーンに1つしか置けない。
デフォルトではクリックなどの入力を処理するStandaloneInputModuleコンポーネントがくっついている。
Oculusコントローラからの入力を受け付けるにはStandaloneInputModuleの代用であるOVRInputModuleを使用する。




その他

レーザーを描画したい場合はLine Rendererが便利


Line Rendererは指定した２点間に線を描画してくれる




手順

事前準備

Oculus Integrationをインポートしておく
UnityでVR用開発ができるように設定しておく


3Dオブジェクト・uGUI共通

カメラの入れ替え & コントローラの表示


シーンのデフォルトカメラ(MainCamera)を削除
シーンにOculus/VR/PrefabsのOVRCameraRigを配置
OVRCameraRig/TrackingSpace/RightHandAnchor以下にOculus/VR/PrefabsのTrackedRemoteを配置 (上の画像の感じになっていればOK)


EventSystemの入れ替え

シーンにEventSystemを追加
EventSystemからStandaloneInputModuleコンポーネントを削除
EventSystemのOculus/VR/Scripts/UtilのOVRInputModuleを追加
OVRInputModuleのRay TransformにOVRCameraRig/TrackingSpace/RightHandAnchorを設定
以下のようになっていればOK




コントローラのトリガーが押されたら何かする

以下のGetDownメソッドでトリガーが押されているか判定可能

if (OVRInput.GetDown(OVRInput.Button.PrimaryIndexTrigger))
{
    // do something
}

ここまで設定が間違っていなければ、コントローラのボタン入力系イベントには対応できるようになっています。他のイベントはOculus Goのコントローラ入力の取得まとめを参照してください

3Dオブジェクト用
OVRPhysicsRaycasterを利用したイベント発火方法がわからなかったので、以下のように自作スクリプト内でPhysicsRaycasterを利用しました

コントローラを起点とした当たり判定の追加 & レーザーの表示

適当なEmptyObject(Laserと名付けた)をシーンに配置 
LaserにLine Rendererコンポーネントを追加
Line Rendererコンポーネントの設定値を適宜変更(以下は参考程度に)



Laserに新しいスクリプト(ControllerLaserRendererと名付けた)を追加
スクリプトにレーザー描画のコードを実装する(以下は例)


rightHandAnchorにはOVRCameraRig/TrackingSpace/RightHandAnchorを指定する
lineRendererにはLaser自身を指定する




ControllerLaserRenderer.cs
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class ControllerLaserRenderer : MonoBehaviour {
    public Transform rightHandAnchor = null;
    public LineRenderer lineRenderer = null;
    public float maxRayDistance = 500.0f;

    void Update ()
    {
        // 右手のコントローラの位置と向いている方向からRayを作成
        Ray laserPointer = new Ray(rightHandAnchor.position, rightHandAnchor.forward);

        // 作成したRay上にColliderがあるか判定
        RaycastHit hit;
        if (Physics.Raycast(laserPointer, out hit, maxRayDistance))
        {
            // Colliderがあれば、衝突箇所までレーザーを描画
            renderLaserToHit(laserPointer, hit);
        } else {
            // Colliderがなければ、最大長のレーザーを描画
            renderLaserFullLength(laserPointer);
        }
    }

    private void renderLaserToHit(Ray ray, RaycastHit hit)
    {
        renderLaser(ray.origin, hit.point);
    }

    private void renderLaserFullLength(Ray ray)
    {
        renderLaser(ray.origin, ray.origin + ray.direction * maxRayDistance);
    }

    private void renderLaser(Vector3 from, Vector3 to)
    {
        // Line Rendererの1点目と2点目の位置を指定する
        lineRenderer.SetPosition(0, from);
        lineRenderer.SetPosition(1, to);
    }

}




Rayがゲームオブジェクトに当たったら何かする
Raycast時のRaycastHitから衝突したGameObjectが取得できるので、それに対して好きな方法でメソッド呼び出しすればOK

Colliderのついているオブジェクトをシーンに配置する
そのオブジェクトに適当なスクリプトを設定する (以下はRayが当たった時だけマテリアルを変える例)


Hoverable.cs
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class Hoverable : MonoBehaviour {

    public Material normal;
    public Material hovered;

    private Renderer rend;

    private void Start()
    {
        rend = GetComponent<Renderer>();
        rend.material = normal;
    }

    public void OnPointerEnter()
    {
        rend.material = hovered;
    }

    public void OnPointerExit()
    {
        rend.material = normal;
    }
}



Raycast時に当たったオブジェクトに対してメソッドを呼び出しを行う


RaycastHitからゲームオブジェクトを取得
ゲームオブジェクトに対してSendMessage



GameObject pointedObject = hit.collider.gameObject;
pointedObject.transform.SendMessage(""OnPointerEnter"");


uGUI用
Oculus Go 開発メモ - uGUI編を参考にさせていただきました

SampleシーンのOVRGazePointerをPrefab化する
OVRGazePointerを自作したければ、OculusGoでEventSystemを使ってオブジェクトをクリックする方法(not uGUI)を参照してください。(私はめんどくさがってPrefab化しました。)

Oculus/VR/Scenes/UIを開く
OVRGazePointerをPrefabにする


(作業していたシーンに戻って)Canvasの設定

Canvasをシーンに配置
CanvasのRender ModeをWorld Spaceにする


Render Modeとは?という方は【Unity】uGUIのCanvasとRenderModeについてを参照してください


CanvasのEventCameraをOVRCameraRig/TrackingSpace/CenterEyeAnchorに設定
CanvasのGraphicRaycasterコンポーネントを削除
CanvasにOculus/VR/Scripts/UtilのOVRRaycasterを追加
以下のようになればOK



OVRGazePointerの設定

Prefab化(または自作)したOVRGazePointerをシーンに配置
Ray TransformにOVRCameraRig/TrackingSpace/RightHandAnchorを設定



RayがGUI部品に当たったら何かする設定

CanvasにGUI部品を設置
ボタンならばOnClick()に呼びたいメソッドを設定


最後までわからなかったこと

OVRPhysicsRaycasterを使用してコントローラ操作


OVRCameraRigに取り付けて使用するらしいが、RightHandAnchorのRayを指定する方法がわからず断念


OVRGazePointerを使用しないで動かす方法


OVRInputModuleの実装内でOVRGazePointerを参照しているところを消せば動くかもしれないが、そもそもOVR系スクリプト同士がどう関連し合っているのかわからないのであまり手を出したくない。
OVRRaycasterを使わず、GraphicRaycasterを使う方法を模索する方が良いのかもしれない。



",False,https://qiita.com//okyk/items/efca6be5bc874333e8eb
"こんにちは－！
HTMLとCSSになれてきたみなさん
自分のサイト、いつも使ってるサイトに比べて
なんていうか質素。シンプル。機械的って思いませんか。
それはなぜか、、動きがないからです。
動きってどうやって出すんでしょう
CSSだけじゃ、まだまだ、ぎこちない。
そこでHTMLとCSSに動け！変化しろ！と命令できる者こそが、、
JavaScriptなんです。
じゃあ初めからJavaScript教えてよ！と思うかもしれませんが
HTMLがないとJavaScriptはどこを変化させればよいかわからなくなってしまいます。
HTMLはWebページを構成するテキストや画像をPCに理解できるように意味づけをする役割を担ってます。
CSSが装飾できるのも、JavaScriptが素敵な変化を加えられるのも
HTMLのおかげなんですね。
では、JavaScriptっていったいどんなやつなのか、、仲良くなっていきましょう。

用語確認

オブジェクト
JavaScriptの指示を与える対象のこと。
window.alert('Hello');

これは画面(window)に‘Hello’と表示させる文章です。
しかし基本的にこの「window」オブジェクトは省略されることがほとんどです。

プロパティ
オブジェクトのもつデータを、そのオブジェクトの「プロパティ(所有物や性質)」といいます。
対象となるオブジェクトの「現在の状態」を知ることができたり、状態を変化させることができます。例えば
alert(scrollY);
=> 330

これは現在の画面が上から330pxのところにあるという意味です。
‘scrollY’のYはY軸(縦軸)のことです。

メソッド
メソッドは動きを指示します。何個か挙げてみます。
prompt('あなたの年齢は？'); //記入を必要とする質問をする
alert('決済が完了しました。'); //一方的な通知をする
confirm('予約をキャンセルしますか？') //Yes or Noで答える質問をする

これもどんな種類があるのか調べてみましょう！

文字列を書いてみよう！
それでは実際にコードを書いてみましょう
'Hello';

「Hello」と表示されているはずです。
文字列はシングルクォーテーション('')またはダブルクォーテーション("""")で囲みます。
どちらを使用しても問題ありませんが、必ずどちらかに統一させましょう。
今回はシングルクォーテーション('')を使っていきます。
また、文字列中の改行は
'Welcome\nGeekSalon'; //改行エスケープシーケンス\n
=>""Welcome
GeekSalon""

で表すことができます。

数値の計算
JavaScriptでは演算子という記号を組み合わせて四則演算などの計算をすることができます。
10 + 5; //足し算
=>15
5 - 8; //引き算
=>-3
6 * 2; //掛け算
=>12
81/9; //割り算
=>9
10 % 3; //余り
=>1

それでは先ほどの文字列との違いに注意して問題を解いてみましょう！
'3'+'3';

これは何と表示されるでしょう？予測してみてから実際に打ってみましょう。
5*(2+4)+3;


こーたさんに借金返済を迫るJavaScript
ここで問題です。こーたさんに、月500万円貸していたとします。
１年たって、あれ？いくら貸したんだっけとなったときどのように知らせましょうか。
alert('いつまでに'+500*12+'万円返してくれるんですか？');


変数について
変数とは、一時的なデータの保存場所です。
変数というハコにデータをいったん置いといて、必要な時に取り出します。
中身は記憶するデータによって変わってくるので、変数といいます。
使い方は
1.変数に名前を付ける
変数はvarキーワードで宣言する　
var name;

変数には命名規則があります！これはググってみましょう。
2.変数にデータを記憶させる
3.利用する、引き出す
これだけです！
それでは手を動かしてみましょう～

チャラいJavaScript
var kawaii;　//kawaiiという変数を用意
kawaii = 'Arisa';　//変数の中に'Arisa'を記憶させる

console.log(kawaii); //変数の中身を表示させる

なんと言ってくれましたか？？？笑
一方で、さくらちゃんに対してはなんというのでしょう。
kawaii = 'Sakura'; //変数の中に'Sakura'を記憶させる
console.log(kawaii);

え、、、、！
console.log(kawaii);
=> sakura

なんどやってもSakuraしかいわなくなった、、、！！
JavaScriptは最新のものしか覚えられないということがわかります。

魔法の鏡JavaScript
JavaScriptはユーザーから情報を得て、その情報を使って表示させることができます。
var message = 'kawaii'; 　//messageという変数の中に代入
var name = prompt('whats your name?');　//名前を聞いて変数にしまう
alert(message+name);

さてさて、「かわいいのはだぁれ？」と
=> kawaiiArisa

ええ子だ。

健康のお供JavaScript
それでは応用編に入っていきます！
BMIって知ってますか？
体重(㎏)×{身長(M)×身長(M)}で求めることができる肥満指数のことです。
この練習ではユーザーから体重と身長の情報を得てから
実際にBMIの計算をしてみたいと思います！

var weight = prompt('あなたの体重は？');　// 体重を聞いて変数に収納

var height = prompt('あなたの身長は？(ｍ)');// 身長を聞いて変数に収納

var bmi = weight/(height*height);　// BMIを求める式を変数に収納

alert('あなたのBMIは'+bmi+'です。');　//警告ダイアログに表示させる

=> あなたのBMIは【ひみつ】です。

基準は
18.5未満：やせ
18.5~25未満：普通
25~30未満：肥満（1度）
30~35未満：肥満（2度）
35~40未満：肥満（3度）
40~：肥満（4度）
だそうです（wikipediaより）
食欲の秋ですなぁ

条件ごとに仕分けする
回答によっては動きを変えたいことは多々あります。
相手のでどころを探り探りという感じです。
それでは女の子が京都までの行き方を尋ねているという設定で確認していきましょう。
以下、コピペしていきましょう。

index.html
<!DOCTYPE html>
<html lang=""ja"">
<head>
  <meta charset=""UTF-8"">
  <title></title>
  <link rel=""stylesheet"" href=""css/style.css"">
</head>
<body>
  <script src=""js/app.js"">
  </script>
</body>
</html>



app.js
var budget = prompt('京都までね。いくらあるの？');　//変数bugetにユーザーからの情報をしまう
budget = parseFloat(budget);　//文字列を数値に変換

if (budget >= 12000) {　　　　　　　//所持金が12000円以上の場合
  alert('新幹線の切符を購入');
} else if (budget >= 4000) {　　　 //所持金が4000円以上の場合
  alert('夜行バスの切符を購入');
} else {                          //所持金が4000円未満の場合
  alert('こーたさんに連絡');
}


実際にcromeで見てみましょう！
(1)20000(2)9000(3)300　で回答してみるとどうなりましたか～！

おまけ（こんなことができるようになるよ）
file:///C:/Users/ala-n/Desktop/Geeksalon/yasashiijs/sample_js/fontbomb.html

さてさてお疲れ様でした！JavaScriptとは仲良くなれそうですか？
一回では覚えられないです！何回も見て打ってみて理解していきましょう！
",False,https://qiita.com//arisa5122/items/875a5acfbfe555c11d8a
"

まずは結論を
[マウスが乗った要素の背景色を変えるよ]
http://gsken.sakura.ne.jp/jQuery/js02.html
コードは以下のとおり。
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">
    <title>マウスが乗った要素の色を変えるよ</title>
    <link rel=""stylesheet"" href=""css/rest.css"">
    <link rel=""stylesheet"" href=""css/js02.css"">
</head>
<body>
    <main>
        <div class=""list-title"">
            <h2>EQUIPMENT</h2>
        </div>
        <ul class=""list"">
            <li class=""list-item"">Backpacks & Bags</li>
            <li class=""list-item"">Ropes</li>
            <li class=""list-item"">Climbing & Safety</li>
            <li class=""list-item"">Avaianche Safety</li>
            <li class=""list-item list-item-b"">Sleeping Bags</li>
        </ul>
    </main>

    <script src=""js/jquery-3.3.1.min.js""></script>
    <script>

        $("".list-item"").hover(function(){
            //hoverメソッド。mouseenter（要素にマウスが乗ったとき）とmouseleave（要素からマウスが離れたとき）を一発で指定
            // 参考サイト（ http://blog.webcreativepark.net/2013/12/02-140448.html ）
            $(this).addClass(""selected"")
            //cssを事前に用意しておいて、そのクラスを追加（削除）する。
        },function(){
            $(this).removeClass(""selected"")
        });
    </script>

</body>
</html>

一応ですがCSSも
body{
    text-align: center;
}

main {
    width:320px;
    margin: 0 auto    
}

.list-title {
    text-align: center;
}

h2 {
    width: 240px;
    font-size: 24px;
    padding: 8px 0;
    margin: auto;
}

li {
    width: 240px;
    padding: 8px 0;
    font-size: 16px;
    margin: auto;
}

li:nth-child(odd) {  
    border: 1px #000000 solid;
}

li:nth-child(even) {  
    border-right: 1px #000000 solid;
    border-left: 1px #000000 solid;
}

.list-item-b {
    border-bottom: 1px #000000 solid;
}

.selected {
    background-color: #dcdcdc;
}


本日の課題は
タイトルにもある通り、マウスがのった要素の背景色を変えるというもの。

hover(　)について
$("".list-item"").hover(function(){
// hoverメソッド - mouseenter（ ）とmouseleave（ ）を一発で指定
    // mouseenter( ) - 要素にマウスが乗ったとき( )内のアクションを実行
    // mouseleave( ) - 要素からマウスが離れたとき( )内のアクションを実行

    $(this).addClass(""selected"")
    // addClass( ) - ( )内のクラスをcssを追加。
    // 事前に背景色を変えるcssを用意しておき、
    // そのクラスをマウスが乗った .list-item に加えることで背景の色を変える。

},function(){
    $(this).removeClass(""selected"")
    // removeClass( ) - ( )内のクラスをcssを削除。

そして、その加えられたcssというのがこちら
.selected {
    background-color: #dcdcdc;
}

hover、mouseenter、mouseleave に関しては以下のサイトを参考にさせていただきました。
http://blog.webcreativepark.net/2013/12/02-140448.html
ありがとうございます。
hover( )　よりも mouseenter( )　& mouseleave( ) の方が使い勝手がいいとのことですがなんでだろう？
検討さえつきません。

ちなみに
hover( ) のアドバイスをもらって僕が考えたのはこちら
$("".list-item"").hover(function(){
    $(this).css(""background-color"", ""#dcdcdc"")
},function(){
    $(this).css(""background-color"", ""#ffffff"")
});

hoverアクションで直接cssに手を加えて背景色を変更するというものでした。
addClass( ) 、 removeClass( ) の方がスッキリするような気がした一日でした。
",False,https://qiita.com//kenichiro_komatsu/items/4a27cb9104690940a99e
"

はじめに
GoFのデザインパターンを紹介している『増補改訂版 Java言語で学ぶデザインパターン入門』を読んで、学んだ内容についてまとめます。
※随時更新を行っていきます。

パターン一覧

生成に関するパターン

Abstract Factory
Builder
Factory Method
Prototype
Singleton


構造に関するパターン

Adapter
Bridge
Composite
Decorator
Facade
Flyweight
Proxy


振る舞いに関するパターン

Chain of Responsibility
Command
Interpreter
Iterator
Mediator
Memento
Observer
State
Strategy
Template Method
Visitor

",False,https://qiita.com//mk777/items/4e13505b654fc21c76a2
"

はじめに
GoFのデザインパターンを紹介している『増補改訂版 Java言語で学ぶデザインパターン入門』を読んで、学んだ内容についてまとめます。

TemplateMethodパターン

TemplateMethodとは
処理の骨組みをスーパークラスで定義し、具体的な実装についてはサブクラスで定義するパターンのことをTemplateMethodパターンと言います。
この内、スーパークラスで「処理の骨組みを定義しているメソッド」がTemplateMethodになります。

登場人物
TemplateMethodパターンで使用するのは以下のクラス図に登場するクラスです。


抽象クラス


AbstractClass
テンプレートメソッドを実装するクラスです。
テンプレートメソッド内で呼び出すメソッドについても宣言を行いますが、実装についてはサブクラスのConcreateClassで行います。
そのため、テンプレートメソッド以外のメソッドは抽象メソッドとなります。


実装クラス


ConcreateClass
テンプレートメソッド内で呼び出されている抽象メソッドを実装するクラスです。
ここで実装したメソッドがスーパークラスであるAbstractClassから呼び出されます。


具体例
具体例として、「手順書」と「作業Aの手順書」、「作業Bの手順書」をもとに説明します。

抽象クラス


AbstractManualクラス
AbstractManual(手順書)クラスは抽象クラスで、作業手順の骨組みを実装します。
ここではoperation()メソッドが作業手順の骨組みを実装しているメソッド(templateMethod)になります。
その他のstart()メソッド、work()メソッド、end()メソッドに関しては宣言を行い、operation()メソッド内で呼び出しを行なっていますが、実装は行なっておらず、具体的な処理についてはサブクラスのOperationAManual.classとOperationBManual.classで定義します。


AbstractManual.java
package templateMethod;

public abstract class AbstractManual{
    public abstract void start();

    public abstract void work();

    public abstract void end();

    public final void operation() {
        start();
        for (int i = 0; i < 3; i++) {
            work();
        }
        end();
    }
}



実装クラス


OperationAManualクラス
抽象メソッドのstart()、work()、end()を実装しています。
コンストラクタで文字を受け取りwork()メソッドで出力を行っています。


OperationAManual.java
package templateMethod;

public class OperationAManual extends AbstractManual {
    private char ch;

    public OperationAManual(char ch) {
        this.ch = ch;
    }

    @Override
    public void start() {
        System.out.println(""<<作業A開始>>"");
    }

    @Override
    public void work() {
        System.out.println(ch + ""を処理しています"");
    }

    @Override
    public void end() {
        System.out.println(""<<作業A終了>>"");
    }
}




OperationBManualクラス
OperationAManual.classと同様に抽象メソッドのstart()、work()、end()を実装していますが、実装の内容については異なっています。
コンストラクタで文字列を受け取り、work()メソッドで出力を行っています。


OperationBManual.java
package templateMethod;

public class OperationBManual extends AbstractManual {
    private String string;

    public OperationBManual(String string) {
        this.string = string;
    }

    @Override
    public void start() {
        System.out.println(""**作業B開始**"");
    }

    @Override
    public void work() {
        System.out.println(string + ""を処理しています"");
    }

    @Override
    public void end() {
        System.out.println(""**作業B終了**"");
    }
}



実行クラス


Mainクラス
OperationAManualとOperationBManualのインスタンスを生成し、TemplateMethodのoperation()を呼び出しています。



Main.java
package templateMethod;

public class Main {
    public static void main(String[] args) {
        AbstractManual d1 = new OperationAManual('F');
        AbstractManual d2 = new OperationBManual(""HelloWorld"");
        d1.operation();
        d2.operation();
    }
}



実行結果
Main.javaを実行した結果は以下になります。
同様のoperation()を呼び出しましたが、異なる出力がされていることが確認できます。

実行結果
<<作業A開始>>
Fを処理しています
Fを処理しています
Fを処理しています
<<作業A終了>>
**作業B開始**
HelloWorldを処理しています
HelloWorldを処理しています
HelloWorldを処理しています
**作業B終了**



メリット
TemplateMethodパターンを活用することで、処理の骨組みを共通化できます。
このようにすることで複数の作業者が各々好き勝手に処理の骨組みから作成を行い、一律にすべき部分が一律になっていない、ということが防げます。

まとめ
処理の骨組みを規定するTemplateMethodパターンに関して学びました。
以下でサンプルコードをアップしていますのでよろしければ参考にどうぞ。

TemplateMethodサンプルコード

また、他のデザインパターンに関しては以下でまとめていますので、こちらも参考にどうぞ。

[随時更新]Javaでデザインパターンまとめ


参考文献

増補改訂版 Java言語で学ぶデザインパターン入門

",False,https://qiita.com//mk777/items/acb39f8f3c3ee2f1c252
"

はじめに
最近だんだんと投稿の質が落ち、くだらない備忘録を垂れ流し続けている新卒エンジニアです。
この未経験からスタートした半年は研修とPHP/Laravel中心の業務でした。
新しく半期が始まったということもあり、弊社では新卒にも自分で決めた目標が課されます。
今回は目標にした、新しい技術を使ってアウトプットをするという目標の第０章になります。
最終的な成果としては、Vue.jsを使ってSPAのポートフォリオサイトを公開しようと思っております。
その前段で、触ったことのないVue.jsを触ってみたという投稿です。

現状のスキル
サーバーサイド言語はPHPとRubyを少し触ったことがある
フロントエンドはHTML/CSSの読み書きができる、JavaScriptも調べながらなら書ける程度です。

Vue.jsインストール
$ mkdir vue-study
$ cd vue-study/
$ npm install -g vue-cli
$ vue init webpack my-project


@567000 さんのVue.js を vue-cli を使ってシンプルにはじめてみるを見様見真似でここまでは順調に進みます。
いくつか質問されますが、vue-routerも使ってみたかったため、僕は全部Enterを押していましたw
To get started:

  cd my-project
  npm run dev

と言われたので実行します。
Your application is running here: http://localhost:8080

うまく言ったみたいです。
いざアクセス。
すると、ERR_CONNECTION_RESETのエラーが。
ネットで調べていくとどうやらESETというセキュリティソフトが悪さをしているそうです。
会社のパソコンなのでソフトの設定を緩くするわけも行かないので設定ファイルを見に行きます。
config/index.js
    host: 'localhost', // can be overwritten by process.env.HOST
    port: 8080, // can be overwritten by process.env.PORT, if port is in use, a free one will be determined
    autoOpenBrowser: false,
    errorOverlay: true,
    notifyOnErrors: true,
    poll: false, // https://webpack.js.org/configuration/dev-server/#devserver-watchoptions-

15行目くらいにありました。ここのportの番号を変えてあげるとアクセスできるようになります。
この後https://orizuru.io/blog/vue-js/vue-01/の内容を丸コピしながら学習をしました。

やったこと

routerを使っての画面遷移
データバインディングによるテキストの受け渡し

まずはsrc/routes/index.jsに見様見真似でルーティングを追加して行きます。

import Vue from 'vue'
import Router from 'vue-router'
import HelloWorld from '@/components/HelloWorld'
import About from '@/components/About'
import Skill from '@/components/Skill'

Vue.use(Router)

export default new Router({
  routes: [
    {
      path: '/',
      name: 'HelloWorld',
      component: HelloWorld
    },
    {
      path: '/About',
      name: 'About',
      component: About
    },
    {
      path: '/Skill/:msg',
      name: 'Skill',
      component: Skill,
      props: true
    }
  ]
})

この時点で、ここに遷移のページ分だけ足せばいいんだなと理解。
今度は対応するvueファイルを追加して行きます。
src/components/About.vue

<template>
    <div class=""hello"">
        <h1>About</h1>
        <p>
            <router-link to=""/"">HelloWorld</router-link>
        </p>
    </div>
</template>

<script>
    export default {
        name: 'About',
    }
</script>


src/components/Skill.vue
<template>
    <div class=""hello"">
        <h1>Skill</h1>
        <h1>{{ msg }}</h1>
        <p>
            <router-link to=""/"">HelloWorld</router-link>
        </p>
    </div>
</template>

<script>
    export default {
        name: 'Skill',
        props: [ 'msg' ]
    }
</script>


src/components/HelloWorld.vue
<template>
    <div class=""hello"">
        <h1>{{ msg }}</h1>
        <input type=""text"" v-model=""msg"">
        <p>
            <router-link to='/About'>About</router-link>
            <router-link :to=""'/Skill/' + msg"">Skill</router-link>
        </p>
    </div>
</template>

<script>
export default {
  name: 'HelloWorld',
  data () {
    return {
      msg: 'Welcome to Your Vue.js App'
    }
  }
}
</script>

<!-- Add ""scoped"" attribute to limit CSS to this component only -->
<style scoped>
h1, h2 {
  font-weight: normal;
}
ul {
  list-style-type: none;
  padding: 0;
}
li {
  display: inline-block;
  margin: 0 10px;
}
a {
  color: #42b983;
}
</style>


このファイルは<router-link :to=""'/Skill/' + msg"">Skill</router-link>のところでmsgを渡している。
これがSkill.vueに行った時に渡したものを表示してくれる。

まとめ
浅い理解だが、
（親）index.html ←　（子）　main.js ←　（孫）　HelloWorld.vueという入れ子構造でデータがimport,exportされているのかな。。。となんとなく雰囲気はつかめた（？）
とにかく、実際に動くページができたのでよかった。半年でしっかり成果作って完成物はもっと丁寧にアップします。
",False,https://qiita.com//kazuki5555/items/75fbe297bc77e12b6021
"インストールがサッと終わりとても便利だったので、実践記録として残したいと思います

step1 composerを使いlaravel-debugbarをインストール
composer require barryvdh/laravel-debugbar


point
Laravel5.5以前は、読み込むServiceProviderやFacadeをconfig/app.phpに記述する必要がありました。
こちらのライブラリはLaravel5.5から導入されたAuto Discavaryに対応しているため、記述する必要がなくなりました。

step2 デバッグバーの設定ファイルを生成
php artisan vendor:publish --provider=""Barryvdh\Debugbar\ServiceProvider""


step3 .envにデバッグバーを有効化するコードを追加
詳しくは、step2で生成した設定ファイルで定義されています。

.env
DEBUGBAR_ENABLED=true



step4 composerの読み直し
composer dump-autoload


step5 config等のキャッシュをクリア
&&  で繋いで複数のコマンドを書くと、前のコマンドの実行後に次のコマンドを順番に実行してくれるので、とても便利です！
php artisan cache:clear && php artisan config:clear


step6 ビルトインサーバーを起動
今回は開発環境でDebugbarを使用したいので、ビルトインサーバーを起動します。
php artisan serve --host 0.0.0.0 --port=80


step7 デバッグバーの表示を確認
ブラウザのウィンドウ下部に、デバッグバーが表示されます！

葉っぱのマークをクリックすると、そのページで呼んでいるbladeテンプレートがわかります。

この他にも、
矢印のマークをクリックすると、ルーティング関連の情報がブラウザ上で確認できたり

クエリを発行している場合はSQL文がそのまま表示されます

番外
ルーティングの設定次第ではDebugbarのjsとcssがうまく読み込まれず、表示されないこともあるようです。
今回はログイン画面のみDebugbarが表示され、ログインされるとDebugbarが表示されないという残念な結果に...。
使えるように試行錯誤していきます
",False,https://qiita.com//tom01938572/items/f34b0882bacf6995cd82
"

経緯
なんやかんやで機械学習をしなくてはいけなくなったため、やってみたものの奥深いっていうか、ごちゃごちゃしてきたためまとめてみた。
英語のドキュメントを読むこと多いから、英単語も載せていくよ

知識レベル
取り敢えず、この記事は確率統計およびDeep Learningの基本的な知識があるものとして話を進める。Deep Learningについての知識は下の本を読んで理解していればいいかな。この本はすごくわかりやすいのでおすすめ。
ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装

環境
macOS Mojave
Python 3.6.2
pip3 10.0.1

正規分布(normal distribution)

random_normal
tf.random_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.float32,
    seed=None,
    name=None
)

正規分布から乱数を出力する　
大体名前から引数の意味はわかると思うので省略する。seedは話が長くなりそうだから他の記事で書くかも...

使用例
# -*- coding:utf-8 -*-
#!/usr/bin/env python3

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

sess = tf.InteractiveSession()
x = sess.run(tf.random_normal(shape=[20000],mean=0.0,stddev=1.0,dtype=tf.float32))
fig = plt.figure()

ax = fig.add_subplot(1,1,1)
ax.hist(x,bins=100)
ax.set_title('random_normal')
ax.set_xlabel('x')
ax.set_ylabel('y')

plt.show()



truncated_normal
tf.truncated_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.float32,
    seed=None,
    name=None
)

切断正規分布から乱数を出力する
標準偏差の2倍までを出力

使用例
# -*- coding:utf-8 -*-
#!/usr/bin/env python3

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

sess = tf.InteractiveSession()
x = sess.run(tf.truncated_normal(shape=[20000],mean=0.0,stddev=1.0,dtype=tf.float32))
fig = plt.figure()

ax = fig.add_subplot(1,1,1)
ax.hist(x,bins=100)
ax.set_title('truncated_normal')
ax.set_xlabel('x')
ax.set_ylabel('y')

plt.show()



random_uniform
tf.random_uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.float32,
    seed=None,
    name=None
)

一様分布から乱数を出力する

使用例
# -*- coding:utf-8 -*-
#!/usr/bin/env python3

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

sess = tf.InteractiveSession()
x = sess.run(tf.random_uniform(shape=[20000],minval=-1.0,maxval=1.0,dtype=tf.float32))
fig = plt.figure()

ax = fig.add_subplot(1,1,1)
ax.hist(x,bins=100)
ax.set_title('random_uniform')
ax.set_xlabel('x')
ax.set_ylabel('y')

plt.show()



matplotlibの解説を少し
plt.figure()

これで何も描かれていないウィンドウを作成。
add_subplot(nrows, ncols, index, **kwargs)

戻り値 : Axesのサブクラス
index : その図の場所
nrows = 2 , ncols = 3 の時



1
2
3




4
5
6



hist(x,bins=100)

ヒストグラムを作成。
bins : 棒の本数
詳しい説明はこちら

InteractiveSession()とSession()の違い
random_uniformは次のようにして実装することも出来る

使用例
# -*- coding:utf-8 -*-
#!/usr/bin/env python3

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


x = tf.random_uniform(shape=[20000],minval=-1.0,maxval=1.0,dtype=tf.float32)
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
with tf.Session() as sess:
    y = x.eval()

ax.hist(y,bins=100)
ax.set_title('random_uniform')
ax.set_xlabel('x')
ax.set_ylabel('y')

plt.show()

さっきと違ってrun()がeval() InteractiveSession()がSessionになってる。
結論から言うと、どちらも同じ。
このサイト様が詳しく説明してくれてる。とても参考になりました。
Tensorflow run() vs eval() と InteractiveSession() vs Session()

英単語



英語
日本語




normal distribution
正規分布


mean
平均


standard deviation
標準偏差



",False,https://qiita.com//kai0706/items/bd3816342d725508fec2
"いつもはSourceTreeを使ってGit管理しているのだけど、そのSourceTreeがバグってしまってどうしようもない＆時間がないときのためのGitターミナル操作講座です。
操作編はまた改めて書きます。

まずPCにgitが入っているかどうか調べましょう
$  git --version

すでに入っている場合は数字（gitのバージョン）が表示されます
 入っていなかったらgitを入れましょう。
Git - Gitのインストール

gitの最低限の設定をする
gitが入っていることが確認できたら最低限の設定をしましょう。
gitに紐付けられるメールアドレスとパスワードが入っているかどうかを確認します
$ git config --global user.name

$ git config --global user.email

それぞれのデータに入っている値が返ってくるはずです。
入っていないor設定が間違っていれば設定をします。
$ git config --global user.name “HOGE（ユーザー名）”

$ git config --global user.email “hoge@hoge.jp（メールアドレス）”

また、文字が全て同じ色で表示されると大変読みづらいため、
Git関連で表示される文字に色をつけましょう。
$ git config --global color.ui auto

設定はここまでで終了です。
",False,https://qiita.com//nabe_kurage/items/b5873f1f38ec92c50db8
"

単一の結果を返す Python の lambda にて複数 (行) の処理を実行させる
JavaScript のアロー関数っぽく実行したかった・・・

実行結果をまとめちゃえばいいじゃん (いいじゃん)

test.py

def exe_callable_lambda(callable_lambda):
    if callable(callable_lambda):
        return callable_lambda()


def sou_desu(): return [
    print('ou'),
    print('iesu'),
]


exe_callable_lambda(
    lambda:
    [
        print('ai'),
        print('amu'),
        print('san'),
    ]
)

exe_callable_lambda(
    sou_desu
)



じっこうけっか
$ python test.py
ai
amu
san
ou
iesu

",False,https://qiita.com//ysKuga/items/9e60a1b4aefa48febd30
"アプリの開発の時はMySQLを使っているのだが、文字列型の適切な選択や、判断方法についてはあやふやだったので、まずはcharとvarcharについて復習してみた

MySQLの文字列型の種類
MySQLには大きく分けて、以下の8つの型が存在する。

CHAR
VARCHAR
BINARY
VARBINARY
BLOB
TEXT
ENUM
SET


CHAR型(M)

固定長文字列
Mで文字数を指定
0から255文字まで可能
取得する時に末尾についた空白は削除された上で取得される

[補足] 固定長文字列

指定した文字数以下の文字を格納した場合には文字列の末尾に空白を必要なだけ付け加えて指定の長さの文字列として格納する

consoleで確認(Rails)
childテーブルにはchar型のcharカラムが存在するとする
irb(main):005:0> a.char = ""a""*254
irb(main):006:0> a.save! #=> 成功

irb(main):005:0> a.char = ""a""*255
irb(main):006:0> a.save! #=> 失敗

irb(main):007:0> a.char = ""あ""*254
irb(main):006:0> a.save! #=> 成功

irb(main):009:0> a.char = ""あ""*255
irb(main):006:0> a.save! #=> 失敗

文字数に依存していることがわかる

VARCHAR型(M)

可変長文字列
Mはバイト数を指定する
0~65535バイト

[補足] 可変長文字列

末尾に空白を付けるようなことはしません。また現行のバージョンでは末尾に空白がある文字列であっても空白が付いたまま格納される

consoleで確認(Rails)
aモデルにはvarchar型のvarcharカラムが存在するとする
varchar(21800) DEFAULT NULLで設定した
irb(main):024:0> child.varchar = 'b'*21800
irb(main):025:0> child.varchar.bytesize #=> 21800
irb(main):027:0> child.save! #=> 成功


irb(main):024:0> child.varchar = 'b'*21801
irb(main):028:0> child.varchar.bytesize #=> 21801
irb(main):027:0> child.save! #=> 失敗

irb(main):024:0> child.varchar = 'あ'*21800
irb(main):030:0> child.varchar.bytesize #=> 65400
irb(main):027:0> child.save! #=> 成功


irb(main):024:0> child.varchar = 'b'*21801
irb(main):032:0> child.varchar.bytesize #=> 65403
irb(main):027:0> child.save! #=> 失敗

bytesizeメソッドでbyteサイズが測れる

VARCHARは65532までが最大有効長なのに、なぜ今回は21800にしているか
UTF-8は最大3バイト食うので、utf-8を使っている場合は、入力文字が全て3バイトだった時の文字数しか使えない！！
なのでMAXでもVARCHARでは、 65532➗3　= 21845文字になります。

VARCHARで、バイト数を指定する時の決め方
tokensテーブルにtokenをvarcharで保存するとする時、Railsではマイグレーションを下記のように記述する

db/migrate/20181029165624_create_tokens.rb
class CreateTokens < ActiveRecord::Migration[5.2]
  def change
    create_table :tokens do |t|
      t.string :token, limit:1500

      t.timestamps
    end
  end
end


今回は、tokenのバイト数の指定を1500としているが、バイト数の決め方の判断として
サンプルtokenのバイト数を計測し、保存するであろうtokenが必ず治るバイト数を指定するのが望ましいと考えられます。
$ sample_token = ""hoge...........""
$ sample_token.bytesize #=> 15

# sample_tokenが暗号化前の時は、
$ len   = ActiveSupport::MessageEncryptor.key_len
$ salt  = SecureRandom.random_bytes(len)
$ key   = ActiveSupport::KeyGenerator.new('password').generate_key(salt, len)
$ crypt = ActiveSupport::MessageEncryptor.new(key)   
$ crypt.encrypt_and_sign(sample_token).bytesize # => 138になる！

暗号化の方法に関してはこちらを参照にしました
https://api.rubyonrails.org/v5.2.1/classes/ActiveSupport/MessageEncryptor.html
",False,https://qiita.com//TT-nasu/items/55fe283e4f2ed8d51e88
"最近フロントエンドの技術を学びたく、とりあえずVueから始めてみたのですが、なんて便利なんだ！！と個人的にはお気に入りです。
といっても他のフロントエンドの技術を学んでいないので生まれたての雛状態なんですが...。
そんなVueを学習中の私が、早めに知っておきたかったことを簡潔にまとめておきます。
詳細は公式ドキュメント等を参照してください。

axios

GitHub
Vue.js axiosを利用したAPIの使用 


Promise based HTTP client for the browser and node.js

と書かれているようにブラウザやNode.jsで利用できるPromiseベースのHTTPクライアントです。
export default {
    methods: {
        getRequest() {
            axios.get('/myapi?param=value').then(response => {
                //レスポンス取得後の処理
            }).catch(error => {
                //エラー発生時の処理
            })
        },

        postRequest() {
            axios.post('/myapi', {param:value}).then(response => {
                //レスポンス取得後の処理
            }).catch(error => {
                //エラー発生時の処理
            })
        },
    }
}

Edge,IEだとPromiseのfinallyがサポートされていないのでローディング画面を作る場合には考慮が必要そうです。

Vue Router

Vue Router

Vue.jsの公式のルータです。
ルーティングとコンポーネントを紐づけることにより、このルーティングの場合はこのコンポーネントを表示するといったことができます。

app.js
const routes = [{
    path: ""home"",
    component: HomeComponent
},
    path: ""app"",
    component: AppComponent
}]

const router = new Router({
    mode: history,
    routes
})

const app = new Vue({
    el: ""#app"",
    router
})



app.html
・・・
<div id=""app"">
    <router-link to=""/home"">Home</router-link> <!-- 押すとHomeComponentが表示される -->
    <router-link to=""/app"">App</router-link>   <!-- 押すとAppComponentが表示される -->
    <router-view />
</div>
・・・


ルートのネストやルートパラメータ、transitionの設定などできることが多いのでSPAを作るうえで非常に重宝しそうです。

Vuex

Vuex

アプリケーションの状態管理を行うためのライブラリです。
const store = Vuex.Store({
    state: {
        val: 0
    },

    getters: {
        doubleVal (state) {
            return 2 * state.val
        }
    },

    mutations: {
        setVal (state, payload) {
            state.val = payload.val
        }
    },

    actions: {
        setValAsync ({commit}, payload) {
            return axios.get('/myapi').then(response => {
                commit('setVal', response.data)
            }).catch(error => {

            })
        }
    }
})

const app = new Vue({
    el: ""#app"",
    store
})

ステートでアプリケーションの状態を保持します。
ゲッターでステートから別の値を算出します。
ミューテーションでステートを更新します。
アクションで非同期処理や外部APIとの通信を行います。必要に応じてミューテーションを使用してステートを更新します。

vuex-router-sync

GitHub

Vue Router のroute情報をVuexのステートとして状態管理することで、ゲッター、ミューテーション、アクションでもroute情報を参照できるようになります。
sync(store, router)   //stateにroute情報を設定

//state.route.path, state.route.params, state.route.queryが参照できる


vuex-persistedstate

GitHub

通常リロードをするとVuexのステートは初期状態に戻りますが、vuex-persistedstateを利用するとローカルストレージにステートが保存され、リロードをしてもリロード前の状態に戻してくれます。
import createPersistedState from ""vuex-persistedstate""

const vuex = new Vuex.Store({
    state: state,
    getters: getters,
    mutations: mutations,
    actions: actions,
    plugins: [createPersistedState()]     
})


VeeValidate

VeeValidate

フォームのバリデーションを簡単に行うことができようになります。（ざっくり）
日本語用のメッセージも用意されていますが、自分でカスタムができます。
バリデーションも自分用に作成することができます。
import VeeValidate, {Validator} from 'vee-validate'

const dictionary = {
    message: {
        required: (n) => n + 'は必須項目です',
        min: (n,e) => n + 'は' + e[0] + '文字以上で入力してください'
    }
}

Vue.use(VeeValidate)
Validator.localiza(""jp"", dictionary)
Validator.extend('password', {
    getMessage: 'パスワードは大文字、小文字、数字を混在した8文字以上で入力してください',
    validate (val) {
        let regex = new RegExp(/^(?=.*?[a-z])(?=.*?[A-Z])(?=.*?\d)[a-zA-Z\d]{8,100}$/)
        return regex.test(val)
    }
})

<input 
    type=""text"" 
    name=""id"" 
    v-validate=""'required|min:4'""
    data-vv-as=""ID"">
<br>
<p>{{ errors.first(""id"") }}</p>

<input
    type=""password""
    name=""password""
    v-validate=""`required|password'""
    data-vv-as=""パスワード"">
<br>
<p>{{ errors.first(""password"") }}</p>

判定順序はv-validateの記載順になり、errors.first(name属性)によって最初のエラーが取得できます。
エラーの項目名はデフォルトではname属性になっていますが、data-vv-asで変更することができます。

Bootstrap Vue

Bootstrap Vue

VueでBootstrapが使用できます。…ハイ。
コンポーネントもそれなりに用意されているのでデザインはある程度楽になるかもしれないです。
個人的にはコンポーネントの構築をしっかり学びたいので現在は使用していません。

Vue Material

Vue Material

マテリアルデザイン用のコンポーネントが多数用意されています。
個人的にはBootstrapよりこちらが好みです。

Buefy

Buefy

Bulmaをベースに作成されたコンポーネントが用意されています。
上記2つもそうなのですが、個人的には使うというよりコンポーネント構築の参考にしています。。

Vue.js devtools

GitHub
chrome ウェブストア
Firefox Add-ons

ブラウザ上でコンポーネント、Vuex、イベント履歴を確認することができます。
Google Chrome、Firefoxのプラグインとして提供されています。

おわりに
正直まだ始めたばかりなので試行錯誤な毎日です。
今回上げたものもまだまだちゃんと理解できていません。
とりあえず何か一つ形にできれば良いな。
Vueについておススメの情報があれば教えていただければ幸いです。
",False,https://qiita.com//b1san/items/28100a2734e59aa45beb
"

サウンドプログラミングしたい。
ゴリゴリの文系、音楽の素養ナシのwebエンジニアが0から音楽系サウンドプログラミングを学んでいく話です。ちなみに最終目標はコーラスを合成するGUIアプリ。道のりとおし。
それまでに学んだことをぼちぼち落としていきたいと思っています。

サウンドプログラミングとは?
サウンドプログラミングと一口に言っても、音声認識だったり音楽だったり、電話の信号変換の話もあったりします。とにかく幅が広い。
私が指しているサウンドプログラミングとは、デジタルオーディオの世界の方です。

誰向けか?

サウンドプログラミングに興味があるけどどこから手をつけるべきかわからない人
数学ができない人
プログラミングはやったことある人
シンセサイザーを自作してみたい人


さあ始めよう!
しかし、いざサウンドに手を出してみると、知らなければならない情報が多すぎて何を勉強したらいいのかわからないのも事実。そもそもこの業界のエンジニアが少ないのか企業機密になってるんだかで、エンジニア向けのまとまった参考文献があまりありません。
今回はまず

足がかりとなった情報源の紹介
使用している言語の紹介

をしていきます。

ひたすら知識を詰めよう..

MIDI検定
これは音楽制作の知識が全くない場合はやるべきです。どういう風に音楽がデジタルで作られていくのかなど、音楽制作全般の知識が身につきます。MIDIはサウンドエンジニアの手となり足となる規格です。この勉強をすると、だいたいどうやって設計していくかイメージが取れるようになるでしょう。
私が勉強に使っている本です。
ミュージッククリエイターハンドブック
数字が出てこなくて読みやすい!嬉しい!

MIDI規格書
MIDIは最近になってようやく規格自体がオープンになったらしいです。規格書はPDFで無料ダウンロードできるようになっています。これもシンセサイザーなどの音がどうやってできているのか知る手がかりになります。
私はまだ読み途中ですが...。

数学の勉強
中学時代から数学ができなかった私ですが、サウンドの世界はもろ数学です。文系が頑張る話ではじめましたが、文系が理数系に転向するぐらい勉強する必要がありそうです。
現段階に来るまでに利用した数学は中学〜大学レベルの広範に及びます。大学レベルは理解することを諦めました。でも高校の正弦波あたりはきちんと理解しておいた方がいいです。音は波ですから。
以下は私が勉強に使っている参考書です。


中学数学
数学の解き方をひとつひとつわかりやすく。

高校数学
忘れてしまった高校の数学を復習する本―高校数学ってこんなにやさしかった!?

大学数学
マンガでわかるフーリエ解析



サウンドプログラミング参考書
私はまだ一冊しか購入していません。なぜなら色々買っても数式だらけで???となるだけだから。
ただこの本は数字がわからなくてもCが読めればある程度言っていることがわかる本です。
サウンドエフェクトのプログラミング　Cによる音の加工と音源合成 

使用言語について
サウンドプログラミングの参考書をamazonで漁るとC言語の本が多く出てきます。
だがしかし、webエンジニアにCは難しすぎる。
GCが標準装備された言語で文字列結合をこちょこちょやっているような人間がひょいとできるような代物ではないのです。本当に。
読んでもよくわからないというのが関の山。

Rustってすごいね
そこで私はRustでCのサウンドコードをリバースしながら勉強しています。Rustなら安全なメモリ管理もできるし、よくわからないエラーに延々と悩まれされることもないです。だってコンパイラーが全部教えてくれるからね!
でもヒープ領域とかスタックとかまだうまく使えてないから配列はスライスとベクターしか使ってません。先にメモリ領域確保とかって話まだよくわからない。

音声合成はRust
先にRustについて紹介しましたが、私がRustを主に使っているのは音声合成の部分です。sin波形の関数だったりノコギリ波だったり、基本的に合成というのは数学演算になってきます。速度、安全性、静的型づけってところでかなりやりやすいかなと。
まだ新しい言語ですが、数学系やwaveファイル操作系のcrateはあるので安心して使えますよ。

解析はpython
私はコーラスを合成したいので、人間の肉声を録音し、スペクトラム分析、ケプストラム分析にかけて声紋分析をしています。
基本的に分析系はpythonがライブラリ最強なのでいいかなと思います。最初マイク入力もpythonにしていましたが、処理が遅すぎてやめました。この子はプロットとcsv出力だけできればいいです。

マイク入力はC++
なぜC++かというと、答えは簡単!open frame works(以降of)が使えるから!
ofは割合アーティスト向けなライブラリですが、マイク入力値をリアルタイムで波形表示したり、リアルタイム音高判別なんかもやりやすかったです。これはツールを自作したのでまたいつかお伝えしたいです。ただ個人的にC++はエラーメッセージの不親切さとか安全面の不安とかがあってアプリ制作には使いたくないところ。

GUIはnodejs
先に言っておきます。RustでGUIはまじ無理。リスカしよ。って感じです。
GUI系のライブラリはまだいまいちうまく噛んでくれません。Rustが急速にバージョン変わったりするからかもしれませんが、とにかくしんどいです。
私はnodejsのGUIライブラリNW.jsと、Rustとnodejsのツールチェインをしてくれるneonを使ってなんとか組み合わせ中です。もしかしたらどこかで詰むかも..。

今日はここまで
ざっと概要を説明したつもりですが、ここは触り中の触りです。
今後がっつり技術的な話をしていこうと思います。Cの参考書をRustで書き直したコードについても説明したいですし。
サウンドやってみたいなーって人に種を巻けてたら嬉しいです。お疲れ様でした。
",False,https://qiita.com//kamiro/items/62f648f7201cdb0097cf
"

はじめに
こんにちは。
みなさん、noteは使っていますか？
情報発信やコンテンツの販売場所として今や大人気のnoteですが、
記事によっては縦読みの方がいいなと思い、適当なスクリプトを書きました。
タイトルは大げさなので、お手柔らかに、、🙏笑

TL;DR

noteで適当な記事を開く
下記のスクリプトを実行する
noteがbookに！！（個人差あり）

var b_props = {
  writingMode:'vertical-rl',
  width:'1000px',
  overflowY:'scroll',
  margin:'0 -200px',
  textOrientation:'upright'
}
$('.ns-note .note-body p').css('height', '680px');
$('.ns-note .note-body img').css({'cssText': 'height: 100%!important;'});
$('.body-wrapper').css(b_props);


対象

noteを縦読みしたい方

以上

感想
CSSってすげえ〜〜
",False,https://qiita.com//nake226/items/0974a96865833c31b45a
"

発端
ぶっちゃけ僕自身も「ど」が30個付くくらいWebに関しては初心者なのですが、それでも50個くらいつく人よりは多少の知識はあるもので、HTMLについて友人に教える機会が少なからずあります。
教えていく中で「タグとか属性とかってどうやって覚えればいいの？」と質問をされ、「やんわりこういうってのがあればそんとき調べりゃいいから別に覚えなくていいよ」と言ったものの、やはり覚えているに越したことはありません（作業効率も上がるしね）。
僕自身Emmetとか使ってるのでa要素の属性とかlink要素の属性とかぶっちゃけ覚えてません。勝手に挿入されるますし。ただたまにEmmetとか入ってないところであたふたすることが稀にあるので、これを機会に少しは覚えようかなと思います。

覚え方
HTMLってよくわかんない名前多いですよね。htmlとかheaderとかは直接的なんでまだしも、aとかiとか1文字だけだとまじで何言ってるかわかりません。
でもaにもiにもその名前になった理由はあるはずです。意味（語源？）を調べることで、そのタグに対する理解を深めていきましょう。ニックネームをつければ親近感が湧くように、本来の名前を知ることでタグもニックネームじみてきてきっと親近感が湧き自然と頭に入るはずです。

名前 ⇔ 由来



名前
由来
意味
備考




rel
relation
関係/関連



href
hyper reference
ハイパー参照
直訳 ハイパーリンクのハイパー


a
anchor
錨
anchorを置いて目印にするという意味


p
paragraph
節/段落



div
division
境界/仕切り



alt
alternate
代わり
imgが表示されない代わりに～


h
heading
表題/見出し



br
line break
次の行へ



em
emphasis
重点/主張



s
strikethrough
取り消し線



i
itaric
イタリック
イタリック体のイタリック


iframe
inline frame
列をなして
IT業界では文中に埋め込む意味で使用


ul
unordered list
順序付けなしリスト



ol
ordered list
順序付けありリスト



pre
preformatted
整形済み



q
quotation
引用





あとがき
さっと自分が覚えたいものだけ並べたので、「他にこういうのあってもいいんじゃないの？」なんてのあったらどしどしコメントください。ただ調べてみるとやはり要素の意味と名前の意味が一致しているものが多く（当然ですが）、しっかり覚えてると文書構造とか意識するときにも役に立つかもしれないですね。
あと僕のTwitterです。あんま更新してないけど見てね。
Twitter : @uguni123 
",False,https://qiita.com//uguni123/items/1b4974b692c4cafd4874
"pythonモジュールのインストールについて扱うページです．
本記事は一応WindowsOSベースで記述していますが，UNIX系OSでも流れはほとんど同じです．
本記事内で <ほげほげ> と書かれていたら，ケースにより文字列が違ってくることを意味します．
記事の抽象度を上げるために，「おそらく類似ケースすべてに適用できる手続き」のみを記載していますので，細かいケースのエラーに対するリカバーは当記事では行えません．ご容赦ください．

正直これでどうにかなる
pythonはすでに導入済みであることを想定しています．
コマンドプロンプト立ち上げてこれを打つと，ほぼ確実に成功します．
pip install <module-name>

バージョン互換性ミスったので消したい，という方はこちら
pip uninstall <module-name>

さすがにこれだけだと記事にする価値がまったくないので，
pipだとうまくいかなかったりする場合についての説明を加えます．
私の環境ではC/C++のファイルをpythonのファイルへ変換しているようなモジュール，
具体的にはpyOpenCLやpyCUDAあたりはpipでどうにもなりませんでした．
そこで，「GitHubからフォルダごと持ってきて自分でビルドする」方法を以下に記述します．

自分でビルドする
ここからは「C/C++の関数をpythonで使えるようにする系のモジュール」に絞った記述です．
C/C++のコンパイラが後々必要になるので，Visual C++のランタイムを入れておいてください．
gitも必要です．gitはお好きな方法で入れてください．
(zipもらってきてもいいんですが，どちらにせよ後でシェル叩くことになるので最初からシェルでいいんじゃないかなー，という考えです)
gitを入れたらPowerShellなりgit bashなりでPythonモジュールのあるディレクトリに移動します．
(多分，<YourHomeDir>\AppData\Local\Programs\Python\Python<Version>\Lib\site-packages)
そうしたら，欲しいパッケージ名でググってGitHubページへ移動します．
GitHubページのやや右上あたりに「Clone or Download」という緑のボタンがあるのでそこをクリックします．
アドレスが出ますので，それを控えてから，シェルを次のように叩きます．
git clone --recursive <adress>

--recursiveはサブモジュールがあったら一緒に引っ張ってきてくれるオプションです．
引っ張ってきたら，モジュールのディレクトリが増えているので次のように叩きます．
cd <moduleName>
python configure.py

こうすると，ディレクトリ内にsiteconf.pyというファイルが新しく生成されています．
お持ちのエディタで開いてこんな記述を探してください．

siteconf.py
CL_INC_DIR = ['<必要なヘッダのパスをここに']
CL_LIB_DIR = ['<必要なlibのパスをここに']


入れたいモジュールが「もともとC/C++の関数をpythonで使えるようにする系」であるならば書いてあります．
元のC/C++ライブラリをインストールしてきて，"".h"" "".lib""というファイルを探してきます．
上のCL_INC_DIRにヘッダファイルのパス，下のCL_LIB_DIRにlibのパスを書きます．
そうしたらファイルを保存，シェルを次のように叩きます.
python setup.py build

すると文字列がダーッと出てきます．
ここで，なんだか「プログラムを止めたければCtrl-Cを押してね」的な文言と一緒に
プログラムがカウントダウンを始めた場合，Ctrl-Cを押して中断して，文言を読んでください．
大抵は，「この作業をやってないからビルドはきっと失敗するよ」と言っています．
文章をよく読んで指示に従ってから，上述のコマンドを打ち直してください．
(コマンドが書かれていたらそれを反射で打ち込んでいいと思います)
ビルドが始まると大量にwarningが出てきますが，たいていは無視していいです．
途中でビルドに失敗したら，エラーをよく読んでください．
大抵「ヘッダファイルがない」か「libファイルがない」のどちらかです．
対応するファイルを先ほど指定したパスの下に入れて(本当はよくないけど...)やり直します．
ビルドに成功したら，最後の仕上げです．次のコマンドを打ちます．
python setup.py install

これで，pythonで使えるモジュールが生成されます．

おわりに
正直なところ，直接ビルドするのはとっても厳しかったです．
何しろ，普通であればpipの一行で済むことに対して

コンパイラの準備
対応ヘッダとライブラリの準備
コマンドラインの扱い
(英語)
(エラー対処力)

あたりが必要になってきます．
本記事は最初に述べた通り，細かいケースの対応力がないため
実際のところはモジュールの導入で困っている方の助けにはならないかと思われます．
ただ，抽象的に「大体こんな流れです」というページが個人的に欲しかったので書きました．
ざっくりとした知識程度に受け取っていただければ幸いです．
",False,https://qiita.com//ANNEX_IBS/items/6ffde6c379f5db435880
"

前提
この記事は以下の項目全てに当てはまる方に推奨する言語として、
JavaScriptを推奨してほしくない理由を述べます。

プログラミング初心者
特段目的はないがいずれかのプログラムをちゃんと勉強したい人
独学


JavaScriptが初心者に向かない理由
JavaScriptはWebエンジン上で動作するスクリプト言語です。
C/C++/Javaなどのコンパイル言語ではないため、
初心者には環境構築も容易で、特段HTMLやCSSも簡易なものであれば、
難しいといったことはないから初心者にオススメできる。
確かに数年前はそうでした。
今もそう捉えようと思えばそう捉えられないことはないです。
じゃあなぜ推奨しないか。
初心者がきちんとJavaScriptを勉強しようとするには、
JavaScriptに関する情報量が多すぎるからです。
以下にその事例をあげます。

jQueryの甘い蜜
一度JavaScriptをきちんと勉強しようとすれば、
十中八九初心者は、技術記事やアドバイスなどで
この単語を見かけると思います。
jQueryはJavaScriptのライブラリで、
主にDOM（Document Object Model）を操作する処理や、
Ajax（非同期なクライアントとサーバとの間の通信）処理を容易に、
ブラウザエンジンごとの仕様を気にかけることなく行うことができます。
jQueryを否定するわけではありませんが、
昨今のWebエンジンはブラウザエンジンごとの仕様が以前より
改善されてきているため、jQueryを用いる場面もかなり減ってきています。
ですがこれらの事実は勉強したからわかる話であって、
JavaScript初学者は、
技術記事でよくみかけるであろう煽り文句とともに、
jQueryの直感的な操作のみに焦点を当ててしまい、
それが便利でとても魅力的なものに見えてしまう。
クライアントサーバモデルやAjaxという
言葉にも触れられる機会は少ないでしょう。

数々のライブラリやフレームワークの乱立
jQueryの甘い蜜を、
なんとか啓蒙記事を見るなどして乗り越えたら、
次に直面するのがこの問題。
Node.js？　Vue.js？　React.js？　AngularJS？　 ES5？　ES6？
はてなの連続です。
どれが一体JavaScriptと呼ばれるものなのでしょうか。
言語仕様という言葉もわからない初心者にとって
この問題はとても罪深いです。
ES5/ES6にしても、
どれがES5に基づく技術記事で、どれがES6に基づく技術記事なのか。
なかなか初心者には判断が難しい。
最近、私もReact.jsのバージョンによる仕様が変わったことで、
推奨される記述方法がガラッと変わってしまい驚きました。
初心者にとってはもうはちゃめちゃです。
自分なら多分オーバーヒートしますね。

プログラミング初心者に教えるにあたって
ECMAScriptというJavaScriptの標準規格に、
独学でたどり着いた人が中にはいるかもしれません。
ですがJavaScriptの言語仕様にたどり着くことさえ、
初心者にとっては難しいと容易に想像できます。
また、初心者になんらかのプログラミング言語を勧める場合、
自分が経験した言語で、ある程度の道しるべを示すべきだと思います。
最後に、こういう記事を書いていて、
世に出すのであれば正確な情報、
わかりやすい情報を提供することが重要だと思いました。
",False,https://qiita.com//olt/items/ded3010c755314286231
"みなさんはエンジニアライフを日々いかがお過ごしでしょうか？
私は2018年6月頃から、ショボいながらもQiitaでのアウトプットや副業でベンチャー企業さんのお手伝いを始めました。  
色々始めてから半年ほど経ち振り返ってみると、非常に自身のプラスになる点が多かったです。
アウトプットを始める前の自分もそうでしたが、
「エンジニアとしてアウトプットしないといけないのは分かってるけど、腰が重いなぁ。」
「アウトプットって言っても、何していいかよく分かんない。」
と感じておられる方々の励みになればと思い、自身の経験をまとめました。  

アウトプットの定義
私の個人的な定義ですが、 第三者が見える（アクセスできる）何かしらのモノをweb上に残すこと を「アウトプット」としています。
そのため、下記のようなものが主なアウトプットに該当するかなと思っています。

自身でプロダクト作ってGithubに掲載する


AWS/GCP/herokuなどで実際に動いており、誰でもアクセスできるとさらに望ましい


技術ブログ(Qiita, Meduim, はてぶ)を書く


言語やフレームワークの話
IDEや便利ツールの話
組織文化やチームビルディングの話
キャリアの話


他の会社で副業して開発に貢献する


そこで得た知見や貢献した事項を（情報漏洩にならない範囲で）記事に落とし込む




目的
私の場合は主に、自身の技術力の向上と転職を目的としていました。
ある程度プログラミングに慣れてきて仕事での開発が安定してきた頃にふと、今後の技術レベルを高めるためにどうしようかなと悩んでいました。
いわゆる、「市場価値を高める」ためにどうすべきかなと考えた結果、技術的アウトプットをする必要があるという考えに至りました。
また、色々あり「チクショー転職だー！」という状況に至り、
当時はポートフォリオや技術ブログへのアウトプットなど全くない状態で転職活動を始めたのですが、面白いほどの高確率で門前払いを受けました..。
(アウトプット云々以前に、単純に自身のスキルや業務経歴が浅いことも相まっているとは思います。)
あまりにもウケが悪かったので「こりゃ偶然や運が悪いわけじゃないな」と観念し、何かしらのアウトプットを残そうと思いたちました。

アウトプットのやり方

技術ブログ
私の場合、基本的には普段の業務でコケた事項の深堀りや職場で実践して上手くいったノウハウ（ただし社外に出しても問題ない範囲で）をQiitaの記事にしていました。
Qiitaに手を出した理由としては、手っ取り早く始められる（メンテ要らない、SEOとかもやってくれてる）からです。
その他、Meduimやはてブなどを使うのも良いかもしれません。
私の場合、MediumにはQiitaの英語版の記事を出し、はてブには直接プログラミングには関係ない内容を挙げています（最近あまり更新していないですが...）。
例えばもしあなたが初めてRubyでCSVデータを扱うタスクを担当した場合、RubyでのCSV操作の方法や注意点などを記事に載せるのがいいかなと思います。
理由としては、 業務の中でのつまずきポイントは、ググってその場で何となく動くようにしてお終い にしがちであり、根本的な知見を身につけるに至らないことが多いからです。
その場しのぎはそれ以上の知識や知恵にはならないので、同じ事象に出会う度に何度もググる羽目になります。
または、よく分からないけどこうすれば動く、というテンプレ程度の知識しか残りません。
私の場合は業務で苦戦したポイントをメモっておき、週末にそれに対する深堀やおさらいをして記事にまとめました。
この手の内容を記事にするメリットとしては、記事（アウトプット）の肥やしになり、自身の知見も深まり、その上、同じような事象で困っている人たちの助けになることです。
また、もし間違えたことを書いていた場合でも、読者さんから マサカリを投げて頂けたり 優しくご指摘頂けたりして知識の修正が入るので、結局は自身の力につながります。
上記のように、技術ブログを書くことに対する損失/リスクはほぼないので、やらない理由はないかなと思います。
ちなみに、世間では（特に初心者の実施する）技術ブログによるアウトプットに対する賛否が語られていますが、私個人としてはショボくてもドンドン外部にアウトプットを残すべきであると考えています。
誹謗中傷や意図的な嘘の情報など、いわゆるモラル的にダメなものを除いて、アウトプットは万人に与えられた権利である（インターネット万歳！）と思っていますので、個人的にはどんどんやるべきかなと思います。
（私もクソ記事を書いて世界の検索汚染に貢献しています。  ）

副業
私の場合は主にwantedlyを転職目的で使ったいましたが、「副業で一緒にやってみて、そのまま合えば社員としてご一緒しませんか？」というスタンスが結構ウケました。
企業としても応募者としても、副業で始めてみて両者のことをよく知った上で正社員になるのが、お互いにとって低リスクです。
ただ、 企業さんがあなたのことをそれなりに欲しい人材と見ている 場合に限るかもしれません。
※このターゲットに入るべく、普段から技術ブログなどでのアウトプットをきちんとしておくことをお勧めします。
※副業専用の募集サイトなどもあると思いますので、副業としてやりたい場合にはそちらを使われた方が企業側との話はスムーズかと思います。
私の場合は結局正社員としてのジョインに至るケースはありませんでしたが、この半年の間にベンチャー系の会社さん2社（A社、B社）でお手伝いさせて頂きました。

A社：スクラム開発体制の導入のお手伝い＆開発環境へのDocker導入


稼働期間


1ヶ月稼働時間:80時間/月


業務内容


開発組織の立ち上げのお手伝い
Dockerを用いたローカル開発環境の構築


Dockerの扱いはこの時が初めてでしたが、プラスアルファのお手伝いとして勉強しながら取り組ませて頂きました。






B社：ECサイトの開発


稼働期間


2ヶ月、稼働時間:120時間/月


業務内容


Ruby/Railsを使ったシステム開発





転職には直接繋がりませんでしたが、副業を経て自身のスキルの幅と深さを身につけられた（＆勿論稼働に対する報酬も頂きました）ので、やって良かったなと思っています。

副業の注意点


エンジニアがエンジニアとして出稼ぎを始めてみたにも記載している通り、あくまでもメインは本業のため、副業へのリソース配分には十分ご注意ください。


副業をするにあたっては社内で説明をしてから行った方が後々問題になりにくいかと思います。
法的には所属元の会社が従業員に対して副業を禁止することはできません(競業他社での勤務とかはダメです)が、だからといって勝手に始めると社内で肩身の狭い思いをする羽目になる可能性もあるので、筋を通してからやった方がいいかなと思います。


副業先との契約形態は事前に合意を取っておくべきです。


報酬やコミットする対象（時間？成果物？）はあとあと揉め事になるので、事前に両者間で契約書ベースの合意を取るべきです。また、キチンと契約書を交わしてから稼働を開始すべきです。
詳細はこちらの 請負契約と準委任契約、どっちで契約すればいいの？～請負契約・準委任契約・労働者派遣契約の違い～ が非常に参考になりますので是非ご覧ください。


副業での収益が1年間の間で20万円を超える場合には確定申告が必要のため、そのための準備は事前にやっておいた方が後々楽です（20万円未満でも住民税の申告は必要です）。


少なくとも、先方に送付する請求書と交通費の履歴(ICカードの履歴)はちゃんと手元に残しておきましょう。




アウトプットを通しての変化

自身の技術力が向上した
副業先で関わらせて頂いた方は私よりも遥かにスキルが高かったので、OOPらしいコードがどんなものか、パフォーマンスを考慮した実装がどんなものか、という知見を深めることができました。
また、自身が如何に井の中に居たのかを思い知らされたので、エンジニアとして今後も自己研鑽しないとヤバイという焦りや励みにもなりました。

技術に対する興味が増した
普段の業務の中だけでコードを書いたり調べたりしているだけでは知らなかった・経験できなかったことに沢山触れることができ、「もっと知りたい、やってみたい」というメンタルになりました。
知れば知るほど面白さが増すのが、エンジニアの世界なんだと思います。

成長フェーズが異なるサービスでの開発を経験できた
本職で扱っているサービスとは成長フェーズや対象業界の異なるサービスの開発に携わらせて頂いたお陰で、今まで経験したことのなかったものをたくさん得られ、刺激的でした。

転職活動にプラスになった
「アウトプットを継続的にしている」というだけでも転職市場にはプラスに働くようで、転職サイトでのスカウトを頂く機会が増えたり、面接でもプラスの印象を持って頂くことが多くなりました。
意欲的に活動している人材は、たとえ現在のスキルがそれほど高くなくても入社後のキャッチアップが早そうなどのポテンシャルを感じて頂けることが多い印象です。
もちろん「全ての面接で無双できるようになった」といったほどの成果は出ていませんが(単純に自身のレベルがそれほど高くないだけですが...)、少なくとも以前よりはお声がけ頂ける機会が多くなり、面接も（門前払いではなく）ある程度のところまでは進めるようになりました。
また、QiitaやGithubを閲覧された企業の採用担当者さまから直接ご連絡いただく機会もありました。
転職サイト上でのメッセージはありましたが、まさかわざわざ連絡頂ける機会が来るとは正直驚きでした。

まとめ
確かにアウトプットを始める際の最初の一歩は心理的障壁が高いですが、始めてみればどうということはなくなります。  
また、アウトプットに伴うデメリット（アウトプットするための時間が必要など？）に比べて、アウトプットをやらないことのデメリットの方が遥かに大きいと思うので、まずは小さくてもいいので始めてみるのがいいと思います。
日々の小さな積み重ねが、半年後や一年後に大きな成果として自身に返ってくると思います。
というわけで、アウトプットを躊躇っているそこのあなたも、今日から一歩踏み出してみてはいかがでしょうか。  

合わせて読みたい

エンジニアがエンジニアとして出稼ぎを始めてみた
転職活動を思い立ってから実施したこと
転職活動の際に使える質問チートシート

",False,https://qiita.com//yu-croco/items/806730a671b6f9e607f6
"
よく眠る

",False,https://qiita.com//isoken26/items/200ab2a7427b51e21539
"

参考
以下のDevOpsのロードマップをアレンジしたもの
https://github.com/kamranahmedse/developer-roadmap/blob/master/images/devops.png

対象
・　研修でプログラミング研修をやったけど、自分のPCでしか動かしたことがない。これって役に立つの？？
・　作ったサイト・サービスをどこからでもアクセスできるようオープンにしたい
・　レンタルサーバってやつをとりあえず借りてみたけど何すればいいのかよくわからない
・　とりあえずサーバを操作してみたい　　などなど

ロードマップ


学習について（随時更新予定）

2.OSの特性を学ぶ
□　おすすめ書籍
Linuxのしくみ ~実験と図解で学ぶOSとハードウェアの基礎知識 
□　おすすめ学習サイト
ローカル開発環境の構築 
https://dotinstall.com/lessons/basic_localdev_win_v2
□　学ぶべきこと
Windowsしか触っていない人がLinuxを触ってみる。
WindowsとLinuxとの違いを理解する

3.OSの基本操作を学ぶ
□　おすすめ書籍
Linuxコマンドポケットリファレンス
□　おすすめ学習サイト
UNIXコマンド入門
https://dotinstall.com/lessons/basic_unix_v2
□　学ぶべきこと
Linuxでコマンドを打ってすらすらCUIで操作できるようにする
LPIC Level1程度

4.ネットワーク・セキュリティを学ぶ
□　おすすめ書籍
なし
□　おすすめ学習サイト
さくらVPS 接続時のセキュリティの設定
https://www.sakura-vps.net/ssh-firewall-security-settings-for-sakura-vps/
□　学ぶべきこと
ドメインの役割を理解する
SSH仕組みを理解する・設定する

5.セットアップをおこなう
□　おすすめ書籍
nginx実践入門
超速! Webページ速度改善ガイド
□　おすすめ学習サイト
なし
□　学ぶべきこと
Webサーバのインストールから設定方法を学ぶ
大量アクセスに備えフォワードプロキシ、リバースプロキシ、キャッシュサーバー、ロードバランサー、ファイアーウォールの仕組み・設定方法を理解する

6.環境構築のコード化
□　おすすめ書籍
Docker/Kubernetes 実践コンテナ開発入門
□　おすすめ学習サイト
Docker入門
https://dotinstall.com/lessons/basic_docker
Ansible入門
https://dotinstall.com/lessons/basic_ansible
□　学ぶべきこと
今まで行ってきたソフトウェアのインストールから設定をコード化する技術を経験する
Dockerでコンテナを学ぶ、コンテナを動かす
Ansibleで自動的に環境構築を行う

7.CI/CDツール
□　おすすめ書籍
Jenkins実践入門 ――ビルド・テスト・デプロイを自動化する技術
□　おすすめ学習サイト
なし
□　学ぶべきこと
バージョン管理（Githubなど）と連携して作ったプログラムが自動的にデプロイできるようにする

8.監視ツールを学ぶ
□　おすすめ書籍
なし
□　おすすめ学習サイト
なし
□　学ぶべきこと
監視ツールを使いサーバのCPUやメモリの容量が大きくなった場合アラートが飛ぶようにする
監視ツールのプル型・プッシュ型の違いを学ぶ

9.クラウドサービスを使ってみる
□　おすすめ書籍
なし
□　おすすめ学習サイト
各クラウドサービスの公式ページ
例）
・さくらVPS　入門
https://vps.sakura.ad.jp/guide/?a8=SG_fMGlctJ8FmQ_wJOAgrVa7B_QMyVIgqc6fuVIKh2aRpD_czJTSoVaJ.whDgn_ShG.BtDT0fVbLxs00000001717002
・AWS　WordPress ウェブサイトの起動
https://aws.amazon.com/jp/getting-started/tutorials/launch-a-wordpress-website/
□　学ぶべきこと
今まで学んでことをクラウドサービスを使ってお金をかけてサーバーを操作してみる
",False,https://qiita.com//haruto167/items/4ac6d66bbf8e55b86120
"最近、Qiitaの記事の品質低下に関する議論記事やネタ記事を見かけます。真面目な話をすれば、「品質」は、顧客の評価で決まるものです。つまり、利用するユーザによって基準が違うもの。例えば記事で言うと、初心者が満足する記事と、上級者が満足する記事は違います。そのため、品質が高い／低いなどとは一概には決められません。
でも今問題視されているのはそういうレベルの話ではなく、単純に「記事の内容が正しくないこと」のように見えます。「正しさ」を二の次にして、注目を浴びやすい記事を書く人が増えたのでしょう。自分が「いいね」を押しても損する人はいませんから、目を引くわかりやすいタイトルを見てなんとなく「いいね」を押す人も多いと思います。
個人的には人が増えればそうなるのも自然な話だと思いますし、それに対するQiitaでお金が稼げるなら質の高い記事がもっと増えるのではないかという提言などの議論は面白いのですが、結果的に議論・ネタ記事ばかりがトレンドに載り、「プログラミング」というサイトの本質からズレてしまっているのは少し違う気がします。

Qiitaの「いいね」がダメならば
もはやQiitaのトレンド、つまり「いいね」の数だけで記事を評価するのは難しい。それならば、「いいね」以外の尺度を見てもいいんじゃないか？ということで、今回は実験的に、「Twitterのリツイート数」に注目してみました。
リツイートするには、「いいね」よりも強い、「フォロワーに共有したい」という意志が必要です。つまり、リツイート数が多い記事は、「みんなに見てほしいぐらい良記事」か「みんなに晒すべきほどのダメ記事」に偏るのではないか？という仮説が立てられます。さらに問題のある記事は周りの反応を見れば初心者でもすぐわかるので、結果として良記事を拾える。かもしれない。
※ちなみに最初は、リツイート数ではなく単純に記事がツイートされた数で集計しようかと考えたのですが、単発ツイートのみで拡散される記事はそうそうないでしょうし、敢えてシンプルにリツイートに絞りました。

作ったもの

Qiitaの人気ツイートをリツイートくん
Qiitaの記事URLを含むツイートのうち、リツイート回数が多いツイートをリツイートします。基準となるリツイート回数については暫定で100回として作りました。ざっくりこんな感じ。
■リツイート対象の条件
・Qiitaの記事URLを含む
・100回以上リツイートされている
・直近7日間以内に1回以上リツイートされている
※以下は対象外
・このbotが既にリツイート済み
・Qiitaの記事がサスペンドされている
・Qiita公式のツイート（トレンドに寄りそうなので）

作り方
シンプルにTwitter APIを利用しました。
やってることはすごく大したことないですが一応書きます。
Standard search API

Search APIの呼び出し
tweepyなどのパッケージを使ってもいいのですが、大した内容でもないので地道にやってみます。基本はこのあたりにわかりやすく書かれています。
Pythonでサクッと簡単にTwitterAPIを叩いてみる
ただし、検索対象に合わせてパラメータを指定しています。
    params = {
        'q': 'qiita.com',
        'lang': 'ja',
        'count': 100,   # 最大取得件数(max100)
        'since': 'YYYY-MM-DD（7日前）', # FROM
        'until': 'YYYY-MM-DD（当日）', # TO
        'since_id': XXXXXXXXXX, # 最小ツイートID
        'max_id': XXXXXXXXXX, # 最大ツイートID
    }

これで、since 〜 until の期間のツイートのうち、IDが since_id 〜 max_id の間のツイートを取得できます。引っかかりやすい（直感的ではない）と感じたポイントは2点。
①Search API は新しいツイートから順に取得する
直感的には古い順に取得していきそうな気もしますが、新しい順です。
また、1回の最大取得件数が100件なので何度も取得を繰り返すのですが、「max_id」には「前回取得時の一番古いツイートのID -1」を指定してあげる必要があります。
例えば1回目に取得したツイートIDが：999 〜 900 だった場合、2回目の取得時の「max_id」は899にします。
②countを100にしても1件のときもある
countは1回の最大取得件数なのですが、100にしたからといって毎回100件取得できるわけではありません。90件だったり80件だったり、まちまちです。最初の数回は100件取得できても、後半は1件ずつということもよくあります。件数に応じて処理を行う際は気をつける必要があります。

リツイート情報の取得
今回は「7日間以内にリツイートされたツイート」を対象とします（7日間：Twitter Search APIの無料枠）。7日以上前のツイート情報であっても、7日以内にリツイートされていれば取得できます。
Twitter API で取得できるツイートオブジェクト（Tweet objects）は、リツイート情報も入れ子の形で含むようです。
参考：Twitterリツイートに関するJSONについてまとめる
from requests_oauthlib import OAuth1Session
import json

twitter = OAuth1Session(CK, CS, AT, AS)
SEARCH_URL = 'https://api.twitter.com/1.1/search/tweets.json'

# ツイート情報を取得
res = twitter.get(SEARCH_URL, params=params)
tweets = json.loads(res.text)

# 取得したツイート毎に処理
for tweet in tweets['statuses']:
    if 'retweeted_status' in tweet:
        retweeted = tweet['retweeted_status'] # 公式リツイート情報（引用リツイート情報は 'quoted_status'）
        id = retweeted['id'] # ツイートID
        retweet_count = retweeted['retweet_count'] # リツイート情報
        ・・・


サスペンドされた記事を対象外にする
ツイートを取得してみると、サスペンドされた記事のツイートが取得されることがありました。見れない記事をリツイートしても仕方ないので、対象外にすることにします。
Python3でURLが存在するかを確認する方法で記事が存在しない場合はリツイート対象外にします。本当はQiitaのAPIなど呼べばいいのかもですが。

結果
作成時のポイントはこんなところなので、最後に実際に動かしてみた結果も残しておきます。とりあえず直近の7日間で対象となったリツイートのうち、リツイート数が上位のものを並べてみます。
一応、集計時点におけるQiitaの「いいね」数とTwitter上のリツイート数も合わせて書いておきます。
1位
Twitter：291 リツイート
Qiita：653 いいね

布団から腕すら出さずに会社を休む [Google Home]https://t.co/YOeaA0UQqT pic.twitter.com/8KmSklEf5D— ポメラニウス (@pomeranian_dev) 2017年10月29日


「会社を休む」というおふざけとプログラミングの真面目さのギャップが良い感じの記事。みんなの休みへの欲求がリツイートさせたのかな。

2位
Twitter：235 リツイート
Qiita：1273 いいね

これでウチなりの AtCoder に入っていくための攻略資料がひとまず完備なものになったと思うのん入門編: https://t.co/6p5cCBIFqe初級編: https://t.co/C4DAGYBRWy中級編: https://t.co/pJzQhWEDg8上級編: https://t.co/BTUBDVjNOCあとは細々と蟻本の「発展的トピック」の類題を集めてるん。— けんちょん (@drken1215) 2018年3月13日

AtCoderというプログラミングのコンテスト？に関する記事。

3位
Twitter：192 リツイート
Qiita：30 いいね

VTuberになるための参考リンク集 https://t.co/NPKY5HDbBM— けもみみおーこく公式 (@kemomimi_oukoku) 2018年11月13日

Vtuberになるための情報を集めた記事。あまりプログラミングっぽくはないですが、多くの人が興味を持ちそうなテーマです。

4位
Twitter：169 リツイート
Qiita：23 いいね

数独はスピン4の非局所イジング模型なので、横磁場の量子アニーリングで一瞬で解けるはずやろと思ってググってみたら、やっぱり試している人がいたhttps://t.co/m0a7Poe7zr— 橋本幸士 Koji Hashimoto (@hashimotostring) 2018年11月3日

「数独」をプログラミングで解いている記事。数独やったことないのでよくわかりませんが、凄いのかな。

5位
Twitter：149 リツイート
Qiita：23 いいね

吹奏楽全国大会への道のりを平成1年～平成30年のデータで分析してみた https://t.co/UpjVTT5mz8— yuhkan (@yuhkan) 2018年11月7日

僕の記事ですみません。
（ちなみにツイートしてくれてるのは知らない方）

6位
Twitter：144 リツイート
Qiita：211 いいね

乱立する深層強化学習のアルゴリズムたちを、体系的に学べるようにまとめてみました。深層強化学習アルゴリズムまとめ https://t.co/Vi5AHdD0yR pic.twitter.com/HZMOxqV0GV— Shion Honda (@shion_honda) 2018年11月5日

深層強化学習のアルゴリズムたちをまとめた記事みたいです。読んでないけど凄そうですね。

7位
Twitter：120 リツイート
Qiita：63 いいね

書きました / VRChatでkawaiiムーブを極める - VRIK Override https://t.co/Ye9U2Iyqj5— よしたか (@TyounanMOTI) 2018年9月7日

VRChat関連記事です。こちらもVtuber記事同様プログラミングっぽくはないですが、楽しそう。

8位
Twitter：117 リツイート
Qiita：838 いいね

【全ての開発者が学ぶべき5つの言語】1. Java2. Python3. JavaScript4. C Programming5. Scalaそれぞれ、なぜその言語なのかという説明が記事でされているので、詳細は記事を読んでみてください！焦らず、一つの言語ずつ勉強していきましょう！https://t.co/oATtLPtzab— Progate@プログラミング学習 (@prog_8) 2018年11月7日

学ぶべき言語系の記事。個人的にはタイミングや環境で大きく変わると思うので強引に言い切る必要はないと思いますが、初心者の指標としては良いのかもですね。

9位
Twitter：109 リツイート
Qiita：677 いいね

オブジェクト指向の入門記事を作ってみました。どうかお手柔らかにご査収ください。https://t.co/RfJkg2GoO6— nrs (@nrslib) 2018年11月7日

オブジェクト指向入門記事。こちらも初心者向けの記事のようです。すごく長い。


良記事とダメ記事に偏ったか？
結果を見て感じたことをまとめます。大きく3つあります。
①ダメ記事には偏る
記事が公開停止されていたので上に載せてはいませんが、数だけで言うと、最近議論の対象になっていたヤバい記事が、実は最もリツイート数が多かったです。やはり、みんなにヤバさ・怖さを共有したいという思いがリツイートを呼び、大きな偏りを生んでいました。
②エンジニア以外にウケる記事にも偏る
一方で、良記事と言っていいのかはわかりませんが、難易度によらず、便利なもの・面白いものを作った記事が多く選ばれている印象も受けました。簡単に言うと「エンジニア以外にもウケる記事」でしょうか。Twitterはエンジニア以外が大半なので、一般向けにも楽しめる内容が拡散されるのは当然と言えば当然ですが。
③Qiitaの「いいね」とTwitterの「リツイート」は比例しない
面白かったのはここですかね。やはりQiitaとTwitterはユーザ層が違うからか、「良い」とされるものが違うようです。各記事の全リツイートを合計したら結果は多少違うかもしれませんが、大枠は変わらないでしょう。

あとがき
結果的に、そこまで有用なフィルターにはなりませんでしたが、「大衆にウケる記事」という意味で参考程度には楽しめるものになったかなと思います。
逆に言えば、「エンジニアにウケる情報」や、「正確な情報」を求めている人は、世間の「いいね」や「リツイート」などは気にせず、小規模でも純粋なエンジニアが集まるコミュニティに入ったり信頼できる人をフォローするのが良いのでしょう。幅と深さを両方求めるのはなかなか難しい気がするので。
ということで、みなさんトレンドなんて気にせず作りたいものを作りましょう。
",False,https://qiita.com//yossymura/items/5d063852f3fef3d0427a
"

はじめに
KerasのRecurrentLayerにおける引数return_sequencesとreturn_stateの違いの認識が曖昧だったので,忘備録として書き残しておきます.
詳細は参考文献.

return_sequences
return_sequences=Trueで出力系列を返す.
from keras.models import Model
from keras.layers import Input
from keras.layers import LSTM
from numpy import array
# モデルの定義
inputs1 = Input(shape=(3, 1))
LSTM(1, return_sequences=True)(inputs1)
model = Model(inputs=inputs1, outputs=lstm1)
# インプットデータの成型
data = array([0.1, 0.2, 0.3]).reshape((1,3,1))
# 予測
print(model.predict(data))

入力のshapeが(3, 1)(timestepが3)なので, 各timestep毎の隠れ層の出力(入力系列に対する出力)が以下のように返される.
[[[-0.02243521]
  [-0.06210149]
  [-0.11457888]]]

LSTMを複数重ねるときや,各出力を組み合わせて使うときなどに用いるらしい.

return_state
return_state=Trueで出力とともに最後の状態を返す.
隠れ層の最後の出力とともに,cellの(最後の)内部状態state_cを返す.
from keras.models import Model
from keras.layers import Input
from keras.layers import LSTM
from numpy import array
# モデルの定義
inputs1 = Input(shape=(3, 1))
lstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1)
model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])
# インプットデータの成型
data = array([0.1, 0.2, 0.3]).reshape((1,3,1))
# 予測
print(model.predict(data))

[array([[ 0.10951342]], dtype=float32),
 array([[ 0.10951342]], dtype=float32),
 array([[ 0.24143776]], dtype=float32)]

出力はそれぞれ,
-LSTMの隠れ層の最後の出力.
-LSTMの隠れ層の最後の出力(return_sequencesとreturn_stateを組み合わせて出力するときに有用(詳細は引用文献)).
-LSTMの隠れ層の最後の状態.
LSTMの最後の状態を他のLSTMの初期状態として用いるときなどに利用する.(ただしセルの数は等しい必要がある)

引用
Understand the Difference Between Return Sequences and Return States for LSTMs in Keras
Keras Documentation

",False,https://qiita.com//satelli/items/0f4810b698b6cebcbf93
"

はじめに
私は教員を辞めてエンジニアになってからまだ6ヶ月のいわゆる新人枠。それでもこの6ヶ月、どのような魔術を使えば圧倒的貢献に繋がるのかを自分なりに試行錯誤し、実行してきました。
そこで、私の失敗談やそこから得た経験が、新人エンジニア同志の方の参考になればという思いで今回の記事を書きました。
周りのエンジニアとのレベルの違いを感じすぎて魔術を使っていい現場なのかと引っ込みがちだった私が圧倒的貢献を目指してやったことが、エンジニアになりたての誰かのところに届けられたらうれしいです。一緒に圧倒的貢献をしましょう！

教員からエンジニアとして今の職場に転職した感想

ベンチャーは技術的負債ほぼゼロから開発しているため、ツールチェインが最新で最高
プログラミングって楽しい
実はメタプログラミング技術はあったらあったで普通に使う
プログラミング楽しいなぁ
競技プログラミングの経験は意外と役に立つ
プログラミングが楽しい
人の書いたコード読むのってつらい
東京つらい、大阪に帰りたい


歯車になりたい
これは僕がいつも思ってることです。若干の能美クドリャフカが入っています。
教員採用試験では「どんな先生になりたいですか？」と面接で絶対に聞かれる。
僕はこう答える。

「他の先生が気持ちを理解できない生徒に寄り添う先生になりたい。普通じゃない考えを持った生徒を許容して伸ばしてあげたい。今の教育に足りていないものだと思うから。」

教員採用試験って声でかくて体力ありそうな人が受かって行くような気がするんですよね。
でも、そういう先生ばかりだと嫌じゃないですか？
僕は嫌です。
いろんな先生がいるべきだと思うです。
閑話休題
僕みたいな駆け出しエンジニアが一人頑張ってもできることってたかが知れてると思っていて。
もっと言うと、どんなに素晴らしいソフトウェアができても営業しなけりゃ誰も使わないと思っている。
僕はモーターをやるのは好きじゃないんです。
つまり、人を使うことは得意じゃなくて、人に使われる方が好きなんです。
歯車の一つとして複雑な機構を支えたいと思っています。

わからないことがあったら聴く、わかった気になってもやっぱり聴く
ベンチャーだからできます。
エンジニア全員同じ島に座ってますし、他の人もそのへんにいます。
わからなくなったらとりあえず聴きます。
わかったと思ってもとりあえずアイデアを話します。
先輩は失敗の数が違うのですよ。
山ほどのバグを生み出して、修正してきたのです。
ありがちな失敗は大体話してるうちに判明するものです。
失敗してから相談するより、スループットが良いと思っています。

知識はいつか役に立つときのために蓄積する
先生をやっていると生徒に「こんなこと勉強して何かの役に立つのかよ」とか「生きるのにこんな勉強必要ない」みたいなことはよく言われます。
社会人でも「そんなものを学んでも仕事で使わない」などと似たようなことを言っているのをまれによく耳にします。
僕はそうは思っていません。
チャンスがきたときチャンスを手にすることができるのは日頃から準備を怠らなかった者だけです。
目の前の業務に終始せず興味をもって何かを学び続けよう。
与えられたタスクをこなすために技術を学んでいくのもとても良いことです。
役に立つ技術を身につけることができるでしょう。
しかし、僕が目指すのは不可能と思われていたことを技術力で可能にするようなエンジニアです。
だってそのほうがワクワクしますよ。
他の人が見えていないものを見るために必要なのは圧倒的な知識だと僕は思います。
いつ役に立つかわからないけど、いつか役に立つかもしれないからこそ今学ぶのです。

競技プログラミングのすゝめ
競技プログラミングを知らない人がいるかもしれないので説明する。
競技プログラミングには色々な形式がある。
最も典型的なものは与えられた問題セットを時間内にいくつ解けるかというタイプのオンラインコンテスト。
プログラミングで問題解決をするパターンを知る練習にもっとも良いと思う。
よく知られたアルゴリズムや、アルゴリズムのオーダーについて知っておくことは非常に価値がある。
勝負をすることが好きな人はコンテストで勝ち負けを競い、レートを上げることで鍛えるのが良いだろう。
あまり勝負に気乗りがしない人は過去のコンテストページを眺めていくつかの問題に挑戦すると良い。
良質な問題で定評がある日本の競技プログラミングのコンテストとしてAtCoderをおすすめしておく。
AtCoderのコンテストは週末に開かれている。
ちなみに僕は週末を楽しみすぎてコンテスト中にゲーセンで音ゲーに熱中していることが多い。

数学って楽しい
数学がわかっているのとわかっていないのとでは見えるものが違ってきます。
僕も数学わからないので勉強中です。
数学はどこにでも潜んでいます。
数学と仲良くなりましょう。
僕も数学わからないです。

勉強会に行く、発表する
会社なんて所詮狭い井の中ですから、大海に出て知見を集めたい。
その道のプロから知識を吸収するのが最も効率の高い勉強方法であるのは明確。
やはり勉強会に行くのがいいのではないか？
東京に引っ越してきて思う、やっぱりその手のイベントが多い。
せっかくなので行くか。

行った感想
寿司！
ピザ！
勉強会終わった後の食事！
楽しい！

せっかくなので発表する
C++の神としてはばちこり発表もしていきたい所存。

突然ですが開発談です

なんだかコードがすごく冗長
一つの操作を行うのに数十行のコードが書かれている。
実は、採用したライブラリがそうせざるをえないような設計になっている。
当然、代わりに使えるライブラリの調査を行う。
しかし、現状の使い方を想定したライブラリは無いように思えた。
ならば、作るしか無い。
そのライブラリがあるとビルドの依存関係もなんか面倒い。
僕がそのライブラリ使うの嫌すぎて3日かけてライブラリを構築した。
ライブラリを間違った運用で使うよりは、自前で作ったほうが圧倒的にマシ。
C++でライブラリを作るためには現状テンプレートメタプログラミングができる必要がある。
ライブラリのユーザーは何をしでかすかわからない。
適切にコンパイルエラーにならないライブラリはただのそびえ立つゴミの山でしかない。
（SFINAE-friendlyになってないライブラリほんま許さんからな）
あと、ドキュメントは必要。Doxygenは早くC++17に対応して欲しい。
当然テストはしっかりとやる、ライブラリがバグってたら目も当てられない。

それクラスにする必要ある？
ある優秀な学生アルバイトさんが、
「クラスなんて作らなくても、標準ライブラリの組み合わせでよしなに実現できますよね？」
と僕に尋ねた。
当時、適当にその時思っていたことをなんか答えた気がするが、あらためて考えてみた。
おそらくは半分正しいが、半分間違っている。
密結合な構造を作らずに疎結合な部品を組み合わせてプログラミングするというのは非常に重要な考え方だ。
それを考えると優秀な学生アルバイトさんの言っていることもうなずける。
問題は、C++の標準ライブラリがそれをそのまま使ってプログラミングが簡易に組み立てられるようになっていないということだ。
どう考えても必要な部品が複数かけているし（i.e. networking）、どう考えても何でもできるようにしておいたから自分でカスタマイズしてくれと主張しているライブラリ（i.e. <random>）も多数見受けられる。
結局のところ、使いやすいようにクラスに包んで使うのが正解なのだろう。
あと、雑魚いプログラマに何でもできる部品を与えるとろくな組み立て方しないからマジで。
Rustとかだったらいいんだけど、C++は危険だからマジで。
型の話もしたいけど、長くなるので今回はやめとく。

まとめ
エンジニアになって6ヶ月、新神エンジニアとか関係ない、技術力で殴っていいんだと思いました。これは私だけでなくこれから神になる人、今神になりたてでもがいてる神、神になって少し慣れてきたけど成長をなかなか実感できない神みんなに共通して必要なことなのではという気持ちでこの記事を書きました。
周りのエンジニアにできないことをできるように、精進を怠らない。そうしているうちに神という限られた力で圧倒的貢献をし、最終的に信仰を集めるエンジニアになれるのではないかと感じました。
1人前の神になれるように日々の精進を絶やさないようにしましょう！
Twitterでも毎日イラストを山ほどRTしてます。よかったら覗いてみてください！
",False,https://qiita.com//_EnumHack/items/3d7d50c43523c71ab307
"

始めに
どうも初めまして。Qiita初投稿のhashimotoと申します。
今年26才にして新卒として人材業界に入り、ひょんなことから会社内での異動でSEとして8月から働き始めました。
いつもQiitaでみなさんの投稿を楽しく読ませて頂き、更に業務中のわからない点を調べるのに、利用させていただいております。
最初の投稿ということで、SEとなるまでPCでニコニコ動画見たりyoutube見たりくらいのネットリテラシーしか持ち合わせていなかった私が、プログラミングを悪戦苦闘しながら覚えてきたこの3ヶ月で、どんなことが難しく感じ、どんなことを簡単だったと感じたかをだらだらと綴っていこうかなーと思います。プログラミング学習をほっぽって、楽しい楽しい初心者向け記事を読み漁った感想を述べていこうかなーと思います。

この記事の目的
　特にありません。というのは明らかにQiitaという技術記事投稿サイトに投稿する理由としてはそぐわないですね（そもそもポエムの時点でダメという前提は今回は目を瞑ってください...）
　この記事の目的は、

最近よく目にする、学ぶべき言語は？とか、初心者向けの記事とは？という素朴な疑問に対する自分なりの解釈を伝えたい
自分と同じ初心者を応援する
自分に興味を持って欲しい

の３つです。
とりあえず、目的がわかった状態で私の文章を読んでいただければ、乱文ですが、少しは私が伝えたいことがわかっていただけるのかなーと思いますので、どこか頭の片隅にでも置いて読んで見てください。

周囲の目って気になりません？
正直、私はキョロ充（古い）です。人の目が気になってしょうがない人です。
試験時間とか、作業する授業とかだと、周囲をチラチラみて「周りの人ってどうやってるのかな？」と意識してしまうような人間です。
これよく見たらただのカンニング野郎じゃないですか…夜中に文章を書いてはいけない(戒め)
　そんな私なので、プログラミングを始めたときには周囲のみんながどれだけ頑張っているかが気になってしょうがなかったです。
　特に、自分と同じ初心者がどれだけ頑張っているのかがわかれば頑張る指標となるし、ロールモデルとなれるかなと思いそんな記事ないかなーと検索していました。

ネットの記事は大体その言語の熟練者が書いている
　読めばわかりますが、記事は大体わかっている人が書いてくれています。なので、大体の記事は、熟練者の方が『あーーー、初心者の頃にこうやっておけばなーーー、もっと楽になったのになーーー』という自戒の念と、初心者への「だから、こうやってやるといいよ！」という善意１００％で書いてくださってます。
　でも、それは私にとってかなり罠だったと思います。成功事例を読むのも楽しいし、初心者への熟練者からのアドバイスは為になるものが多いです。そして全員がみんなに読んでほしいから記事を書いているわけなので、推敲を重ねて記事を書きますので、すごい面白いんですよね。
　でも島根県でフリープログラマーとして頑張っている女の子の話を読んでも、すごいなぁとは思うけれども、結局何を頑張ってたのかに関しては触れられていないし、「成功したよ！」とだけ言われても「過程をくれよ...」という感想しか浮かばない。
　そして、月給20万ちょっとの私が、やっとの思いで「はぁ、はぁ、よし、PHP、javascriptを使って研修用のFAQサイトを完成させることができたぞ！」と達成感に浸っていたときに、
全ての開発者が学ぶべき5つの言語なんて記事を読んで見たときの衝撃たるや。
　全然知らない言語がポンポン飛び出してきたかと思えば、「あれやれ、これやれ、これなら成長できるぞ」
といったガイドラインがあるのですよ。その中でPHPに関しては一切触れられてないし、でも会社的にもPHPをやらねばならないし。
　ものすごくひねくれた見方をすると、自分がやっていることに対して、否定されているとも取れてしまいました。
（もちろん、元記事の投稿者、及び翻訳してくださったpoly_softさんにそんな意図は絶対にないと思います。記事自体、とても興味深く、私も楽しく読ませていただきました。翻訳して頂きありがとうございました）

世の中は初心者に優しくしようとしすぎている
　こんな記事（この言語が次のトレンドだ！とか、この言語をやっておけば間違いない！といった記事など）は、ネットを少し周回すれば、様々な有識者が思う最善の記事として投稿されているものが、大量に見つかります。
それも、そのライターが熟練のライターだけあって、それぞれの記事が物凄い説得力と読ませる文章力を保持していた状態で。
初心者応援するのはいいですし、見てきた人にしかわからないものがあるのも事実です。　
しかし、こういった情報が氾濫すると、最初に『プログラミング学びたいなー』と思ってネットで調べる段階で、情報の濁流に呑まれて何から手をつけたらいいのかわからなくなってしまいそうです（実際自分は異動が決まったときに、最初に前提として学ぶべき言語とかないかなーと思って調べて、いろんな面白いブログ、記事を2時間かけて読んで、異動してから上司先輩に聞こ、という結論に達しました）。

解説記事や初心者向けの記事は駆逐されるべきなのか？
　今までの私の論調を見て、『初心者の為に記事書いても初心者を惑わすだけなら、書かない方がいいの？』
という結論に達してしまうかと思われてしまいそうですが、私の意見は、そんなことは全くないです。
善意で書いてくれて、とても楽しくなるように趣向が凝らされているこういった記事は、その言語を学びたい初心者が最初に読むべき記事だとすら思います。ガイドラインもしっかりしていて、理由も明白だからです。
絶対的に必要なのは、初心者の目的意識です。
データサイエンティストになりたいのならpythonやった方がいいとか、webアプリケーションを作りたいならPHPかJavaとか、iOSアプリを作成したいならswiftだとか、各個人が、何を作りたいのか、何を会社から求められているのかを明確にすることが一番大事なのです。
これを読んでいる初心者のあなたに問いたいです。
あなたはプログラミングを学んで、何をしたいですか？正直、とりあえず手に職つける為。今よりお金を稼ぐ為という人もいるかと思います。
ちなみに偉そうにいっていますが、私はまだわかっていません。やっていくうちに、やりたいことが見つかればと思っています。

　記事を読むのが好きな初心者のあなたへ
　こんな場末の記事を読むほど、活字ジャンキーなあなたへ送ります。記事を読むときに、目的意識を持つことが、非常に大事です。今までの論調から、初心者が読むべき記事を明記することは私は避けさせていただきます。ただ、各プログラミング言語が何に強いのか、そして何ができるのかくらいは見ておくべきかもしれません。
　
　あなたが自分の目的に合う言語を選択できることを心から祈っています。

最後に
ここまで読んでくださりありがとうございます。
　正直、私が上記で色々意見させていただいた、初心者向けに作成された時間泥棒な記事は、とっても大好きです。
面白くて、為になる。
　お金を稼げる言語だとかAIを学習するのにいい言語だとか、わかりやすい言語だとか、フルスタックになる為には知らないといけない言語だとか言語にも色々あることもしれたのはこういった記事のおかげです。
　しかし、一概にプログラミング言語といってもこのような多様性がある昨今、こういった記事を読むだけで無限に時間が取られてしまうことも事実だと思います。
　どうか、初心者の皆様が、無限に時間を取られることなく、自分の目的を達成することに、この記事が少しでもお役に立てれば嬉しいです。
　最後になりますが、私への誹謗、中傷はいくらでも受け付けますが、初心者向けの記事を書いてくださっている方々に対しての批判は無いようにお願いいたします。
　私が一番伝えたいのは、初心者は記事を読むときは目的意識を常に忘れずに読もうね！ということです。
　PHPの記事を読んでいるのに、気がついたらPythonの記事とか読んでいる私からの、自分への言葉でした。
もしよろしければ、コメント、いいねお願いします！
",False,https://qiita.com//charmston/items/df31a419a4e57ebe86ba
"

概要
この文章は下記の記事のパロディです。  

Qiitaでお金が稼げるなら質の高い記事がもっと増えるのではないかという提言
全ての開発者がQiitaへのアウトプットをやめるべき理由
最近のQiitaの記事の質が下がってきている事への考察

タイトルの主語が大きいですが概ねタイトルの趣旨からズレないように頑張るつもりではあります。
つまり、極力「釣り」ではなく「ひろゆき」風にいうと「私の感想です」ということです。
こういったパロディ記事はその時代でのトレンディを物語るのでトレンドが過ぎ去った6ヶ月後くらいに記事を見直して「そういや、そういう時代があったな!」と過去の自分を振り返られるようにするために記事として残すことにしました。
どの記事も部分的に正しいだろうし、間違っているかもしれない。
ただ、このことに気づくともっとQiitaを効率的に利用できるのではないかという提案を提示します。

Qiitaへのアウトプットで得られるメリット
プログラマがQiitaに記事を投稿する理由は人それぞれだと思いますが、大体は次のような理由からだと想定しています。

実装でハマった時の知見の共有
自分なりのライフハックを残すため
普段の業務や勉強に対するアウトプットのため
承認欲求のため
自分の知識を元に他人と議論したいから
有名になりたいから

私の場合は1, 3, 4 がメインとなります。
1と2 に関してはQiitaのキャッチフレーズにもなっているくらいですしね。

特に4の点に関してはQiitaはとても素晴らしいですがそれはまた後述します。
ちょっと昔に「ポータブルスキル」という会社に依存しないスキルを獲得する事が今後生き残る戦略だ！みたいなトレンドで「ポータブルスキル」という言葉が流行ったような記憶があります。
Qiitaへのアウトプットとして記事を投稿していると、それは会社に依存せずにあなた自身の「信用」に繋がります。この「信用」を持ち運びできるようになると考えるとエンジニアの集客を頑張ってくれるQiitaの存在はとても大きいと私は思っています。多分、今有名なエンジニアの方でQiitaでの記事の投稿がきっかけで有名になったという人は多い気がします(私の勘違いかもしれませんが)。
またQiitaには昔よりシステムはイマイチになったらしい「いいね」システムがありますが、
この「いいね」システムはなかなかいい感じに承認欲求を満たしてくれています。
「いいね」の数が多いほど自分が認められている感じがするんですね。
それで、もっと評価されたいためにさらにいい記事を投稿するようになるのでモチベーション維持のシステムとしてはかなり優れてる。
昔Wordpressとかはてなブックマークでも3ヶ月も記事を書き続ける事が出来なかった私ですら今だに記事を書き続けられています。
最もこの「いいね」システムをビジネスに応用してブランディングされて
トレンド記事にノイズが入ってくるといった弊害があるのが最近の悩みどころですよね。
まあ、そういったノイズを言い出すと他のサービスも多かれ少なかれ良い所・悪い所があるはずなのでQiitaらしいかというのが私の思っている事です。

エンジニアが情報発信した方が良い理由

自分の知見を共有することにより信用の前払いが可能になる
自分の情報を共有することによりその分野の情報が逆に集まりやすくなる
自分の知識の評価を他者を経由することにより正当性が高まる

実は1と2はほぼセットみたいな効果があるが、エンジニアにとっての最高の情報収集は実は情報発信する事だったりする。自分の興味のある分野についての情報を集めたい場合には先に自分から情報をネットで共有するのである。
ほとんどの場合、本当に重要な情報というのはネットでは見つからないのである。
すると勉強会や仕事の時に思いも寄らないタイミングで「自分」を知っている人と繋がるのである。いわば、ブログで発信している記事が自分の「名刺」代わりになり得るのである。知らない間に「信用」が溜まっていくのである。
これを「信用の前払い」という。
因みにこのテクニックの先駆者は参考文献の本に書かれている内容だ。(私が勝手に名付けているわけではない)

コンテンツを無料で配布すると、人々はまずお金を使わずにあなたの作るコンテンツにどれくらいの価値があるかを見るチャンスを獲得する。あなたは何かを売ろうなどと思っていないかもしれないが、売る計画がある場合でも、あなたが提供している無料コンテンツの品質が高いことを人々が知っていれば、他ならぬあなたが売っているものだから買おうという気持ちになりやすくなる。彼らは無料コンテンツに感謝の気持ちまで持っているので、いずれあなたが作った製品を支援してお返ししたいと思っている場合さえある - (Soft Skill 他人のために価値を生み出せ！)

最も既にブログやツイッター、勉強会やカンファレンスなどで十分に情報発信している有名人はこの旨みを体感している。
本人達が自覚しているかは別の話しであるが。
因みに私が本気で情報発信しようと思い立った理由は3番の理由である。
自分の主張に正当性を担保させたいがためにQiitaに記事を投稿したいと思うようになった。
というのも最近の仕事の業務にコードレビューが当然のように組み込まれていてレビューをしたり受けたりする時に
本当に自分の主張がエンジニアリング的に正しいのか分からなくなる時が多くなったのである。
コードレビューについては今回の趣旨とは違うのでまた改めて別の機会に自分なりの考えを述べたいと思う。
自分の正当性を担保させる、いわば自信をつけたい場合にはQiita のシステムは非常に優れていると思っている。

Qiitaで投げ銭機能が実装されたら質の高い記事がもっと増えるのかについての考察
こちらは結論から言うと質の高い記事は増えないと思う。
と言うのも技術記事では基本的にまとまった金銭を稼ぐことができないからそれ相当のモチベーションに繋がらない。
「投げ銭」は気持ちの良い言い方だけど、これは使い方によっては「情報商材」とか「アフィリエイト」としても使えてしまうからである。
(私の勝手な想像であるけど、ほとんどのエンジニアって嫌悪なイメージでしかない。)
さらにはビジネス相手がコスパを重視するエンジニアなのでどんなにいい記事があってもその記事の続きが「300円」と提示されたら課金しようとは思わないはずである。
「あり」だと思うのははてなブログみたいに「Pro」を購入した人だけ広告を貼れる権利を付与させる事くらいかな。

Qiita以外のプラットフォームではダメなのか
考えられるプラットフォームでは

Wordpress
note
medium
はてな

などがあります。
Wordpressでもいいと思いますが、モチベーションをいかに維持できるかでしょうね。
というのも趣味として記事を書くのは出来ますがそれを6ヶ月や1年と長期ベースでブログをメンテナンスするのは非常に大変です。
Wordpressだとデザインをカスタマイズできる分こだわりのあるエンジニアであればデザイン面で非常に苦労する仕様だったりする。
noteはやり方としてもプラットフォームとしても稼ぐことをメインにしたプラットフォームなので「あり」だと思っています。
ただ、集客面で全く機能してくれません。ツイッターで集客できる人はありでしょう。
またエンジニアを相手にビジネスする場合には彼らはとことんコストパフォーマンスを重視します。
そんな彼らを相手に課金させるほどの記事を投稿できるか、ここが非常にマネタイズの難しい部分である。
Mediumは元々が海外産なのである意味Qiitaよりもプラットフォーム的にはエンジニアに適している。
ただ、海外の人に読ませる記事となると英語になるので日本語で情報発信をするのならやはりQiitaの方が発信しやすい。
モバイル系のエンジニアは私の観察範囲だとQiitaとはてなを並行して使い分けている方が多いですね。
Qiitaである程度記事が集まったらはてなに誘導させるのが一番ではないでしょうか。
となると、やっぱりQiitaは外せないんですよ。
それでもQiitaは嫌だという方は気にせずお好きなプラットフォームで記事を書けばいいのではないかな。
誰にも読まれない、というストレスさえ克服すれば割と情報発信を続けられるはずである。
最後に私の好きな名言を残して今回の記事を終わろうかなと思います。

Strive not to be a success, but rather to be of value. - Albert Einstein
(成功した人ではなく、価値を生み出す人になるよう努力せよ - アインシュタイン)


参考文献
最速の仕事術はプログラマーが知っている
SOFT SKILLSソフトウェア開発者の人生マニュアル
",False,https://qiita.com//tamappe/items/fc786333b0f97ba7738e
"

概要
基本学習嫌いな自分が第二言語の勉強法として入門書のサンプルのユニットテストを書くと学習が捗りまして何より楽しかったので、単純な写経との比較とかなんで第二言語？とかその辺りを書きます

対象読者
これから Go やってみようかなという自分と同じような方
多分他言語でも共通で使えるので第二言語の勉強法に悩んでる人など

経緯
ペチパーとして生を受けて早１年と半年強ぐらい
そろそろ第二言語勉強しなきゃなって思って（ちょっと遅いけど）言語候補を探していました
そんなとき以下の記事と出会いました
Ruby->Go->Scalaという習得順序がエンジニアの爆速の成長に最適である理由 
ふむふむ、Web エンジニアとしての第二言語は Go を選べばいいんだなと アッホ 素直な自分は第二言語に Go を選択しました
Go に対する知識は、ゴファーとかいう煽りネズミがマスコットであることと Google が設計を行ったことを知っているという程度
そんな状態で勉強方法を探してみると、まずヒットしたのが公式のチュートリアル A Tour of Go
これでも十二分に学習できそうでしたが、なんか本読みたいなって気分だった（雑）ので入門書を購入
選んだ入門書は、 スターティングGo言語 です
その本がたまたま最序盤でユニットテストをやっていたのをきっかけに他のサンプルのテストを書いてみると理解が深まりました

写経との比較して
プログラミングの有名な学習法の一つに写経があります
「サンプル片手に写すだけ」というシンプルさ、継続的に行いやすいため作業癖をつけれるというメリットがあります
ダイエットで言うところの「ウォーキング」に該当するかなと言う点もあり、初学者にはめちゃめちゃおすすめな学習法です
ただ、個人的には写すだけだと飽きが来やすく単純作業となりがちで挫折するパターンが多いです
他にも「写して動かすだけで満足」しがちと言う点もあります
その面、ユニットテストを書く場合、サンプルを理解して適切なモジュールに分割した上でサンプルにないコードを動かす必要があります
以下は、go によるクロージャで、「一つ前の文字列を出力する」と言う実装がされたメソッドのテストのサンプルです
func TestLater(t *testing.T) {
    f1 := later()

    expect := """"
    actual := f1(""One"")

    if expect != actual {
        t.Errorf(""%s != %s"", expect, actual)
    }

    expect2 := ""One""
    actual2 := f1(""Two"")

    if expect2 != actual2 {
        t.Errorf(""%s != %s"", expect2, actual2)
    }

    expect3 := """"
    actual3 := later()(""Two"")

    if expect3 != actual3 {
        t.Errorf(""%s == %s"", expect3, actual3)
    }
}

サンプルにないことを書くことで理解もより深まりますし、何よりたのしいです  
あとサンプルないことを書く -> 本に書かれてないエラーも多発する -> 調べる過程でも学習になる
と良いこと尽くしです
さらにテストを楽しくするパッケージありました（色が付くとたのしいです）
go testの出力を見やすくするツール作ったった

なんで第二言語？
初学者にいきなりユニットテストといっても難しいかなと思ってます。
ユニットテストってこういうもんですよ。とか理解する前に挫折したら本末転倒ですし。（ペアプロとかしてくれる人とかいたら別）
第三を学習しようという頃には、もうエンジニアとしての練度も上がってるだろうし、リファレンスを斜め読みで OSS のサンプルコードとか読んだり、何か適当なものを実装した方が早いかなと言うイメージです
そんなこんなで第二言語向けの学習法かなと思いました

終わり
何より楽しく学習できる方法なので、プログラミングをプライベートでやるって言っても何やれば良いかわかんないんだよな〜って人にもおすすめです
ぜひ実践してみてください  
",False,https://qiita.com//penton310/items/2af559bc78a10db7ee96
"※KPTとYWTのそれぞれの説明については割愛させて頂きます

結論

新人が溜めるものは経験と自信です！


経緯
新人が配属されOJT教育のためにKPTで振り返りをしたところ、非常に困った事が発生しました
まず新人なので基本的には何も知らないし、何もできません
そんな状態で振り返りを実施するとPしか出てきません

次々に溜まり捲るP

少しも溜まらないK

教える全ての事がT


Pばかり溜まるのって本当にテンションが下ります
しかも新人です
スタートからいきなり壁にぶつかってテンションの上る人は非常に稀です
新人も教える方もモチベーションが上がりません
そこで振り返り手法をYWTに変更しました

まず手順書を見せながら業務を説明して、新人にやってもらいます
→(自動的に)Y(やったこと)が増える！
初めての事をやって、できたらできた事がわかります
できなくても、できなかった事がわかります
→(自動的に)W(わかったこと)が増える！
新人の状況や成長を見ながらリーダーが次のOJT業務を指示します
→(自動的に)T(つぎにやること)が増える！

新人の最初のアウトプットはWので「きたか、できなかったか」の選択のみです
これを週1で繰り返していくうちにWが選択ではなく自分の言葉で出てくるようになります
次にTも自分で発言が出来るようになり、遂にTにないYも自分から発言ができるようになります
これはもう自転車の後ろを押さなくても自分で走っていける状態です
手を離しても定期的な進捗確認や質問の回答等のサポートは必要ですが、あとは本人の努力次第で成長できるでしょう

補足
何をやってもP(問題)ばかり出てくるとなると、何もやりたくなくなるのが人の心情ではないでしょうか
ゲームを始めて何かしてもエラーメッセージしか出なかったら、つまらないのですぐ止めてしまいます
チュートリアルのストーリーの中で画面の見方の説明、操作方法の解説をして覚える事に集中させるのがよいと思います
最初はやったことのない仕事をすることに躊躇します
しかし実際に進めていくことでYとWが溜ると、その溜ったYとWがある事で
「前回のこれもできたから、今回もできるのではないか」
という根拠の無い(有る？)自信がでてきます
そうして次に進む事ができるようになるのではないでしょうか
次の新しい事に挑戦する気持を育てることが新人には大切だと考えます

補足2
本人が「自分でやる」という意識付けも成長には大切だと思います
",False,https://qiita.com//lunar_sword3/items/74c546901102b7757901
"この記事は
Qiitaでお金が稼げるなら質の高い記事がもっと増えるのではないかという提言
に対するレスです。
コメントでやれば良いと思ったのですが記事にした方が良いね！してもらえると思うので記事にします。
*いいねの数に対してお金をもらえると仮定した話です。１いいね！はいくらかは知らないが、生活できるレベルのお金がもらえるとして。

1. 技術ブログの記事を転記します
会社ブログや他のテックブログ、それこそMediumの記事をQiitaに転記します。
元々が価値がある記事なのでいいね！されやすいはず。自分で記事を書くわけではないので労力も少ない。

2. ソーシャルワーカーにリライトを発注します
転記で稼いだお金で、よくいいねされる記事をリライトしてもらって再投稿します。
何度も再投稿することで新着に表示させ、いいね！を狙います。
もちろん、過去の他人の投稿もリライトしてもらいます。今でも重複記事多いから問題ないね！

3. Qiita転記システムを作成し情報商材として売ります
自分だけでやっても天井は知れているので、初動で稼いだあとはシステムを複製して売ります。
後は他の人が1,2を繰り返します。

ほぼ確実にスパムらしき投稿が増えると予測されます。
え？スクリーニングすれば大丈夫って？それをやるのは運営で、もちろん運営コストが増える
そうすると、（仮にお金がもらえるとして）記事を書いた人への報酬が減る・・・という悪循環が発生します。
現にYoutubeのスパムや無断転載とか多いですしね。
Youtubeでアニメがあがっていたら明らかに無断転載って分かるので通報しているんですが、
技術記事の転載だと原本探すの大変そうですね。

いまでもQiitaを使って利益を獲る方法　→　ポートフォリオとして使う
記事に対して報酬がもらえないQiitaでも十分に利益を享受する事ができると私は思います。

就活、転職活動にQiitaのアカウントを提出する
逆に最近、Qiitaをスクレイピングして転職オファーメールを送るとかいう会社があるっぽい

Stargzr のような、Qiitaをベースにスコアリングしてランキング＆仕事につなげるサービスを利用する


最後に
私は闇落ちしないので、この記事に書いてあるような人でなしな行動は行いません。
あくまで私が考えつくコンテンツで収益を分けてもらえるようになった時に悪用できるパターンを考えてみただけです。
",False,https://qiita.com//mikkame/items/30b66f68723c5a2ff088
"

Qiitaでお金を稼げると良いのか？
エンジニアがアウトプットする際に、
お金を稼ぐという物理的報酬が、
モチベーション向上につながるのか？
　⇒否、つながらない
という考え方もあるよ、と紹介したい。
背景としては以下の記事：

Qiitaでお金が稼げるなら質の高い記事がもっと増えるのではないかという提言

この提言自体は大変素晴らしい提言である。
一見すると、良い報酬があれば良い成果を生む、
という誰もが納得するような話だ。
しかし「行動経済学」の「アンダーマイニング効果」によると、
必ずしもそうとは限らず、報酬を与えることで、
逆にモチベーションが下がる場合もある。
お金が絡むと金儲けに走る人が出るからダメとかそういう理由ではなくて、
人間は必ずしも合理的に動かないという、オモシロ心理学的な話。
Qiitaへの記事投稿に限ったことではなく、もっと一般的に、
エンジニアが個人開発でサービスやアプリを作る場合にもあてはまる。
個人開発したサービスやアプリの価格、有償提供か無償提供か、
考えたことのある開発者は多いハズだ。
また、企業内の、ボトムアップ的な草の根活動にもよくあてはまる。
イノベーションを起こそう、とか、何かのイベントをやろう、等の活動だ。
「物理的な報酬」が発生すると、モチベーションが下がってしまうことがある。
Qiitaに記事を書くにしても、
個人開発を行うにしても、企業内での草の根活動をやるにしても、
モチベーションを上げよう！と思って外的報酬を設定して、
意外な落とし穴にハマることの無いように、初耳の方は下記一読をオススメしたい。
私自身、ごく最近知った概念で、
特にエンジニアにあてはまることが多い気がしており、
上記の記事をきっかけとしてまとめておく。

行動経済学とは？

今までの経済学は「人間は必ず合理的な経済行動をするもの」
という前提で構築されてきました。
ところが普段の私たちは、それでは説明できない
非合理なふるまいを多くしています。
行動経済学とは、従来の経済学では説明しきれない人間の経済行動を
人間の心理という視点から解明しようとする新しい経済学です。
　　～ 行動経済学まんが　　ヘンテコノミクス　より～




「行動経済学」なんていうとさも知識人っぽいが、実は、
最近ちょっと流行ったビジネス書（まんが）を読んで、
本書には23話の「行動経済学」が書いてあるのだが、
その第一話目、エンジニア業界に最もあてはまりそうな、
「アンダーマイニング効果」（報酬が動機を阻害する）
に興味をもったので、
Web上で調べたことを含め、掘り下げてまとめておこう、という話。

アンダーマイニング効果を考えるエピソード

堀のらくがき

いたずら好きな子供たちはおじいさんの堀に落書きをしていました
おじいさんがいくら怒っても、子どもたちはまた落書きをします
そこで一計を案じたおじいさん
子どもたちが落書きをするたびに「元気があってよろしい」とお小遣いをあげました
次の日も次の日も、落書きをするたびにお小遣いをあげました
数日後、「すまんがもうお小遣いはやれなくなった」とおじいさんは言いました
子どもたちは「えー、落書きしてやったのにくれないのかよー」と言いました
その後、子どもたちは落書きをしなくなり、おじいさんには平和な日々が戻りました


私たち人間は通常、報酬があるとやる気を起こします。
しかし、人間とは不可思議な生きもので、
時として、報酬によって逆にやる気をなくしたりします。
　　～ 行動経済学まんが　　ヘンテコノミクス　より～


国民栄誉賞を２度も辞退したイチロー選手
国民栄誉賞辞退の理由はなんですか？
「今の段階で国家から表彰を受けると
モチベーションが低下するのではないか、と懸念しています」

「Wikipedia」が「Microsoft」に完勝

Microsoftが以前だしていた百科事典エンカルタ（Encarta）
プロに記事の執筆、編集を依頼した最新鋭のデジタル百科
90年代に急速に成長した人気製品であった
しかし、2000年以降、Encartaの人気は衰退
最大の原因は、無償で編集された「Web上の無料百科事典Wikipedia」


エジソンの発明

私は一日たりとも、いわゆる労働などしたことがない。
何をやっても楽しくてたまらないから。
　　～　トーマス・エジソン　～


GoogleやAtlassianの革新性

Googleの20%ルール
Atlassianの「FedEx Days」


24時間、いつもの仕事以外の好きなことをやる




子供のゲーム

ある子供は毎日いつもゲームをしていました
もしかしたらプロゲーマの才能があるかも？と思ったお父さん
毎日欠かさず、２時間ゲームをしなさい、とルールを決めました
ゲームクリアまでの作業工程やノルマを決めました
工程やノルマを達成するごとにお菓子とお小遣いもあげました
うまくいかない場合はアドバイスもしてあげます
やがて子供はゲームが嫌いになり、別なことをして遊ぶようになりました


Qiitaのらくがき

いらずら好きなエンジニアたちはQiitaに落書きをしていました
Qiitaに落書きをするたびに「お小遣い」が貰えることになりました
・・・あとはご想像にお任せします・・・


人間の２種類のモチベーション
アンダーマイニング効果は人のモチベーションの話なので、
まず、人間の２種類のモチベーションの話をする

外発的動機づけ
内発的動機づけ


外発的動機づけとは？
いわゆる「アメとムチ」。
頑張ることで物質的な報酬や評価を得ようとする意欲。
外部から与えられる目的（物質的な報酬・利益・評価）を
達成するためにがんばる意欲のこと。

内発的動機づけとは？
活動することそれ自体がその活動の目的であるような動機。
趣味や仕事などの、内容そのものに面白さや充実感を感じてがんばる意欲のこと。
お金のためでもない、怒られないためでもない、
その活動がしたいからするという、趣味の活動における意欲。
有能感、自己決定感、仲間との交流など、
自分自身の満足感のためにがんばる自主的な意欲。
（実際に報酬や罰が発生するかどうかではなく、気持ちとしてどうか、ということ。
　逆に、報酬や罰がなくても、それを狙っているのであれば外発的動機づけになる）

外発的動機づけと内発的動機づけの違い
内発的動機づけによる活動は、外発的動機づけによる活動よりも、
質が高く、集中力が高く、楽しく、継続性がある、と言われている。
特に、クリエイティブな作業に対しては、
内発的動機づけの方がはるかに生産性が高いとされている。
（いくつかの実験で示された、とされている）
実験しなくても、自身の実感として、そう感じませんか？

アンダーマイニング効果とは？
内発的動機づけはちょっとしたことがキッカケで、
簡単に外発的動機づけになりさがってしまう、という現象のこと。
人は自分で自分の行動を決めていると自覚している時には、
内発的動機づけが得られるが、
他者から統制されていると感じた時には、外発的動機づけにすぐかわってしまう。
自分の好きでしていた活動（内発的動機）であっても、
にわかに外からの物理的報酬（外発的動機）を与えられると、
今後はその報酬なしで活動をするのがバカらしくなってくる。
（報酬が継続すれば活動は継続したとしても、動機がかわっているため、
　「活動の質」がかわってきてしまう）
報酬が動機を阻害する現象

人間とは、かくもヘンテコな生きものなり。
　　～ 行動経済学まんが　　ヘンテコノミクス　より～


では、アンダーマイニング効果を防止するには？
東京オリンピックの無償ボランティアとか、
やりがい搾取とか、なんかブラックな話を推奨したいわけではない。
金銭的報酬があってはいけない、というわけではない。
または、言語的な報酬＝褒める/称賛するなども、
場合によっては「褒められるために活動する」ことになるため、
無償であれば良いというわけでもない。

子供が科学に興味を持つエピソード

ある子供はなぜ夕日は赤いのか、興味をもって図書館で調べていました
お母さんは「エライ！」と褒めました
お父さんはお小遣いもあげました
子供は・・・

ちょっと待って。
なぜ子供は「夕日が赤い理由を調べようとした」の？

自分が興味をもったから。
純粋な知的な好奇心から。
調べて新しい知識を得ることが嬉しかったから。

褒めることも、お小遣いをあげることも、必ずしもNGではない。
でも、最も注目するべきは「調べた行為」「勉強した行為」ではなくて、
「興味をもったこと」「好奇心があること」「学習による喜び」のほうでは？
あなたがお父さん、お母さんなら、子供にどのように接しますか？
Qiitaへの記事投稿のモチベーションや質をあげるには、どんな報酬が良い？
個人開発のモチベーションを高めるには？必ずしもサービスでのお金儲けが必要？
企業内のボトムアップ活動を促進するためにはマネージャはどう促す？

まとめと後書き
アンダーマイニング効果を認識しておかないと、
他の人への促しにおいても、
自分自身のモチベーション向上においても、
良かれと思った報酬や施策が、
かえって逆効果を産む場合がある。
内発的動機に火がついたのなら、
それを安易な外的報酬で消さないように
報酬を与える際には細心の注意をはらわないといけない！
必ずしも、無償が良いとか、やりがいを全面に出すとかではないし、
Qiitaでお金を稼ぐというのも全く否定をしているわけではない。
方法や設定を間違えたら逆効果になるよ、という話。
個人的には、個人開発しているスマホアプリ/PCソフトについて、
なぜ無償提供で作っているのか？という理由を、
初めてヒトコトで言語化できたような気がして、
一度まとめておきたいテーマであった。
（アプリ/ソフトを売れば儲かるよね、よくと言われる。
　売って稼ぐよりも、多くの人に使って欲しいから？
　人のため、そんな聖人みたいな理由じゃないよね、の違和感。
　自身の内発的動機がアンダーマイニング効果によってかわるのを
　防止するため、そう、自分のためなんだ！という観点は大きい。
　もちろん、どうせ大した金にはならない、という理由も大きい）
また、Qiitaそのものについて言及すれば、
どのような外的報酬（称賛）を与えるか、ということについて、
お金よりも栄誉のほうがしっくりくる印象はある。
（お金がダメとは言っていない。前向きな検討はおおいに称賛の気持ち。
　お金でうまい方法を設定するのは難しい。投げ銭とかはいいと思う）
Qiitaの殿堂は、「殿堂入り」という「栄誉」ならば
投稿者の内発的動機を阻害せずに、ほどよく称賛でき、
良記事を促進する仕組みになるのでは？
という思いも込めて、自身の内発的動機だけで作ったサイトだ。
思想も汲んでご覧いただければ大変嬉しい。

【無料】Qiitaの殿堂を作った物語【簡単】
あなたの時間を無限に奪うQiita記事集できました

最後に、
この記事は一応ポエムのタグをつけているが、
個人開発の「設計思想」や、
自身の開発プロセス（モチベ管理）を考える上で考慮するべき事柄、
人間の心理学的な観点からサービス開発時の構造上の設計ミスを防ぐノウハウ、
などとして、ソフトウエア開発にかなり密接に関わる内容と考えている。
（２３話ある行動経済学の話のなかで、
　最も関係性の高いものを選んだのだから）
この記事が、誰かの動機の阻害を、未然に防いでくれることを願う
おしまい。
",False,https://qiita.com//youwht/items/1c7ae618c8b24984836e
"円周率  3.141592653...で出てくる数字を順番に、
奇数を「ズン」、偶数を「ドコ」に置換していくと、
いつか「キ・ヨ・シ！」に巡り合えるだろうか。
",False,https://qiita.com//jinoji/items/c9b3b36d47344d563e0b
"

きっかけ
Qiitaに対する意見記事が最近連発しておりその中の全ての開発者がQiitaへのアウトプットをやめるべき理由にたいしてコメントをすることによりバズることに手を貸した1。
コメント欄でも炎上を狙った記事だとの批判もあったがOSS(オープン・ソース・ソフトウェア)の概念について私の信念と異なっていたので意見した。
一連の流れでQiitaでお金が稼げるなら質の高い記事がもっと増えるのではないかという提言という記事が出たのでそのことについて主に意見を述べる。
コメント欄では長くなるので記事でします。

Qiitaの目指す世界


プログラミングに関する再利用性・汎用性の高い情報が集まる場をつくろう
「あなた」と「誰か」がつながる場としていこう

これこそOSSやフリーソフトウェア運動に繋がる理念だと思っていますがそれと違う理念でOSSを論じてQiitaに重ねることでQiitaでマネタイズできないと語っていたので、「すべての〜」に反対意見を述べた、またQiitaにお金を介在させることでOSSの理念にそぐわない場になるのではないかと懸念しているので意見を述べる。

オープンであることとフリーであること
オープンはもちろん開かれていることでOSSならソースが見れることである。
フリーは無償と捉えられがちだがフリーソフトウェア運動では自由の意味である。
OSSが進んでいくためには開発する人が必要で開発する人はないように通じている必要がある。そして自由に出入りできるので情報の共有が肝になる。
こういった点はQiitaの情報共有とにている思う、だからこそQiitaはオープンでフリーな場だと考えてるし、そうあるべきだと思っている。

マネタイズによって壊されるオープンでフリーな性質
私はお金が介入することでこのオープンでフリーな性質が壊れると思っています。
元記事にあった個人ブログはQiitaに対するではないので割愛として

有料記事化
これは言わずもがな。読むためにお金という障害がある以上オープンな場ではありません。

イイネに応じた報酬
冒頭で述べたように私はある記事をバズらせ(炎上せ)ました。
イイネが有料だったら記事元にお金を発生させたくなかったのでやってなかったかもしれません。
私はコメントによる間違いの訂正はオープンなQiitaの強みの一つであると思っていますし一連の記事やコメントでも同じ考えが多くいると認識していますがお金が発生することによりそれが阻害される可能性があります。
そうでなくてもセンセーショナルなタイトルやさらには間違った内容が増えると思います。
イイネに加えてヨクナイね、ふつうだネやClap(複数回可能なイイネ)の提案もありますがスパムやサブアカウントの問題などもあります。
そしてそれを運営が監視するということは、オープン性が縮小していくことだと思います。
「Qiitaでのアカウント停止について」の大事な3点。HRTは最重要ガイドライン・スルースキルを身に着けよう・停止理由は「技術的誤り」ではない。のコメントあるようにコメントに込められた攻撃性が指摘されていますが、ことお金が人は絡むとより攻撃的になります。
人の心情の自由ははお金によって簡単に動きます。これはフリーからは程遠いと思います、つまりフリーであるためには無料であることが必要だと思います。

報酬をどう渡すか
報酬を渡すとなると銀行口座を教えてもらうなど何か手段が必要になります。最近流行りの仮想通貨でも良いですが全員がやってるというわけでもないですし。
となると何か個人情報を登録してもらうという心理的障壁ができてオープン性が下がると思います。

ひとまず結論
Qiitaに望むことはありません。今までのままのプラットフォームの提供をお願いします。
そして今までのままの理念を貫いてほしいです。

関係する雑記

Qiitaを使ったマネタイズ
これが無理だとは思っていません。それはフリーソフトウェアは使い方は自由だからです。アイディア次第でどうにかなるとは思っています。
殆どのアイディアはコメント欄で出たもののパクリです。

個人ブログ
利用規約に下のようにあります

著作権を侵害しない
他人の書いたソースコード、そのほか写真・イラスト・音楽などに関して、権利者の許諾なく複製して掲載する著作権の侵害はやめましょう。引用する場合は、配布元のライセンス規約に従い、適切な表記で掲載してください。

Qiitaに投稿された記事自体の著作権は下のようなので書いた人の好きにしていいと思います2。
つまりマルチポストNGではないのでQiitaと自分ブログに投稿しておくということは規約違反ではないと思います。
同じ内容だとQiitaしかみないんじゃないか。という場合は索引ページなどを自分ブログに作るなどすれば自分ブログに引き込めんじゃないでしょうか。

何か作ってそれに関する記事を書く

宣伝や販売を主目的とした記事は投稿しない
技術に関する情報共有を目的としていない、宣伝や販売を主目的とした記事は投稿しないようにお願いします。なお自社や自作の技術的な解説等を主目的としている記事は、宣伝や販売には当たりません。 ご自身の書いた記事が「宣伝や販売に当たるのか判断に迷う」といった場合は、サポート窓口へお問い合わせください。

とあるのでなにか作ってそれに関する記事をかけば規約上はセーフだと思います。実際、WebAPIを作ってデプロイしてみた系の記事はちらほら見ますし完成品もリンクがあったりを見たことはあります。
(多分非常にセンシティブな部分なので機能解説と実装まで入れるべきだと思います。)
QiitaもAPIがあるらしいのでそれを使って何か作ってみて別のアドレスからAPIを叩いての便利ツールなどはできそうですし需要もありそうな気がします。

meta Qiita
@mpyw さんがコメント欄で

Meta Stackoverflow 的なノリで Meta Qiita つくろう （たぶん流行らない）

現状のQiitaが好きなので本体は変えてほしくないけどお金を取るとより記事が良くなると思うなら作ってみるのも一手じゃないかと...
デザインも機能もシンプルなので技術に自身がある人ならなんとかなるんじゃないかと思います。
お金が絡むと運営が大変になりそうなのでそっちのほうが問題になりそうな気もしますが...
@Cj-bc 

これは現在でも可能な事ではあるのですが、投げ銭をデフォルトでサポートするなどあれば良いのではないかなと思いました。

私は怠惰で最新技術を追ってないので間違ってるかもしれませんが全員が自由に投げれて受け取れるシステムというのは今の日本では難しい気がします。
プラグイン的に外部にそういう機構を入れるとか、アカウントの追加機能ならできそうですがすべてのは難しいと思います、誰でも参加できるの障害を上げれば可能だと思います。

Qiitaをマネタイズに使うアイディアの最後に
@YumaInaura さんがコメント欄で 

エンジニア=道具を使うのがうまい人が集まっているわけだから、きっともっと上手にできるはず。

と言っています、にQiitaを有料化しろとか、個人ブログにするとかは3、何かを作るエンジニアとしては安直すぎると思います。

最後に、何故、記事が炎上したかの考察
「HRT」=「Humility（謙虚）」、「Respect（尊敬）」、「Trust（信頼）」が足りなかったから、これはユーザー間でもそうですが何よりQiitaへのHRTが足りなかったように思います。
有るアカウント停止に端を発しQiitaでのアカウント停止に関する考えについてにある

Qiitaでは「技術的にレベルが低い」「技術的に間違っている」「Qiitaというサービスや運営に批判的である」といった理由で記事の非公開化やアカウント停止という措置を取ることはございません。

を試すような記事が出た。私がバズらせるに加担した「全ての開発者が〜」はそれた特に顕著だったと思う。
これはQiitaの運営に対してHRTを足りないと思う、特にQiitaのオープンでフリーな性質を好んでいた私にとって不快に感じたので @qiitadaisuki さんに攻撃的になったと思う。
一連の記事は特にQiita運営に対してHRTを欠いた記事があったように感じる。HRTを欠いたものを見ると反応もHRTを欠いてしまう。




炎上とも言うがニュートラルな表現としてここではバズるを使う。 ↩


少なくとも共有のために引用可は求められると思います。 ↩


私のアイディアもまだ足りないと思っています、やる気も技術も ↩



",False,https://qiita.com//forl_head_officer/items/b35f38a57f02d5cc36a3
"

最近聞いた話から考えてみた
小学生のテストで、
りんごを2つずつ3人に配りました。
その時の式と答えを書きなさい

答え
  式： 2 × 3
  答： 6

みたいなのがあって、2 × 3 だと正解で、 3 × 2 だと間違いになったとか。
「結果は同じなんだからそんなの算数(数学)としておかしい！！」
「いや、考え方として順番が大事だ！！」
という論争がありましたとさ。

わたしの考え
正解？不正解？それは、
「先生が何をテストしたかったか次第では？」
結局よくある けーす ばい けーす の話ですね……
小学生のこの年代くらいであれば、先生からの教え方で 「2つ」のものを「3人」に配るのなら「2 × 3」と書きましょう、順序も大事です 、みたいな教え方をされているのだと思います。
この手のテストでこの手の採点方法のケースでは、答えが6であることではなく、学校で教えられた通りの解法で解くことがテストされているのだと思います。
であれば、掛け算を使わない 2 + 2 + 2 はテストとしては不正解ですし、逆順の 3 × 2 もテストとしては不正解です。
ただ単に答えの6を求めたかったのか、2 × 3 = 6を求めたかったのか、それはテスト出題者の意図次第だと思います。
良いか悪いかは置いておいて、世の中の「問題」と「答え」はそういうものだと思います。

世の中のテストというものは、出題者が決めたルールの中で高い点数を取るというゲームです。
ただ単に「論理的に、科学的に、答えがあっていればいいというものでは無い」というのがわたしの理解です。
電卓を使ったらダメなのも、ググるのもダメなのも、横の人に答えを聞いたらダメなのも、名前をローマ字ではなく漢字で書くのも、周りが習っていないテクニックを使って答えを出すことがダメなのも、出題者が決めたルールですよね？
ボールをゴールに入れるだけなら手で持って走ればいいのに、足しか使っちゃいけないというサッカーのルールと同じです。決められたルールの中で高得点を出すというゲームがテストです。
※ ただし、その「ルール」が明確にされていることは重要です。おそらく今回の算数のテストのケースでも先生は「順序も大事ですよ」というルールを事前に言ってくれてた……と……信じています。

ということで、わたしは学校の教え方として、かけ算の順序を教えるのはおかしい！順序を意識させた方がいい！ そもそもそんなテストはおかしい！ みたいな議論をしているわけではなく、まあ、テストってそんなものだよね。。。という考えです。

なんでQiitaでこんなことを？
結果が同じでも、様々な書き方が出来るもの…… そう！ソースコードです！！
どなたかがTwitterで、
「算数のしぐさみたいなものなら道徳の時間にやるべきではないか」
みたいなことを書いてました。
「え！？ コーディングも同じ結果になることを様々な書き方出来るんだけど…… コーディングスタイルとかも道徳でやるべきなの？？」
という違和感を覚えたからです（汗）

言った本人は「算数なんて当たり前なものは道徳ででも…」と思ったのかもしれませんが、小学生低学年のかけ算は初めて習うことで全然当人には当たり前じゃないですからね（笑）大人のプログラミングくらいには当たり前じゃないと思う。。

ソースコードもこれじゃないとイケないなんてものは無いモノの、やっぱり慣習として、ベターな書き方があります。if文の書き方、for文の書き方、==の書き方、インデントの書き方、などなど。
それに対して「答えが同じなんだからどっちでも良いべきだ」と言われると、違うかな…… と思います。可読性が変わってきます。可読性が変わってくるとレビューの効果・効率が落ちたり、保守性が悪くなったりします。

わたしが考えるキレイなソースコード
最初の算数の問題に非常に近い話ですが、「2つを3人に」であれば「2 × 3」と書かれているのが、キレイなソースコードの１つの要素だと思います。
それが絶対な正解では無いにせよ。やりたいことが素直にストレートに表現されている、文学的なソースコードというのが、がキレイなソースコードだと思います。
ちょっと極端な言い方ですが、やっぱり一般的な日本語の文脈の通りに2 × 3と書いてくれる方がソースコードとしては読みやすいです。相手が日本人ならなおさら。

もちろん速度を重視するために一部をアセンブラに書き換えるようなこともあるとおもいます（わたしもやってました）そうなると可読性もあったものじゃないので、結局はケースバイケースなのは百も承知です。それでも、やりたいことがストレートに表現されているコードを、わたしは素直でキレイなコードだと思います！！


終わりに
これは完全に私見ですけど、、、そういう意味では、初学者に対して、算数のテストで 2 × 3 を正解として、3 × 2 を不正解にしてくれる方が長い目で見たときに、良い職業プログラマーに育ってくれるかなー などなど思いました。
大人になったら、ひらがなの「はね」「とめ」なんて気にしなくても通じるから大丈夫ですけど、習った当初にはそういうことをテストというツールを使ってでもしっかり身に着けさせることは、あながち悪い事ではないと思います。

でも、不正解にしながらも、先生にはちゃんと「本当はどっちでもいいんだけどね。でも今回は数の順番もテストの範囲だから書き方を気を付けてね^^」「大きくなったらPythonでも使って計算を解いていいから、今だけ我慢してね^^」というフォローはしていただきたいと思います。じゃないと理不尽な記憶だけが残ってしまいますので。。。

",False,https://qiita.com//567000/items/d50803be2716d04dde9e
"でも、いかんせん私の舌はそんなに早く回らないんだ。
歯がゆいよ、もっと言いたいことはあるのに、素晴らしいの一言に気持ちを込めるしかないんだ。
でも、それだとお前は自分のどこが素晴らしいのか理解しようがないわけだ。
できることならね、お前のどこが素晴らしいのか、万の言葉を用いてレポートをまとめたいところだよ。

書いている人
よこのじと申します。
よこのじ.workというサイトにて、技術関連の勉強したことを書いています。例えば、Pythonでのスクレイピングやサイト作成のためのHTML,JavaScript関連のことなど。
Twitter（@yokonoji_work ）では、これ役に立ったなど、ちょっとしたことをつぶやいています。

はじめに
Qiitaというサービスに対しての意見記事は恒常的に投稿されています。この記事を書いている時点ではこのような記事が投稿されています。

最近のQiitaの記事の質が下がってきている事への考察
Qiitaでお金が稼げるなら質の高い記事がもっと増えるのではないかという提言
全ての開発者がQiitaへのアウトプットをやめるべき理由

この手の記事は良くも悪くも途切れることはないと思っていますが、私自身はエンジニアとしてのアウトプットについて考える機会となり、結構好きな系統の記事だったりします。
そういうわけで、私もQiitaを利用してアウトプットしようと思ったのですが、Qiitaの何が良いのか、その思いを書いてみます。

千の言葉を用いて褒める
Qiitaって「プログラミングに関する知識を記録・共有するためのサービス」です。
細かい規約部分については置いといて、Qiitaのようにプログラミングに関するあらゆる情報が各個人から集まる場があるというのは本当に素晴らしいことだと思うのです。
私はWordPress（よこのじ.work）にて技術関連の記事投稿をしていますが、WordPressではなかなか起こりにくいことがQiitaでは当たり前のようにあると感じています。
それは、ヒントが連鎖することです。Qiitaでは記事へのコメントや、ある記事から発展した新たな記事が作成されるなど、ちょっとした情報発信から議論が生まれるのが素晴らしい。これがあるから、ちょっとした情報発信にも価値はあると思うのです。
もちろん、各WordPressの間でも無いわけではないですが、エンジニアがエンジニアに向けて書くQiitaではその内容がとても濃い。
Qiitaを利用することで直接の金銭的な利益は得られませんが、あらゆる情報が集約されていて様々な技術・考えに触れられるというのは、エンジニアとして、あるいは人生においてすごく大きな利益だと思います。Qiitaという場が自身の第2の脳になる、これにあえて価格を付けるとしたら、どれだけの価格になるのだろうと思ったりします。
人が技術を発展させてこられたのは情報を共有することで、技術を改良・応用したり、あるいは何十年・何百年前から続く問題に挑み続けることができているからだと思います。
こういうことを考えると、今までWordPressで記事を書いていた私は、Qiitaという場を利用して、より良いアイデアをもらったり、間違いを指摘してもらったりしながら技術を吸収して共有、さらに改善を重ねていくということをやってみたいと思いました。
何を言いたいのかと、思われている頃でしょうか・・・
話を戻して、なぜQiitaでアウトプットしようと思ったのか？についてですが、「私も情報共有して貢献しながら、その中で知見を広めたいと思った」です。

さいごに
こんなつまらない記事を見たあなたが（見てくれてありがとうございます）、アウトプットについて考えたり、つまらない記事を除外する仕組みを考えたりしてくれれば、私が記事を書いた時間は無駄ではなかったということになるわけです。
そうそう、ブロックチェーン関連のプロジェクトにALISやsteemitという、記事を作成して得られた「いいね」に応じて金銭的報酬が得られるサービスがあります。これは記事の質を高める仕組みになるでしょうか？
ある経済学者による研究によると、報酬の多さとパフォーマンスは比例するものではないようですね。むしろ金銭的報酬は悪い影響すらあると。
真のパフォーマンスは、自主性から生まれるそうです。金銭的報酬がないWikipediaが巨大になったのは、ユーザーが誰かの役に立ちたいという自主性により支えられているところが大きいようです。

チベーションを研究する科学者達が、すでに私達が取るべきアプローチを示してくれています。この新しいアプローチは、自分が大切だと思うからやる、やりたいから、好きだからやる、面白いからやる、社会を変える重要な動きの助けになるからやるという、内から湧き上がる動機に基づいています。
Wikipediaはなぜ""タダ""で記事を書いてもらえるのか？
人のやる気を引き出す、3つのキーワード

Qiitaは自主性により支えられているサービスになっているでしょうか？これについて考えるところがあれば、新たなサービスを生むヒントがあるかもしれませんね。
以上です。ありがとうございました。
よこのじ／よこのじ.work
",False,https://qiita.com//yokonoji/items/8033deefbba5d002a5fc
"

TL;DR
bookmarkletに呼び出しやすい名前をつけておくと、アドレスバーで呼び出せて便利。


ユーザーストーリー
Developersの気になるニュースをキュレーションしているサイト、Developers Milkを公開しています。
毎日、サイトを巡っては、抑えておいたほうが良いと思うニュースを厳選してサイトに登録しています。
サイトのアーキテクチャはQiitaに公開されています。サイトのURL、タイトル、一言紹介文などをGoogle SpreadSheet登録しておくと、Webサイトに公開されるようにしています。
サイトの登録は、毎日、何度も行う作業なので、できるだけ作業効率を上げておくことが望まれます。
Developers Milkでは、Web Commandsをつかってサイトの登録作業を効率化しています。

Web Commands
Web Commandsは私が考えた造語です。
仕組みはほとんどbookmarkletとして知られている昔からある技術ですが、それに名前をつけておき、アドレスバーから呼び出すことを形式知化しているところに新規性があ（ると思いこんでお）ります。
bookmarkletとは、通常URLを登録、呼び出すためのブラウザの機能、つまりブックマークにJavaScriptを登録しておくことで、任意のWebページ上でJavaScriptを実行できるようにする技術です。
bookmarkletを登録するときにそのブックマークを呼び出すときのコマンドを名前（Chromeだと名前、Firefoxだとタグ）につけておくことで、アドレスバーにそのコマンドを入力すると、JavaScriptを呼び出すことができます。
Chromeだと、同一のGoogleアカウントでログインしておくことで、スマートフォンでもPCでも同一のWeb Commandsを呼び出すことができます。

Developers MilkでのWeb Commandsのユースケース
「mklt」という名前でJavaScriptを登録してあります。Developers Milk用の小さなアプリケーション→MilKLeTからもじったものです。
これぞ、と思ったWebページで、アドレスバーにmkltと呼び出すことで候補に表示されるので、mkltを選択します。
mkltには、GitLab Pagesに登録されたJavaScriptを呼び出すコードが登録されており、GitLab PagesのJavaScriptはそのページをDevelopers Milkに登録するためのコンソールを表示します。
コンソールには、そのページのURL，タイトルを抽出してデフォルト値とし、また、ページ上の画像を押すとその画像を抽出、入力欄に保存する機能があります。「Send」ボタンを押すだけで、Google Apps Scriptを経由してSpreadSheetに登録してくれます。

作り方。
mkltは、Chromeのブックマークとして、以下が登録されています。

bookmarklet
javascript:s = document.createElement(""script"");s.src=""https://{GitLab Pages上のJavaScriptのURL}"";document.body.appendChild(s);


これにより、Web Commandsを実行したページ上にGitLab Pages上のJavaScriptが読み込まれます。GitLab上のJavaScriptには以下のようなコードが登録されています。

!function () {
    var title = document.title;
    var siteURL = pageURL.substring(0, pageURL.indexOf(""/"", 10));

    var baseElement = document.createElement(""div"");
    baseElement.style.position = ""fixed"";
    baseElement.style.left = ""0"";
    baseElement.style.width = ""100%"";
    function dc(base, value, textarea) {
        var elem = document.createElement(""div"");
        base.appendChild(elem);
        var input = document.createElement(textarea ? ""textarea"" : ""input"");
        elem.appendChild(input);
        input.style.width = ""100%"";
    }
    dc(baseElement, ""title"", title);
    dc(baseElement, ""siteURL"", siteURL);

    var buttonArea = document.createElement(""div"");
    baseElement.appendChild(buttonArea);
    function appendButton(area, name, func) {
        var button = document.createElement(""button"");
        button.innerHTML = name;
        button.onclick = func;
        area.appendChild(button);
    }
    appendButton(buttonArea, ""CANCEL"", function () {
        document.body.removeChild(baseElement);
    });
    appendButton(buttonArea, ""SEND"", function () {
        var url = endpoint + ""?siteURL="" + encodeURIComponent(siteURL));
        var script = document.createElement(""script"");
        script.src = url;
        document.head.appendChild(script);
        document.body.removeChild(baseElement);
    });
    document.body.appendChild(baseElement);
}();

キモは、document.body.appendChild(baseElement);でしょうか。昔ながらのJavaScriptでガリガリとUIを作成、Developers Milkに登録するコンソールを作っています。

みなさんも、Web Commandsで自分の業務をプチ改善してみては？
昨今、Web上で行う作業は多岐に渡ります。
例えば勤怠入力。標準時間を自動入力、登録する機能をJavaScriptにしてbookmarkletに保存、ktnr（勤怠入力=KinTaiNyuuRyoku）としてコマンド呼び出しすることで作業を効率化することができるかもしれません。
例えば集計処理で、Web上の表をCSVにして出力することができるかもしれません。
どちらも対象のWebアプリケーションの作りによるところはありますが、チャレンジしてみてはいかがでしょうか？
",False,https://qiita.com//ukiuni@github/items/f6482040d3f6ba241f4c
"最近Qiitaで、全ての開発者がQiitaへのアウトプットをやめるべき理由という記事を見つけました。この記事を読んだ率直な感想は、「云わんとしていることは分かるが、解決策間違ってね？」というものでした。
僕は逆に、「Qiitaがこのように変わってくれたら、もっと良記事が増えて、Qiitaへ来る人が増えて、日本のエンジニア界隈がさらに盛り上がるのではないか」という視点で、Qiitaに投稿するという形で意見を書きたいと思います。
意見とは、タイトルにもあるように、Qiitaでお金が稼げるなら質の高い記事がもっと増えるのではないかというものです。

なぜエンジニアはQiitaに記事を書くのか
エンジニアがQiitaにアウトプットをする一番大きな理由は、
日本のエンジニアが集まるプラットフォームとして最大級だから
だと思っています。Qiitaに質の高い記事を書けば多くのエンジニアが注目してくれて、半ば勝手にはてブやTwitter等で拡散されて多くのエンジニアの目に触れます。また、Qiitaのコントリビューション数をアウトプットの証明として使うこともできます。
ゴミ記事問題や、アカウント停止問題など様々ありますが、これらもQiitaを多くの人が使っているからこそ話題になっている話だと思っています。

「お金が稼げる」というモチベーション
シェアリングエコノミーが流行っていますが、その中でも「何でも売れるし、何でもお金に変わる」という部分に注目しています。
若者を考察するための”視点”｜ハヤカワ五味｜note
情報だってお金に変わるのです。そして、お金が稼げるというモチベーションってめちゃくちゃ強くないですか？情報でお金を稼ぐという文脈で、3つのサービス・方法を取り上げて見たいと思います。

個人ブログによる広告収入
note
Medium

です。これらの良い点・悪い点を整理してみます。

個人ブログ
個人ブログでお金を稼ぐ方法の大部分は、広告収入だと思います。

良い点：プラットフォームとして成長したら広告収入が安定してたくさん入って来る
悪い点：ブログ自体の構築が大変
悪い点：ある程度の収入を得るまでがとても大変

個人ブログは、収入を得るまでに「記事を書いて拡散してPV数増やして」という部分が大きな障壁になります。

note

続いて、noteです。

良い点：有料記事・マガジンの作成ができる
悪い点：人が集まっていないため、拡散チャンネルが別に必要

有料記事等を簡単に作成できる一方で、SEOがめちゃくちゃ弱いため、それを購入してもらうには別で拡散チャンネルを持っている必要があると思っています。つまり、そもそも有名な人でないと売れない印象です。

Medium

続いて、Mediumです。

良い点：clap（いいね)に応じて収入が得られる
良い点：人が集まっている（主に英語圏）
悪い点：日本人が集まっていないため、英語で記事を書く必要性がある

人が集まり、いいねによってお金が稼げるという最高のプラットフォームですが、メインは英語圏のユーザーです。英語で記事をかける人にしかメリットはありません。

Qiitaに質の高い記事がもっと増えるのではないかという仮説と要望
じゃあQiitaはどうかというと、

良い点：誰でも記事が簡単にマークダウンでかける
良い点：日本のエンジニアが集まっている、つねに注目されている

上で挙げた、個人ブログ・note・Mediumの悪い点を全部クリアしている印象を受けます。あとは、お金稼ぎができるという部分があったらもっと盛り上がるのでは？というのが僕の主張です。
ある、めちゃくちゃ優秀なエンジニアの知り合いがボソッと言っていました。昔はQiitaに記事を書いていいねをたくさん稼いで有名になっていたが、今はそれをすることによるデメリットの方が多い。価値のあるコンテンツを公開するだけの時間がもったいない。でもnoteとかだったらそれがお金に変わるからいいよね。
確かに、コンテンツを公開してお金を稼げるのは最高だ、それならみんなもっと頑張るのではないかと強く思いました。
次に、Qiitaでお金を稼げるようになるとしたら、どのような方法がいいのか検討したいと思います。

noteのように有料記事がかける
Qiitaの良いところは、全ての良記事が当たり前のようにタダで読める点です。Qiitaで「この記事いいな」と思って見てみたら途中から有料だったら萎えます。基本的に大部分の人は100円であっても払いたくないのです。「Qiitaでいいなと思う記事は基本的に有料だよね」とか「無料の記事って基本的に質が低いよね」という風潮になって、Qiitaに人が集まらなくなる懸念があります。

Mediumのようにいいねに応じてお金がもらえる
Qiita運営側としては集客で得た広告費の一部をユーザーにあげる形になると思います。そのため、ユーザーとしては今まで通りに記事をみて「いいね」をするという行動は変わりません。記事を書く人も質の高い記事を書こうというインセンティブが働きます。結果、「いいね」を獲得できるような良記事が増えるだろうと予想します。

結論
Qiitaに「いいね」の数に応じて記事の作成者がお金をもらえる機能があったら、さらに良記事が増えて素晴らしいプラットフォームになるのではないか。

最後に
もちろん、捨てアカウントで「いいね」を稼ぐなどと言った行動をスクリーニングしないといけないなど、アイデアだけでは実行できない部分もあるので一筋縄ではいかないとは思いますが、アイデアとしては良いかなと思っています。
どうですか？ぜひみなさんの意見をください。Qiita運営側も意見くれたら嬉しいです。
そして、この記事はデータに基づいているわけではなく個人の感覚で書いているだけなので、定量的に間違っているよなどという意見も大歓迎です。
Twitterでも色々と発信しているのでぜひ見てください。
👉https://twitter.com/hiromu_bdy
",False,https://qiita.com//HiromuMasuda0228/items/aa3729882cb59fda9838
"

概要
ソシャゲでレイドシステムというのがある。上手く負荷を捌くにはどうすればいいだろう

きっかけ
マギレコというゲームで新たにレイドシステムが実装されたのだが、何度も落ちて臨時メンテナンス祭りだ。開発会社は相当優秀であるにもかかわらず。（一年間の運営状況や実装状況をみるに、少なくとも日本の他のモバイルゲームと比較すると圧倒的に技術力がある）。
じゃあレイドシステム固有の難しさとはなんだろう？考察したい。

01. ソシャゲの一般的な特徴
本稿で取り上げるソシャゲの特徴。日本のソシャゲは大体当てはまると思う:

通信方式


HTTP(S) でのステートレスな通信


負荷パターン


ユーザ数が非常に多い
ユーザ間の連携は少ない（フレンドシステムくらい）
突発的な負荷の集中（イベント時・新キャラ実装時など）


サーバとクライアントの担務


サーバ側は重要資産のみを管理


主にユーザの所持品管理


クライアントがゲーム機能の大部分を提供



なお、最近では、PUBG, Minecraft のマルチプレイのようにフレンドのアクションをリアルタイムに反映するような即応性の高いソシャゲもあるが、本稿では考慮の対象外とする。
この章で何が言いたいのかというと、サーバサイドだけ見ると ECサイトとソシャゲは結構似てて同じような負荷対策が有効である ということ。

02. ソシャゲのスケーリング手法
レイドシステムの前に、一般的なソシャゲの性能上の考慮点を記載しよう

02-1. シャーディング
日本のソシャゲはソーシャルな側面は非常に薄く、ゲームシステムのほとんどはシングルプレイだ。したがってユーザIDを使ったDBインスタンスの物理的な分割（シャーディング）がぴったりはまり、性能問題は起こりにくい:

シャーディングで負荷分散できる機能:



ユーザ情報管理機能: 石とかスタミナとかキャラ一覧とか

ガチャ・ショップ機能: 課金石やゲーム内通貨とアイテムの交換（購入・売却） 

シナリオプレイ機能: ステージ情報をサーバから引っ張ってきてプレイ

強化機能: アイテムを使ってキャラや武器を強化



一方、ユーザID によるシャーディングを選択する場合、いくつかの「ソーシャル」な機能に関してはインスタンスをまたいだ処理が必要になる

シャーディングで（直接的には）負荷分散できない機能:



フレンド管理機能: 検索、追加、削除、一覧機能

サポート要員選択機能: シナリオプレイ時に選択（非フレンドも対象になる）

PvP機能: 他ユーザとの対戦



これらに関しては、2.3で再考する。

02-2. マスタの複製保持
ソシャゲでは、メンテナンス時間を設けることが許容されている。そこで、多くのマスタデータ（ステージ情報・ガチャ情報・武器やキャラマスタ）を、全てのシャードに格納して、マスタDBに負荷が集中しないようにすることができる。新ステージ実装時や、新ガチャ実装時には、すべてのシャードに配置された複製を更新すればよい。これは手間だし、不整合を生じさせうるが、運用でしっかりカバーすれば性能上の問題をだいぶ軽減できる。
ここで一つ補足が必要だろう。ECサイトと異なり、ソシャゲでは（ユーザ情報以外の）マスタデータのサイズはものすごく小さい。なぜならクライアント側にアセットがほとんどあるので。サーバ側が管理するのは、ユーザ固有の情報とかガチャ情報とかステージ情報とかそんなもんだ。したがってシャードDBにマスタデータを配布するという同期作業のコストは面倒ではあるが、それほど時間がかかるというわけではないと指摘しておこう。

02-3. クライアント側キャッシュ
シャードをまたぐ機能は性能がスケールしないので、リアルタイムな情報を取得しようとするのではなく、キャッシングするという妥協（という仕様上の制約）が必要。使用頻度の多寡で設計をするのが原則。
サーバサイドでキャッシングするという選択肢もあるが、ソシャゲの場合クライアントが賢いのでクライアント側で実施する方が実装上楽（なぜかを考えてみてほしい）。

頻度が低いので対策が不要な機能例



ユーザ検索機能: 知り合いなどを検索する機能


比較的頻度が高いのでキャッシュして、更新頻度を抑える機能


たとえば 必要に応じて15分に1回最新化

フレンドサマリ機能: フレンド一覧とログイン日時情報

サポート要員選択機能: 他ユーザの候補一覧を更新




02-4. レコード分割によるオンメモリ化
これまでは機能に注目したが、それとは別の軸でも負荷を軽減できる。
高頻度でアクセスされる transient な情報に関してはオンメモリになるよう設計するという手法がある。例えば「サポート要員選択機能」は非常に高頻度にアクセスされる（たとえキャッシュによりその頻度を抑えられるとしても）。この時必要になるのはユーザの全情報ではなく、ごく一部の情報のみだ。例えば、

ユーザ名
ログイン日時
サポート要員としての強さ: Lv・ステータス値・装備

DBに格納する際に、フラットにユーザの全情報を詰め込むととてもじゃないけれどメモリに収まりきらないが、上記のような情報程度ならメモリにおさまることができる。これによりディスクアクセスのない高速なアクセスに対応できる。
数字を挙げよう。1シャードあたり 10万ユーザを格納するとする。上記のような厳選データとして 4KB 使うとすると 論理データ長はたったの 0.4GB  だ。一方、ユーザのすべてのデータを格納しようとすると、ユーザ当たり 1MB くらい必要になると思うので、メモリは 100GB くらい必要になり、メモリに乗りきらない。
このように、レコードをよく使う部分と、そんなに頻繁に使わない部分に分けて格納するようにデータをモデリングすることが特にソシャゲでは必須である。（UserSummary テーブルと UserData テーブルのように二つのテーブルに分けるわけだが、1:N ではなく 1:1 の対応関係になる。こういうのは一般にレコード分割とか垂直分割と呼ばれる）
なお、上記のような厳選データは、DBに格納するのではなくキャッシングミドルウェア(memcached や redis)に格納することも可能である（ミドルが異なると整合性のチェックがとりにくくなるという欠点はある）

03. レイドシステム
以上の知識をベースにレイドシステムについて考えてみたい

03-1. レイドシステムの仕様
あまり詳しいわけではないので何とも言えないが、ここで考察するレイドシステムは以下を想定（マギレコに準ずる）:

とても強いBOSSが出現


初戦はBOSSを発見したユーザが戦う。


勝てれば終わりで報酬がもらえる。
負けてもBOSSのHPが減った状態で何度も再戦できる
再戦のたびにBOSSのHPは減るので一人でもなんとか倒せる


二戦目以降は救援要請が可能（他ユーザに戦ってもらう）


他ユーザ ＝ 非フレンドも含む
救援要請対象のユーザを指定することはできない
もちろん救援要請後に自分が再戦してもよい
自分・他ユーザが累積してBOSSのHPを削り切ったら勝ち
報酬を山分け（与えたダメージで比例配分）




累計BOSS打倒数で報酬がもらえる


500万体倒したらユーザ全員にガチャチケットをあげるとか
自分が発見したBOSSの打倒数でも報酬がもらえる（50体倒したらガチャチケットとか）




03-2. レイドシステムの技術課題
上述のレイドシステムを実装する上での技術的課題を洗い出そう。
a. 状態を持つステージ
既存のゲームでは（少なくともサーバサイドでは）ステージは状態を持たなかった。つまり、クライアントからステージ X をチャレンジするよ、とリクエストが来た場合、ユーザのスタミナを必要分減らして、ドロップアイテム等の情報を生成してクライアントに情報を返すだけでよかった。あとはクライアントサイドでゲームが進行する。ステージクリアか失敗かの情報がサーバに来ればそれに応じたユーザ情報の更新を行えばよい。
一方、レイドシステムはこのようなステージとは概念が異なる。残HPであったり、誰がどの順番で何回挑戦し、どれくらいのダメージを与えたか、それらの情報をサーバサイドで覚えておかねばならない。500万体のBOSSがいればそれだけの情報を覚えておかねばならないのだ。
b. 実装基盤の抜本的な変更
前述の考察を進めると、レイドシステムのためのサーバが新規に必要になる、と想定される。というのも、500万体のBOSS分のレイドの状態を保持し、時々刻々その状態をアップデートするというのは、全く異なる負荷パターンを結構なスループットで捌かなければいけないからだ。1時間に 10万体のBOSSが倒されるとなると、 ざっくり計算でも 1時間当たり 100万件位のデータのアクセスが要求されるのではないだろうか？そうなると1台のレイドサーバでは厳しく、複数台のレイドサーバを用意する必要がありそうで、しかも（ユーザIDでなく）レイドIDでのシャーディングが必要になりそう。このように考えると、実装基盤に対する変更は非常に大きくなると想定される。
c. キャッシュ効果の薄さ
既存のステージプレイでは、シャードをまたぐような重い処理はサポート要員の選択程度だ。これは何回ステージにチャレンジしようと、15分に 1回リフレッシュすれば十分であり、性能上の問題にはなりにくかった。
一方、レイドシステムではこの描像にならない。次々とBOSSが登場するので、ある意味レイド情報は使い捨てで、どんどん情報が必要になる。クライアント側のキャッシュが効きにくい構造になっているといえる
総括
レイドシステムの実装には、特にサーバサイドにおいて、これまでのイベントとは別次元の難しさや困難さが伴うと想定される

03-3. レイドシステム実装Tips
まあ、私は作ったことがないので勝手な妄想でしかないが、もし実装する立場になったら以下のような点を気を付けようと思う
レイドの排他制御
排他制御を厳密に行わない。つまり救援要請があった BOSSと戦う際にロックをとらない。すると同時に同じBOSSと戦うユーザが出てきてしまう可能性があるが、そこはOKとしよう。100 のHPだった場合、3人が100を与えてしまったら、報酬を3人に分け与えればよい。報酬を三等分するのではなく、3倍にするということ。
排他制御なんかしたら、BOSSと戦うボタンを押した際に、「他の誰かが戦っています。別のBOSSを選択してください」というださいメッセージをださなければいけなくなる。これは絶対避けたいシチュエーションだ。
レイドの生成
レイドはどのタイミングで生成されるのだろうか。一番ナイーブに考えると、ユーザがレイドサーバに create 命令を発行することだ
マギレコでは、BOSSの手下をクリックすることでサーバ側に刺激を送り、確率的にBOSSの生成を促しているように見える。サーバサイドで１ユーザあたりの最大同時BOSS数の確認をして、条件を満たせばBOSSを生成してそのIDをユーザに紐づけるのだろう。これはなかなかうまいやり方だな、と思ったが、手下をクリックするたびにサーバとの通信が発生しあまりよいUXとは言えなかった。
私なら、イベントを開始したユーザのクライアントが、裏で非同期に事前にいくつかのレイドを予約して、クライアント側で確率的にBOSSの存在を明らかにする。これならブロッキングな通信の発生頻度を抑えられるだろう。
レイドのファイナライズ
ファイナライズのトリガを引くのは、BOSSを倒したユーザのクライアントでよいと思うのだが、排他制御をしていない場合、トリガが複数回引かれる可能性がある（二つ前の排他制御の項を参照）。配分が二重にならないように配布フラグをつけるなどしておく必要があるね！

まとめ
レイドシステムの実装上の困難さを考察し、実装時の考慮事項を検討した。
",False,https://qiita.com//41semicolon/items/7b35e6ae0dab65c43b4f
"

まえがき
弊社の社員旅行のしおりアプリ（以下、しおりアプリ）を作成したというお話です。
長いので、読みたいところだけかいつまんでお読みいただけると幸いです。
アプリ作成の経緯から製造を経て実際の運用まで、採用したアーキテクチャ等に触れながら私の思いや思惑も書いています。
最初にことわっておきますが、アプリ作成時に度重なる危機や困難など、ドラマ性はありません。

しおりアプリと開発経緯
しおりアプリとは社員旅行の「旅のしおり」をアプリ化したものです。
それまでは紙を冊子にして配布するという形態をとっていましたが、社員が増えてきたこともあり、冊子にする方法は次の2点を理由に難しくなってきました。

紙と印刷代のコストがかかる
紙資源を浪費する

いわゆるペーパーレスの流れがあるわけです。
しかし、ただペーパーレスを理由にするのであれば、アプリでなくPDFなどで配布すればいいわけです。
そこをあえてアプリにした理由は、弊社がシステム開発会社であること、それを営業トークに使いたいという思惑があったことなどがあったかもしれません。
実際はどれも建前で、本音はおもしろそうじゃね？　という好奇心から始まったものだと信じています。

3度目のアプリ化
そんなしおりアプリですが、今年で3度目のアプリ化となります。
1度目は私が2年目のとき、先輩社員が作成しました。
当時は何時にどこに行くのかという旅程を確認するというシンプルなものでした。
2度目はその翌年、機能が少しリッチになり、観光マップ機能などが追加されました。
これは先輩社員がiOS版、私の同僚がAndroid版の作成を担当しました。
さらにその翌年、つまり昨年、2度目のアプリ化ですったもんだがあった影響もあり、PDFで配布することになりました。
3度目、今年は満を持してAndroidアプリ開発をメインにする私がAndroidを担当、iOS版はiOSアプリ開発をメインにする後輩たちに依頼し開発を行いました。

しおりアプリが抱える問題
それまでのアプリ化、しおりアプリは次の3点の問題を抱えていました。

アプリ化する度にすべて作り直している
しおりアプリの作成に膨大な時間を費やしていた
データはアプリに埋め込みで、差し替えのためにアプリの修正が必要

本来、旅程を表示する機能は改修を必要としません。データを差し替えれば済むはずです。
（人によって旅行プランを選択でき、そのプランが多岐にわたるようになったため、単純にデータ差し替えで済まなくなったという事情はあります）
また、データを差し替えるためだけにアプリに手を入れる必要があるというのは極めてナンセンスです。
そして、毎年全機能を作り直すことは無駄と言わざるを得ません。
3度目のしおりアプリでは以上3点の問題を改善するべく、開発を行いました。

閑話：社員旅行委員会
弊社では社員旅行の計画立案、運営までを社員旅行委員会という組織が担います。
社員旅行委員会とは、社員から有志を募って組織される集団です。
社内ではSRIと呼ばれます。
（Shain Ryokou Iinkai、頭文字をとってSRIです。）
SRIはいくつかのチームから構成されます。
宴会、バスレク、しおりなどの複数チームで分担して作業を行います。
各チームの紹介は割愛しますが、しおりアプリはお察しのとおり、しおりチームが他チームに問い合わせた情報をもとにアプリ作成を手がけています。
私がSRIに参加したのは昨年、しおりをPDFで配布した年からになります。
このままではいかんでしょ？という思いから、本気でアプリ化してやろうと思い立ち上がった次第です。
しかしながら、その年はあまり仕事等で時間をとれなかったこともあり、アプリ化を断念し、2018年からアプリ化の流れに戻しました。

しおりアプリに求めたもの
私がしおりアプリに求めたものは2点あります。
1点目は、仕事の成果物と同じ品質です。
ユーザ体験を含む、あらゆる面で可能な限り高い品質を追求したかったのです。
実際のところ、仕事が忙しくなるにつれてリッチな機能から削ぎ落とされていき、最低限の機能だけ残る形になってしまいました。
そこは残念なところですが、来年の課題としてコードを書く余地が残っているという点で、次の開発を楽しみにしています。
2点目は、部署メンバーの教育につながるサンプルになることです。
私が所属する部署のメンバーがこの規模のアプリを難なく読み書きできるようになるためのサンプルにするため、ソースコードの品質には特に注意しました。
悪いお手本を参考にコードを書くより、良いお手本を参考にコードを書く方が上達が早いはずです。
私の書くコードが良いお手本とは言いませんが、少なくとも私が関わった今までの仕事の中でトップレベルの品質になったと自負しています。
後述するアーキテクチャやライブラリを社内に浸透させるためのサンプルにするためにも、しおりアプリは極めて重要な立ち位置にあります。
すでに社内ではソースを公開しており、弊社の社員であれば誰でも修正できる状態にしています。

しおりアプリの基本要件
アプリ化当初からある要件は次の4点です。

社員旅行の行程を閲覧できること
社員によってプランが異なるため、個人別の行程を閲覧できること
社員全員の名前、所属を顔写真つきで確認できること
緊急時の医薬品、連絡先等を確認できること

そこに、今年は次の4点を加えました。

行程の予定時間が変更される場合、その通知を行えること
行程の更新を含めたアプリのデータを更新できること
データの更新のためにアプリの更新を必要としないこと
従来のアプリよりもう少しまともなログイン機能を設けること

最終的に、以上の8点を要件として開発することになりました。
本当に最低限の機能です。

アーキテクチャとライブラリ、それと言語
Android/iOSともに、MVVMとClean Architectureを組み合わせたアーキテクチャを採用しました。
その意図は、部署メンバーの教育に直結するアプリにするためです。
新卒を含め、社内の若手メンバーにアーキテクチャの重要性を認識してもらうことができれば、私がこれらを学習した時間より短い時間で習得してもらえると信じています。
なお、本記事ではアーキテクチャの解説は行いません。
ライブラリは以下のものを採用しました。
※ライブラリの説明は省きます。
※特に有名なライブラリはリンクも省きます。
※Android版のみです。
※バージョンの表記があったりなかったりという揺らぎはご容赦ください。



ジャンル
ライブラリ




View
PDFView


View
DataBinding V2


DI
Dagger2


Logger
Timber


DB
Room


JSON Parser
GSON


Image
Picasso


HTTP client
OkHttp3


HTTP client
Retrofit


Other
LeakCanary


Other
RxAndroid/RxKotlin



ライブラリからもお分りいただけると思いますが、開発に使用した言語はKotlinです。

利用したサービス
毎年アプリに手を入れなくてもデータを差し替えるだけで運用できるようにしたいなどを理由に、ファイルを置いておくサーバやAPIなどが必要でした。
しかしながら、しおりアプリ開発自体に予算があるわけではないので、選択肢は基本的に無料で使えるものに限定されます。
また、自前でバックエンドのアプリケーションを作る余力もなかったため、バックエンドとして次のサービス（ツール？）を利用しました。

Googleスプレッドシート
mobile backend

Googleスプレッドシートは社員旅行の行程などテキストで管理できる情報を管理するために、ニフクラのmobile backend は画像ファイルの保管庫としてファイルストアを利用しました。
また、mobile backendはプッシュ通知も利用しました。
Googleスプレッドシートを選択した理由は、SRIメンバーの誰であってもデータの変更を行えるようにする必要があるためです。
日本にいてIT業界に身をおく人でExcelを触らない人はほぼいないと思うので、スプレッドシートなら分かるでしょという安直な発想です。
プッシュ通知はSRIからの連絡手段として利用します。
旅にはイレギュラーな出来事がつきものです。
例えば交通渋滞、宴会の準備が間に合わないなど、実に様々です。
そういうときは予定を変更せざるをえません。
その連絡手段としてプッシュ通知が必要なのです。
（実はプッシュ通知がなくてもよかったのですが、そこはしおりアプリ単体で完結させるためという言い訳をさせてください。）

バックエンドそれはGAS
データをGoogleスプレッドシートで管理すると決めてから、それと連動するかたちでAPIを提供したいということもあり、もっとも手っ取り早いGoogle Apps Script（GAS）を選択しました。
基本的にはGASをAPIとして叩くことでJSONを取得できるようにし、アプリで使用するデータを更新するというという手段をとりました。
これはそのままスクリプトをガリガリ組んだわけですが、本来ならPOSTを使いたいところを全てGETで対応することになりました。
POSTにするとリダイレクトされてしまい、期待通りの結果を得ることができませんでした。
期待通りの結果を得られるようにアプリを修正すればいいといえばそうなのですが、正直そこまでの労力をかけたくなかったというのが本音です。

閑話：JSONの渡し方
旅程情報や社員情報などのデータはそれぞれGoogleスプレッドシートの1セルにまとめて格納して、GASでそれを取得して渡すという方法をとっていましたが、データ量が多すぎたため、1セルに収まりきらないということが分かりました。
そのため、データを事前にJSONファイルに書き出し、それをGAS経由で渡すという手段に切り替えました。

データの管理、取得方法
テキストとして渡すことのできるデータは全てスプレッドシートで管理しました。
シートをテーブル、行をレコードという扱いで、DBのように扱いました。
（繰り返しになりますが、SRIメンバーの誰でもデータの変更を行える必要があるためそのようにしました。）
スプレッドシートに入力したデータをGASでJSON形式のファイルに出力しました。
JSON形式のファイルをダウンロードすることで、アプリはデータの更新を行えます。
そのため、1シート1ファイルにし、できるだけ分割して必要なデータのみ更新できるようにしました。
この状態では、テーブルの1部を更新するために、多くのデータをダウンロードする必要があるという欠点があります。
しかし、諸般の事情を考慮した結果、この構造がもっとも都合のいいものでした。
APIのようにその都度データの一部だけ取得できるようにしなかった理由は次の2点です。

オフラインでも動作するようにするため（他の諸般の事情による）
GASの動作速度が遅いため
スクリプトを動作させられる上限に達する恐れが高いため

しおりアプリを利用する社員数とGASのスクリプトを叩く回数を考えるとスクリプトを動かせる上限に達してしまう恐れが高いため、頻繁にアクセスしないようにする必要があります。
GASにアクセスする回数を抑えるために、ユーザに手動でダウンロードさせることでその問題を解決しました。

求めたのはアプリからのプッシュ通知
プッシュ通知をアプリから送りたかった。
mobile backend を選択した最大の理由はこれです。
多くのサービスではアプリからプッシュ通知を送るためにWebの管理画面あるいは別途APIを構築する必要があるように見えました。
それを作る余裕はなかったのです。

社内オープンソース化
有り体に言えば、ただ社内でソースコードを公開しただけです。
マージリクエストは大募集していますが、仕事でもないソースコードを見るなんて酔狂な人はそういません。
何を目指しているのかをまだまとめられていないので、機能追加が難しいというのもあると思っています。
なので、issueなどを作成して、マージリクエストを出しやすくするというのも検討中です。

今後の課題
さて、ここまでしおりアプリ開発のもろもろを書きました。
しかし、最終的にできあがったものは当初の計画のおおよそ半分程度の機能しか実装できていません。
そのため、次は当初の計画にあった機能も実装していくことを課題としています。

あとがき
アプリの機能に関してはほとんど説明しませんでした。
「旅のしおり」にあるべき機能を最低限盛り込んだアプリであることを考えれば、説明するほどのものでもないでしょう。
プッシュ通知を送る処理など、個別のものについては公式でサンプルもありますが、どこかで記事を書くことがあるかもしれません。
今思えば、もっといい作りにできたのではないかというところが数多くあります。
Kotlinのバージョンが1.3になり、コルーチンが正式版になったということもあります。
来年はそのあたりをソースに反映することになるかもしれません。
しおりチームのメンバー構成についても時間があるときに追記したいと思います。
",False,https://qiita.com//shmz/items/d3141fc234a7192c99e7
"

Kritik der kritik der Qiita
全ての開発者がQiitaへのアウトプットをやめるべき理由 - qiitadaisuki
この記事が論争を巻き起こしている。興味深い問題であるためこの記事を投稿しようと思う。

Kritik der Qiita
まず、上の記事で展開されている批判の概要を述べておく。

Qiitaに記事を投稿するよりも自分でブログを開設した方が技術力が評価されやすい
Qiitaに専門性の高い知識が投稿するのは無意識の ダンピング である
Qiitaには意図的に質の悪い記事を書く人間が存在する

以上の批判は、Qiitaの内包する次の三つの性格から生まれている。

自己アピールの場としてのQiita
市場におけるQiita
誰でも参加できるQiita


自己アピールの場としてのQiita
自分の技術力をアピールするのなら、自分でデザイン、保守、SEO対策をしたブログを開設した方が良いのは明白である。 
しかし、これは自分の技術力が評価されることを望む技術者だけに当てはまるものである。Qiitaを利用することで簡単に技術記事を投稿し、Qiitaを利用する技術者の目に触れることで自分の学びを確実なものにしようとする利用者にも言えるものではない。

市場におけるQiita
Qiitaが生産手段(記事を投稿する場)を提供する代わりに記事の価値を搾取しており、同時に専門性の高い記事がQiitaの競争力を極端に高めている、という。
ここから前掲記事の筆者が導いたのは、「自分でブログを開設することで搾取から開放さるべきだ」という命題であるが、「ダンピング」の否定にはそぐわない。自分のブログで記事を公開しても、自分にアフィリエイト収入が入るだけで、「ダンピング」であることに変わりはない。
そもそも、Qiitaに専門性の高い記事を投稿することは本当に「ダンピング」なのだろうか？
いらすとや が絵描きの職を奪ったのだろうか？フリー素材が写真家の職を奪ったのだろうか？これらは確かに、安い値段である程度使えるレベルの絵を書いたり写真を撮ったりする人間の職は奪っただろうが、根本的な破滅を引き起こしてはいない。むしろ、仕事の効率を上げるのに役立っている。
Qiitaもこれと同様ではなかろうか？絵描きや写真家といった芸術家と同列に比べることはできないが、Qiitaに投稿される記事のレベルが上がるにつれて、職場でレベルの低い質問をする後輩が減り、質問をするにしても基礎的知識を踏まえた上での質問がしやすくなるだろう。
更に、前掲記事の筆者は、Qiita運営の果たす役割を過小評価していると言えよう。サーバーの保守などについても言うことはできるが、複数の人の記事を保有するQiita という観点から見ると、Qiita運営側が果たす役割を評価することができる。Qiitaのシステムに言及しておこう。
まず、Qiitaに存在する記事はQiitaのインターフェイスを利用することで簡単に検索ができる。これはGoogle検索では不可能なことだ。すべてのプログラマーが自分のブログで記事を書いた場合、こうした統一的な手法で記事を探すことは難しくなるだろう。
そして、Qiitaのトレンド機能である。Qiita内の記事から同一の基準でトレンド記事を抽出している。これもGoogle検索にはまだ不可能なことである。
つまり、多くのプログラマーがQiitaに集うからこそ享受できるインターフェイスが存在するのである。

誰でも参加できるQiita
Qiitaには意図的に質の悪い記事を書く人間が存在するというのは、賛成する人間が一定数いるようだ。もしかしたらこの記事も質の悪いそれの一つなのかもしれない。基準を定めることは難しい。
私はこの問題に対して有効と思える批判をすることはできない。オープンな場において常につきまとう問題であるからだ。
これに対し、Qiitaから離れないという立場から、一つの提案をしたい。
Qiitaで質の悪い記事、著作権を侵害した記事を淘汰できるシステムが作られるべきである。
現状でも記事を報告する機能はあるが、冒頭記事で参照されているQiitaブログの通り、質の悪い記事を淘汰するシステムは作られていない。こうしたシステムを構築することで、質の悪い記事を功利主義的に淘汰していくしか道はないだろう。

まとめ
冒頭にURLを貼った記事の指摘は的確なものであり、納得できるものである。その中で疑問を感じた部分に関して批判を述べた。
この記事への批判も歓迎したい。
",False,https://qiita.com//junfred/items/6b53d276c8e147b54237
"

このような傲慢なタイトルが多く、業を煮やしました
ここでは、私なりのプログラミング上達論を言います。
なお、爆速で成長するという意見も、傲慢極まりないと考えます。

はじめに

あなたなら、完璧なプログラミング言語をどのようにデザインしますか？

に、アラン・ケイ氏が回答されておりますが、
「完璧であることとより良い事は、必要であることに対する敵です。」

最初にJavaを学ばないでください
ある専門学校では、一貫してJavaを教育します。
欠席すると教師が
「このタイミングで休む事をどう思いますか？」
と教室の生徒全員に聞きます。
生徒たちも、「信じられません」と言います。
この専門学校を卒業した生徒の就職率はとても良いです。
会社の評判では、とても良く飼いならされていて使いやすいそうです。
そして、幾人かはその考えに染まりきれず脱落し、
かなりの人間が柔軟性のなさから、生存率を下げ心を病むのです。

オブジェクト指向は　哲学？宗教？
Quoraの回答に対する低評価とその見解
十分に抽象化されたオブジェクト指向のソースでさえ、
激動のプロジェクトを目の前にすれば、それが如何に無力なものかを理解します。
私のプログラミングスタイルでは、引数が２つオプションで追加され、
処理が渡されるまでのコーディングは１０秒ほどでバージョンは２つ飛んでも問題なく管理でき、NPMにパッケージが上がります。
派生バージョンを考え尽くし、利便性と言語学に研鑽し、手短な開発を実現していきます。
必要であればシングルトンを書き、ファクトリーに入ったインスタンスが使用された後に削除されます。
インスタンスそのものに対してのプログラミングは最小限になり、一つのセオリーの記述にのみ専念できるスタイルを取ります。
ポリモーフィズムのための不用意な継承は悪とすら考えるほどですが、誰でも同じレベルでプログラミング出来るわけではありません。
そして、そんな私にもミスはあります。

大切なのは、本来の楽しさ
そこで、まずはパラダイムの事は脇に置きましょう。
そして、scratchをおもむろに開くのです！
もし、近くにcoder道場があるなら、
そこのメンターになることで、scratchの理解は早まるでしょう。
もし、Scratchが肌に合わないならSqueakをおすすめします。

なぜ、この２つの環境が重要なのか？
昨今、Node-Redや、IBM BlueMixなど、ブロック接続型のVPLも利用者が増えてまいりました。
数学的にも、圏論を主体としたプログラムを矢印で結ぶことの大切さが問われ始めています。特に、Squeakは、Smalltalk由来で、そのままソースコードを見てSmallTalkに親しむことで、Squeak Smalltalk環境でのコーディング障壁を下げることが出来ます。

ある、Qiitaの記事のコメントで…
和訳ですが「日本では、まだJavaとVBScriptを使っていると思っている」というコメントが入りました。こういうコメントが入ることを、我々は真摯に考え直さなくてはなりません。

抽象化すべきものの正体
抽象化するべきは、現実世界ではなくあなたの目の前にあります。
そう、端末の計算資源です。
オブジェクト指向は、目の前の計算資源に対して現実世界を専門的に取り扱うための仮想の計算機があるという事を実現したかっただけなのです。

終わりに
あらゆる言語は、その考察された経緯があります。
ある言語を叩くのは、自分がただ、その言語が嫌いなだけという偏見の押し付けにしか過ぎません。
逆に、ある言語だけを褒めるのも同じだと思っています。
言語の製作者の皆様には、畏敬の念を忘れてはならないだけです。
そういう真摯な気持ちが、プログラミングやデザインを超えた問題領域の解決に繋がるだけですので、深呼吸をして、プログラミングに取り組んでみてください。

補足、参考など
コーディングのスピードを上げるためには、どんなトレーニングやテクニックが効果的ですか？
Swiftと圏論への応用
",False,https://qiita.com//johnny-shaman/items/d29485179587c48f1da1
"

我々はそろそろ、真剣にBlueprintについて考えるべきではないか
あなたは、本当にBlueprintをご存知だろうか。
Blueprint、もはやこれは、地球規模のプログラミング言語なのだ。
あなたも無関心ではいられない、無関心であることがノードベースプログラミング迫害につながっているのだ。
ゲーム業界ではかなりノードベースプログラミングが迫害されているというほうこくがある。
我々は長期にわたり、さまざまなBlueprintの潮流を体感し続けてきた。
ゲーム業界はBlueprint無しでは生きられなくなるだろう。
今回公開する記事を通し、その事実を今一度、思い出していただきたい。
【あなた方の知りたくない真実】 MaterialはBlueprintとは別のものであり、組み方とかが違う

イベントドリブン

for Unity(MonoBehavier)
インプットを取得するには、以下のようなコードを書く必要がある。

Input
void Update () {
    if (Input.GetKeyDown(""return"")){
        //エンターキーが押されたときの処理
    }
}



for UnrealEngine(Blueprint)
インプットを取得するイベントは、標準で組み込まれているのである。
　
登録したキーが押されたときに発火するActionMappingsや、スティックの倒され具合を正規化してくれるAxisMappingsもあり、わざわざ余計なことをしなくても、体感的に正しくやってくれるのである。
【あなたが知りたくない真実】 一見Blueprintには見えないが、LevelもBlueprintの一部である。

UIについても、MonoBehavierだと、「表示されるべき値が変更されたときに、UIの値も変更する」といった、気の利くことはやってくれない。常時UIを書き換えるか、気の利くことをやってくれるかもしれないUniRxを使用するかである。
Blueprintでは、表示されるべき値を監視することで、「値が変更されたときに、UIの値も変更する」という、気の利くことをやってくれるのであり、もっとUIを最適化したければ、イベントドリブンで更新することも簡単なのである。
【あなただけが知らない真実】
イベントディスパッチャーも、ブループリントインターフェイスも、発信されたものを受信して行動しているので、ReactiveExtensionsであるのだ。

便利な機能
あなたは回数を限定して実行するときにDoNを使用することで、

DoN
void Update () {
    if (counter > 10) { return; }
}



if文とカウンター変数を省略する事ができるし、
一度だけ実行するときにDoOnceを使用することで、

DoOnce
void Update () {
    if (flag == true) { return; }
}


if文とフラグを省略する事ができる。
【気づきづらい真実】 Unityのオブジェクトは多重継承だが、UnrealEngine4のアクターは単一継承である
MathExpressionは画期的な機構である。数式を記入するだけで、なんかいい感じにコーディングをやってくれる。
もはや複雑な計算を実現するために、多重カッコのスパゲッティーに悩まされることもないのだ。
出典　我々はそろそろ、真剣にバンブーについて考えるべきではないか。（ダイハードティルズ・@NJSLYR）
",False,https://qiita.com//Polaris1080/items/c7d76645ff59a9fea5f5
"

前書き
今から書くのは、プロトタイプ開発に対する批判ではなく、プロトタイプ開発”もどき”への批判です。
正しく管理されたプロトタイプ開発ひいては、アジャイル開発は素晴らしいものです。
これは実際の経験から、書いているので、読んでハッとする方もいるかもしれませんが、あくまで例だと考えて読んでください。
（これを、システムを発注する側の人間が読んでくれた嬉しいのになあ…）

技術検証をしていない
プロトタイプ開発のメリットとして、事前にネックとなりそうな技術的要素を検証するできることにあります。
そうすることで、対処できない段階になってから発覚することを防ぐことができます。
例えば、ウェブサービスの開発と一口に言っても、必要なものはウェブサーバーだけではないのです。
負荷分散・可用性・認証・CI/CDツール・セキュリティ対策に必要なものや脆弱性診断ツール・ソース管理ツール・ログ管理などなど。
一般的なコンシューマー向けサービスであれば、上記に加えて要件により必要なものを加えるとキリがないくらいです。
そして、それらを実際に導入することで起きうる問題を予め潰すことで、スムーズな開発を進めることができます。
「面倒だから」と怠れば、間違いなく後半になりプロジェクトは炎上します。
その時かかる人件費、どころか完成できないリスクを考えれば、有料であろうがなんであろうが、外部サービスの導入は早めにするべきです。

リリースをしない
コンシューマー向けだろうがなんだろうが、リリースをしていないものに価値はありません。
技術的な検証のみならず、実際のユーザーのフィードバックを取り入れなければ、そのプロダクトはただの空想の産物なのです。
勘違いしている方が多そうなので言っておくと、とりあえずお客さんに見せたいから作るレベルのものは、プロトタイプではなく「ワイヤーフレーム」です。
そんなものは、２週間程度で作られるものであり、何か月もかけて作るものではありません。

品質が最悪
プロトタイプ開発だからと、スピ－ドだけを重視した結果、品質が犠牲になっていることがあります。
これは、リリースをしていない場合、より顕著になりそうですが、顧客の要望を変則的に受け入れる以上テストコードなどの品質担保ツールはどうしても必要なはずです。

途中で人員を入れ替える
下請けに限らず、プロトタイプ開発から人材を入れ替えることは、スキルセットのマッチングを意図した人材を確保しなければいけないなどの、問題が多く発生します。
また、今までの顧客との信頼関係を再度築かなければなりません。
プロトタイプ開発やアジャイル開発において、顧客との信頼関係は最重要です。
総員入れ替えなど、もってのほかです。
また、大きな声では言えませんが、これは大手ベンダーなどがリリース時、または開発後半における責任逃れのために行う行為です。
下請け発注し、、責任と残業を押し付け、自らの社内での立場が悪くならないよう汚いことでもなんでもやります。
（大手ベンダーの圧政実力主義の闇が見えますね。）
違法にしたほうがいいです。笑

まとめ
これを読んで、少しでもプロトタイプのまがい物を買わされる詐欺被害者が増えないことを祈ります。
",False,https://qiita.com//zzzzz/items/95b6941f95f1a4f20d8b
"※この記事には「ネタ」タグがついていることからお分かりいただけると思いますが、意味のある記述はほとんどありません。それを快く思わない方は速やかにお戻りください。

問題提起
最近、Qiitaでの記事の投稿について、「○○な記事を書くべきだ/でない」といった記事が多いように見受けられます。その件について、個人的な主張をまとめたいと思います。

私の考え
Qiitaの利用に際して、利用規約は必ず守らなければなりませんが、それを守り、ガイドラインに従ってさえいればユーザーには自由に記事を投稿する権利があります。その権利を敢えて制約するように働きかけると、逆に記事を書きにくくなり、有意義なアウトプットの妨げになるのではないでしょうか。
投稿された記事を読むかどうかも、ユーザーに委ねられています。自分が欲しくない情報は読まなければよいだけなのです。内容の薄い記事に対して文句を言っているよりは、良記事を見つけ出してそれに「いいね」をつけるほうが、よほど建設的だと思います。良記事に「いいね」がたくさんつけば、相対的に内容の薄い記事は人の目につきにくくなります。
Qiitaの使い方は人それぞれです。技術的な情報を共有するために使っている人もいれば、備忘録として使っている人もいます。はたまた、プログラミングのパラダイムなどについての議論の場をQiitaに設ける人もいます。Qiitaが広く利用されているのは、こういった幅広い使い方が可能だからではないでしょうか。Qiitaは自由なアウトプットの場であるべきです。初心者が書いたような内容の薄い記事に対しても、もう少し寛容であってもいいのではないでしょうか。
内容の薄い記事が増えると、初心者の学習の効率が下がる、といった意見もあります。しかし、必ずしも良い記事を読めばそれが身につくとは限りません。いきなり専門書を読める初学者はいません。色々なレベルの書き手がいるのと同様に、読み手のレベルも様々です。書き手が読み手のレベルを想定して書くのではなく、読み手が自分のレベルに合った記事を選ぶべきなのです。記事のレベルをよく見極められない初心者は、とにかく色々な記事を読んで、自分に見合うものを探すのが、学習の第一歩です。

結論
Qiitaは自由な情報共有の場であり、情報の発信も利用も、ユーザーに委ねられています。自分の求める情報が簡単に見つからないからといって、不平不満を言うのは止めましょう。調べる努力を惜しんでいては、進歩はありません。
「○○な記事を書くべきだ/でない」という記事は書くべきではありません。ところで、この記事のタイトルを見ると、このパターンに一致しています。したがって、この記事自体、書くべきではありません。さて、私はどうしてこの記事を書いたのでしょうか。
",False,https://qiita.com//hogefuga/items/18a824f419262b771dec
"（タイトルは流行に合わせて付けました）
エンジニアのアウトプット、Qiita記事の質、おかしな記事のアカウント停止等、Qiita周辺で色々と話題が耐えない昨今ですが
Qiitaでのアカウント停止に関する考えについて - Qiita Blog
という記事が発表され

Qiitaでは「技術的にレベルが低い」「技術的に間違っている」「Qiitaというサービスや運営に批判的である」といった理由で記事の非公開化やアカウント停止という措置を取ることはございません。

と明言されたのでこの機会にQiitaに記事を書くのをやめるべきだと思う理由を書かせていただきます。

アウトプット＝Qiitaではない

「エンジニアのアウトプットは大切」
「Webでアウトプットが確認できると採用可能性が上がる」

これらは事実でしょう。しかしアウトプット先がQiitaである必要は全くありません。

むしろ個人サイトのほうが技術証明になる

Qiitaに登録してMarkdownで記事を書いているだけの人
個人サイトでWordPress等のシステムを運用して記事を書いている人

これだけで比較した場合、技術力の証明という意味では後者のほうが優れているのではないでしょうか？
特にWordPressやRails、React等、需要の高い技術を使って個人サイトをしっかりと構築すればそれ自体がポートフォリオとなるでしょう。

技術記事には価値がある
ダンピングという言葉を知っていますか？
不当廉売 - Wikipedia 

不当廉売（ふとうれんばい、英語: dumping, ダンピング）とは、市場の健全な競争を阻害するほど不当に安い価格で商品を販売すること。

技術的知識には一定の価値があります。どんな軽いことでも時間を費やして得た知識には価値があります。それどころかとても高度な記事をQiitaに投稿する人もいますよね。皆さんが無償でQiitaに記事を書く行為もこのダンピングに近いのではないでしょうか？
自己運営のブログに公開すれば直接的でなくともアフィリエイト等で本人に対価が行く可能性もあったはずです。より高度な知識なら、個人でもnoteや技術系同人誌などの方法で販売することもできたはずです。しかし皆がQiitaに無償寄稿し続ける限り儲けるのはQiitaだけです。

SEOによる悪循環
皆がQiitaを利用し大量の記事が集まることによりQiitaはSEOに極端に強くなり、何で検索しても結果上位がQiitaとなっています。これは主に以下2つの結果をもたらします。

質の低い記事でもQiitaにあるだけで上位に表示される
個人サイトへのアクセスが減りQiitaだけが儲かる


Qiitaをやめればおかしな人達を排除できる
意味のわからない記事を書いて炎上しトレンドに入る人、注目されやすさを狙った記事を書き売名を狙う人、それらの人達がQiitaに来る理由はQiitaに人が集まっているからです。既に人が集まっているからそういう人達が寄ってくるのです。これはQiitaの人気がこのまま続く限り避けられません。
しかし高品質な記事を書く人達がQiitaを使うのをやめれば、次第にQiita自体の価値が下がるでしょう。そしてQiitaに目を向ける人が減れば、上記のような注目されたいだけの人も行き場を無くすでしょう。

まとめ

Qiita以外のアウトプット場所を自作する方が技術力の証明になる
Qiitaに投稿することで各個人が本来取り得た対価を損なっている
Qiitaへの一極集中が解消されれば


質の低い記事を目にすることも無くなる
注目目当ての人達の居場所も無くなる



以上のことから、全ての開発者がQiitaを離れるという選択肢は様々な一考の価値があると私は考えます。
将来Qiitaが廃れたときのリスクヘッジの意味でも、早いうちに個人サイトを作成して独り立ちしておくことをお勧めします。
",False,https://qiita.com//qiitadaisuki/items/2160a390ce91283707a1
"Qiitaでは最下部にある「投稿」を押すと、新着順投稿一覧を見ることができます。
あまりに目立たなすぎて、そんなに新着順見せたくないのかという気になります。
そしてここ、タグ検索やトレンドでは殺されてしまったコメント数表示が唯一残されているところでもあります。
しかしどうも、昨日あたりからコメント数表示がおかしい。

適当に新着をもってきたところ、4記事にコメントがついているように見えます。
しかし実際にコメントがついているのは青丸の1記事だけで、他の3記事にはコメントがついていません。
他の記事でもこのような例が散見されます。
私はミュート設定などを一切していないので見えないコメントがあるというわけでもなく1、またタイミングからしてSPAMが削除されたというわけでもなさそうです。
最初の\itemsを拾ってきた段階で既にコメント件数は入ってきているので、こちらから原因を窺い知ることはできなそうです。
これはいったい何なのでしょうかね？
あと青丸記事の人は即日仕事辞めたほうがいい。




というか未ログインの別ブラウザでも発生する。 ↩



",False,https://qiita.com//rana_kualu/items/0797252f00314aadeac8
"はじめまして、PHPプログラマーのアリスです。私はこう見えて日本人ですが、母親はイタリア人です。さて、今回は入社3日目の初心者PHPプログラマーが仕事でどんなことをしているか書きます。

1日目
1日目は、最初に自己紹介を行いました。
「山田アリスと申します。専門学校でプログラミングを学びました。バックエンドプログラマーとして配属されましたが、最初はわからないことが多いと思いますが、よろしくおねがいします。」
と言いました。歓迎ムードではなく、みんな無表情で手をぱちぱちしていました。
その後、「新人は入社後の手続きをしてください」と言われ、そのとおりにしました。その日の記憶は、それしかありません。

2日目
配属先で何をするかと思ったら、上司から「自分のPCの環境を完璧に設定しろ。後からバカみてぇにコロコロといらねーもん入れたら承知しねーから、今のうちに必要なものは入れておけ」と言われ、EmacsとGNUのツール一式、puttyなどを入れました。ちなみにPHPとPostgreSQLの環境はちゃんと入っていました。
上司に確認すると「おい、完璧にって言っただろ。diffを取るツールは入ってるのか？」と言われたので、「GNUツールとEmacsを組み合わせればdiffは取れます」というと、「上司よりも知っているような態度を取るなバカ」と怒られてしまいました。気をつけたいです。
結局、２日目は環境設定だけで終わりました。

3日目
3日目は、「お前は初心者だから、バカみてぇに学習用プログラム書いてりゃ良いんだよ！」と言われ、PHPとPostgreSQLを使って掲示板プログラムを作りました。
掲示板は、匿名掲示板を真似して作るように言われただけで、具体的なことは自分で考えろと言われました。私は何かフレームワークを使おうと思いましたが、上司は「俺達はフレームワークなんて一度も使ったことはねぇし、それがここで書かれているプログラムなんだよ、偉そうにフレームワークなんて使うな」と言われたので、スクラッチで作りました。
大雑把に言えば、PostエンティティとUserエンティティを定義し、Userエンティティには匿名ユーザーの属性をCookie内のIDと対応させるように設計し、Postエンティティは、Userエンティティと多対1の関係を結びました。Postにはタイムスタンプと投稿内容、投稿したUserのIDが保存されます。
フロントエンドは苦手でしたが、とりあえずbootstrapを使って簡易的に作成しました。
上司に作ったものを見せると、上司は「オメーは偉そうにbootstrapなんて使うな。学習目的ならCSSをイチからバカみてぇに書いてりゃ良いんだよ」と言われたので、CSSをちゃんと書くことにしました。
すでにここまでで3時間ほど経ってしまいました。実は上司は「こんなクソみてぇに簡単なものは2時間で作れなきゃ困るな！」と言っていましたが、すでにその時間を大幅に過ぎてしまっていたのです。
それでもなんとかCSSを完成させて、デザインをちゃんと見直してから上司に見せると、「オメーはなんでPHPコードを関数で分離してるんだ。俺達は分離なんてしないでデケェコードを保守してんだよ。自分だけわかりやすく書こうとするな、他の人間に揃えるんだよ！」と言われたので、分離されたコードを一枚岩の巨大なコードに変更しました。
食事後、すでに4時間経ちましたが、上司の口の悪い言い方で不安になっていたので、上司にコードを見せるのが怖くなっていました。それでもあきらめたくないので、テストコードを書きました。一枚岩の大きなコードはテストすら難しい状況でしたが、各段階でassertを書くことでテストを行いました。
テストを行い、少し自信を取り戻しましたが、そのテストを書くのに2時間かかっていました。すでに6時間経っています。急いで上司にコードを見せました。
上司は「おい、テストコードなんて俺らは書かねぇんだよ、バカ。俺らはバグが発生した時点で担当者にメールを送るコードを書いてデバッグしてんだよ、自分だけズルするな。」と言われたので、最後の修正を行いました。

学んだこと

コードは分離してはいけない。
テストコードは書いてはいけない。代わりにメールを送ってデバックする。
bootstrapは使ってはいけない。
フレームワークを使ってはいけない。
上司よりも知っているような態度をとってはいけない。
私はバカなので、努力しないといけない。

がんばりたいです。
",False,https://qiita.com//alice1992/items/630662ef7238e60a811e
"先日Qiitaを賑わせていた「今後必要になるマスター言語」が気になったのでPersonality Insightsに食わせてみました。

背景
同年代のエンジニアなので若干のひいき目で見ようと思ったが、コメント欄の意味不明な発言や人間性を疑問視してしまうような発言も多々あったので、この人物がどのような性格なのか興味が沸いてきた。

下準備
既にQiitaからはユーザ自身がサスペンドされているので、記事を直接見ることはできないのでweb.archive.orgから原文のみを拝借。
https://web.archive.org/web/*/http://qiita.com//administrator1974/items/387aab2a42bf57e3b215

調理
WatosonさんのPersonality Insightsに怪文書を放り込む。
Personality Insights
ここの「デモを試す」から「性格分析 Personality Insights を試してみる」を選択。
表示されたページの「テキスト入力」「任意のテキスト」に本文を貼り付けて「分析」。

分析結果
【性格特性】
1056 分析された単語: 確度が低い解析結果
Watosonさんでもこの文章だけでは判断に迷ったようです。
通常はこれくらいの単語数があれば精度の高い分析ができると思うのですがさすが怪文書。以下に分析結果の総評を張り付けておきます。
コメント欄の発言も貼り付ければ、もっと精度は上がると思われる。
結果
表現に富むタイプであり、熱くなりやすいタイプであり、また独特なタイプです.
哲学的なタイプです: 新しいアイディアに興味をそそられ、進んで受け入れ、探求することを好みます. 
冒険的なタイプです: 新しい経験をすることを熱望しています. 
また、自己主張が強いタイプです: 遠慮なく発言し、その場をリードする傾向があります。また、集団を統率できます.
現代性につながる体験を好みます.

生活を楽しむことと伝統の両方にあまりこだわりません. 
単なる個人の楽しみよりも大きな目標を伴う行動を優先します. また人が通った道よりもわが道を行くことを大切にします.

下記のような傾向がありそうです______
・自動車を買うときは維持費用を重視する
・歴史映画を好む
・社会貢献のためにボランティア活動をする

下記の傾向は低そうです______
・商品を購入するときは家族の影響を受ける
・商品を購入するときは商品の実用性を重視する
・ドラマ映画を好む

ざっくりまとめると「独特のタイプで冒険好き、自己主張は強くその場をリードすることができる。人が通った道よりも我が道を行く」タイプのようです。我が道を進んでいるのだから、周囲とズレていてもあまり気にしない人なのかもしれない。

総評
Watosonでは文脈や語彙量、普通体 or 丁寧体、感嘆詞の有無などで分析をしていると思われますが、文章の中身が正しいかどうか判断することも重要ではないかと感じたので記事にしてみました。
",False,https://qiita.com//bathclin/items/4a1f035c85cb18886cb4
"（前提）
弊プロジェクトでは===は使わずに==を使う慣習があります
<?php

$str = ""0"";
$cmp = ""test"";
var_dump($str == $cmp); // false

$str = 1;
$cmp = ""test"";
var_dump($str == $cmp); // false

$str = 0;
$cmp = ""test"";
var_dump($str == $cmp); // なんとtrueになる！


",False,https://qiita.com//miyatahirotaka/items/f64858e82fb9442088cc
"どの言語を学ぶべきかがトレンドで大盛況ですね。
でも私はその全ての論争を終結し得る、もっと習得が必要となる３つの言語をピックアップしました。
きっと全ての開発者がこれに賛同してくれることでしょう。

1.日本語（母国語）
日本語は日本国における公用語ですが、日本人にとっては母国語に当たります。
母国語が日本語ではない方は、日本語の記述を母国語に置き換えて読んでみてください。
これははっきり申し上げますが、日本語を習得せずに日本で仕事をするなど不可能です。

日本語ができることのメリット

日本語の資料を読み書きできる
日本で仕事をすれば間違いなく日本語の資料が存在します。
顧客の要望や仕様書、プレゼンテーション、メールに至る日本人からのドキュメントは全て日本語です。
日本語を正しく読み理解する能力がなければ、その人が何を希望しているのか汲み取ることはできません。
日本語を正しく書くことが出来なければ、自分が何を言いたいかも伝えられません。
Qiitaも書けないし読めないんです！

日本人と話せる
日本語の聞き取りとスピーキングができれば、日本人と話すことができます。
読み書きよりもエビデンスが残りにくいというデメリットは存在しますが、その場で迅速にコミュニケーションを取ることが出来ます。

日本人を顧客にできる
日本語の喋れない日本人を相手にビジネスしたいですか？
私は嫌ですね……時々いますけど。
仮に日本生まれの日本人なのに、日本語が喋れない人と日本国ベースで仕事ができる人が居たら教えてください。

コミュニケーションロスが減る
自分が考えている内容を考えている通りに文章や言葉にできれば、相手から「それはどういうことですか？」と聞き返される回数が減ります。（人間は意識を共有できる高等な生物では無いので、ロスを０にすることはできません）
日本語は特に主語が抜けやすい言語ですが、正しく表現することができれば有用な言語です。
ロスを減らすこと、それはすなわち早く仕事を終わらせることに繋がります。

日本語ができないことによるデメリット

言ってることがわからない
文章や話の理解力を欠いた状態で仕事することはできません。何故なら人の要望を理解せずに、その人の希望するものを作り出すことはできないからです。

思っていることが伝えられない
支離滅裂な文章を想像してください。

オレンジが欲しいので天気が、いいですよね、

この文章から筆者の考えていることを想像することができたら、あなたは狂気に満ち溢れています。
自分の考えを正しい日本語で伝えることは、認識の齟齬を生まないためにも絶対に必要なことです。
単純なSlackのメッセージだろうとも、正しい日本語を使いましょう。コミュニケーションロスが減ります。

以上、日本語の必要性でした。
全ての開発者だけでなく、日本人はちゃんとした日本語を使いましょう。
主に、訳のわからないメッセージを飛ばしてくるそこのあなた!
書きたいことはもう以上で書ききったので、あとはおまけです。
ここまで読んでくれたあなたはもう気がついていると思いますが、これは業務時間中の暇つぶしに読むためのネタ記事なので、いいねする暇があったら仕事してください。

2.各種プログラミング言語
ここでいうのはDだとかCだとかそういう次元の話ではありません。コンピューターへ命令をするための言語全てです。
Qiitaが主に対象としているソフトウェア開発者の方々は、何らかのプログラミング言語を使用できるはずですが。

プログラミング言語ができることのメリット
ソフトウェア開発者としてお仕事ができます。
仮にマネージメントを専門とする方でも、仕事を円滑に進めるためにも触りは理解しておくことをお勧めします。

プログラミング言語ができないことのデメリット
ソフトウェア開発者としてのお仕事ができません。

3.英語
英語は英国発祥の言語です。昨今では世界共通言語のデファクトスタンダードとなっています。
ですが英語を母国語とする人は、存外少ないようです。

英語ができることのメリット

英語の情報リソースを活用できる
開発者が何らかの問題にぶち当たった時、解決方法をGoogleへ尋ねると思います。
その時の検索の対象として、日本語だけではなく英語も範囲に含めることができたら、あなたの問題解決力は飛躍的に向上するでしょう。
何故ならそもそもこのIT業界というのは英語圏であるアメリカで飛躍的に発達し、今も強い影響を及ぼしています。情報量は英語の方が圧倒的に多いわけですから、使わない手はないですよね。

非日本語圏の人的リソースを活用できる。
私は以前、ベトナム人開発者を活用したオフショア開発を担当していました。彼らとは英語でやりとりしていました。
この日本においてソフトウェア開発者が不足している中、人材としてカウントできる人種を増やすことができれば、根本的な問題解決を図れます。
外国人と仕事をする場合は、双方が同じ言語でコミュニケーションを取る必要があります。その場合に用いられる可能性が高い言語が、英語なのです。

英語ができないことによるデメリット
これは議論が別れるところでしょう。
日本というのは幸せな国でして、多くの有志によって海外の文献からネットの資料まで、日本語訳されたものが出回っています。
最新情報もタイムラグはあるものの、有用なものは翻訳されて発信されます。多くの国は母国語での情報が無いために英語を使うことを強いられているのですから、これは大変幸せなことです。
Qiita,Teratailなど開発者向けのサービスも豊富ですし、英語ができないからといって開発者になれないことはないと考えています。
ただし、英語を使うことができれば前述のメリットを享受できます。

最後に
似たような記事が乱立しているのですが、普段から強く感じていることなのでキーボードを叩かせていただきました。
日本語がおかしい人と仕事するのが本当にストレスなので、英語とかプログラムとかの前にまずは正しい母国語を使っていきましょう。

関連記事
書こうと思ったことを先越されましたが、意見は違うので是非とも御一読を。
https://qiita.com/Gaccho/items/e12e998b0a1788d55f5f
",False,https://qiita.com//Mister_K/items/1b29544da104dff64fe9
"やあ （´・ω・｀)
ようこそ、バーボンハウスへ。
このテキーラはサービスだから、まず飲んで落ち着いて欲しい。
うん、「また」なんだ。済まない。
仏の顔もって言うしね、謝って許してもらおうとも思っていない。
でも、このタイトルを見たとき、君は、きっと言葉では言い表せない
「ときめき」みたいなものを感じてくれたと思う。
殺伐とした世の中で、そういう気持ちを忘れないで欲しい
そう思って、この記事を立てたんだ。
じゃあ、注文を聞こうか。

前置き
今回記事を起こした理由としてはWindows7→10へアップグレードした人達には昔のスペックのままPCを使用しているのではないかと思い記事にしてみました。
経験上4GBのメモリだとすぐにメモリ使用量が100％近くまで上がりストレスがしゅごいのぉぉぉぉってなってしまうのではないかと思います。
下手をすると2GBのままアップグレードした人もいるのではないでしょうか。
そんな方の少しでも役に立てれば・・・・

注意事項
あくまでPCに関しては素人が試行錯誤している部分の記事なので、
ここに記載されている方法を行ったことによる不具合等々については自己責任でお願いいたします。
間違った知識等を載せる可能性がございますので、まずは行う前にググったりして安全をご自身の目で確認したうえで自己責任でお願いいたします。
というかおとなしくメモリを増設したほうが安心安全ではある。

PCスペック紹介
OS：Windows10 PRO
CPU：Intel I7-7700(3.60GHz)
メモリ：8GB
HDD：1TB 7.2k（回転速度？）　SATA接続
電源：180W　※ブロンズとかシルバーとかではないらしい
立ち上がり直後のメモリ使用率は約45％～55％ほどです。

Cortana強制停止

グループポリシーとかレジストリ変更とかそんな生易しいものじゃねぇ！
フォルダ名を変更しCortanaの起動自体を止めてしまうというもの。
下記の作業を行う際には元のフォルダ名、アドレス等々を絶対にメモっておくこと。
どうにもWindowsUpdateとかで不具合が起きる可能性があるらしい。
あと代償としてスタートメニューでファイルの検索ができなくなります。
その為ウィンドウズキー→アドレス直入力→フォルダ移動ができなくなります。

やり方
1．タスマネからCortanaのフォルダを検索
多分この辺り
C:\Windows\SystemApps\Microsoft.Windows.Cortana~~~~~~~~
~~~の部分はおそらくランダム文字列？
2．このフォルダの名前を変更する。
　 そうすると使用中となっているため変更ができませんとメッセージが出てくる。
　 メッセージは消さずにそのままに。
3．Cortanaをタスマネでタスクキルと同時に2のダイアログにて再試行を押下する。
4．やったねたえちゃん家族がへったよ！
上にも書きましたがWindowsUpdate等で障害が出る可能性があるので、バックアップは確実にとっておいたほうがよいです。

Dell製のアプリケーション停止
読んで字のごとくです。
私が使っているPCはDell製なので色々常駐アプリがあるので片っ端からアンインストールしています。
日々のバックアップを取っていれば特に問題ないかと思います。

オーディオ系のアプリケーション停止
こちらも読んで字のごとくです。
Dell製な為かRealtekHDオーディオマネージャーは入っているのですが、他のオーディオ系のソフトが常駐しているのでこちらもアンインストール。
音響系は特にいじらない仕事なので消しています。

superfetchの停止
ほぼWeb上に落ちている情報をうのみにしていますが、
どうにもアプリケーションの起動を早くするための機能らしいですが、こいつがまぁメモリをよく食べる食べる・・・
メモリのダイエットを狙うだけなら停止してもいいかもしれません。
説明が長くなるので下記の検索結果より適当なWebサイトを開いてやってみてください。
https://www.google.co.jp/search?q=Windows10+superfetch&oq=Windows10%E3%80%80superfetch&aqs=chrome..69i57j0l5.10693j0j7&sourceid=chrome&ie=UTF-8

GoogleCrhome周りを整理する
最近やたらとGoogleCrhomeのメモリの食い方がすげぇんです。
使っていない人は関係ないですが、使用している人はお気を付けください。
こまめにタブを閉じることをお勧めいたします。
あとアドインを大量に入れるとそいつらもバックグラウンドで動き続けるっぽいので
不必要なアドインは消しておいたほうが省メモリ化を図れるかと思います。
お勧めのブラウザがあったら是非とも教えてください。

Crhomeの自動メモリ解放機能
どうにもシステムのメモリが少なくなった時にメモリを自動解放してくれる公式の機能らしい。
ただ完全にサポートしきっているわけではないので色々と自己責任とのこと。
ちょっと調べてみたところFlashを多用する艦これ等々は不具合が出やすいらしいのでお気を付けをくださいませ。
機能としてはタブを切り替えた際に自動的に再読み込みをするとかなんとか。
アドレスバーに
1．chrome://flags/を入力
2．Automatic tab discardingの項目をEnabledへ変更
3．crhomeを再起動で完了とのこと。

まとめ
正直検索すれば色々出てくるとは思うのですが、ざっと効果のありそうな部分だけまとめてみました。
個人的にももう少し色々やってみようとは思っているので気が付き次第更新します。
この記事を見た方も何かいい方法があったらリクエストやコメント等々是非ともお願いいたします！！！
泣いて喜びます。
",False,https://qiita.com//tsukayou/items/27942ce5fcdee1276e9a
"

すべての開発者が学ぶべきたった一つの言語という記事
なんかバズってるし、私もパクって駄文を一つ書いてみることにしました。
ヤマト運輸を待ってる最中なので暇なんですよねー。
ZOZOのジーンズ早く来ないかなー。

職場でメインに使われてる言語
例えば、日本国内の職場ならメインの言葉はたいてい日本語。
そういう職場では日本語ができないと何もできない。
仕事にならない、

英語
大半のライブラリーやプログラム言語の解説は英語。
この言語ができないと調べものもできない、知識の習得もできない。
超重要な言語。

コミュニケーション能力
言語能力についで必要な能力。言語が荷車の片輪だとするとこの能力はもう片方の車輪。
上司がなにか言う前に忖度していろいろやる技術・・・・・ではもちろんない。
相手を尊重し、相手を理解し、その上で自分を理解してもらう能力である。
様々な書籍があり、さまざまな研究がなされているがこれ、ものすごく難しい。言語能力に加えてこの能力があれば最強である。
プログラム言語だけできる人は多い。人間の言語がきちんと話せる人も多い。しかし、コミュニケーション能力がある技術者は少ない（ように思う、とくに某記事への叩き方を見ると）。
この能力があれば、
なんの役にも立たない意見、人を馬鹿にしたような意見、があっても
よってたかって叩かず、ネタにせず、静かに笑っていられる人
になれます。（多分）
",False,https://qiita.com//TakaakiFuruse/items/75890603ac6e6c3b2328
"ここ最近、Qiita上でクソ記事書くなとか、炎上YouTuberとか、今後必要になる～～とか、全ての開発者が～～とか、
そしてそれらの記事に対する反論記事にアンサー記事、言ってしまえばこの記事だってそうなるけども
そういうモノが多い気がしませんか
Qiitaって、そういう目的のサイトだったんでしょうか？
確かにPHP触らない人は基本PHPの記事は見ないし、C#について調べている最中にHaskellの記事は読まない。
Pythonで機械学習しようとしてる人は3Dオブジェクトが云々の記事を見ないし、AWSマスターだと言える人はAWS初心者向けはあまり開かない。
そのため、記事のジャンルによってはいいねの数も付きやすさも変わって、大衆受けするものがトレンド入りしやすくなるし、まして""全ての開発者""なんて広い主語の記事は大体の人は気になって見に行くし、結果いいねが付きやすくなる。
大人数のユーザーを持つOrganizationは組織票でつくこともある。
そこについてはしっかり理解していますし、恐らく他のQiitaユーザーも分かってやっているのだと思います。
じゃあQiitaはバズる記事を書けばいいのでしょうか?単純に自分が有名になるだけの記事を書けばいいのでしょうか?自分がこのフレームワークつかって、ここで失敗したからバッドナレッジってことで、同じところで躓いた他の人の助けになるような事を書いたり、自分で実装した処理を「このサービスと言語でこんな風に実装しました!」って事書いたり、僕はこんなサービスリリースしました!ロジックはこうこうこんな感じです!って紹介したりして、そういう記事を増やすことで、こんな処理したいけどどうすれば・・って人が検索した時にQiitaでの記事を見つけて前進出来たり、あーこの人はこんな所で失敗したんだな・・自分は気をつけようって他人の経験から学ぶ事が出来たり、こんなサービスはこういう風に動いてるんだなぁって見た人のノウハウになったり、
そういう目的が主となっているんじゃなかったんでしょうか
確かに、今後こんなプログラムが必要だ!って言われたら気になって見てしまいますし、その記事は面白いでしょう。
全ての開発者はこの言語を学べ!気になりますね、そうかこんな言語が今需要あるのか。見てて楽しいですね。
でもそれってQiitaの目指す所なのですか。プログラムに関する再利用性・汎用性の高い情報が集まる場をつくろうとしているサイトで必要ですか。いらないとは言いません、私も面白いとは思っています、ですが、今回の事をきっかけとして、またQiitaの本来求められた姿を取り戻せるように、記事の質を変えていきませんか。

補足
たくさんのコメント、反論意見に同調意見ありがとうございます
この記事に対する意見を、また記事として書く、ではなくコメントとしてたくさん頂けてるということは少しなりとも皆様の考えに私の意見が反映されているのではないかと考えています。
私も、目を離したすきにここまで伸びてるとは思わず、全員に返信は時間もないのでこちらに書かせていただきます
記事の質が高い低い、言葉の強さに関して、執筆中確かに感情的になっていたせいもあってそのような言葉使いになってしまったことは申し訳ありません。
また、運営側にどうしてほしい、というよりはイチユーザーとして記事を書く時は少しでも私の意見を思い出していただきたいという思いが強く、記事にさせていただきました。
通報・ミュート機能があるからそうすべきとの意見に関してですが、上でも書いてある通り私も、今回の記事で題に上がるような記事は楽しく見させていただいていること。
その他に、意図的に質の低い記事を書こうとしている人はいない。そう考えているため通報に関しては考えていないです。もちろん、悪意を持って記事を書いている方がいれば通報や運営側からのユーザー削除などの措置があると思います。
質のいい記事を書いている人だけが意識すればよい、との意見に関しても同じように、意図して質の低い記事を書こうとしている人はいないと考えているので、「この記事を見た方に伝われば良い」として述べています。
Paizaランクが高い人のみかけるようにすればよいのか?とありましたが、私はむしろ初心者こそ記事を書いていくべきと考えていまして、理由としては初心者である自身はどこで躓いて、どう解決したか、それらのナレッジを共有することが他ユーザーにとっても有益なことであると思っています。もちろん、技術力の高い方の書く記事もとても重要です、どちらも重要ですが、初心者である私目線で考えた場合、初心者こそ記事を〜〜という意見になっています。
勘違いされないようにもう一度読んでいただきたいのですが、レベルの低い人が書くレベルの低い記事が質の低い記事ではなく、誰かの記事を真似て同じような事しか書かれていない記事や、コメントで完結する内容を述べた記事を、ここでいう質の低い記事として私は捉えていますし、そのように書いてあります。
とはいえ、誰かの記事に対するアンサー記事でも、とても面白いものも見受けられます。ポジティブな感想ですので、例としてトレンド入りしている記事をリンクとして貼っておきます。
「今後必要になるプログラム言語」を読むに当たって
重要な自然言語
また、@koshian2さんのコメントがとても納得でき、興味深い事が書かれており私の記事以上に考察されています。(私の記事の質が低い事が原因でもあります・・)
意見記事としてあげたものですので、これに対する反論や同調意見などはもっと見てみたいと考えているので、どれだけ単純な事でもよいです。この記事を読んで、何か思う事があればぜひ教えていただけませんか。
",False,https://qiita.com//wannabe/items/294b34334a94e8e3618c
"ひどいブーメラン記事だという自覚はあるけど流石にトレンド汚染も酷いし本来的な使い方してるユーザーにとっては迷惑そのものでしかなかったので

事の経緯
ここが詳しいけど、まとめると時代錯誤甚だしい内容の記事を誇大したタイトルでツッコミどころ満載の記事を挙げてそれに対するツッコミを何故かわざわざ別記事で捕捉する奴らが大量発生した。










11/08 17:00現在のトレンド。これはひどい（某炎上系Youtuberも含む）

言いたいこと
某炎上Youtuberさんやタイムトラベラーさんが香ばしくてついついちょっかいかけてしまうのもわかります。
でもそれコメントで完結できない？
いいねしてわざわざトレンド汚染してる奴らもどうかと思うしコメント欄も酷かったことはよく知ってるけど。
実際にこれを反例として何か学べることがあるならいいけど、寒い笑いしか提供できてないようなのが大半です。
一応捕捉しとくともちろん中にはこういう時にしかバズらないようなおもしろい読み物もあるけど。
というかゴミ記事問題についてバズったの先週とかここ数か月の話なのによくやるよ
面白い記事がこんなしょうもないので埋め尽くされて目につかないのは残念だなぁと思った次第
どうせすぐ鎮火するから消せとかまでは言わないけど、つまんないパロディ記事については年がら年中見かけるので、せめて面白い内容にしてくれ
",False,https://qiita.com//bobbyorogun/items/c9db7d6232b3e76f144d
"

はじめに
本記事は、オブジェクト指向をざっくり理解している人向けに書きます。

オブジェクト？
そもそも、オブジェクトの捉え方ですが…
それは圏論の言葉で言うと「圏」であり…

それ難しいから（笑）
そもそもから話そうではないか我が友よ（笑）
そもそも数学嫌いが多いじゃないか
重要なのは対話じゃないか

アレルギーの方へ…
例えば、1+1=2　という数式で説明します。
実は、証明できます。
しかし、そんな事をしなくても自明の理（当たり前）です。
この、ある値がとり得る当たり前の図式 を結論だけ言って整理すると…
（間はめちゃくちゃ長いです）
1に1を足すと2になるというメソッドで切り出せるということになります。
（すごく乱暴です。）
こういう、当たり前の図式　の事を、自然変換という名前で読んでいるだけです。
で、それを、分かりやすい言い方に変えるとメソッドになります。
分かりづらいなら、
x+y=z
xにyを足すと z になる
という動詞だということに着目してください。
これを普段のオブジェクト指向から説明すると、
オブジェクトAをupdateするとAの各プロパティは B になる
つまりは、メソッドはオブジェクトが持つ自明の理を実装しているに過ぎません。

メソッドが自然変換だと考えたら出来ること
ここもゴニョっと言えますが…
乱暴に片付けてしまうと、オブジェクトが持っているプロパティが、ある値になる。
それは、～～～～～～の理由で…
という風に記述しますよね。
つまり、あまりにも長いメソッドは、リファクタリングをします。
細かい記述に書き直せるというわけです。
このリファクタリングされたメソッドは、もしかすると
オブジェクトが持っている値を直接書き換えない場合が多いのではないでしょうか？
ところが、リファクタリングしても、切り出したメソッドの再利用価値が低いなどということも考えられます。

メソッドの分解 = 自然変換の分解
あるメソッドは、リストをイテレート（巡回）した結果を返すとします。
イテレートの際は、必ず幾つかの約束を果たさねばなりません。
よくある設計ですよね？
そこで、意気揚々とリファクタリングします。
実は、リファクタリングは、自然変換の分解です。
分解すると、より小さなメソッド（つまり関数）に切り出せます。
そういった概念を、「圏論」の言葉でいうと「関手」、「射」と読んでいるだけです。
つまり、関数型プログラミングの最初の理解に大切なことは。

メソッド内だけで、関数型プログラミングをする
本当にミクロな書き方が出来ると思います。
そういうところで便利さが分かってくると面白くなってきますので、
怖がらずにまずは
ラムダ（無名関数）
を使った表現に変えてみることから、初めてみてください。
",False,https://qiita.com//johnny-shaman/items/90002ac38a8cae06e538
"

この記事の目的
JAVA = JAVAscriptであることを示します。
一応、末尾にまじめな解説とかもつけました。
なお、一部誤りがあったので末尾に謝罪もつけました。

コード
同じコードを２つ貼ります。
/*
<script>
var System = {out: {println: function(e){console.log(e);document.getElementsByTagName('body')[0].innerHTML=e;}}};
window.onload = function() {
    eval(document.getElementById('code').innerHTML.replace(/[*]/g, '/').replace(/(public|static|void|String\[\] args)/g, ''));
}
</script>
<div id=""code"">
*/
class Main {
    public static void main(String[] args) {
        System.out.println(""Hello, world!"");
    }
}
/*
new Main().main();
</div>
*/

/*
<script>
var System = {out: {println: function(e){console.log(e);document.getElementsByTagName('body')[0].innerHTML=e;}}};
window.onload = function() {
    eval(document.getElementById('code').innerHTML.replace(/[*]/g, '/').replace(/(public|static|void|String\[\] args)/g, ''));
}
</script>
<div id=""code"">
*/
class Main {
    public static void main(String[] args) {
        System.out.println(""Hello, world!"");
    }
}
/*
new Main().main();
</div>
*/


JAVAで動かす
実行方法は正直なんでもいいのですが、paizaというサイトに貼り付けて実行します。
うごく！Hello, world!ってでる！すごい！

JAVAscriptで動かす
上記のコードをhtmlとして保存して、ダブルクリックするなどして適当なブラウザで開きます。
検証した環境はchromeでした。
Hello, world!ってでる！すごい！
ひょっとすると、一部のブラウザの画面には変な文字列が出ているかもしれませんが、真の男はそういう事は気にしません。
一部のブラウザを使っている場合も、開発者ツールを見てみましょう。Hello, world!ってたぶんでる！すごい！

ポエム
真の男にとっては、JAVA=JAVAscriptであることは自明なことだ。
JAVAとJAVAscriptは違うなどとゆうあほや腰抜けも居るが、メキシコの荒野ではそんなのは通用しない。
それがわからないやつは、最初は威勢だけはいいが、やがてコメント欄に吹き荒れるメキシコの風に倒れ、酒場で知り合ったベイブと慰め合い、刹那の快楽に酔い、子を産み育て、「何も面白いことが無い一生だった」などと酒場でぼやきながら・・・・やがて老いて死ぬ。THE END OF MEXICO....。
だが、おまえがまだ真の開拓精神を持っているならば話は別だ。今からでも遅くはない。JAVA=JAVAscriptをやれ。

うそです
JavaとJavaScriptは別物です。すみませんでした。
なぜ別物なのに同じだと主張する人が出てくるのか？という背景については、
https://qiita.com/pik/items/1c95d69debee4aedffea
が詳しいです。

ちょっとだけ解説
JavaとJavaScriptは、コメントの形式などもC由来で似ている言語なのですが、一方で予約語の振る舞いなどが色々違います。とくに、スペースだけでトークンを配置した時の扱いも貧弱です。
そこで、素朴なやり方ではどちらでも動くコードを書くのは不可能かなと思い、邪道に手を出しました。

コメント
まず、HTMLを使う事によって、JavaではコメントだがJSではコメントではない、という領域を作り出します。
Javaの世界では、これによってJavaScript用のコードを全てコメントとして扱えるようになります。Javaにはマクロもないので、コードを実行時に動的に書き換えるのは少し難しく、コメントを除けばJavaとしては普通のコードになるようにします。

コードの書き換えとeval
上述のとおり、素朴なやり方では邪魔な予約語が沢山あるので、コードを書き換えます。キモになるのは、evalとreplaceです。

eval
evalというのは、パフォーマンス的にもセキュリティ的にもやってはいけない事で、文字列をJavaScriptのコードとして評価します。

replace
予約語とコメントの片割れが邪魔なので、正規表現で置換します。予約語以外も、一部面倒なので消してしまいます。

ワンポイント
実は、このコードには、例えばphpを乗せることも不可能ではないのですが、出力をHello, world!だけにするのがちょっとめんどくさいなと思ってやめました。

お詫び

その１
コピペミスっていて、Javaのソースが古いバージョンになってました。直しました。すみません。

その２
replaceは以下の通り、一回で良かったです。すみません。

改良版
/*
<script>
var System = {out: {println: function(e){console.log(e);document.getElementsByTagName('body')[0].innerHTML=e;}}};
window.onload = function() {
    eval(document.getElementById('code').innerHTML.replace(/[*]/g, '/'));
}
</script>
<div id=""code"">
*/
class Main {
    /**/ public static void
    main(
        /**/ String[] args
    ) {
        System.out.println(""Hello, world!"");
    }
}
/*
new Main().main();
</div>
*/


おしまい！
",False,https://qiita.com//sasanquaneuf/items/d800df9081ca2d352dc5
"

結論

""英語""が今の時代で1番重要な自然言語

はじめに
こんにちは、がっちょです。
今回は""自然言語""について話そうと思います。

自然言語とは
Wikipediaによると

自然言語（しぜんげんご、英: natural language）とは、人間によって日常の意思疎通のために用いられる、文化的背景を持って自然に発展してきた言語である。
(Wikipedia - 自然言語)

つまり、簡潔に言うと私たちが会話をするために話している言語のことです。
英語・日本語・中国語・スペイン語・アラビア語・ラテン語・シルボ語……などなど
これらの言語を自然言語と呼びます。
エスペラント語のように人工的に作られた言語を人工言語と呼びます。

英語
英語は重要な自然言語です。
開発者にとって絶対に欠けることが出来ない自然言語だと思います。
では、具体例を見てみましょう。
以下のソースコードはC言語で文字列を出力するものです。

Source.c
#include <stdio.h>
int main(void) {
    printf(""Hello, World!\n"");
}


皆さんお気づきでしょうか？
実はこのソースコード、英語が使われています。

英語
include //含める
main //主要な
void //空(から)の
print //表示される
Hello, World! //こんにちは、(このプログラミング言語の)世界！


このように多くのプログラミング言語は英単語が多く登場します。
多くのプログラミング言語の入門書に書かれているHello, World!も
英語ですので、英語の重要さがわかると思います。

日本語
この記事を見ているほとんどの方は、日本語が母語の方がほとんどだと思います。
母語が日本語で、日本語に自信がないという方はまず日本語を勉強すると良いと思います。
私は日本語が苦手です……英語も苦手です……
皆さんはどうですか？
日本語がある程度できないと英語力は身に付きません。
例えば、
英語：dog
日本語：犬

dogという英単語があります。
その英単語は、日本語では犬という意味らしいです。
しかし、日本語の犬という単語を知らないと
dogという英単語の意味はわかりません。
このように日本語の語句を知らないことによって理解できない英単語が出てきます。
逆に考えると日本語の語句をたくさん知っている人のほうが、英単語をたくさん覚えられます。
まずは日本語を勉強すると良いです。

最後に
自然言語を学んで、開発がもっと楽しいものになれば良いですね😊

読んでみると良い記事(この記事いいね👍)
全ての開発者が学ぶべき5つの言語
重要なプログラミング言語が知りたかったらこちらの記事を見ると良いかもです。
5 Programming Languages Every Master Developer Should Learnの翻訳記事みたいです。
全ての開発者が学ぶべき1つの言語
D言語について詳しく解説している記事です。
全ての開発者が学ぶべき1つのNim
Nimについて詳しく解説している記事です。
はじめてNimの存在を知りました。
追記
超天元突破ギガドリル並みにタイトルの主語が大きかったので、主語を消滅させました。
",False,https://qiita.com//Gaccho/items/e12e998b0a1788d55f5f
"みなさんの組織では、役職者の運用はうまくいっていますか？
技術顧問としていくつかの会社と関わる中で、
能力も経験も実績も申し分ないはずなのに、いざ然るべき役職に就けてみると期待していたほどの成果が挙がらず、チームにも停滞感が漂っている...。
もちろん、本人も決してサボっているわけではなく一所懸命に頑張っているが、なんだか思うようにいかないようだ...。
というような厳しい事態に遭遇することも多く、新たな役職を任せるということはとても難しいものなのだなぁ... と感じ入る今日この頃です。
とはいえ感じ入ってばかりじゃなくて何か顧問っぽいことしないとなー契約解除されちゃうなーと思いまして、複数の業界にわたって収集した成功事例群を分析してみたところ、「どうやら役職の任命のやり方にも作法があるようだ」という気付きを得ることができました。
(「誰を任命するか？」という役職候補者の選抜ではなく、「任命する」という行為自体の作法です)
この記事では、その中で際立って優れた成功事例の 1つ、某国の王様が勇者を任命した際の事例をご紹介いたします。
...というネタ記事です。
※この記事では「役職」の意味を明確に定義しませんが、管理職に限らず、ある程度の裁量をもって成果に責任を負うロール全般を幅広くふんわり曖昧に指したい気持ちです。

勇者という役職の任命
王様:
よくぞ来た！ 偉大なる勇者の遺志を受け継ぎし若者よ！

背景と目的
王様:
そなたも知ってのとおり、十数年前に厄災の大穴より現れし魔王により、世界は滅亡の危機に瀕しておる。
大穴にほど近い、かの辺境の大国は、強大な魔物の軍勢を前に為す術もなく一夜にして陥落したと聞く。
このままではやがて世界は魔王に滅ぼされよう。
解説:
役職は理由があって任命されるものです。
まずはその背景や目的をしっかりと説明し、我々はどういう問題や苦難に直面しているのか、何を解決しないといけないのか、手を打たないといつ何がどうなってしまうのかを正しく理解してもらわないといけません。
背景や目的を理解せずに目先の瑣末事ばかり追いかけているようでは、いつまでも望んだ成果が挙がらないまま組織は疲弊してしまいます。 _:(´ཀ`」 ∠):
ここで王様は、

超強い魔王の侵攻により世界が滅びようとしていること
それは十数年前からの長年の懸案事項であり、最悪の未来も予想される手強い問題であること

という情報を伝えています。
なるほど状況は芳しくないが可及的速やかに魔王を何とかしないといかんのだ、ということが簡潔に伝わってきますね。

任命理由と期待される役割
王様:
魔王の存在をいち早く察知し、これを討とうと直ちに我が国を旅立ったそなたの父は並ぶ者なき歴戦の勇者であったが、魔物との死闘の末、火山に落ちて亡くなったそうじゃな。
その父の遺志を継ぎ、幼き頃より厳しい修行を積んできたそなたの勇名はこの城にまで届いておる。
この十数年、そなたの父の後に続く勇気ある者を待ち続けたが、ようやく時はきた！
今、魔王討伐に旅立たんとするそなたに、正式に我が国の勇者の称号を授けよう！
そなたならば魔王に屈することなく必ずや勇者の使命を果たすであろう！
解説:
なぜ他の人ではなく自分がその役職を任されたのか？
動機付けのためにはその納得感も大事ですよね。
「社員が増えてきたから〜」とか「勤続年数が長いから〜」などという理由では、やる気も出ませんね。
「あなただからこそ期待できる成果がある」という説得力ある理由が示されれば、やり遂げる意思と自信がもりもりむりむり (ง `ω´)ง  と湧き上がるものです。
この勇者様の場合は、血筋もあるかもしれませんが、それよりも日頃の努力＆その証としての勇名が認められたようです。
「血筋」という固定的な属性ではなく「結果に結びついた本人の研鑽」という成長思考を重視しているのがポイントで、いかに勇者の家系であったとしても、真の勇者となるための鍛錬を怠っていれば、こうして勇者の称号を授かることはなかったでしょう。
役職候補者の見極めにおいては、本人の現在の能力や実績のみではなく、その能力や実績に至るまでの行動、成長という要素にも目を向けることが大事ですね。
また、任命理由の説明とともに、期待する役割を明文化することも欠かせません。
「組織の中でどういう存在になってほしいか」を明確に定義し、周囲に与えるべき影響や形成されるチームへの期待をお互いに確認し合いましょう。
ここで王様は、強大な魔王を前にしても屈しない勇者であれ、と言っています。
それはつまり、土地を追われ国を捨て魔王の目の届かない場所に隠れ住んだり、投降＆隷属して生き延びるようなことは望まれておらず、あくまで戦闘的な手段で魔王を排除することで、平和とともに人々の誇りを取り戻したいという期待が込められているわけです。
さらに、先代の勇者は魔王討伐に失敗しており、今回は 2度目のチャレンジであることが述べられています。
魔王の侵攻が差し迫る状況の中、先代の勇者が倒れた後に 10年以上も待ったということから、登用可能な人材が著しく不足している極めて難しい役職であることがわかります。
満を持して任命した当代・勇者様に寄せられる期待の高さが伺い知れますね。

求められる成果
王様:
ゆけ！ 若き勇者よ！
魔王を討ち取り、世界に平和を取り戻すのだ！
解説:
求められる成果を曖昧にしたまま雰囲気で仕事を進めてしまうと、先々の展望をもたないがゆえに職務の舵取りが安易かつ場当たり的になり、次第に目先のタスクで手一杯になってしまいます。
これは「忙しいぬるま湯」であり、こんな状態では目の前に積み上がった作業をひたすら消化することにしか注意が向かないため、本当に求められる成果を挙げることは不可能です。
これを防ぐため、どのような状態になったら役割を全うしたと言えるのか、仕事の方向性と到達点を明示しましょう。
達成したのかしていないのか、していないならばどの程度足りないのか、恣意的な解釈を差し挟む余地がない程度に明確に定義する必要があります。
王様は非常に明確で揺るぎないメッセージを発しています。
すなわち、魔族による脅威の排除を目的に「魔王を倒すこと」ただ 1点を成果として求めていますね。
敵は世界中の国々を相手取ってなお一方的に滅亡の脅威を感じさせるほどの絶大な力を持つ軍勢ですから、いかに勇者といえど真正面からぶつかっては勝ち目がありません。
しかし、あくまで求める成果は「魔王を倒すこと」であり、つまり「魔王軍すべての勢力の殲滅」ではなく「ゲリラ戦による魔王暗殺」という現実的な戦略を示唆しています。
非常にストレッチされた難易度の高い目標ではありますが、決して達成不可能とまではいえないところが秀逸ですね。
また、期日は明言されていませんが、世界が滅びる or 復興不可能なほど蹂躙されるまでと考えるのが妥当でしょう。
情勢は重要拠点の防衛や陥落具合に大きく左右されますし、また、勇者による道中での敵勢力の各個撃破なども期待できるため、絶対的な期日の提示が難しいと王様は考えたのかもしれません。
具体的で測定可能で達成可能で、もちろんこれ以上ない価値を持っていて... 期日設定に若干の甘さがありますが、SMART な目標設定の基本を押さえた良い目標ですね。

想定されるリスク
王様:
魔王は人の足では到底踏み入ることのできぬ魔境に居城を構えておると聞く。
まずは世界各地を巡り、古の文献に記された伝説の不死鳥を蘇らせるのだ。
神の乗り物と伝えられる不死鳥ならば、いかなる魔境であっても攻め入ることができるであろう！
解説:
先ほど感極まった王様はつい「ゆけ！」と旅立ちの声を上げてしまいましたが、ゴールが定まったからといって即送り出すのは NG です。
正しい方向へロケット・スタートを決めるためには、先人の知見を足掛かりにすることが肝要です。
特に、過去に実を結ばなかった取り組みや、そこから導かれる懸念事項などの一見ネガティブな情報こそ、同じ失敗に至りかねない回避すべき選択肢を刈り取り、初手の成功率を高める最重要の知見と言えるでしょう。
うっかりそのまま送り出そうとしてしまった王様ですが、考え直して、魔王城に攻め入るためにはすでにこの世から失われている伝説の不死鳥が必要だと教えてくれました。
復活方法は定かではないようですが、その手掛かりは世界中に残されており、勇者ならば必ず蘇らせることができると確信しているようです。
先代の勇者は 1秒でも早く平和を取り戻すために、逸失している不死鳥復活の手続きをショートカットして無茶を承知で一直線に徒歩での魔王城突入を目指したのかもしれません。
歴戦の勇者である自分ならばと単身で踏破しようとしたところ予想外の強敵が立ちはだかり道中で力尽きた... その失敗を活かして王様は、年若く経験が浅い当代・勇者であっても成功率が高いと思われる別アプローチを勧めているのでしょう。
「伝説の不死鳥」という徒歩に代わる行軍手段をあらかじめ提示することで、先代と同じ轍を踏む危険を回避するとともに、難攻不落の魔王城に突入するための手掛かりに最初から気を配って旅をすることになり、道中の手戻りも少なく抑えられそうです。

与えられる支援とリソース
王様:
勇者の称号を持つそなたには、この世界の理を司りし精霊の加護が与えられておる。
たとえ魔物との戦いにおいて命を落とすことがあろうとも、その勇気を失わぬ限り、何度でも蘇ることができよう。
共に旅立つ仲間を街のギルドで募り、これで装備を整えるがよかろう。
(  パパラパパラパ〜ン！ 150ゴールド、ヒノキの棒、布の服を手に入れた！)
解説:
責任と権限は一体です。
責任だけ押し付けてあとはよろしく、ではいけません。
課せられた責任を果たすのに充分な、円滑な職務遂行の助けとなる支援措置とリソースを計画的に与えましょう。
今回、勇者様には以下 2点が与えられました。

魔物にやられても生き返る
仲間をスカウトする権利、小銭、貧弱な装備 (...棒切れと服だけって)

項目1 は、強大な武力を誇る魔王を相手取るうえでこれ以上ないほどの助けになりますね。
魔王討伐というプロジェクトにおける最大のリスクである「戦死」を完全に回避して、逆に積極的に戦いを挑むことを可能にしています。
プロジェクトにスピードを注入することに成功しており、すでに平和を脅かされつつある時間的余裕のない現状に対して極めて効果的な措置といえるでしょう。
一方、項目2 は一国の王とは思えないほどケチくさいようですが... 実はこれは計画的なリソース設計の結果と推察されます。
王様はやろうと思えば金に物を言わせて腕の立つ精鋭、最高級の装備、潤沢な路銀を用意することもできたでしょう。
このままでは世界は滅亡してしまうのですから、当然金を惜しんでなどいられないはずです。
ですが、そうしなかった。
なぜか？
それは... 今、目の前にいる新米勇者に、いきなり最高のリソースを与えたらどうなるでしょうか？
きっと戦い慣れた仲間たちにおんぶにだっこで、勇者としての心構えはいつまでも養われることがないでしょう。
人知を超えた脅威である魔王に抗う唯一無二の拠り所は、幾度倒されても挫けない勇者だけが持つ本物の勇気に他なりません。
その勇気を... 仲間との絆、信頼を培うことが重要だと王様は考えたのではないでしょうか。
幾多の死線をくぐり抜け、揺るがぬ勇気に率いられた最強チームこそが魔王討伐を果たすことができる... そのチーム・ビルディングを成功させるために、王様はあえて最低限のリソースしか与えなかったのでしょう。
一見アンバランスに見える、強力な精霊の加護と貧弱な初期リソースは、実はよく練られた一体の支援だったのです。
チーム・ビルディング、大事。ヽ(•̀ω•́ )ゝ✧

勇気の対価は
こうして勇者は困難なミッションに立ち向かうこととなりました。
魔王を倒すまでのその旅路は、文字通り、死んでも戦い続ける修羅の道です。
勇者の背を押すのは、人々の救世への願いか、はたまた亡き父の軌跡に想う憧憬か。
この旅が終わったとき、果たして勇者は何を得るのでしょうか？
過酷な使命に挑む勇者に、王様は何を対価として支払うことができるのでしょうか？
金銭、名声、権力、人脈、経験...、望むものは人それぞれですが、
「この役職を全うすれば自分が欲する対価を手に入れることができる」
この確信こそがモチベーションの源泉になるのではないでしょうか。
役職の任命においては、そんな対価設定が必要なのだと思います。

エピローグ
〜〜王国〜〜
兵士:
申し上げます！ 厄災の大穴にて突如激しい地響きが轟きました！
崩落はありませんでしたが、封邪の祠の司祭殿によりますと、時を同じくして、大穴より常時発せられていた邪悪な波動が感じられなくなったとのことです！
大臣:
もしや、大穴より異界に突入した勇者殿が、大魔王を...？
この世界で魔王と呼ばれていた魔物が、異界を統べる大魔王の尖兵に過ぎぬと知ったときはもはやこれまでかと思ったものですが、なんと、その大魔王までをも倒してしまうとは！
王様:
...穴は崩れたわけではないのだな？
引き続き厳重に監視し、勇者が大穴より帰還することがあればすぐに王宮へ連れて参れ！
絶対に他国の手に渡すでないぞ！
〜〜異界〜〜
賢者:
大魔王が倒れて、この地を覆っていた闇の封印も完全に解けたようだな...。
皆、このあとはどうするつもりだ？
戦士:
この異界には他にも人が住む大陸があるみたいだし、アタシは旅を続けるわ。
もともと帰る家もないしね。
武闘家:
ボクも一緒に行っていい？
たった一人の家族だったお兄ちゃんも、もういないし...。
戦士:
もちろんよ。
<賢者> はどうするつもり？
賢者:
ああ、俺はまた遊び人に戻って、気楽な日々を送ることにするよ。
「世界を救った賢者様」なんて呼ばれるのは性に合わんからな。
<勇者> は？
勇者:
私... 本当は勇者なんかじゃなくてお店屋さんになりたかったんだ。
世界中を旅して色んな珍しいアイテムを見てきたから、目利きには自信ある！٩(๑❛ᴗ❛๑)۶
あの吟遊詩人さんの開拓村でやらせてもらえないかな。
武闘家:
家族のところには帰らないの？
勇者:
...帰ったら多分、王様に殺されると思う。
きっと、お母さんやお祖父ちゃんを人質にとられてしまう。
戦士:
そうね。アナタは今や大魔王を凌ぐほど強いんだから、王様も内心ではアナタをさぞ恐れていることでしょうね。
武闘家:
<勇者> ... 商売をやるなら、絶対お兄ちゃんみたいになっちゃダメだからね。
勇者:
うん、きっと村の人たちを大事にするって約束する。
...あの町で反乱が起きる前にお兄さんを止められなかったこと、本当にごめんなさい。
武闘家:
ううん、お兄ちゃんはいつかああなると知りながら、それでも住人を酷使して町の発展を優先することを選んだんだから。
誰のせいでもないよ。
勇者:
ありがとう... <武闘家>。
<戦士> も <賢者> も、私たち、また会えるよね？
その後、勇者とその仲間たちの姿を見た者はいない。
かくして勇者の伝説は語り継がれる。
Fin

トゥルー・エンド
王様:
これから旅立つそなたが首尾よく使命を果たしたとしても、そなたの真の望みはワシらでは叶えられんだろう。
それを知りながらこのまま送り出すのは心苦しい。
ならばせめて、僅かばかりの報酬の前払いとして、そなたが望みを叶える助けとなるであろう古き言い伝えを授けよう。
その言い伝えによると、世界のどこか、英雄のみが足を踏み入れることができる神の塔があるそうじゃ。
遥か天空までそびえる塔の先には、いかなる願いをも叶える神竜が住んでおる。
その力は世界の理を超え、死者の魂すら呼び戻すほどの奇跡を起こすといわれておる。
魔王を討ち取った暁には、不死鳥を駆り塔を探し出し、神龍のもとへ赴くがよい。
世界を救った勇者ならば、あるいは塔を昇りきり、神龍の奇跡が与えられるかもしれん。
その奇跡の及ぶ先は、精霊の加護を失い力尽きたそなたの父とて例外ではなかろう。
過酷な戦いの果てに、そなたの願いが成就することを祈っておる。
解説:
個々のパーソナリティに寄り添い、望みの対価を提示することができれば、きっと最高のモチベーションを発揮してくれることでしょう。

あとがき




エピローグ時年齢
性別
性格
職歴
特技




勇者
20才
女
父親似
勇者 » 商人
ゲリラ戦


戦士
36才
男
ガチムチおネエ
魔法使い » 戦士
ぱふぱふ


武闘家
17才
女
ボクっ娘
僧侶 » 武闘家
納棺


賢者
29才
男
イケメン
遊び人 » 賢者 » 遊び人
シャンパンタワー



この物語はフィクションです。
登場する人物、団体名等はすべて架空のものであり、実在の弊社顧問先とは一切関係ありませんが、名もなき勇者様たちは今日もそれぞれの現場、それぞれの役職で粉骨砕身戦い続けています。

補遺
プログラマのための技術情報共有サイトQiita にこういうテーマの記事を投稿して大丈夫かな？ かな？
",False,https://qiita.com//kkitta/items/270dcee8c1b8c3def180
"定期的に話題になるコンビニの年齢認証パネル。
年齢確認タッチパネルを拒み続ける「大人」たち　トラブル経験のコンビニは2割弱も
それっぽく実装して納得してみよう。

年齢認証パネルがない時
function cashierOperation(items, guest) {
  // 商品の合計を計算しつつ、酒類の商品をチェックする
  let total = 0
  let alcoholic = false
  items.forEach(item => {
    total += item.price
    if (item.alcoholic) {
      alcoholic = true
    }
  })

  // 酒類があった時の処理
  if (alcoholic) {
    // 見た目で確認してみる。this(店員)の力量が試される
    const checked = checkaApearance(guest)
    if (checked < 20) {
      // 20歳未満と判定されれば身分証を見せてもらう
      // 同期処理で5分ぐらいかかる
      const id = guest.getId()
      if (!id) {
        // 身分証を持ってなければ売らないで終了
        return false
      }
    } else {
      // ここにきた時に、実は未成年の場合非常にまずい
    }
  }
  return true
}


年齢認証パネルがある時
function cashierOperation(items, guest) {
  // 商品の合計を計算しつつ、酒類の商品をチェックする
  let total = 0
  let alcoholic = false
  items.forEach(item => {
    total += item.price
    if (item.alcoholic) {
      alcoholic = true
    }
  })

  // 酒類があった時の処理
  if (alcoholic) {
    // 年齢認証パネルでお客さんに年齢を提示してもらう
    const checked = guest.isAdult()
    if (!checked) {
      return false
    } else {
      // ここに来た時に実は未成年だったとしても、お客さんが嘘ついたので、
      // 仕方ないのではないか
    }
  }
  return true
}


比較
個人的には、処理が単純になっているし、嘘をつく必要もないので、年齢認証パネル対応は賛成。
トラブルになるケースは、guest.isAdult()で、俺が未成年に見えるか Exeptionが飛んでくるようなもの。
わざわざキャッチしたくない。
しかも、例外を飛ばしてくるのは、具象クラスなので、把握しきれない。guest instanceof 山田太郎みたいな判定が増えていく事になる。
理想は、全ケースguest.getId()の実施だが、時間がかかるので、別の問題が発生しそう。
入場時認証などで、会計前に認証済みになっているので一番いいかも。

結論
みなさん大人しく押しましょう。
特別扱いはめんどくさいのです。
",False,https://qiita.com//subaru-git/items/95abc4ad530996bdef0b
"

今後必要になるプログラム言語を読む前提知識
時代背景と当時の知識がないと読むのが非常に難しいので、僕の知っている範囲内の前提知識を書いておこうと思います。

当時の背景
当時というのは、ここでは 2000 年前後の Java 黎明期を指します。
僕個人で言えば PostPet 2001 などの開発に携わっていた頃のことです。
この少し前にインターネットの世界ではウェブブラウザが誕生しました。
そして、元々はセットトップボックス用に開発された言語 Java (Oak と呼称されていた)も開発されていました。
誕生間もない Java と誕生間もないウェブブラウザが出会うのは必然だったかもしれません。
この２つが出会ったことで、ウェブブラウザ上に動的なコンテンツを表示する Java Applet という技術が誕生しました。

Netscape Communications と Sun Microsystems
次に、Java とウェブブラウザを作っていた企業の話です。

Netscape
Netscape Communications は Netscape というウェブブラウザを開発していました。
後に AOL に買収され Mozilla へ繋がって行く企業です。

Sun Microsystems
Sun Microsystems はワークステーションや CPU, OS を作るなどコンピューター全般を業務範囲とする企業で早くからインターネットに注力していました。
この Sun がセットトップボックスという新しいハードウェアを作るに当たって生み出されたのが Java です。
Sun は後に Oracle に買収され Java も Oracle が所有します。

LiveScript
Netscape はその頃自社のブラウザで動作する動的コンテンツ環境を実現するために LiveScript というスクリプト言語を開発していました。
このとき、Netscape と Sun は業務提携していた関係もあり LiveScript を JavaScript という名称に変更しました。
当時から Java Applet との連携が考えられていたのかもしれません。

Java Applet と JavaScript
Java Applet と JavaScript は以下の２つの事象によって結びつきます

Ajax 以前の世界では、JavaScript 単体でサーバと通信するプログラムを作るのが難しかった
Java Applet も awt などで構成されていたため GUI 構築が煩雑で面倒だった

この解決をするために HTML + JavaScript + Java Applet という仕組みが考案されました。



技術名
役割




HTML
文書


JavaScript
UI の構築


Java Applet
Java が用意する豊富なライブラリへのブリッジ（UI 構築には使っていない）



この仕組みを使ったウェブページが主に官公庁系 SIer によって量産されました。

動的データの取得方法
Java Applet を用いた通信のため Java Servlet を使うのが一般的でした。
具体的には

JavaScript から Java Applet の機能を呼び出す
Java Applet は Java Servlet にアクセスする
Java Servlet は SQL を使って DB にアクセスし、データを Java Applet に返す
Java Applet はデータを JavaScript に返す
JavaScript は、そのデータを使って UI を構築する

という流れです。
こうしてみると、著者が Java を前提にし、JavaScript と Java を混同（JS から Java のライブラリを呼べるので）し、それ以外をサブルーチンと呼ぶ意味がわかるかと思います。.
また、突然 SQL が言語欄に出てくるのも自明です。

まとめ
あの記事は HTML + JavaScript + Java Applet + Java Servlet を使う Java 黎明期の知識がないと読むのが難しいと思います。
恐らく、この辺りで時代が止まっている物と思われます。
MSDN が C++ と主張しているのも、この時代にはまだ C# が無く Win32 API のオンライン・リファレンスな意味合いが強かったため、と思われます。
これらを頭に入れてから、もう一度読んでみてください。
…やっぱり全然わかりませんね！！
追記
コメント欄はわりかし読めるようになってるかも
追記2
元記事はアカウント停止で読めなくなったようです
",False,https://qiita.com//pik/items/1c95d69debee4aedffea
"

はじめに
『今後必要になるプログラム言語』1という記事が話題になってたので、便乗して書きました。

今後必要にならないプログラミング言語
独断と偏見で選びました。

FORTRAN77
C++98/03
Python2


FORTRAN77
科学技術系の数値計算でお馴染みのFORTRAN77。古い言語で、コンパイラが今ほど賢くなかった時代のものなので、文法にかなり制約があります。レガシーなFORTRAN77のコードを引き継ぐ必要がなければ、Fortran90/95でコーディングしましょう。
科学技術系の数値計算の言語といえば今はJuliaが熱いので、Juliaで書くのも1つの手ですね。

C++98/03
C++11から

range-based for loops
スマートポインタ
nullptr
ムーブセマンティクス
autoによる型推論
ラムダ式

などの機能が導入され、GCC6.1でデフォルトのC++コンパイラがC++14になったので、今からC++98/03を使う理由はないと思います。

Python2
Python3が当たり前になってきたので、今からPython2をやる理由はないと思います。Python2は文字列まわりがつらい。
Unix系OSにプリインストールされているPythonは、まだまだPython2が多いですが…

最後に
雑な記事ですみません。コメントは歓迎します！

追記
はてなブログで反省文を書きました。
『Qiitaにクソ記事を載せたことに対する反省』




ユーザーのサスペンドにより元記事が閲覧できないので、魚拓のURLを貼っておきます。https://megalodon.jp/2018-1107-1545-49/https://qiita.com:443/administrator1974/items/387aab2a42bf57e3b215 ↩



",False,https://qiita.com//h6akh/items/d241ce878533f6086aa7
"

はじめに
・今まで全くプログラミングをしたことない人で、iosアプリを出してみたいが勉強方法が分からない方や、本一冊読み終えて何していいか分からなくなった方など初心者の方々向けにこの記事を書かせていただきました。
・あくまで勉強方法の一例です。
・記事内にコードや技術的な話は書いていません。
・プログラミング経験自体は５ヶ月程で、内訳は2ヶ月がPython、今3ヶ月程になるのが
Swiftです。
・勉強方法は書籍、ネット上で見つけたコードの写経、勉強会への参加です。

作ったもの

https://itunes.apple.com/us/app/ar-piling-bottles/id1441125552?mt=8&ign-mpt=uo%3D4
AR機能を使った、瓶を30秒以内にできるだけ多く並べて、その後並べた数によって
リザルト画面が変わるというシンプルな連打ゲーです。

プレイ画面の一例

こんな感じで結果が出ます。

勉強方法
結果としてリリースまで以下の5つのフェイズに分かれました。
「独習Python入門―1日でプログラミングに強くなる！」という本を実際に手を動かしながら２ヶ月ほどかかって読了。
↓
「絶対に挫折しない iPhoneアプリ開発「超」入門 第6版 【Swift 4 ＆ iOS 11】完全対応」という本を実際手を動かしながらこちらも2ヶ月ほどかかって読了。
↓
Hacking With Swift(https://www.hackingwithswift.com/read)
というサイトに載っている サンプルコードと解説をよみながら10日で10個ほどプロジェクトを完成。
↓
思いつきでToDoアプリを二週間アレンジしてオリジナルのアプリを作成しAppStoreに出すも、Minimul FunctionalityによってReject。
↓
ARで適当にモデルを動かして遊んでいてなんとなく思いついたゲームを二日で作成、
一度リジェクトがありその後二日後にリリース。
*紹介した二冊の本、そしてサイトは本当に初心者にも優しいつくりになっていて、
オススメです。(サイトは英語です。)
また勉強に行き詰まった時は勉強会に出て、新しい知識を得たり経験者の方から進め方の意見を聞いたりしました。

勉強方法の背景
pythonから入った理由としては、他の言語よりシンプルらしい、という情報をネットで見つけたからです。
ところが本を読み終わったあと、これをいつまでやれば自分の手でアプリケーションが作れるようになるのだろうかという不安が生まれ、Xcodeがあれば実際に動いている様子がすぐ確認できるiosアプリ開発の方が、自分のアプリを作るイメージが浮かびやすいと予想し、言語をSwiftにチェンジしました。
Hacking With Swiftというサイトを使ったのは、サンプルコードが載っているサイトがSwift2で更新が止まっており、写経しても全く動かないということがちょくちょくあったので、完全にSwift4に対応しているこのサイトを使いました。

勉強方法について今思うこと
自作アプリの作成はもう少し早く始めるべきであったと思います。二冊目の本を読み終わった後、サンプルアプリの写経をし始めたくらいがベストタイミングだったと思います。やはり、自作アプリを作っているときは、集中力も違って、かなり楽しく取り組むことができました。また勉強してインプットしている時もこの機能は今のアプリに使えそう、とか少し違う観点から学べると思います。ありものの改変でいいので、早めに取り組むべきだったと思います。
pythonから始めた点については本が丁寧なこともあって、プログラミングの基礎へのとっかかりとしては良かったと思います。ただ、未経験からiosアプリをリリースしたいという目的が明確であれば、やらなくても良いと思います。

プログラミング未経験からiosアプリをストアに出す効率的方法
あくまで個人的な意見です。
1.「絶対に挫折しない iPhoneアプリ開発「超」入門 第6版【Swift 4 ＆ iOS 11】完全対応」を実際手を動かしながら最後まで読む。
↓
2.Hacking With Swift(https://www.hackingwithswift.com/read)
というサイトに載っている サンプルコードと解説をよみながら手を動かしてできるところまでやる。(英語が苦手な方はグーグル翻訳などに頼ってください。)並行して今まで書いてみたアプリに自分が思いつくオリジナルの機能を追加してみる。
↓
3. オリジナルの機能を追加するのに慣れてきたら、自分のアプリを作ってみる。今まで作った機能を組み合わせたり、その機能を変更したり、用途を変えたりetc..
↓
4. できたものをAppStoreに提出し、Rejectと戦う。

最後に
乱文ですが最後まで読んでくださりありがとうございました。
少しでも未経験の方や初心者の方の助けになれれば幸いです。
",False,https://qiita.com//nakachima/items/b5bed4551dede047e304
"

はじめに
私が高度情報で１発目に抜いたのはエンベデッドシステムスペシャリストなのですが、この試験に関しては、高度情報の中では結構特殊な試験なので、そもそも「受けない」とか「よくわからん」的な人が多い気がしています。
ただこの試験、逆に統計をみると不思議な感じを受けるんですよね。
エンベデッドシステムスペシャリストは当然高度試験なのですが、他分野と比べて学生の合格者が一定数コンスタントにいるんですよね。（しかも高度情報の対策本が出だす2003～4年より前から受かってる人がいたりしてたわけです。）
その反面、IoTで飯くってるはずの現役エンジニアが受けても受からないという話も聞くのです。これはどういうことなんでしょうか。(誤解を招かないように結論を先に書いておくと、学生のレベルが高かったり低かったりというわけでもないですし、現役でIoTで飯食ってるエンジニアのレベルが高かったり低かったりというわけでもないです。単に試験制度が合う層が、学生に一定数いる、というだけの事です。)
私としては、ここにどうにもギャップがある気がしてまして、いわゆるところの「ＩＴエンジニア」的なもの（想定している層としてはＳＥＳとかオープン系とか混ぜた感じです）とエンベデッドシステムスペシャリストの何が違うのか、あまりわかりやすいネット上の情報が無い気がするので書き残しておこうかと。
合格報奨金などで良いお小遣いになる会社もまだまだ多いと思うので、役に立てば幸いです。

まず強く意識しておく事
エンベデッドシステムスペシャリスト試験の説明のHP ( https://www.jitec.ipa.go.jp/1_11seido/es.html ) には、次のように記載があります。

２．役割と業務
組込みシステムに関するハードウェアとソフトウェアの要求仕様に基づき、組込みシステムの開発工程において、開発・実装・テストを実施する業務に従事し、次の役割を主導的に果たすとともに、下位者を指導する。 
　(1)組込みシステムを対象として、機能仕様とリアルタイム性を最適に実現するハードウェアとソフトウェアのトレードオフに基づく機能分担を図り、設計書・仕様書の作成を行う。

そう、エンベデッドシステムスペシャリストに対して求められる最重要ポイントは、「ソフトの世界とハードの世界を（ソフトウェア側から）つなげられる」事なのです。

受かるためにやる事
身も蓋もないのですが、ざっくり言うと(やっぱり)３点しかやる事はないです。

適当な教科書で基礎打ち
午前1、午前2対策
レベルを上げて物理で殴れ

適当な教科書に費用がかかるのと、過去問印刷に費用がかかる感じはあります。
過去問に関しては、会社が試験取得を推奨してるのであれば会社で刷っちゃうのもいいとおもいます。
(発注先企業の社員でもなければ、請ける時のアピールにも使えますし、資格が取れてプロジェクト的にミスマッチでなければ普通は会社にそれ以上に還元されるので…)

1. 適当な教科書で基礎打ち
他の分野では出てこない「コンピュータ構成要素」「ソフトウェア」「ハードウェア」「システム開発技術」は他の分野の教科書では拾えないです。（例えば伝送路符号とか… ＮＲＺ符号やＦＭ符号なんかは磁気券などでも使われてたりしますが、実運用で触れる人は極めて少ないでしょうし。）
なので本を買って参考書的に引けるようにしておくことが大事です。（ただし、そもそも内容が大きく変わる分野でもないので、お古を先輩からもらえたりするのであればそれでもいいと思います。本書いてる人には怒られそうですが、毎年買いなおす必要も無いでしょう。）
なお、解答能力を上げるために参考書が要るのは圧倒的に午前2ですが、無いとさっと疑問が解決できないケースはあるので１冊ぐらいは買っておいてよいでしょう。出版社に関しては翔泳社とかメジャーどころであればどこでもいいと思います。見てフィーリングが合うのが大事。

2. 午前1、午前2対策
セスぺ対策で書いたのとほぼ一緒になるのですが、簡単に言うと過去問を7年分ぐらい目途でやりましょう。
紙ベースでやるのなら試験本番とは異なりますが、片面印刷を推奨しておきます。
(※注：理由はミスノートを作るのに都合がいいから。やり方は後述します。)
あまりIPAの試験を受けてない人には認知されていない気がするのですが、
午前１・午前２の多岐選択試験(基本情報や応用情報、セマネの午前もたぶんそう)は
問題と選択肢まですべて一緒の形で、過去問からそっくりそのまま再出題されます。
（※注：そっくりそのまま出てこない場合は新問です。）
また、前回試験の問題で出た問題は出ません。
よって、前々回(2021年の試験受験者に対して、ここを基準に言うと2019年試験)から遡って7年分(2013年試験)ぐらいの過去問をやっておくとサクッと抜けます。
あと、用語を選ばせるタイプの問題は用語の説明が何も見ずとも
最終的に半分～６割ぐらいはできると良い感じです。
（だいたいこうだよね レベルでいいんですが。）
午前1が(試験時間50分＋確認25分)×7回
午前2が(試験時間40分＋確認20分)×7回
ぐらいで平均的に7割ぐらい回答できるようになると思います。
はじめは時間がかかると思いますがやっていくうちに過去問で見たことある
問題ばかりになるので、そのうち1回30分ぐらいで流せるようになります。
なお、間違った問題はＡ４のルーズリーフにテープのりを使って貼って集約し、
ミスノートを作ると高速に取りこぼしが取れるようになっていきます。
ミスノートはこんな感じです。(下記例はDBスペシャリストのですが。)

テープのりは100均とかでもいいのですが、コクヨのドットライナーがストレスフリーに使えて詰め替えもできるので超楽です。こういうところにはお金を使いましょう。

3. レベルを上げて物理で殴れ
この試験に関しては、現実世界の物理法則とＩＴをつなぐという特性上、午後に関してはこれをやっとけば１００％取れるというのが無いんですよね。出題範囲のＩＴがだいたいわかっている前提で、強いて言うのであれば物理のリテラシーがあってそれとＩＴを結び付けられるかが大きい気がしています。
先の「学生の合格者が結構昔からいる」というのもおそらくそこらへんに理由があって、ロボコンとか出てる学生さんだと、嫌でも午後試験で出てくるような事象は幅広く経験できる(というか経験してしまう)ので、解答できてしまうんだろなという気がします。(私も高校が電子科、大学の研究室がフィードバック制御だったので午後の解答は余裕でしたし…)
その上で、そんなことをわざわざ改めてやってられない社会人は要点を抑えてシミュレーションしつつ、過去問を３年分ぐらい解いてみるのが良いのではないのかなと思っています。ぱっと思いつく要点は以下にまとめておきますので、ここらへんを気にしつつ解いてみるのが良いのではないかと思います。
・速度と加速度について説明でき、解析・フィードバック方法を説明できるか(要は微分すると一次元下がるので…って事がわかってればよいでしょう。)
・ロボットアーム系の出題は結構多いので、1つの平面のあらゆる方向を向くのに、1周360度、これをXYZ軸で3軸もてると空中のあらゆる方向を向ける事が理解できていて説明できるか（3DCGの極座標系、アフィン変換とかとも絡む感じの概念ですね。）
・自然界にはノイズ源があるのでそれの捌き方（統計でいうところの最大・最小値を捨てて後を平均取るなど）と、そのメリットデメリット（動きが外乱(ノイズ)の影響を受けにくくなる半面、データを得られるのが平準化している時間分だけ遅くなるのでオーバヘッドが発生する みたいな。）
・物理世界には慣性が存在するので、入力があってから出力をフィードバックするまでのタイムラグにも車とか飛行物体は進んでたり動いてたり、外部の状況が変わっていたりする（ので、フィードバック制御等だとオーバーシュートが発生したり…）
・力のつり合いが取れると加速度がなくなったり、車やロボットの移動が止まる。併せて、〇力　ってつくキーワードはだいたい解答できる様に引き出しを作っておく。（重力とか…）

おわりに
セスぺ編が地味に受けている気がするので嬉しくなって続編を書いてみた次第です。セスぺがまだの人はこちらもどうぞ。
https://qiita.com/ditflame/items/21908ef8fb2f9e35d544
ちなみにバリバリの制御系スキルスタックだった私がエンベデッドシステムスペシャリストをしばらく受けるのをためらっていたのは単純にプロジェクトロックインされたくなかったが為です…（オープン系とか業務系システムも開発したかったのですが、組込みのチームにひょんなことから入ってしまってかなり深くまで浸かってた時期がありまして…）
組込みは2010年ぐらいまでやってましたがエンベデッドシステムスペシャリスト取ったのは2014年だったかな。(そんな経緯だったので試験の傾向性はずっと追ってました。)
個人的にはやっぱりIoT系で客観的に刺さりやすい資格というと、今もエンベデッドシステムスペシャリストが頭一つ抜けていると思うので、メリットがあるなら取得する価値はあると思います。
何か気づいた事があれば追記しますね。
",False,https://qiita.com//ditflame/items/549511c0430fdb0fed56
"弊社の勤務記録表を作り直してみようと思ったことがすべての始まり。
そして頓挫したフェルマータ的ぽえむ。めげないぞ。
ただの愚痴と言われても仕方がない
書いている人間はノリツッコミがひどい

はじめに
作り直すに至ったまでの経緯

背景
筆者が所属する部は、昨年度まで別の会社の部署だったのだが、今年度から同じグループ会社である弊社に移籍した部署である。4月から弊社の規則に従うようになり、弊社の利用する勤務記録表を使うようになった。ようやく半年が経過したところである。
しかしこの勤務記録表がかなりの問題児であった。

初めての勤務記録
説明書となるエクセル文書と実際に勤務記録を行うxlsファイルを渡されれば、とりあえずはまあ説明書を見ながら記録をしてみようとなるはずだ。私もそうだった。
まず説明書を見ながら、勤務記録ファイルを立ち上げようとする。
「他のエクセルを開いたままこのファイルを開かないでね！」←エラーメッセージ
f**k!
やめたらこの説明書
気を取り直して説明書を軽く全部読んで暗記。
さあ開くぞと意気込み、開いていろいろ弄ってみた。
使いづらさには目をつむるとして…
ボタンが押せないのにも目をつむって……
リストボックスが動かないのにも目をつむって……
勤務時間計算がおかしいのにも目をつむって……
とりあえず記入を完了した。
最後に保存をしようと思い、何気なくCtrl+Sを押す。(わりと当然だと思う。)
「(なんかのエラーコード)(正直どんなのか思い出せない)」→エクセル落ちる
f**k!!
　
まだ大丈夫、まだ大丈夫と持ち直して入力し直すために再度勤務記録ファイルを立ち上げる。
ファイルが破損していて開けない。どうやら勤務記録ファイルごとファイルが破損したようでもう二度と立ち上がらない。
f**k!!!!!!!!!!!!!!!!

エラーの洪水
勤務記録表はExcel2003-2007様式で作成されており(xls形式を用いるためか？)、Excel2016で開くと何故か落ちる。開けるエクセルのバージョン(2013で開けるのは救い)で開いても、少し意にそぐわない動作(Ctrl+Sもダメ！)をするだけでエラーの嵐。落ちるエクセル。戻らないデータ。さらには実際の勤務形態に即しておらず計算がまともにできない。勤務記録表の意にそぐうように自らの勤務時間を変更する始末。
いやあかんやろ。
それってもう勤務記録表として成り立ってへんやろ。
業務でのスキマ時間ができたので弊社のえげつなく使いづらい勤務記録表を作り直してみることにした。
というわけでとりあえず中身のVBAを見てみることに。

修正の記

問題点の洗い出し
眠らせたマクロを起こさないように、さながらオオカミの腹をかっさばいて中をのぞくような感覚でプログラムをのぞいてみた。
詳しく述べるのは避けるが……臓器が臓器の体をなしていない。そこに機能をもつものなど存在せず、茫洋たる荒海が広がっているだけだった。原生生物の構造を見るほうがまだ面白いものだ。
赤ずきんもおばあさんも、どこかにはいるのだろうが目を凝らしても波が邪魔をする。同じような波が何百と押し寄せてくる。(個人的に2度以上同じコードを書くくらいなら切り出せよと思っているので開いた口が塞がらなかった)。
この体に与える食物への消化処理も、食物側に依存している。入力制御等の最低限のチェックはされているものの、その関門をひとつ突破してしまえば何のその。体内の奥深くに入って初めて異物であることを認識する(たまに認識すらしない)。
デザインに関して一つだけ言うなら保護を外すと罫線が1本1本外れる。
最初から作り直すほうが早いのでは？

十分条件の洗い出し
xls形式を守っていくことのメリットが存在しないのもあり、いっそ1から作ってしまうほうが簡単であることに気づいた私は逆に何が必要なのかを見直すことにした。最終的に

作業工数と各時間がわかるcsvの出力
当月の休暇取得日数等がわかるcsvの出力
各csvを見やすくまとめたエクセル表

この3点があれば良いことがわかった。csvを作成するにあたっての条件等は現行プログラムにかかれている条件を精査することで実現可能だ。
さあつくるぞ。

勤務記録表の作成→完成
周囲からの希望も織り込んだ上で、

記述が少ない
修正しやすい
画面移動が少ない
一般的な使い方をしてもエラーが出ない

以上の要件も満たした勤務表を作成することに( 当たり前では？などと言ってはいけない )。追加で「Webで記入してそのまま提出できたら嬉しい」等の希望もあったが、まず今までの体裁をそのまま利用できてデザインを大して悩む必要のないxlsm形式を採用した。
メンテナンスを行いやすいような設計を目指し仕様書を作成、上司に仕様の確認を取る。メソッドの切り出しに注意し、個別でのテストが行えるようにテストコードを作成(大したものではないのでこの部分はもっと勉強したい)。総時間にして5日ほどで一旦完成した。
現行プログラムにある条件を考慮し、部内の希望も叶えた勤務記録表である。

挫折の経緯
テスト使用ということで数人にテストをお願いした。他部署の人間と関わることがほとんどないうちの部署だが、私の知る数少ない他部署の人間にテストをお願いしたときのことである。
「ここの条件がテストに含まれていない」
と教えてもらった条件の中に、自分たちが使っているものとは明らかに違う条件が出てきた。
何やらおかしいと思い詳しくすり合わせてみると、どうやら以前から存在していた部署には通達されていたルールがたくさんあるらしい。今年から移籍してきた筆者の部署にはそれが伝達されていなかった。
いやええんかそれ。あかんやろ。
他部署の上の人間に必要な項目を改めて伺おうにも勝手に作製するなんてと叱責されて終わってしまったので結局真に必要な項目がわからず作業を中断するしかない。
うーん。なんだかな。

反省
個人の空き時間とはいえ作ることを先に(もっと上の人間にも！)言っておけばこの時間はなかっただろう。VBAの勉強ができたのはありがたいことなのでスキルとして繋げさせてもらうが。
それにしてもなんだかすっきりしない心情のまま――今日も懲りずに作業の効率化を図るのであった。
",False,https://qiita.com//deficient_corde/items/d5d894ea3275f451954f
"

すみません
タイトルは釣りですが、釣られて下さってありがとうございます。

書いたもの
_(losand)._
$(dsand).$()

基底ライブラリ:　_(losand)._

最初は、モナドそのものの実験的な実装だけをやりました。
そのうち、javascriptがとり得る最低限の圏に対しての自己関手ぐらいは実装したものにしようという意欲が湧きました。
前作、de.jsより、既存コンストラクタのprototypeを出来る限り拡張しないでも実装出来ないだろうか？
全て、_で繋いで文脈を理解させるのも限界がある等、色々失敗した後だから色々試しました。失敗も多かったです。
例えば、JSONを丸渡ししてオブジェクトの振る舞いを書ければ…
なども候補にありました。
losandそのものはstateモナドとして実装されているのですが、使う側のことを考えたらstate側を明示で呼び出すのは苦しい言い訳がある（要は使いづらい）ので、モノイドをかなり意識しました。
最後に　._　で閉じるまでは、すべての命令を |> で繋いだ表現として書くことも可能です。
<script src=""https://cdn.jsdelivr.net/npm/losand@1.2.0/losand.js""></script>

で読み込み出来ます。
以下は例です。
//ex
_({a: 1})
.draw({b: 3})
.map(
  o => _(o)
  .vals
  ._
  .reduce((a, b) => a + b, 0)
)
._
// 4

_({a: 1, b: 3, c: 5})
.hold(""a"", ""c"")
._

_記号にサンドイッチされた計算機構と考えたので、
losand
になりました。
これが460行です

VDOM on Pure Javascript $(dsand).$()

上述、losandを使って凝集度を上げに上げて作った結果、745行のVDOMまでを賄うフレームワークを書きました。
まずは読み込み…
<script src=""https://cdn.jsdelivr.net/npm/dsand@0.3.5/dsand.js""></script>

最初に、\$.dataに閲覧可能な状態(state)を書き込みます。
閲覧対象の指定は、各 Render（後述します。） の持つ、markメソッドに、$.dataの持つプロパティ名を入れてあげることで、指定した閲覧を可能にします。
閲覧は、RenderFunctionの持つlookで出来ます。
_($.data).draw({
  test: {
    state: ""success on load at test state""
  }
});

次に、役割のための振る舞いを記述します。
この振る舞いは、あるHTMLエレメントで指定されたイベントが発火したときに、
そのエレメントが持つclass名をキーにして実行されるように出来ています。
引数に渡されるのは、eventオブジェクトです。
ここには、MVCで言うところの　Mの持つドメインロジックを記述していきます。
_($.role).draw({
  doTest (e) {
    alert($(e).look.state);
    return ""marked data loaded on .look""
  }
});

次に、見た目を変更する記述をします。
これも、HTMLエレメントで指定されたイベントが発火したときに、
そのエレメントが持つclass名をキーにして実行されます。
第1引数はeventオブジェクトが渡りますが、
第2引数は、$.roleで行った振る舞いのreturn値が入ります。
生成されたエレメントの見た目をごっそり書き換えたい場合、
.seem が便利です。
ちなみに、packという名前の由来ですが、振る舞った後は、unpackされたという言語的概念に即してみました。
_($.pack).draw({
  doTest (e, d) {
    $(e).seem(d);
  }
});

いよいよレンダリングです。
各タグがそのままエレメントを返すためのgetメソッドとして機能しています。
\$.bodyはbodyタグを表し、.\$は基本的にappendを行います。
ちなみに、\$(Element).\$(undefined)なら、その要素をremoveします。
const myButton = button
.mark(""test"")
.class(""doTest"")
.$(""load state data"")

$.body.$(
  p.$(myButton)
);

745行ほどで、この実装が出来ています。
主なサポート対象はchromeで、phonegap を使ったSPAを素早く作れます。
AndroidアプリのwebViewで正常に動作するので、ネイティブっぽいSPAを作ることも可能です。
グローバルのgetメソッドに各タグの為のレンダーが当たっているのは、コンポーネントを実際に生成する場合は、グローバル領域を使わないようにして欲しいという気持ちがあります。
なお、formタグ内のname属性を持つElementに対してであれば、
.set({[name]: value})
で値の設定ができ
.get
で、オブジェクトが返ってきます。
返ってきたオブジェクトを
_(Object).json

とするだけで、JSON取れます（笑）
JSONStr.json._

でパースされて戻ってきますが、
JSONStr.json

で、losandに代入されたオブジェクトとして扱えます。
理由は…
_({a: 3}).json.json._

というstringifyしてparseするというところで明示をしたかったためです。
以上、お読みいただきありがとうございました。
",False,https://qiita.com//johnny-shaman/items/3b7ea472125d2131ee7c
"

1. ゲームの楽しみ方
ある程度ゲームを楽しめる段階に達すると ルール以外の概念でゲームを把握するようになる
例えば将棋:


駒の働きが先手の方が勝っているので指しやすいと思う
先手は角交換を嫌がっているようです
これは焦点の歩ですね (などの手筋 = 部分的なパターン認識)
後手の王様が遠く 速度で勝てると踏んでいるのでしょう

ポーカーでは

この形は インプライドオッズ を考慮すればベットできる
今はルースにプレイするにはブラインドが小さすぎる
セットが入ったので、ポッドを育てるための振る舞いが必要だ

数独では:


双子の数字に注目して候補を減らすことは基本戦略
あと少しなのでバックトラックが成功しやすそうなマスを見つけよう


2. 実況・解説の有用性
野球中継やポーカー中継などTVショーで実況者・解説者が言語化するのは、前述したような ある程度大局的な観点で、（正解かは置いておいて）論理的な思考力を使えば理解できるようなサマリだ。
こういったものを自動生成するような枠組みがあるのかなあ？機械学習でニュース記事の自動生成をやってたり、強いAI を作ることができるので、技術的には可能なのだろうけど、あまり世に広く普及はしていないようにみえる。将棋なんか完全に 機械学習が商用ベースに乗っているのに実況ソフトは見かけない（その分野にあまり詳しくないだけかもしれない）

3. 実況の種別
少しの考察。学術的にあるんだろうけど、まあ素人考えで。
実況には大きく N種類あると思う。1. 要約の実況 2. 予想の実況 3. 思考の実況 4. データベース探索
「要約の実況」は、ゲームの現況を上手く要約するものだ。パターン認識が得意とする領域:

先手は四間飛車を選択しました
角換わり腰掛銀の形になりそうです
先手は詰めろをかけました

「予想の実況」は、次の一手を当てるものだ。これはまあ普通の機械学習。DQNとか:

最善手は△３二飛。評価値 +120。後続手は・・・・
事前手は△９五歩。評価値 -20。後続手は・・・・

「思考の実況」は人間の思考をトレースするような実況。感触や部分的な最適解を積み上げてどのような推論が可能なのかを提示:

銀冠が完成する前に後手は仕掛けたいです
金をうわずらせて 将来銀の割打ちの筋が生じさせるようにしたのでしょう
駒得よりも速度を優先させた攻めになります

「データベース探索」は、ゲームの現況に合致した過去のデータを提示するもの。長考中の時間の穴埋めや、対局者固有の情報を提示:

この局面では先手の攻めが成功した事例がありました
先手は現在５連勝中で勢いに乗っています
この局面のような角換わり６八金型が流行しています


4. 社会的背景と照らして
意思決定を受け入れるためには、何らかの形で人間がその決定を理解しないといけない。興味があるのは過程であり、結果ではないから（少なくともゲームを見る側にとっては）。
実際、ゲームの実況だけでなく 株価の推移、SNS での炎上過程、野生動物観察、などなど情報を消化しやすいように加工するニーズはいくらでもありそうで、それらの基盤技術はこれから伸びていきそう。関連する会社の株があれば買いたい、まじで。
",False,https://qiita.com//41semicolon/items/43506ebd02a90d00eecc
"

はじめに
はじめまして、ましはるです。
自分がプログラミングの勉強をはじめて先月で半年になりました。
しかし、始めてから先月までなかなか上達せず、何度も挫折しそうになりました。
プログラミング初心者の方にはそういう方はとても多いのではないでしょうか？
何故、自分がなかなか上達しなかったのか、何度も挫折してしまったのかをプログラミング初心者やこれからしようと思っている方に向けて自分が犯してしまった、大きく分けて3つ目のミスについて書いていこうと思います。
文章を書くことがとても苦手なのでその辺はご了承くださいm(*_ _)m

１つ目のミス
まず、皆さんは何からプログラミングを始めたでしょうか？
私は今流行りのProgateを使ってはじめました。ゲーム感覚でできるし、開発環境のことなんて考えなくのもいいので、プログラミングがどんなものかを知るという意味ではとてもいいツールだと思います。
しかし、私の場合はこれをやり込んでしまったのです。
これが1つ目のミスです。
色々な言語を満遍なくやり、勉強をした気になっていました。
やり込むことは悪くは無いと思うのですが、やり方の問題です。なにも考えず、とりあえずやるのではなくノートにメモをしたり、Twitterなどにアウトプットしていく事がとても大切です。アウトプットしていない自分が言うのでは説得力がありませんが、初心者の方でなんらかの媒体でアウトプットしている人は上達スピードが全く違います。どんな事でもいいのでアウトプットしていきましょう。
自分もこれを機にQiitaなどに投稿していきたいと思います。

2つ目のミス
2つ目が参考書やネット記事のチュートリアルをひたすらしてしまったというミス。
これも先程の話と同じなのですが、参考書をたくさん読んで
「俺はこんなに勉強したぞー！！！！」という感覚に陥ってました。
結果、全然身になりませんでした。自分は馬鹿なので記憶力も良くないし、要領もよくない、そんな自分がこんなやり方で上手くいくわけがないのです。
ほんと時間を無駄にしたなと思います。
チュートリアルは1つしたらそれを改造してみるのがいい勉強方法だと思います。
参考書は一周ほどしたら辞書代わりとして使いましょう。
勉強を始めたばかりの時に色々読むのではなく、1つアプリを自分の力で作ってみた段階で読む方が内容がちゃんと理解できるし、何より面白いと思います!!

3つ目のミス
3つ目のミスはいろいろな世界にこんにちは！してしまった事です。
要するに初心者なのにいろいろな言語に手をつけてしまったということです。1つの言語も理解出来ていない段階で他の言語をやっても理解できる訳ないです。自分の作りたいアプリにはどの言語が向いているのかと探ってるうちに無駄な時間を過ごすことになります。Ruby、PHPやPythonなどこれらの言語なら簡単なアプリならどれを使っても作れるので、とりあえず1つの言語に絞って勉強した方が良いです。
自分の場合はこれに本まで買っていたので、時間だけでなくお金まで飛んでいきました。

効率よく成長する
こんな感じでいろんなミスをして時間を無駄にしてきたわけですが、これらを解決でき、爆速で成長できる方法があります。
それはメンターをつけることです。
1人で勉強していくことはもちろん出来ます。
しかし、メンターや教えてくれる人がいるだけで成長スピードが全然違います。自分はある会社にインターンに行き5日間一緒にアプリ開発をしたのですが、物凄いスピードで開発を進めることが出来ました。分からない所の解決方法を教えて貰うだけでなく、検索の方法も教えてもらったので、インターンが終わった後の開発もスムーズに進めることが出来ています。1人では1ヶ月ほどかかっていたことも一日でできたりするので、プログラミングをしていて楽しく感じてきます。
プログラミングを始めたての人がインターンを行くのは難しいものがあるので入江さんのMentaや迫さんのSkillHacksなどを使うと分からないことをすぐに聞ける環境を作ることが出来ると思います。
プログラミングスクールや専門学校という選択肢もありますが、正直オススメはしません。理由はここで書くと長くなってしまうので、また、違う記事として書きたいと思います。

まとめ

Progateをやり込みすぎない
参考書やチュートリアルばかりやらない
言語は一つに絞る
学んだ内容をアウトプットする
メンターや教えてくれる人をつける

これらを抑えておけばどんどん成長していくのではないでしょうか？
以上、下手くそな文で書いていきましたがどうでしたでしょうか？
自分と同じミスをする人が1人でも減ればいいなと願っております。
あと、プログラミング友達がいないので誰か友達になってください(切実) 
一応Laravelやってます。
早ければ今月末にアプリのβ版をリリースする予定なので良ければTwitterのフォローの方<(_ _)>よろしくお願いします。
ここまで読んでいただきありがとうございました。
",False,https://qiita.com//m4haru/items/0a9202b50b7c689c2039
"

まとめ
canonicalは正式なHTML仕様。
canonicalを使うことで、検索エンジン(クローラー)に対して自分を無視させcanonical先に正の影響を与えることができそう。
301リダイレクトではないので検索エンジンに重複して表示される可能性はあるし、所々の記載場所の中で内容を閲覧することができる。
つまりはサービス内の読者にはcanonicalであるかどうかはあまり関係がない。
よって、検索エンジンでは重複せずとも別々の場所で同じ記事を複数回見かけることになるかもしれない。(その形態自体は宣伝や誘導目的のマルチポストとは言えない可能性が高そう)
それはちょっと体験がよくなさそうなので各サービスは特色を活かした方面へ特化してほしいし、投稿者もcanonicalをつけつつも完璧に同一な記事ではなく各サービスに合わせたユニークな記事になっていれば読む方は幸せなのかな。
とうすぼんやり考えました。

前段
最近、Qiitaに似たサービスがいくつか出てきて、カノニカル、カノニカル投稿といったワードを目にするようになりました。
浅学ゆえ存在自体しらず、「カルーセルスライダーみたいなものなのかな？」と思っていました。
canonicalの意義と使い方自体は調べればすぐにわかる内容ですが、調べた記録を残すとともに便乗で考えたことなどを。

canonical(カノニカル)とは
同一内容1を投稿した際、オリジナルのURLを指定できる。
検索エンジンのための情報。
<link rel=""canonical"" href=""https://qiita.com/khsk/items/～"">

linkタグなので、href先との関係がcanonical、href先が標準・オリジナルだと宣言している。
ネットではcanonicalタグというワードも散見されるが、見てわかるように属性である。2
同じ内容をいろんな箇所に書くとGoogleからペナルティをくらう、といったような話を聞いたこともあるような気がするが、SEO方面には明るくないので突っ込まない。
おそらくそういったものへの対策か、同一内容で検索結果を汚さないためのユーザーライクのためなのだろうか。
どちらが強く影響するのかは気になるところ。

検索エンジンのための記述ということはrobots.txtのようなもの？
検索エンジン向けの記述と言えば、(ワードサラダなどSEOハック的なものを除けば)
robots.txtを思い出した。
Robots Exclusion Standard - Wikipedia

このプロトコルは全く拘束力がない。あくまでもボット側が協力することが前提であり、robots.txt で指定したからといってプライバシーが保たれることを保証するものではない。robots.txt で見られたくないファイルを指定したとしても、ウェブブラウザを使えば世界中の誰でもそのファイルを閲覧できる。また、このプロトコルを無視するボットを運用することも可能である。
robots.txt プロトコルにはRFCや公的な標準化団体が全く関与していない。 

robots.txtは無視できるということもなんとなく知っていたが、なるほどRFCやHTMLに関する標準仕様ではなく、「お願い」といったもののようだ。

canonicalは仕様
ではcanonical属性はどうだというと、見出しでオチているのだが、
HTML Standard 日本語訳 links.html#link-type-canonical

canonicalキーワードは、href属性で指定されるURLが現在の文書の優先URLであることを示す。これは、検索エンジンが重複コンテンツを減らすのに役立つ。詳細については、The Canonical Link Relation仕様を参照のこと。[RFC6596]

RFC 6596 - The Canonical Link Relation

April 2012

とあるので決して新しい仕様ではないようだ。

クロスポストとは
「canonicalを利用した～」という文章には「クロスポスト」というワードが続くようだ。
「複数のURLに投稿する」程度の意味だろうか。
そもそもSEO、検索結果に配慮がされているといえ、人間から見れば「マルチポスト」なのでは？っと掲示板世代としては思ってしまったので、こちらも少し調査。
クロスポスト - Wikipedia

クロスポスト（英語：crossposting）とは、ネットニュースにおいて、1本の記事を複数のニュースグループ（カテゴリ）を指定して投稿することをいう。
ネットニュース特有の機能であり、クロスポストを行う場合、内容に関連する数グループ程度までを指定して投稿する。この程度のクロスポストなら、あまり問題にならない。 

ネットニュース - Wikipedia

ネットニュースは、電子メールと並んで、コンピュータネットワークの初期につくられた情報交換システムの1つである。いずれもインターネットが一般に普及する以前から存在しており、当時はUUCPで配送されていた。この経緯により、ネットニュースの記事の形式と配送方法はEメールのそれとよく似ており、UUCPの特徴であるバケツリレー式の配信を前提としたものになっている。ただし、Eメールは通常一対一のメッセージ交換に主に使われるのに対し、ネットニュースは不特定多数の利用者間の議論の場として使われる。 

なんとも古い時代からあったものなのですね。
クロスポストのWikiだけでは、最近ニュースサイトで見かける
「提供元:ほげほげ(リンク)」
のような引用？形式のことかと思っていたので、勘違いがとけた。
マルチポスト - Wikipedia

マルチポスト（英: multi-post, multiple posting, multiposting）とは、同一の内容の文章を複数のニュースグループや掲示板に別の記事として投稿すること[1]。クロスポストとは区別される。
本来はネットニュースからの用語であるが、現在[いつ?]は電子掲示板などで多く使われる語である。 

クロスポストと前後して生まれた？（主観）主に手作業のようにポストしていくことを指す言葉のようだ。
関連:マルチポスト（クロスポスト）とは - IT用語辞典

canonical的・ユーザー的に言うと？
サイト単位で言うと…マルチポストじゃないですか？
Googleなどの検索エンジン側では重複表示されないかもしれません。その違い？

クロスポストが増えると私はどう思うか予想する
(Mediumやdev.toなどすでに運用しているところは未観測なので言及できずすべて推測になってしまうのですが)
CrieitやQrunchなど、日本語圏のQiitaライクの技術的記事を(も)投稿出来るサイトが今後、仮にたくさん生き延びることができたとして。
それぞれが活性化するとなると多分私もそれぞれを監視するようになると思います。
でも、それぞれのサイトで同じ記事がクロスポストされていたら…
コメントの有無はともかく内容的にはほぼ同一なので、どこか一つで見れば事足りるわけですよね。
それは知恵袋やOKwaveなどでおきていた
「(質問者からすると)マルチポストではないけれど裏でサービス同士が連携しているのでどこでも同じ内容が表示される」
状態ですよね。
検索エンジンでなら除外検索でフィルターできますけど、Webサービス内ではフィルターは難しいのでは…？
ユーザーミュートでは「クロスポストではないそのサイトだけでの投稿」が見えなくなりますし3
ほしくない情報が目に入るという問題は「ゴミ記事問題」にもつながるので深入りはNGですが。
「ゴミ記事が多すぎる問題」に対する私見 - Qiita
(一連の流れ・コメントは読んでいません。本文もあまり…)
ユーザーがうまくフィルターするのが責務。ということはわかります。
「ゴミ記事」との違いは「良記事もクロスポストされる」ということでしょうか。
真に有益な記事ほど各知見集積サービスで(タイムラインなどでフィルターしていても)何度も目に入ることになって辟易する。
なんてことにならないかなと。とくにそれが「トレンド」など有限の枠をその有益さ故に各所で消費されると跨いで見ている身としてはもったいなくなるかもしれませんね。
そういった記事の(技術・ツール的な)フィルター方法も思いつかないので目視の無視フィルターになるのかなと。
個人ブログ → Qiita ← 個人ブログ
のような個人からパブリックへの流れなら巡回場所が減るので大歓迎ですが、各知見集積所にメッシュ状にクロスポストされると、
かわいい記事でも「もう見た」なんてポプ子状態にならないかなと思いました。4
そもそもその、適当に名付けましたけど「各知見集積所にメッシュ状にクロスポスト」が一部ユーザーの行動でしかなく、クロスポスト推奨側の期待している行動ではない可能性もあるので、杞憂感もありますね。

優先を示すcanonicalがあるのならば、非優先を示す属性はあるのか？
記事のコピー(canonicalはそれだけに限りませんが)先に付けるcanonicalとはいえ、複製なんて簡単なので、
仮に百個のcanonicalを記述されたクロスポストがあると仮定します。
そして例えばcanonical先がhttps対応しました！となり優先URLが変更になると、
100個のHTMLファイルの編集が必要になりますよね？
では逆に優先URL先のHTMLに、
「このURLたちより私を優先してほしい」
と非優先URL、逆canonical属性のようなものがあれば変更ファイルは一つで済むんじゃないかと思いました。
…検索エンジン的には優先URLを巡回しない限り個々の非優先URLを別々にインデックスしてしまう欠点がありますが、そういうのはあるのだろうかと思いました。
結果的にはそのようなものはありませんでしたが。
HTML Standard 日本語訳 links.html#linkTypes
canonicalはあくまでcanonicalが書かれているURLとcanonicalが参照しているURLの一対一の関係。


canonicalは記事の盗用に役立つのか？
これも考えればすぐわかりますが、ならない。と思います。
あくまでcanonicalは自分を謙譲して参照先を優先させる機能なので、盗用者が盗用元を記述するわけもなく。
先に仮定した非優先属性があったとして、盗用されたらそれは無視するように非優先に片っ端から入れていくならどうだろうか？
と思いましたが、これも盗用先が盗用元を非優先だと主張しだして(検索エンジンにとって)よくわからない状態になるなと思いました。
盗用者がわざわざ盗用元のURLを記述してくれるとなれば観測が楽になる効果はあるかもしれませんが…
HTMLってそういうことのための言語なのか？と言われると困ります。そんな属性ないのですけれど。

その他
カノニカルとは？ - Crieit

質問はマルチポストになってしまうためカノニカルによるクロス投稿は行わないでください。

「クロス投稿も質問も大いに歓迎するけれど、例外的にクロス投稿のうち質問はマルチポストに分類される」
という解釈でしょうか。
このあたりの線引はネット文化に足を踏み入れそうで、明確な定義はないのかもしれませんね。




に限りませんが ↩


linkタグはrel属性が重要なので慣用的に(ref属性)タグと呼び習わしているならごめんなさい。 ↩


そんなシーンの実益があまり思いつかないのですが
技術的なものはQiitaとクロスポスト、雑談はAサイトのみに投稿→Aサイトでそのユーザーをミュートしていると取りこぼす？ ↩


似た内容や初歩的な内容は長く居れば何度も遭遇することになりある意味慣れましたが、このパターンに忌避感を覚えるのはその慣れがまだないからなのかもしれません。 ↩



",False,https://qiita.com//khsk/items/6f8142a663f84b8b261c
"Architecture no one needs is server side templating (HN)を読んで。
昨今では猫も杓子もJavaScriptが大好きなようで、「Reactを使っていない」などと言おうものならバッシングを受ける時代ではありますが、そんな2018年も終わろうとしている今、どうしてテンプレートエンジン(笑)などというダサいソリューションを使わなければならないのでしょうか?

テンプレートエンジンは遅い

Templating language is basically string interpolation which builds the presentational layer at load time.

""load time""とはいつを指すのでしょうか? どうやら元記事の著者は、テンプレートエンジンが「ユーザーがリクエストを読み込むとき」、文字列を連結していると考えているようです。これは明らかに、大抵のテンプレートエンジンの動作ではありません。

テンプレートエンジンはテンプレート文字列を読み込み、コンパイルする
入力されたモデルを使い、コンパイルされたテンプレートがストリームに対して出力を行う
必要なら出力をキャッシュする

第一に、テンプレートはコンパイルされるので単なる文字列の結合より高速です。第二に、出力は結合された文字列として生成されるのではなく、出力ストリームに直接出力を行います。その場合、文字列結合にかかわるオーバーヘッドはありません。
一般的に言って、SPAよりもテンプレートエンジンのほうがずっと高速です。
時々、「SPAはリンク先をプリフェッチするから速い」などという人がいますが、これはユーザーの観点からすれば最低最悪なので絶対にすべきではないです。

SSR
サーバーサイドレンダリングは本当にいいところが何一つないソリューションで、遅く、複雑で、必要性のない依存関係と必要のないレイヤーをシステムに持ち込む、何故使われているのか理解に苦しむものです。

テンプレートエンジンの学習コストが高い
テンプレートエンジンの学習コストがJavaScriptの学習コストより高いというのは相当無理のある主張ではないでしょうか? それともJavaScriptはできて当然ということなのでしょうか? テンプレートエンジンの構文は大抵どれも似通っていて、それほど負担になるとは思えません。必要な時にリファレンスを見れば十分ではないでしょうか。

エラーハンドリング
元記事の著者はなぜか、テンプレートエンジンはエラー発生時に500を返すことしかできないと思い込んでいるようですが、全くそんなことはありません。

クライアントサイドのコードが別々になってしまう
これは確かに問題であり、テンプレートエンジンにうまい解決策はありません。ただ言えることがあるとすれば、テンプレートエンジンを使うときは「その流儀」に合わせて設計を行うべきであり、例えばAJAXリクエストを極力減らして作るべきです。

コンポーネント志向
多くのテンプレートエンジンがコンポーネント志向でないというのは本当に問題なのですが、これは単に多くのテンプレートエンジンがそういう設計になっているということであり、コンポーネント志向のテンプレートエンジンがないわけではありません。

SPA: Good parts
SPAは動的なWebアプリケーションを作るのに適した唯一のソリューションで、テンプレートエンジンにはできないことです。

まとめ

Summary of the article: ""Why do we need screwdrivers? I have a hammer, and everything looks like a nail to me!""

HNのこのコメントが、完璧な要約です。SPAが必要ならSPAを使えばいいし、テンプレートエンジンが上手いソリューションならテンプレートエンジンを使えばいいのです。
",False,https://qiita.com//minebreaker/items/bef8da0e76a7f7c2245c
"

話さないこと

Gitとは
VSSとは
絶対こっちのほうがいいに決まってる！
Gitのここがよかった！（たくさん記事があると思うので今回は割愛します）


話すこと

VSSのここが恋しい！
今後はVSS？Git？


前提

プロジェクト概要

開発メンバー約６人
フレームワークは自社製のフレームワークを使用
各プログラムから呼び出す共通のクラスファイルあり
言語はC#
開発ツールはVisual Studio
コードレビューする文化はあまりない（あってもGitは不使用で行う）
単体テスト終えたら、コードレビューせずにmasterにマージする


お話の舞台

工程は開発〜単体テストの部分のお話（現在進行中）


私のGit経験

Git経験９ヶ月
しかし今まで開発メンバーが自分だけのプロジェクトで使用していた
複数人でGitを使用するのは初めて
複数人ではVSSを使用した開発をしていた


開発メンバーのGit経験

全員Git初心者
私がGit操作など教えた
VSSユーザーからVSS非ユーザーまで


VSSからGitに乗り換えたきっかけ
プロジェクト開始時、普段使っていたVSSサーバーが死亡していたため、VSSを使用することはできない状況でした。
同時期に、私が別プロジェクトでGitを使用するということで、社内に新しくGitサーバーを立てました。
新しくGitサーバーができたということで、そちらを使用しようとなりました。
ちなみにGitサーバーは

オンプレで、社内ネットワークからのみ接続可能
GitBucketとPostgreSqlを使用
Windowsサーバー（OS失念）

なぜGitにしたかですが、私がGit信者だったからです。
Gitはナウい！サポート切れてない！
Git使っているとコミット履歴残るからいちいちコメントで修正前のコードを残さなくていい！
というミーハーな理由で社内サーバーにGitサーバーを構築しました。
ちなみに、Gitの前はAccuRevを使おうという動きがありましたが、ライセンス費の都合で、Gitになりました。
Git信者だった私ですが、いざ、Gitで複数人で開発していると、
Gitのデメリットが見えたので記事にしました。
そう、Git信者、目が覚めた。

Gitを使い始めて、VSSの恋しくなったところ

共通ファイルが常に最新になること
どのプログラムからも呼び出す共通クラスのファイルがあったため、非常に厄介でした。

作業ブランチで共通ファイル編集してしまい、そのファイルだけmasterに反映させることができないため、ファイル単位でmasterにマージしたい！という要望多数。
わざわざ共通ファイル編集用ブランチ作成するのも面倒。
誰かが共通ファイル編集したたびに、リベースするの面倒。
共通ファイルのファンクション名変えたら悲劇。使ってるところ全部修正しないと……、面倒だな。

ということで、ゴリゴリに共通ファイルを編集する、開発〜単体テストの期間は、
Gitのような分散型ではなく、VSSのような集中型のほうがラクだなと思うようになりました。

学習コストが高い
あくまでツールだから、なるべく時間をかけずに使いこなせるようになりたいものですが、
Gitは使いこなせるまで時間がかかりました。
Git初心者の開発メンバーに教えていると、概念でつまずいたり、使い方でつまずいたりと、
つまずきポイントが多くて、同じ内容をいろんな人に教えることになりました。
って、これはGitが悪いのではなく、学習方法や指導方法が悪いのです。
まあでも、Gitに限らず、学習コストが高いものって避けがちです。
どれだけスマートに学習できるかの人間側の能力が試されてる気がします。

今後の運用
工程に合わせて使用する管理ツールを変えるのがベストかもと思いました。
移行の面倒さはありますが。
例えば、開発〜単体テストまでは中央集中型で、結合テスト以降をGitにすれば
両者のいいところが活かされるのでは？と思います。
とはいえ、私はまだまだGit初心者なので、 Gitだけでこの課題を解決できるGit運用の方法があるかもしれません、私が知らないだけで。
Gitの運用方法で回避している場合はコメントにてご教授いただければ幸いです。
",False,https://qiita.com//tsuyuri104/items/f5d330df607e8cff63d5
"ポエムなので、いつもの、ですます調ではありません。

AIは旋盤工に取って代われるのか
悪の組織（大学や企業の研究所）が日夜、旋盤工を失業させるために頑張っているイメージだが、
現状のAIは旋盤工を失業させ得るとは思えない。
多くの人が思い描くのは、旋盤工に代わって、NC旋盤の前に立つロボットだろうが、
NC旋盤自体がロボットのようなもので、ロボットがロボットを制御するのは非効率だから、
そういったことは起こり得ない
また、多くのNC旋盤が、インターネットに接続されていないし、
旋盤という仕事をAIに教え込むために必要なデータの定義や、
ビッグデータが（秘密裏に）取得されているようには見えない。
しかし、油断は出来ない。
なぜなら、旋盤という仕事は、確率とくにベイズ推論の世界なのだから。

旋盤の仕事は確率論的である
例えば材料である。鉄材の場合、SS400のように成分がすべて均質ということはありえない。
材料の中に石が所々に詰まっていて、偶然石に当たった場合に切削していたチップが
摩耗していくというイメージだ。
例えばチップである。チップは工業製品なので均質と思いがちだが、
個々には微妙に大きさや品質が違っていて、うまく削れる場合もあるし削れない場合もある。
例えばタップやドリルである。材料にもよるし、切削条件にもよるが、ある回数以上になると破断する。
豊富なデータを使用できれば、切削条件や、使用回数をAIが決めてくれる未来の可能性もあるだろう。

NC旋盤は未熟である
結論、AIに学習させるというAI対応NC旋盤が出てこない限りは、旋盤工は失業しないであろう。
以上
",False,https://qiita.com//matumoto_onga/items/fba700c2089b66816d13
"新卒としてweb企業に就職しそろそろ若手ではなくなってきたので、
自分への戒めも込めて書きたいと思います。
(エンジニアリングの話は、ほとんど出てこないです。)

信頼貯金を意識する
信頼が仕事をするうえで一番大事だと思います。
信頼がない状態で自分の意見を言っても聞いてもらえないですし、
信頼がない状態で仕事を任せてもらえるようにはならないと思います。
信頼はすぐに得られるものではなく、
日々の行いを通して少しずつ得られるものであり、
貯金のようなものだと思っています。
悲しいことに信頼は貯めるのは難しく時間がかかりますが、
無くなるときは、一瞬で無くなります。
自分の信頼貯金がいくらほどあるかをちゃんと把握して、
信頼貯金に応じた行動をすることが大切だと思います。
ただ信頼を得ることだけに意識を向けて人の顔を見て仕事をしてしまうと、
社内政治が始まるので、それは避けるべきだと思います。

オープンな存在になる
仕事は一人では行えないので他の人と関わりますが、
自分がどういう存在で何を考えているか・何に重きを置いているかなどを、
発信してオープンな存在にすることが大事だと思います。
オープンな存在の方が周りが自分を理解する部分が多くなるため、
自分のキャリアプランニングに沿った経験をさせてもらうこともできますし、
自分が働きやすい環境になると思います。
また教えてもらう立場の時には教える方も、
何がわからいのか・どこで詰まっているのかを知りたいので、
自分は何がわからないのか・何を教えてほしいかを明確に示すことも大事です。
あと、人は忙しいときやミスとしたときはクローズな状態になりがちですが、
そういった状態でもオープンな状態を維持して、
一人で抱え込まないことを意識すべきだと思います。

相手を尊敬、尊重する
エンジニアはプロダクトの開発に責任を負っているプロフェッショナルですが、
企画やデザイナーは同じプロダクトの別の部分の責任を負っているプロフェッショナルです。
一つのプロダクトにおいて別の責任を負った人がいるので、
必ず意見が食い違う時がありますが、否定的にとらえてはいけないと思います。
しっかりとなぜ相手はその意見になるか・相手は何を考えているかを考え、
相手の意見を尊重した上でシステムに落とし込むべきです。
またエンジニア同士でも、お互いの得意分野や設計に対する思想が違うので、
相手のコードや設計を否定せずに、どのアプローチがプロダクトに一番合っているかを考慮し、
コードレビューなどを行うべきだと思います。

自分の論を持つ
若手は教えてもらうことが多いのであまり意識しないかもですが、
何事にも自分の論を持つことが大事です。
自分の論がない状態で他の方に質問や意見を求めて回答が返ってきて、
それをそのまま取り入れても何も自分の成長には繋がらないと思います。
自分ではできない仕事でもいったん自分で出せる考えを出したうえで、
他の方に自分の意見をぶつけてみることが大事だと思います。
違っていたらそのズレから自分がどれだけ足りない部分があるかを、
知ることができるかと思います。
また自分の論を作るときにはしっかりと自信を持つことが大事です。
自信がない付け焼刃の論を持つぐらいなら、持たない方がましだと思います。

コストを意識する
コストといっているのは、時間的なコストの話です。
社会人は時間に対してお金をもらっているのでできるだけ無駄な時間を減らすことが
コスト面では大事だと思っています。
コミュニケーション部分でのコストは他人と自分の時間のコストがかかるので、
コーディングといった自分だけの時間のコストよりも意識する必要があると思います。
例えば他の人に質問をするときには事前に分からない内容をまとめておくことや、
会議に議題を持ち込むときには何を話すかを明確にしておく、などです。
仕事で話すときにちゃんと会話のゴールを決めて、
無駄な時間を使う手探りの会話を避けることが大事だと思います。
ただ仕事外の雑談は大いにすべきだと思います。

振り返りをちゃんとする
定期的に自分の振り返りを行うことが大事です。
普段の業務に対する振り返りもちろんですが、
自分のキャリアの棚押しも振り返りに含めて行うべきだと思います。
振り返りで意識すべきことは振り返りをしたうえで、
ちゃんと次の行動を明確にすることです。
振り返りをただ漠然として、
次に行う行動を決めないと意味がないので注意です。
またチーム内での振り返りもチーム力の強化になるのですべきだと思います。

チームを意識する
チームとして働くうえでは自分の仕事はもちろんですが、
チーム全体の問題点を自ら考え改善していく動きを自主的に行うことが、
チームにとっては大事だと思います。
チームに貢献しようとすると今までの自分視点ではなく、
チーム全体視点で物事を見る必要があるのですが、
これがすごく大事だと思います。
いかに自己組織化ができるようになるかが、チーム力につながるかと思います。

できるだけプロダクトの話をする
エンジニアは案件を遂行するのが仕事ですが、それだけがすべてではないと思います。
案件の裏にはKPIであったりサイトとしての目指している姿があるわけで、
それが単にシステムタスクとして落とし込まれたのが、案件です。
なので案件を行うときには最低でも一回は企画の方などと、
なぜ案件を行うのか・この案件終了後どうしていきたいのかといった、
プロダクト大枠にかかわる部分をちゃんと話すべきだと思います。
そうすることでシステムとしてのモチベーションも上がりますし、
何より話すことでより仕様書には書かれていないような企画の意図などを知ることができ、
より効率的なアプローチなどを考えることができるようになり、
案件をより良いものにすることができます。

若手のメリットを生かす
あまり意識していない人が多い気がしますが、
若手ってすごく貴重な時間だと思います。
メンターの方に質問をしたら親身になって教えてくれるし、
少しミスをしてもチームがカバーしてくれるし、
手を上げたら大体のことをやらせてもらえるし、
何よりミスをしたら、ちゃんと怒ってもらえることができます。
中途やベテランになるとそうはいかなくて一人の社会人になるので、
教えてもらう機会や注意される機会も少なくなって、
シビアに評価されてしまいます。
なので若手というメリットを最大限使って、
恐れずにできるだけ手を挙げていろいろ経験するのが本当に大事だと思います。

まとめ
エンジニアはエンジニアリングスキルも大事ですが、
エンジニアリングスキル以外にもいろいろ大事な部分があるかと思います。
(もちろん一定のスキルレベルはマストだと思いますが。)
コメントやツッコミいただけると嬉しいです。
",False,https://qiita.com//kojikaa/items/23867c6caa10821af7c5
"

概要
LaravelというかPHPを2018年9月(要するに2ヶ月前)から触り始めて練習がてらにToDoアプリを開発しました。
なんだ「ToDoアプリかよ」と思われるかもしれませんが、自分が欲しい機能を盛り込んだアプリになっています。　　
下書き&タスク管理アプリ Chekera(チェケラ)
(データをユーザーごとに管理するため最初はログインページです)




Chekeraに関して
すでにあるToDoアプリは「メモ(やること)」をメインにしていると思います。
ブログの記事を更新するときの掃き溜めのスペースとして使う予定です。
普通のTodoはログインページはない気がしますが、保存したタスクは自分しか見られないようにしたかったので、
ユーザー認証を使ってユーザーにIDを設けてそれで管理するようにしました。
これだけを伝えればサーバーサイドの方でしたらどう実装しているかが把握できるかなと。

参考にした書籍
PHPフレームワーク Laravel入門
絶対に挫折させないアプリ開発　はじめてのLaravel(ダウンロード版) 
発端は先月10/8に開催された技術書典でした。
もともと技術書典ではiOSの本を探していたのですが、目的が20分程度で達成してしまいました。
20分の滞在で引き返してもよかったのですが、「次いでだし」ということでちょっと周りのブースにも足を運びました。
さらに本のクオリティに対して値段がとんでもなく安く感じましたので試しにLaravelの本で(欲を言えばサンプルアプリがサクッと作れる)レベルの高そうな本を探しました。  
すると、プラムザさんがLaravelの本を出していました。
(ただし、プラムザさんという会社の存在は初めて知りました。)
で、ちょっと話しを聞いて本をペラペラめくってみたら、「Todoアプリ」を3回作るらしいとの事でしたので私のニーズとジャストミートしていました。
「これって私のために書いてくれたんじゃない？」って思うぐらいジャストミートでした。
まあ、3回作るっていうプロセスが謎でしたが、本の通りに3回作りました。はい。
(1回目はピュアPHPでの実装、2回目はVのみの実装、3回目はMVCを意識といった構成)
ちなみに宣伝じゃないです。
Chekeraはそれにアレンジを加えました。
もともとは9月ごろに「PHPフレームワーク Laravel入門」を元にLaravelを勉強していてちょうどフレームワークの使い方に慣れてきましたので次のステップに進みたかったのですね。

なぜ作ったのか
仕事中に思いついたヒラメキをできるだけスマートに落書きとして残したかったからです。
またQiitaやはてなで記事を書くのですが、Qiitaには下書きの制限があって10個までしか保存できません。
はてなはあくまで日記という感じなので「メモ」という感じではない。
また、過去に実行してきたタスクを見直すようにもしたかったのでタスク完了したページも欲しかった。これから個人のアプリ開発をしていくときにも思いついたタスクをバンバン残して行ければ効率的じゃね、と意味でです。

使っている技術・フレームワーク
Laravel 5.5
PHP 7.1.19
heroku　(デプロイ)
PostgreSQL (開発環境ではSQLiteを使用)
JQuery (tableの閉会)
Bootstrap4 (フロント側の装飾のため)
IDE: PHPStorm (補完がないと死んじゃう子のため)

機能

CRUD( Create / Read / Update / Delete)
ユーザー認証
Markdownのライブラリ
JQueryでtableのcellをパカパカ開く
完了したタスク表示ページ
独自ドメイン化(httpsはまだ対応できていない)

この部分まで実装しています。

普段何をやっている人なのか
本業はサーバーサイドでもなければウェブ開発でもありません。会社先でiOS・Androidアプリのモバイルアプリのエンジニアをやっています(いわゆる常駐型)。じゃあ、モバイルアプリを開発すればいいじゃん！と思うのですが、いかんせん本格的なアプリを開発するためにはクライアントサイドだけでは十分な機能を実装できない事に気づきます。

なぜRailsではなくPHPなのか
そこでWebかAPIかですが、Webというかツール系の開発ができるようになりたいためウェブサイトの開発を検討する事にしました。開発言語の選定としてRubyかPHPかで悩みましたが、本当はRubyを選定したかったのですが、
下記の理由によりPHPを選ぶ事にしました。QiitaではRubyの方が圧倒的に人気ですね。(それとPythonも。)
ただし、RubyというかRailsだと

SNSとかWeb開発しかできない-> SNSが作りたいわけではない
保守・運用のときの外注コストが高い -> PHPだとエンジニアを集めやすい
安いレンタルサーバーのほとんどで対応していない
Wordpress・ECサイトの案件に弱い
バージョン管理が面倒そう
SQLの勉強に不向き
地方の案件が少ない
アフィリエイトに不向き
学習コストがPHPよりも高い

こんな感じのデメリットがあります(ありました)。2、3年前に一度Railsを触ってウェブ開発をやりましたが、私の周りの案件や友人はPHPが多かったのでRailsができてもサポートできない歯がゆさがありました。しかし、PHPができればそういったわだかまりは少ないです。(いやマジです。)

これから追加する機能について
また、ChekeraにはFeature Release ページも設ける予定ですが、今後予定している機能として

Release Note
User Feedback Form
Request Form
SNSログイン機能

などを考えています。

難しいと思った機能について

セッション管理
クッキー管理
(heroku)サーバーデプロイ
DB接続(ピュアPHPができなかった)
HTTPS化
ドメイン関係
ライブラリの追加・実装
DBの取り扱い、切り替え(MySQL断念)
メール送信
ユーザー認証
SQLのDateカラムの対応
マイグレーション
Laravelのミドルウェア機能全般

こんなところです。結構つまづいて半日から1.5日ぐらいハマって進まなかったです。イライラしながら寝たりとか。

今後開発していくアプリについて
すでに決めていますが、今後開発していこうと思っているアプリがあります。

TwitterBotクライアントアプリ
Qiita -> TwitterBotアプリ
アンテナサイト
Wordpressのオリジナルテーマ
写真共有サイト
投票サイト

このあたりを開発していきたいですね。(自分はいったい何エンジニアなんだ笑)
モバイルアプリエンジニアの弱点はこういった外部のサービスを自在に操れない点です。
逆にウェブからモバイル移転はUIデザインの設計の難しさによってなかなかの難易度ではないでしょうか。

開発にかけた時間
今年9月1日にPHPを触り始めてからおよそ2ヶ月間ですが、勉強コストも含めChekeraにかけた時間を言いますと、
平日: 2~3時間
土日: 8時間
のペースで開発を進めました。
土日はもっというとどちらかがプライベートの用事で1日潰れていました。ですが、肌感覚でトータルでは80~100時間は費やしたでしょうか。その間にもQiitaに記事を投稿していましたので結構ハードスケジュールでした。

個人開発でメンタルを保つために行ったこと

BGMとしてYouTubeのBGMを聴きながら作業する
Chekeraにやりたいことを書きなぐりながら改修していく
git-flowで開発をやっているのでできる限りその日に「ブランチ」を切るようにする(ブランチ名は2018-11-2という感じで日付にした)
doneリストを見ながらこれまで達成したことを振り返りニヤニヤする
ファーストリリースまでに盛り込む機能を制限する
夜遅くまで作業せずにハマった場合はすぐに寝る(睡眠不足よりも翌日に作業する方が閃きやすい)

特に個人開発の場合下手すると後半ダラダラと開発することになるので「1ヶ月のみ」という制約をかけて自分にプレッシャーを掛けるようにしました。自分の中でこの制約を掛けることを「サーキット」と名付けています。サーキット感覚で開発を進めるという意味で。

PHP習得のための最短ロードマップ
プログラミング経験者でしたらPHP未経験でも下記の順番で進めば簡単なWebサイトが作れるようになることがわかりました。


ProgateのPHPコース全部　3回やる
PHPフレームワーク Laravel入門

絶対に挫折させないアプリ開発　はじめてのLaravel(ダウンロード版) 

書籍版は絶対に挫折させないアプリ開発　はじめてのLaravel(製本版) 
RailsだとRuby on Rails チュートリアルというエグい量のチュートリアルが有名なのでこれをやり切る気力があれば習得できます。
Railsが簡単とは言ってもPHPの方が学習コストは低いかなというのが所感です。
WordpressとECに対応できたら意見が変わります。)

参考になった記事
ジャバさんの
開設後３週間で収益１０万円を得た個人開発サイトでやったことの全部を公開する
はもちろんのこと
もっと気軽にアウトプットできる技術ブログサービス「Qrunch（クランチ）」をリリースした【個人開発】
【お知らせ】半日でwebサービス「Peing（質問箱）」を作ったら6日でTwitterトレンド1位（49000ツイート）になりました
といった個人開発でリリースしちゃった系の記事ですね。
これらのサービスには全然敵いませんが、Webサイトを公開できるぐらいにはスキルが身についたと思っています。
ただ、やっぱりウェブ系のデザインの操作がかなり大変でした。個人開発している方ってデザインどうしているんでしょうかね。Laravelには標準でBootstrapとかVueが搭載？されているのでイメージしていたデザインはだいたい反映できました。iOSならもっと細かいディテールまで考えちゃいそうですね。
そんな感じで気軽にChekeraを使って頂ければ私としてはとても幸せです。
また、いいねを頂けるともっと幸せになります。
",False,https://qiita.com//tamappe/items/f0d7f98367f21ca32cc2
"

はじめに
この記事は「新人エンジニアの私が圧倒的成長するためにやっていること - Qiita」のパロディーです。
「ロートル」という言葉が既に死語なので，タイトルの意味がよく分からない人も少なくないでしょう。
これは「老人」という意味ですが，ふつう否定的なニュアンスで使われます。年を取って能力が衰えたり，時代遅れになったりということを捉えて言います。他人を揶揄するときにも，自嘲的にも使います。
こんな古い言葉を使っている時点で既にロートルなわけです。
この記事は，ロートルを自任する筆者が，ソフトウエアエンジニアとして日々感じている老化に抗（あらが）うために「こうすればいいな」と思いつつやれていないことを記します。

勉強し直し
私は JavaScript がちょっとだけ書ける。私の仕事の範囲では，ちょっとしたスクリプトが書ければ十分なので，あまり困ってはいない。
しかし，私の JavaScript の知識は 10 年以上前のものだ。
この 10 年で JavaScript は大きく変わった（らしい）し，周辺技術はもうワケわからんくらい新しいものが出てきた。
最近書かれたコードが読めないんである。
then てなに？　ぷ，プロミス？　そりゃあプロだってミスはするさ。
もう一度 JavaScript を新しい気持ちで勉強し直さんといかんのだ。
いや，手をこまぬいているわけではない。
実際，入門書でゼロから勉強しかけた。おかげで let と const は習得したぞ（笑）。
しかしロートルゆえにまだ then が出てくる章まで行っていない。
禅の境地に達するにはまだ修業が足りないのだ。

好奇心を保つ
老いとともに衰えがちなのが好奇心だ。
しかし，好奇心こそ新しいことを学ぶ原動力ではないだろうか。
手持ちの技術でどうにか仕事がやれてしまっていると，新しいことを学ぶ意欲が十分にわいてこないことがある。これは老化のサイン。
私の尊敬する一世代上のあるエンジニア（故人）は，亡くなる直前まで実に旺盛な好奇心を見せていた。自分がキャリアを積み上げた技術に取って代わるような新しい技術にも意欲的に取り組み，面白くて仕方ない，といったふうだった。
私もそのようにありたいと思う。実際，私の中にも好奇心の火はまだ燃えている。
だが。
新しい技術を学ぶのは大変だ。けっこうな時間を費やさなくてはならない。
新しい概念や，ときには新しい思考法を身に付けなければならないこともある。
解説を読むだけではダメで，自分でコードを書いていろいろ試してみなければ理解が進まない。
それだけやって得た技術が，実はスジの悪いものだったり，すぐに廃れたり，目的に合わないことがあとで分かったりすることも少なくない。
正直なところ，React.js や Chef に多大な時間を費やさなくてよかったと思っている（これらが悪いというのではなく，自分の目的などなどに照らして得るものが割に合わなかっただろうという意味）。
そこで，「何を学ぶかはよく見極めて」ということになる。これ自体は必ずしも間違っていない。
ところが，そうこうするうちに，いつの間にか新しい技術への好奇心をも失っている自分に気付く。
学ぶかどうかはともかく，「えっ？　なになになにソレ？」っていう気持ちは持っていたい。

健康管理・体調管理
健康管理はすべての人に重要だが，ロートルにとっては若い者よりずっと切実だ。
高血圧になって降圧剤を飲むことになったとしよう。よくは知らんが副作用で眠気を催すことがあるらしい（薬によるのかな）。
これでは我々の仕事に大いに差し支える。
（降圧剤を飲んでいるエンジニアをダメと言っているのでは全くない。降圧剤を飲まなければならないことが，頭をシャープに保って仕事をするうえで不利になりうることを言っている）

睡眠
我々の仕事にとって，きちんと睡眠を取ることが大事なのは論をまたない。
若い頃は気合いでどうにかなった場合でも，ロートルになると無理がきかない。
夜更かしはやめよう。たとえあの OSS のあの issue が気になったとしても。

運動
我々のような座り仕事（まあ立ってコーディングしてる人もいるけど）では，生活の中に意識して運動を取り入れないと極端な運動不足になりうる。健康を保つうえで良くないのは間違いない。
が，ぜんぜんやれていない。
プログラマーの中には筋トレの好きな方がけっこういらっしゃるらしい。その効用も，一部の人が信奉するほどではないかもしれないが，実際にあるのだろうと思う。
老化に抗うために良さそうなことをやろうという気が起きないこと自体が老化を意味しているようだ。

英語力をつける
言うまでもなく，ソフトウエアの仕事で英語力は不可欠だ。
いわゆる四技能のうち，「読む」が圧倒的に重要で，ライブラリーのドキュメントや解説本・解説記事などを素早く正確に読み取る能力があるとないでは大違い。
Google 珍訳翻訳もある程度は役に立つが，肯定と否定を取り違えるなどの誤訳も多く，とても信用できるものではない。（どんな単語が出てくるかを眺めるにはよい）
OSS（オープンソースソフトウエア）に issue を立てたり開発に参加したり，自分の作ったものを公開したりするには，「書く」力も必要となる。
仕事の種類，職場の状況，関わるプロジェクト，顧客などなどによっては「聞く」「話す」も必要になるが，私の仕事では必要ない。
私は「読む」も得意でなく，とにかく時間がかかる。仕事中に必要に駆られて英文を読むことが多いが，読むことにいくらでも時間が使えるわけではないので，イライラしながら読むが，理解度も十分ではない。
いまちょっと具体的に思い出せないが，副詞なんかで，何十回も辞書を引いてきたのに，中学英語くらいの単語なのに，どうにも覚えられなくて，出てくるたびに辞書を引くものがいくつもある。時間の無駄だ。
語彙力はある程度辞書でカバーできるが，かといって辞書を引けば大丈夫，というものでもない。
文中に「get a computer to talk」なんてのがあったとして，「× 話すためにコンピューターを得る（？）」とか「× しゃべるコンピューターを得る（？）」ではどうも意味が通じないので辞書を引こうと思ったとする。
「get」を引けばよさそう，というのは分かるが，「get」の項には膨大な記述があり，その中から当てはまりそうなものを短時間で探そうとしてもムリ。（そこそこの時間があり，ある程度のコツが分かっていればどうにか探せるだろうけれども）
語彙力も問題だが，文法が分かっていないので，構文が取れない。これはもう，場数を踏めばいいとか，慣れとかでどうにかなるものではない。
書くほうで言うと，自分が使っているライブラリー（OSS）のバグを見つけて issue を立てたりする機会はたまにある。
バグを直してもらわないと本当に困るので，苦手でも仕方ない。
ありがたいことに，ある程度はコードに語らせることもできるので，うまくコードサンプルを書いて，単語を適当に並べたら意図を汲み取ってくれることも多い。
しかし，議論になるとまるでダメ。やり取りができない。
また，問題点を文章で説明しなくてはいけなくて，うまく表現ができなくて書くことを諦めてしまうこともしばしば。
そこで。
何度か英語を勉強しようとしたことはある。
しかーし，頭に入ってこない。そして，続かない。老いを感じる。
長々と書いたが，まとめると，「英語を学ぶ必要を感じているがやれていない」となる。

アウトプット
アウトプットの効用は既に多くの方が書かれているので繰り返さないが，適切なアウトプットの努力が良いものであることは私も理解している。
ブログも Twitter もやっていない私は，技術的アウトプットというと Qiita に記事を書くことになる。
Qiita に記事を書き始めて 4 年近くなるが，記事の総数はわずか 68 本。
ここ最近ペースが落ちていて，今年に入ってからは 5 本だ。しかも小ネタ化している。
書こうと思ったネタはいくつもあるのだが，書きかけで止まっているものや，書きかけてすらいないものが多い。
ヤングの中には，1 日 1 本とか，すごいペースの定期投稿を自らに課している方もいらっしゃって恐れ入る。1 週 1 本だってとてもムリ。
この記事だって自らに鞭打つ思いで，ようよう書き上げたんだぜ。
",False,https://qiita.com//scivola/items/0aff062b6d99d0c5ce66
"

まず始めに
この記事はこれからプログラミングを始めようと考えている思っている方向けの記事で私自身都内のスクールのでプログラミングを学んだ身なのでその視点から話します。

結論
これからプログラミングを始めたくて最短で技術をものにしたければスクールに通うべきです！

スクールのメリット
例えばこれから野球を始めてやるときにどうやってボール投げればいいかなんて教えてくれる人コーチみたいな人がいますよね？
どうやってボール投げるか教えてもらった方がすぐ出来るようになりますよね？
それと同じでスクールに通うメリットはその人が長い時間かけて習得したノウハウを短期間でものにできることにあると思います。

スクールのデメリット
費用がかかる
本当に知りたいことが学べるか受けてみないとわからない

スクールに通うべき人
独学で挫折した
近くに教えてくれるような人がいない
どうやって勉強したら良いかわからない
手っ取り早く覚えたい

スクール通わなくても良い人
自分で目的を持ってプログラミングに触れることができる
費用をかけたくない
一緒に学べて教えてくれるメンター的存在がいる
これら以上のことがスクールに通う通わないの基準になるのかなと思います。
簡単ではありましたが参考にしていただければと思います。
もしもう少し深掘りな話を聞きたければツイッターのlinkを貼りますのでメッセージをいただければと思います。
https://twitter.com/jade91509386
",False,https://qiita.com//joe225/items/cf93589775de4b66a672
"

Unityで実装方法が分からない時のググり方
Unityでゲームなどを作るときに、実装方法が分からないという時のググり方（調べ方）を私なりにまとめてみました。
私が主にやるやり方は2つで
""Unity + 実装したい要素""で検索をかける
スクリプトリファレンスを見る
このふたつを交互に行うことが必要だと思っていて、
実装方法に見当がつかない場合は、先にググり実装したい要素に使う機能のワードを知り、その後にスプリクトリファレンスを見てどんな機能なのかを先に学ぶべきだし、
使用するべき機能の名前が分かる場合は、スプリクトリファレンスを見て、その後にググって色んな人のサンプルコードを読み、実際の使い方を学んでいくのが良いのかなと思っています。

1.""Unity + 実装したい要素""で検索をかける
まず、実装方法が思いつかない場合、ある程度見当をつける必要があります。
そのためにはまず""Unity + 実装したい要素""でググります。
例えば""Unity 武器 切り替え""や""Unity ジャンプ""など、実装したいものでググるのが1番手っ取り早いかなと思います。

気を付けること

重要なキーワードから順に並べて検索する
サンプルコードはそのままコピーしない
サンプルコードで確認すべきは、Unityのどんな機能を使っているのか
例えば武器の切り替えなら　キー操作に""Input.GetKey""を使っているや、武器が""SetActive""を使用しているなどこの要素を作るために、どんな機能を使用しているのかワードで抽出する。
1~2ページまでしか見ない
そこまでに欲しい情報が無い場合は、検索ワードを変えるor英語で検索する方がいい


確認するべきこと

使用している言語あっているか（C#orJavaScript) 
記事が古すぎないか　


2.使用する機能の名前が分かるなら、スクリプトリファレンスを見る
スプリクトリファレンスは、Unityの機能に関する辞書のようなものでとても優秀です。
スプリクトリファレンス内を検索の部分で""使用したい機能の名前""で検索をかけると、その機能についての説明やサンプルコードが出てきます。

スプリクトリファレンスURL
https://docs.unity3d.com/ja/2017.4/ScriptReference/index.html

最後に
まだまだ私も学んでいるところなので
もっと良い方法やコツがあれば教えてください。
",False,https://qiita.com//saffron_furafoop/items/25a7f8f694d3500c547d
"

経緯
Eclipse以外のIDEではプラグインで動くHibenate-Toolsが使えない。
Gradleのプラグインにいい感じのものがなさそうなので、gradleのTask化した。

実行環境

gradle 4.10.2
MySQL 5.7
OpenJDK 11.0.1 
Hibernate-tools 5.3.6.Final


手順
gradleのdependenciesにhibernate-toolsを追加

build.gradle
testRuntime group: 'org.hibernate', name: 'hibernate-tools', version: '5.3.6.Final'


taskの追加、pathやpackageは環境に合わせて変更。

build.gradle
task hbm2java{
    def basePackage = ""com.hoge.entity.jpa.gen""
    def resourcesDir=""$projectDir/src/main/resources""
    def srcDir=""$projectDir/src/main/java""
    def preparedJdbcConfiguration = [
            propertyfile:       resourcesDir+""/hibernate.properties"",
            revengfile:         resourcesDir+""/hibernate.reveng.xml"",
            packagename:        basePackage
    ]

    doLast {
        project.ant {
            taskdef(name: ""hibernatetool"",
                    classname: ""org.hibernate.tool.ant.HibernateToolTask"",
                    classpath: configurations.testRuntime.asPath
            )
            hibernatetool(destdir: srcDir,
                    templatepath: 'templates'
            ) {
                jdbcconfiguration(preparedJdbcConfiguration)
                classpath{
                    pathelement( path: ""config"" )
                }
                hbm2java(jdk5: true,ejb3: true)
            }
        }
    }
}


hibernate.propertiesファイルの配置

hibernate.properties
hibernate.connection.driver_class = com.mysql.cj.jdbc.Driver
hibernate.connection.url = jdbc:mysql://localhost:3306/hoge_db?useSSL=false
hibernate.connection.username = user
hibernate.connection.password = passwd
hibernate.c3p0.min_size=5
hibernate.c3p0.max_size=20
hibernate.c3p0.timeout=1800
hibernate.c3p0.max_statements=50
hibernate.dialect = org.hibernate.dialect.MySQLDialect


Hibernate Reverse Engineering ファイルの配置

hibernate.reveng.xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<!DOCTYPE hibernate-reverse-engineering
    SYSTEM ""http://hibernate.org/dtd/hibernate-reverse-engineering-3.0.dtd"">

<hibernate-reverse-engineering>
    <schema-selection match-catalog=""hoge_db"" match-schema="".*"" match-table=""foo"" />
    <schema-selection match-catalog=""hoge_db"" match-schema="".*"" match-table=""bar"" />
</hibernate-reverse-engineering>


あとはコンソールでTaskの実行
$ gradle hbm2java

Spring Data JPAにはエンティティージェネレータがない。
皆どうしてるのか疑問だったが、gradleでantを使えばいいだけだったとか、そういうの。

参考資料

https://docs.jboss.org/tools/latest/en/hibernatetools/html/ant.html
https://github.com/institut-de-genomique/hibernatetools-gradle-plugin

",False,https://qiita.com//kineko/items/195a3482af3811bea67e
"

概要
日本の米国株投資界隈で有名な B氏の投資戦略の最適性に関して考察する。B氏の投資戦略は以下:

最初の 1回


資産を10等分して、10銘柄を購入する


月に 1回 1銘柄を購入


購入銘柄は上記 10銘柄のうち、ポートフォリオで時価総額が小さい銘柄
購入代金は 給与 + 配当金



10銘柄が金額ベースでバランスするように、月1でリバランスを行うというものだ。売却も行わないし、購入を手控えることもしない。本稿では、上記投資戦略の有用性と問題点を、数理的な直観で考察するとともに JavaScript を使ったシミュレーションでそれを裏付ける
免責と備考:

私は金融工学を全く知らない素人
B氏についてもっと知りたければブログ村の米国株カテゴリに行く


彼は本も出版している
ブログを見に行ってアフィリエイト報酬に貢献するのもよい
かなり目立つので考察の対象としただけでB氏に特別な感情はない




#00. 考察の範囲
「10銘柄集中投資 + 毎月ありったけ投資」という枠組みは維持したままで、リバランス戦略が妥当であるのかを考察する。例えば以下のトピックは考察の範囲外:

選択された10銘柄の妥当性
購入の手控え、株式売却など戦略全体の構造の抜本的な部分の是非


#01. 戦略の支持要因
B氏の戦略(以降 B戦略)が特定の条件下では最適な戦略であることを示す。
次のようなケースを考えよう:

全銘柄の値動きのモメンタムがなく、ゆらぎのみが存在

この場合、最適な戦略はゆらぎのせいで一番安くなっている銘柄を買うことだ。もし前月のポートフォリオにおいて時価総額ベースでバランスしていたとすれば、今月のポートフォリオにおいて一番時価総額が低くなる銘柄がまさにその銘柄であるので、B戦略は最適となる。実際は前月のポートフォリオはバランスしていないために、B戦略では最適な銘柄を選択できない場合もあろうが、多くの場合最適な銘柄を選択できる

最適戦略を外す例


前月 A銘柄 9% B銘柄 11%の時価総額とする(ややバランスがずれている)
今月 A銘柄の価格 0.8, B銘柄の価格 0.7 とする
B銘柄を購入するのが最適戦略だが
今月の時価総額は A銘柄 7.2% B銘柄 7.7% となるのでA銘柄を選択する



上記の考察、つまりB戦略の最適性は、全銘柄が同一の正のパフォーマンスを持つ場合（全銘柄が一様な割合で値上がりする）や、負のパフォーマンスを持つ場合（全銘柄が一様な割合で値下がりする）にも当てはまる。
調べる価値があるのは、B戦略は 最適戦略をはずすことがたまにあるがその影響はどのようなものか？ということだ。数理的な直観を働かすと、ゆらぎがアホみたいに大きくなければ、市場のモメンタムによらず最適戦略と大して変わらないリターンになるのではないかと考えられる。これについては後でシミュレーションをしよう。

#02. 戦略の不支持要因
前章で考察したように、B戦略はゆらぎが比較的小さい場合には最適戦略を大きく外さないだろうという感触がえられたが、それを覆すような考察をしよう。
前章では銘柄ごとのパフォーマンスが一様であることを仮定したが、実際にはこれは当てはまらない。よくありうる例としては、採用銘柄にクソ株と当たり株が混じるケースがあげられよう。数理的な直観を働かせると、B戦略ではクソ銘柄ばっかり買ってしまうと思われる。
例えば、クソ銘柄が毎月 10%減価して、その他の銘柄の値動きがないとしよう。100の総資産は翌月には99になる。毎月の投入額を1とすると、なんと毎月クソ銘柄を買い付けることになってしまう。この場合、30カ月後にも総資産額は100のままだ。最適戦略はクソ銘柄以外の株を買うことなので30カ月後の総資産はざっと120くらいになるはずだ（クソ銘柄の価値がほぼ0になるので赤字が10、新規投入分が30なので合わせて +20 になるという概算）。毎月ダーツで買う銘柄を決める場合は総資産はざっと117くらいになるはずだ（ダーツで3回クソ銘柄を買ってしまうので赤字額は13で、新規投入分30と合わせて +17となるという概算）。
上記のように、B戦略には相対的に低パフォーマンスな銘柄を買うバイアスがあると言わざるをえない（これについては後にシミュレーションする）。
想定される反論は「10銘柄にはクソ銘柄は含まれていない」「クソ銘柄に見えても長期的にはパフォーマンスはプラスに収束する」というものがあろうが、上記の指摘は「どんな銘柄を厳選したとしても銘柄間のパフォーマンスに差があるであろうことは避けられず、まさにその事実がB戦略の最適性を損ねることになっている」ことを主張するので根が深い。

#03. 配当複利に関する考察
少し寄り道をしよう。B氏の銘柄選定基準は高配当で増配率の高い銘柄である。これらの銘柄はキャッシュフローの多くを配当に分配する。B氏の主張はおそらく以下のようなものであろう:

値下がり時に買えば（買える株数が多いので）配当をたくさんもらえる


配当を再投資することで複利で効いてくる


株価が値下がりしても、企業は減配の決定をしない


むしろ減配しない銘柄を選ぶ



この主張は、ある程度納得できるものがある。一方、以下のように逆の主張も可能であり、どちらの方が説得的であるかは悩ましい（もし神の手が存在するならば、両者の主張がバランスするのだろう。そうでなければ裁定できてしまうので）:

配当を出すことは望ましくない（例：バークシャー）


配当＝利益確定＝税金がとられる
その分を企業が投資に回せば、税金分を繰り延べできる
配当がでない分、株価は上がる



銘柄選択に関しては本稿の範囲外であるので、上記の議論は完全に蛇足だ。シミュレーションでは、1株あたりの配当と増配率が一様であることを前提とする

#04. シミュレーション設計
以上で議題がそろった。実際にシミュレーションをしてみよう。
戦略のバリエーション
B戦略の最適性を議論したいのだから、比較対象となる別の戦略が必要となる。以下のように複数の比較対象となる戦略を用意しよう

S戦略: 買い付け銘柄は完全に事前に順番通りにきめる
R戦略: 買い付け銘柄は完全にランダムにきめる
最適戦略: 神の目線に立ち、最適なリバランスを実施する

上記以外にも、例えば、基準点から株価を測った指標で銘柄を決める、なども考えられるがB氏のパッシブ性を尊重した戦略を採用した。
初期条件
現実的な初期条件で固定したい。以下で固定する

初期の総資産は 100
月投入額（配当除く）は 1
投資期間は 120カ月
銘柄の配当は年率 3%, 増配率は年率 10%
配当は 3カ月ごと
初期の各銘柄の株価を 1 とする
総投下資本220 に対する利益率（百分率）でリターンを測る


120か月後に時価総額が 250 なら 30 / 220 * 100 = 13.6 がリターン



市場を支配する要因
考察したように以下の要因で銘柄の株価が決定するとする

ゆらぎ


ゆらぎの大きさを制御できるようにパラメタ化する
fvalue と呼び、理論価格からの乖離具合を示す
例: fvalue = 0.1 の場合、株価は理論価格の 90%から110% に収まる


パフォーマンス


ベータ：市場全体の年率リターン
アルファ：銘柄ごとに設定される、ベータからのずれ




#05. シミュレーションA
銘柄間のパフォーマンスに差がない、という仮定をおいたシミュレーションを行う。ゆらぎの大きさを「大・中・小」のバリエーション、ベータが「マイナス・ゼロ・プラス」のバリエーションの合わせて9パターンをシミュレーションした結果を以下に示す。
simulationA                                        opt    B      R      S
--------------------------------------------------------------------------------
fval:0,    beta: 0                                  46.34  46.34  46.34  46.34
fval:0.05, beta: 0                                  53.59  49.93  46.46  46.44
fval:0,10, beta: 0                                  61.35  55.07  46.47  46.53
fval:0,    beta: 0.05                               93.34  93.34  93.34  93.34
fval:0.05, beta: 0.05                              101.92  97.82  93.62  93.65
fval:0.10, beta: 0.05                              110.82 103.61  93.58  93.54
fval:0,    beta: -0.05                              16.28  16.28  16.28  16.28
fval:0.05, beta: -0.05                              22.83  19.44  16.31  16.27
fval:0.10, beta: -0.05                              29.72  23.89  16.51  16.39

予想通り、B戦略は最適戦略とまではいかないものの、ほかの戦略と比較しても優位であることがわかる。ゆらぎが大きくなるほどこの傾向は顕著になる。これは #01 で概算した結果と整合的だ。
一つ、面白い現象が見えていることを指摘しよう。初期配置では、配当は3%である。そしてベータが -5% である。直観的に考えると、投資を続けるほど損失が増え、赤字になると考えがちだがそうではない（ベータがマイナスのケースを見よ）。なぜなら株価が下落するに従い、配当がどんどん上昇していき、どこかのタイミングで配当額が時価総額の損失を打ち消してくれるようになるからだ。
具体的な数字を上げよう。5年後には株価は 0.95^5 ≒ 0.77 になる。したがって同じ資金で 1/0.77 ≒ 1.3 倍の株を購入できる。それに加え 増配率を考えると、1.3 * 0.03 * (1 + 0.1)^5 ≒ 7.7 % の配当利回りとなる。これはベータを上回る。減配しない銘柄・増配率の高い銘柄の強力さを実感できる

#06. シミュレーションB
ゆらぎがない、という仮定をおいたシミュレーションを行う。銘柄間のパフォーマンスの差が「大・中・小」のバリエーション、ベータが「マイナス・ゼロ・プラス」のバリエーションの合わせて9パターンをシミュレーションした結果を以下に示す。
simulationB                                        opt    B      R      S
--------------------------------------------------------------------------------
beta:     0, alpha: 0.1 x1 ,-0,1 x1                 95.58  48.61  53.58  53.54
beta:     0, alpha: 0.1 x2 ,-0,1 x2                 99.78  50.67  60.57  60.76
beta:     0, alpha: 0.1 x3 ,-0,1 x3                103.98  53.13  68.40  68.00
beta:  0.05, alpha: 0.1 x1 ,-0,1 x1                161.39  95.47 102.67 102.86
beta:  0.05, alpha: 0.1 x2 ,-0,1 x2                167.55  95.79 112.64 112.40
beta:  0.05, alpha: 0.1 x3 ,-0,1 x3                173.71  96.96 122.38 121.94
beta: -0.05, alpha: 0.1 x1 ,-0,1 x1                 49.74  20.04  22.30  22.09
beta: -0.05, alpha: 0.1 x2 ,-0,1 x2                 52.56  23.40  27.68  27.96
beta: -0.05, alpha: 0.1 x3 ,-0,1 x3                 55.37  27.03  34.16  33.86

予想通り、ベータの数値によらず、B戦略はどの戦略と比較しても低いパフォーマンスしか上げられていない（なお、上昇相場＝ベータがプラスの場合のほうがその差が顕著になっている）。これは #02 で概算したように、B戦略ではパフォーマンスが劣る銘柄を買い付けやすくなってしまうためだ。たった 1,2 銘柄におけるパフォーマンスの差分で、B戦略の欠点が観測できるほど大きくなる。

#07. シミュレーションC
これまでの結果では「ゆらぎがある場合はB戦略は結構イケてる」、「アルファがある場合はB戦略はイケてない」ことがわかった。じゃあ両方混ぜたらどうだろう？現実的と思われるいくつかの設定でシミュレーションをした結果は以下の通りとなる。
simulationC                                        opt    B      R      S
--------------------------------------------------------------------------------
fval: 0.01 beta:  0.05, alpha: 0.1 x1 ,-0,1 x1     161.47  95.49 103.27 102.83
fval: 0.05 beta:  0.05, alpha: 0.1 x1 ,-0,1 x1     162.11  99.85 102.96 103.35
fval: 0.10 beta:  0.05, alpha: 0.1 x1 ,-0,1 x1     164.79 105.93 103.42 103.47
fval: 0.01 beta: -0.05, alpha: 0.1 x1 ,-0,1 x1      49.79  20.03  22.25  22.11
fval: 0.05 beta: -0.05, alpha: 0.1 x1 ,-0,1 x1      50.54  22.88  21.91  22.08
fval: 0.10 beta: -0.05, alpha: 0.1 x1 ,-0,1 x1      53.04  28.43  23.10  22.84
fval: 0.01 beta: -0.05, alpha: 0.1 x2 ,-0,1 x2     168.49  96.74 112.35 112.38
fval: 0.05 beta: -0.05, alpha: 0.1 x2 ,-0,1 x2     172.62 101.85 112.79 112.96
fval: 0.10 beta: -0.05, alpha: 0.1 x2 ,-0,1 x2     178.80 108.18 113.02 113.26

パラメタ次第だとは思うのだけれど、色々いじくった結果としては、「B戦略は最適な戦略ではない」と結論付けられるように思われる。

#08. 一旦まとめ
B氏のリバランス戦略の二つの側面を明らかにし、シミュレーションで実証した

側面1: ゆらぎ効果によるタイミング投資


時価総額ベースで最小の銘柄を選択するということは、すなわち「この銘柄はたまたま下がっているだけで今買えばあとで報われる」と信じるようなものだ。もしそのように価格形成がされた場合、非常にうまくいく。


側面2: 選定銘柄間のパフォーマンス差を粗視化


選定銘柄間に有意なパフォーマンス差が存在する場合、長期的・慢性的に時価総額でのバランスを崩す要因となる。それを是正しようとするリバランスは、劣後したパフォーマンスの銘柄を嗜好することと同義だ。そのような銘柄は相対的に配当が高くなるという効果があるものの、資産の最大化の観点では正当化できない。にもかかわらずこのリバランス戦略をとるということは、選定銘柄間のパフォーマンス差を意図的に無視していると言わざるを得ない。



上述の二つの側面のうち（私の興味があるパラメータ領域では少なくとも）後者が支配的であるため、B氏のリバランス戦略を採用する積極的な意義を見出すことができなかった。むしろ毎月ダーツやサイコロなどで投資先をランダムに決めるようなリバランス戦略の方が総じてパフォーマンスは高かった。

#09. 私の主観を交えたまとめ
私の主観を交えてB氏の投資戦略全体に対する意見・立場を明確にしたいと思う。
私は、基本的にはB氏の投資スタンスに共鳴をする立場だ（でなければこんな長々と記事を書かない）。それは主に以下のような基礎概念を共有していると思われるからだ

中長期的には株式投資が一番報われる資産運用だ


バフェット・ピケティ・マルキールや過去のS&Pのチャートで実感できる


タイミング投資・アクティブ投資をせずに機械的に投資するべきだ


底も天井も素人には予測できない


市場平均程度のリターンでも十分早く資産形成できる
増配銘柄は相場下落時にこそ儲けのチャンス


悲観・絶望が支配する時に投資スタンスを貫きやすい



だからこそ、B氏の投資戦略のリバランス戦略に違和感を持った。つまり、B戦略には機械的な投資という性質よりも、「タイミング投資的な色彩」や「銘柄選択に対するある種の自身＝アクティブな側面」があったからだ。これらに関しては本文を読んだ方には同意を得られるだろう。これなら、S戦略やR戦略のように、機械的に追加投資先を決めたほうが一貫性があるのではないだろうか？本稿はそれをある程度定量的に示すことができたと考える。

#10. ソースコード
バグがないといいなあ。Node v11 で動作確認。
const N = 10; // number of stocks
const T = 120; // number of months to invest
const DIV = 0.03; // dividend
const GDIV = 0.1; // growth ratio of dividend.

// utility
const range = (start, end) => [...Array(end - start).keys()].map(x => x + start);
const sum = arr => arr.reduce((a, x) => a + x, 0);

// convert yearly return to monthly/quarterly return
const y2m = r => Math.E ** (Math.log(1 + r) / 12) - 1;
const y2q = r => Math.E ** (Math.log(1 + r) / 4) - 1;

// stock prices
function priceTableOf (fvalue, rate) {
  const prices = [1.0];
  range(1, T).forEach(t => prices.push(prices[t - 1] * (1 + y2m(rate))));
  return prices.map(p => p * (1 + (2 * Math.random() - 1) * fvalue));
}

// policies
function policyB (shares, prices) {
  return shares.reduce((a, s, n) => shares[a] * prices[a] < shares[n] * prices[n] ? a : n, 0);
}
function policyR (shares, prices) {
  return Math.floor(Math.random() * shares.length);
}
const policyS = (function () {
  let idx = -1;
  return function (shares, prices) {
    idx = (idx + 1) % shares.length;
    return idx;
  };
})();

// simulator: returns average of totalReturn for each policy
function simulation (nSample, fvalue, beta, alphaArr, policies) {
  const result = Array(policies.length + 1).fill(0); // +1 is for optimal policy
  for (let i = 0; i < nSample; i++) {
    doSimulation(fvalue, beta, alphaArr, policies).forEach((x, i) => {
      result[i] += x;
    });
  }
  return result.map(x => x / nSample);
}

function doSimulation (fvalue, beta, alphaArr, policies) {
  const prices = range(0, N).map(n => priceTableOf(fvalue, beta + alphaArr[n]));
  // create optimal policy
  const optimalPolicy = (_, __, t) => {
    const values = prices.map((_, n) => { // value = price + yield
      let value = prices[n][T - 1] / prices[n][t]; // now: 1 -> end: 1.X
      // XXX: dividened
      return value;
    });
    return values.reduce((a, x, i) => values[a] < values[i] ? i : a, 0);
  };

  const result = [optimalPolicy, ...policies].map(policy => {
    const shares = Array(N).fill(100 / N);
    range(1, T).forEach(t => {
      let newMoney = 1.0; // salary
      if (t % 3 === 2) { // dividend
        const ratio = (1 + y2q(GDIV)) ** ((t - 2) / 3);
        newMoney += sum(shares) * y2q(DIV) * ratio;
      }
      const targetIdx = policy(shares, range(0, N).map(n => prices[n][t]), t);
      shares[targetIdx] += newMoney / prices[targetIdx][t];
    });
    const total = shares.reduce((a, s, n) => a + s * prices[n][T - 1], 0);
    const totalReturn = (total - 100 - T) / (100 + T) * 100;
    return totalReturn;
  });
  return result;
}

const nSample = 100;
const policies = [policyB, policyR, policyS];

function printHeader (header) {
  const disp = ['opt', 'B', 'R', 'S'].map(s => s.padEnd(7, ' ')).join('');
  console.log(header.padEnd(50, ' '), disp);
  console.log('-'.repeat(80));
}

// simulationA
const simA = ([fval, beta]) => simulation(nSample, fval, beta, Array(N).fill(0), policies);
const resultA = [
  [0.0, 0], [0.05, 0], [0.10, 0],
  [0.0, 0.05], [0.05, 0.05], [0.10, 0.05],
  [0.0, -0.05], [0.05, -0.05], [0.10, -0.05]
].map(simA);
const labelsA = [
  'fval:0,    beta: 0',
  'fval:0.05, beta: 0',
  'fval:0,10, beta: 0',
  'fval:0,    beta: 0.05',
  'fval:0.05, beta: 0.05',
  'fval:0.10, beta: 0.05',
  'fval:0,    beta: -0.05',
  'fval:0.05, beta: -0.05',
  'fval:0.10, beta: -0.05'
];
printHeader('simulationA');
resultA.forEach((r, i) => console.log(labelsA[i].padEnd(50, ' ') + r.map(x => x.toFixed(2).padStart(7, ' ')).join('')));
return;

// simulationB
const simB = ([beta, alphaArr]) => simulation(nSample, 0, beta, alphaArr, policies);
const aArr = Array(N).fill(0);
aArr[0] = 0.1;
aArr[aArr.length - 1] = -0.1;
const aArr2 = [...aArr];
aArr2[1] = 0.1;
aArr2[aArr2.length - 2] = -0.1;
const aArr3 = [...aArr2];
aArr3[2] = 0.1;
aArr3[aArr3.length - 3] = -0.1;
const resultB = [
  [0.0, aArr], [0.0, aArr2], [0.0, aArr3],
  [0.05, aArr], [0.05, aArr2], [0.05, aArr3],
  [-0.05, aArr], [-0.05, aArr2], [-0.05, aArr3]
].map(simB);
const labelsB = [
  'beta:     0, alpha: 0.1 x1 ,-0,1 x1',
  'beta:     0, alpha: 0.1 x2 ,-0,1 x2',
  'beta:     0, alpha: 0.1 x3 ,-0,1 x3',
  'beta:  0.05, alpha: 0.1 x1 ,-0,1 x1',
  'beta:  0.05, alpha: 0.1 x2 ,-0,1 x2',
  'beta:  0.05, alpha: 0.1 x3 ,-0,1 x3',
  'beta: -0.05, alpha: 0.1 x1 ,-0,1 x1',
  'beta: -0.05, alpha: 0.1 x2 ,-0,1 x2',
  'beta: -0.05, alpha: 0.1 x3 ,-0,1 x3'
];
console.log();
printHeader('simulationB');
resultB.forEach((r, i) => console.log(labelsB[i].padEnd(50, ' ') + r.map(x => x.toFixed(2).padStart(7, ' ')).join('')));

// simulationC
const simC = ([fval, beta, alphaArr]) => simulation(nSample, fval, beta, alphaArr, policies);
const resultC = [
  [0.01, 0.05, aArr], [0.05, 0.05, aArr], [0.10, 0.05, aArr],
  [0.01, -0.05, aArr], [0.05, -0.05, aArr], [0.10, -0.05, aArr],
  [0.01, 0.05, aArr2], [0.05, 0.05, aArr2], [0.10, 0.05, aArr2]
].map(simC);

const labelsC = [
  'fval: 0.01 beta:  0.05, alpha: 0.1 x1 ,-0,1 x1',
  'fval: 0.05 beta:  0.05, alpha: 0.1 x1 ,-0,1 x1',
  'fval: 0.10 beta:  0.05, alpha: 0.1 x1 ,-0,1 x1',
  'fval: 0.01 beta: -0.05, alpha: 0.1 x1 ,-0,1 x1',
  'fval: 0.05 beta: -0.05, alpha: 0.1 x1 ,-0,1 x1',
  'fval: 0.10 beta: -0.05, alpha: 0.1 x1 ,-0,1 x1',
  'fval: 0.01 beta: -0.05, alpha: 0.1 x2 ,-0,1 x2',
  'fval: 0.05 beta: -0.05, alpha: 0.1 x2 ,-0,1 x2',
  'fval: 0.10 beta: -0.05, alpha: 0.1 x2 ,-0,1 x2'
];

console.log();
printHeader('simulationC');
resultC.forEach((r, i) => console.log(labelsC[i].padEnd(50, ' ') + r.map(x => x.toFixed(2).padStart(7, ' ')).join('')));

",False,https://qiita.com//41semicolon/items/ff51616242dc49ceb903
"前回はこちら…

抽象化について
今回は、抽象化についてを視野にお話を進めたいと思います。

練習
抽象化を実際に練習していきましょう。
私の練習方法ではありますが、次の事を意識して文脈が成り立つかどうかを考えます。
主語を削除しても、その文脈は通じるか？
理由についてはそれほど難しくありませんが、ラムダ抽象を例に根拠を幾つか上げることは出来ると考えます。

最大の抽象化とは？
あらゆる主語がとり得る継承でありながら、最大のポリモーフィズムを生み出す最初にして最大の抽象クラス（interface）宣言は何でしょうか？
頭の中で想像してみて頂くと理解できるかと思いますが、
答えは宇宙（現実）です
interface Universe {}


宇宙の真理
ここまで言うと宗教めいても来ますが、より物理学的で数学的なアプローチで説明することが出来ます。
トーラス構造といいます。
このトーラス構造は、チューリング完全上の計算機構が多重に入れ子になっているという説明もできます。
りんごを例に取ると、りんごの花が咲いて実がなる動きの過程で中に芯が形成され、その芯の中に種が形成されていきます。
出入り口が上下にある渦の中央に別の空間があってその中心の渦の…と続きます。

計算機構について
トーラス構造は、空間に量子という状態を保持できるオブジェクトとして説明が可能です。では空間に一つの量子だけをとり得ることも出来るのではないでしょうか？
目には見えないほど小さい量子の一つだけです。

この考え方を機械上で考えてみる。
コンピュータ上で、１ビットの概念を破壊することは容易ではありません。
そのままでは小さすぎて有用性が見えないため、現実世界で破壊し辛い概念を用います。
例えば、数です。コンピュータ上では、プリミティブな値として扱われるものですが、具体的な値は像と言われる通りで扱いづらいものなのです。

値を一つだけ取りうる計算機構
例えば、値一つを取る仮想機械（class）を想像してください。
それに対して、どんなメソッドを追加していけばより便利になるでしょうか？
そういう考えに基づいて作られたclassには、大きな処理を一つのメソッドに実装する必要はなくなります。さらに細分化した時、それに名前を与える価値は何でしょうか？
我々は、自明的な数学の記号に、わざわざ命名を与えたメソッドにするようなことをしないことが多いはずです。以下は、javascriptでの例です。
// わざわざ命名する
function add (value1, value2) {
  return value + value2;
}

add(1, 2) /// 3

// 命名しないまま、関数の実行を考える
(function (value1, value2) {
  return value1 + value2;
})(1, 2)

// 上記の関数を、引数の一つをオブジェクトとした
// メソッドとして実行できないか考える
[1].map(function (value1, value2) {
  return value1 + value2;
}.bind(null, 2))[0];

// 上記をもっと数学的な射を表す記述に変えてみる
/*
[1].map((x, y = 2) => x + y)[0]
*/
[1].map(x => y => x + y).map(y => y(2));

つまり、ある関数を適用するメソッド、mapを使うことで、無名のメソッドが実装できている訳です。
あえて引数２つを取ったうちの一つに値を設定しているのは、記述による表記揺れを防止するためです。
この例は、分かりやすく極端にしているため、実際にはもっと短縮した表現を使います。

計算機構の無名関数は、どうやって実行されているのか？
論より証拠ですが、こんなものを書くと分かりやすいかも知れません
class List extends Array {
  constructor (...a) {
    Object.assign(this, a);
  }
  map (func) {
    let result = [];
    for (let i = 0; i < this.length; i++) {
      result[i] = func(this[i]);
    }
    return new this.constructor(...result);
  }
}

map するためのメソッドを、自力で実装すると、引数にとった関数の第１引数に自身の持っている値を入れて計算しているというのが、明白だと思います。

一番大きいものを引数に取る
よくありますが、こんなコードを思い浮かべてみてください。
(function (state) {
  return state;
})(universe);

(state => state)(universe);

宇宙そのものを関数に入れて実行するという事は、出来るということになります。
そうするとこれは、最初の方でインターフェースで扱うしか無かったオブジェクト指向とは違い、宇宙そのものを扱う抽象関数が記述可能になるということを言っていることと同じになります。

メリット・デメリット
オブジェクト指向は、フォールダウン型の大規模開発に向いていますが、その実細かいメソッドの実装の連続が変更に耐えうるために必要な書き方ということです。
しかし、メソッドに一々命名が伴います。
分かりやすいのですが書く方にとってはあまり優しくないというのも事実です。
関数型は、小規模から開発して規模に合わせたスケーリングを可能にします。
しかし、あまりにも小さいサイズの抽象を扱いすぎることで、静的型付け言語では、少々の苦しい言い訳を迫られることもあります。
それが、なれない人にとっては読みづらさにもなります。
扱う抽象が増えれば、計算機構を増やさなくてはなりません。

宇宙を引数に取る関数は最初と最後だけ
関数型プログラミングでは、副作用（値の変更）は最初と最後だけに発生し、関数の実行結果を別の関数の引数に代入していくという手法を取ります。
無名であれば、メモリ上に残る懸念もありません。
生じて消える生き方に近い潔さが、関数型プログラミングの魅力かもしれません。

最後に
Haskellの進展には目を見張る物がありますが、そろそろRustやGoを勉強しようと悩む私です。
もう少ししたらC2出ますが、Rustを使ったOSのためのフレームワークとか作るのも面白そうだと思い始めました。
そう言いながらDAWの自作が先になる気も？　しています。
ここまで読んでいただき、ありがとうございました。
宣伝にはなりますが、値を一つだけ取る計算機構を扱いやすくしてみたライブラリが以下です。
よろしければ、使用感や要望などもお願いいたします。
m(_ _)m
losand
dsand
",False,https://qiita.com//johnny-shaman/items/e3a36398d594c1b47448
"※この記事は""中学生""以上の方が理解できるレベルで書いています。
※ここでいう""記事""とは""Qiitaに投稿する記事""を指します。

記事を書けばレベルアップ☝☝☝

初心者がQiitaに投稿するメリット
ここから、Qiitaに記事を投稿する利点についてお話します。

1.強い人からの添削文が返ってくる。
これは強力です。
例えば、とあるプログラミング言語を記事の中で書いたとします。

とあるソースコード.kakuchoshi
#取込 出力
開始
{
 出力 < ""Hello, World!"";
}


ここで、強い人から返事がきます。

""出力""は古い関数です。
新しい関数""出力2""を使うと良いでしょう。

強い人からのコメントによって、
より良い書き方があることがわかりました。
このように強い人に助けられることがあります。

2.曖昧な知識が明確になる
記事は曖昧な知識では書けません。
つまり、記事を書くには曖昧な知識を無くす必要があります。
執筆途中に何度もその情報が合っているか調べます。
それにより、今まで曖昧だった知識が明確になります。

3.歴史に刻まれる
あなたが書いた記事はこのサイトが無くならない限り、
基本的に残るはずです。
1年後、昔の自分が書いた記事を見てください。
なんだこのぐちゃぐちゃしたコードは！？
って思うかもしれません。
記事を書くことによって、自分がどれだけ
成長したのかわかりやすくなります。

ゴミ記事問題について
個人の意見ですが、""ゴミ記事""という名称をあまり好みません。
ここでは""ゴミ記事""のことを""薄い記事(うすい記事)""と呼ぶことにします。
最近、Qiitaでは薄い記事戦争が勃発しているとか。
私は""プログラミング初心者が書いた記事 = 薄い記事""とは思っていません。
""プログラミング初心者が書いた記事""は可能性に満ち溢れています。
では、薄い記事を排出しているのはどこなのか？……

トレンドに薄い記事は載りやすい
最近、トレンドに載っている記事を見ると中身の無い記事が増えているように感じます。
特に多いのが、
Organizationランキングで上位に君臨している企業の記事です。
では、なぜOrganizationランキング上位企業の記事に中身のない記事が多いのか……。
それは簡単です。

同じ企業のメンバー同士でいいね！しあっているから。

つまり、中身のない記事でも(身内の)いいねが沢山つくことによって
""人気の記事""だとみなされてトレンドに載ってしまうのです。(よくないね)
つまり、
手順はこうです。
1.沢山の記事を書いてQiitaに記事を出す
2.出した記事に同じ企業メンバーが""いいね""をつける
3.""いいね""が沢山ついているので""人気記事""判定され、トレンドに載る
4.以上の事柄によって企業のランキングが上がる
5.ランキングを維持するために更に沢山の記事を出す
当然、ネタを用意していないで沢山の記事を出すと1つ1つの記事の内容が""非常に""薄くなります。

当然、記事がゴミ化します。
※もちろん、全てのOrganizationランキング上位企業が行っているわけではありません。
あちゃ～～これじゃあ、薄い記事が増えるばかり！

同じOrganization同士で""いいね""禁止したほうがよさそう
※もちろん、Qiitaに意見書を出しました。
この行為を禁止にしたら、トレンドにある薄い記事が減りそうです。
個人が出した優秀な記事が、
企業が出した薄い記事が原因でトレンド入りできなかったら
悲しいです。

フィナーレ
初心者でも上級者でもゾンビでも、
みんなでQiitaに記事を書いていこう！

この記事いいね！
以下の記事は私が言いたいことのほぼ全てを言ってもらっているので、
良ければご確認ください。(ハッピーセットです)
エンジニアは全員技術記事を書くことを習慣化した方がいいぞ
とても良い記事です。
わかりやすい。
拝啓 本当は Qiita を書きたいのに、まだ迷っているあなたへ。
とても良い記事です。
それにしても、YumaInauraさんって記事のタイトルの付け方やリンクの付け方が上手いなと思います。
「ゴミ記事は書くな」はゴミ記事を増やすことにつながるから避けるべきでは？
とても良い記事です。(お前！それしか言ってないじゃないか！)
あなたの時間を無限に奪うQiita記事集できました
とても良い記事です。

きっとここのコメント欄にも強い人がいつか来るはずです……(来なかったら淋しい)
追記: 来ました。
",False,https://qiita.com//Gaccho/items/21a5f1aed58da4b9b5b0
"

はじめに
こんにちは。なおとです。
10/27に初めてLTをしました。今まで興味はあったのですが、何か怖いし、どのように準備していいのかが分からず避けていました。
今回はLTをしてみたいけど、どのように作成して、何を話せばいいのか分からない人向けに自分がやったことを書いていこうと思います。

LTとは？
LTとはLightning Talks（ライトニングトーク・稲妻トーク）の略です。稲妻が落ちるような短い時間で話をすると解釈しています。
LTはいわば「短いプレゼンテーション」で、勉強会やイベント、カンファレンスなどで行われています。
基本的にルールがないので気軽にやってみようのコンセプトだと思います。
IT業界でよく聞く「LT」ってなんなの？3つのメリットを知って、きみもLTしてみよう！

申込み〜準備

申込み
やはり初めてはどのLTで発表していいか分からないです。特に初めてだと何だか怖いという思いが先に来て躊躇してしまいますが、知っている勉強会や雰囲気が分かる所を最初に選びました。

準備

LTで話す内容
いざ申し込んで見たものの、何を話していいかは分かりません。
とりあえず、自分が話せそうなものや話してみたいものをざっとあげていきました。

今までの転職活動を振り返ってみた
勉強会には参加したけど、自分の勉強していなかった
僕のコーディングのスタイル
VueとTypescriptを勉強してみた
ハードルが高いと思っていた公式ドキュメントが段々読めてきた
カタカナ言葉が全然わからん
Podcastを聞き始めました

何も考えずに数を上げてきました。その中で直近で経験したことや話しやすいこと、共有したいなと思ったものを選択しました。
今回は「勉強会には参加したけど、自分の勉強していなかった」を選んでみました。

ざっくりな構成
内容は決まったのですが、どういう構成がいいのかが分かりません。
いきなりスライドには手を出せないので、ipadのGoodNotesを使って手書きで何となくのイメージを書き出しました。
注意した点

ざっくりした話の流れは意識して書く
細かい文言は意識せず書く
10枚以下にする

結果作成したのはこんな感じになりました。


keynoteで書き出す
ざっくりな構成ができたのでスライドを作成していきます。
色々なスライドツールがあるみたいでしたが、いい感じになると言われているテンプレートを見つけたのでこちらを使うことにしました。
更に大体いい感じになるkeynoteテンプレート「Azusa Colors」作った
注意した点

まずはざっくり構成をそのまま書き出す
構成が気になった所は修正する
見た目はとりあえず整えない
話す内容もここでは気にしない


ざっくりした話す内容を書く
作成したスライドをもとに話す内容を書いていきました。
注意した点

文体を気にせずにひたすら書き上げる
とにかく手を止めない!!「えぇー」「あー」などを入れて止めないようにしました。


修正作業
ここまできたらあとは修正していくのみです。修正したい点は沢山ありましたが、自分の考えはアウトプットできているのでそこまで大変ではありませんでした。
この段階で発表スライドを意識して、フォーマットを整えます
注意した点

口頭の説明がメインなので、スライドの内容は簡潔に（話を聞いてもらうため）
1スライドに1メッセージを基本にする
文字数はできるだけ少なくする
話す内容もできるだけシンプルにする


発表練習
修正作業までは順調だったのですが発表練習は当日になっていました。。
実際に声に出すと意外と時間が掛かっている、この辺はもっと言い回しを変えたほうがよいのでは？など色々でてきました。何だかんだ5回ぐらい練習しました。
注意した点

とにかく声に出して練習する
書き上げた内容は暗記せずに、ポイントだけを覚える
時間も意識する（短い分にはいいが、長いと迷惑になる）


実際のLT
少しアニメーションも入れたりしているので伝わりづらいかもしれませんが、出来るだけ話した内容をそのまま書いています。読みづらさや繋がりがいまいちな所も多々あると思いますが、ご了承ください。


勉強会に参加していたはずなのに、ちっとも勉強していなかった話をします。
よろしくお願いします。この右下の番号はスライドの枚数です。9枚なのでサクサク進めていければと思ってます。



まずは自己紹介をします。
twitterとかはうさぎみたいなアイコンで、なおとでやっています。IDは@naoto_7713です。
簡単な経歴ですが、大学を卒業してまずはSIerに入社しました。そうですね、このときで一番記憶に残っているスキルはExcelの罫線を駆使してキレイな図を作成することでした。あと、vlookupも多用して業務していました。
ただ、もっと技術的なことがしたくなり、転職しました。HTML/CSSも分からない状態にも関わらず、Web系自社開発に転職できました。本当に感謝しかありません。主にバックエンドもフロントエンドも経験をできました。
ただ、もっと色々な現場を見たい、新しい技術を触りたいと思い、一年務めず、申し訳無さがありましたが、フリーランスになることにしました。
現在はVue、Angular、Typescriptでフロントエンドをしています。
今回が始めてのLTです。緊張しています。改めてよろしくお願いします。
あっ、でもこういうのも筋トレをすれば全て解決するんですよね？？（前の人のLTに被せるような内容。筋トレについてのLTでした。）



それでは今日はボクが失敗した話をします。
概略ですけど、フリーランスで10月からVueを使った現場に配属されることが決まりました。決まったのが、9月ぐらいなので1ヶ月ぐらいは時間があるので、一ヶ月もあれば何とかなるでしょと考え、面談でもVueは初めてですがこの1ヶ月で勉強してきます！！と言っていました。。
そして実際はどうだったのか？という話です。
今日は駆け出しエンジニアの人が多いとのことでこれを反面教師として聞いていただければです。
筋肉エンジニアの方は僕はどこの筋肉が足りなかったのかを教えてほしいです！！



それでは、Vue.jsを勉強するための僕の思考をお話します。
僕自身そこまで意志が強くないので、平日の夜は早く寝たいし、土日はゲームやアニメなどゴロゴロしたいと考えてしまいます。
なので勉強を続けるには高いモチベーションが必要だ！！と考えました。
そこでやったことが３つあります。
１つ目はツイッターを見るです。僕もつい最近初めたのですが凄いエンジニアの方や今まさに勉強している人のつぶやきを見ると、僕ももっと頑張ろう！！とモチベーションが上がっていきました。
２つ目は勉強会に参加です。twitterを見ると勉強会の情報が流れていき、毎日どこかしらの勉強会が開催されていて凄いなと思いました。実際僕も行ってみて、自分の知らない情報やLTを見ると刺激され、ここでもモチベーションはぐんぐん上がっていきます。
３つ目が人と会って話すです。twitterや勉強会経由で食事を行く機会も増えました。僕自身そこまで社交的でなく、むしろ人見知りするタイプですが、同じエンジニアやエンジニアを目指している人とは共通点も多く、一緒に頑張りましょう！！とモチベーションが上がりました。
ここまで話して分かると思いますが、モチベーションは凄く上がっています。ただ、一日も経つと少しモチベーションが下がり、また上げなきゃという気持ちになるんですよ。。そして、twitter→勉強会→人と会うを繰り返した結果どうなったのか、そうなんです。わかりますよね。vue.jsを勉強する時間がなくなっていました。ただ、実際に行動しているときは僕正直ここに気づいていないです。。自分は頑張っていると感じているんですよ。
しかしある時、ある人から言われたのです。



「あなたが本当にやるべきことを見失っていない？」
 これを言われたとき、結構心にはきました。いや、色々勉強会と人と会ったりで行動しているよ。。など反論したい気持ちも出てきましたが、Vue.jsに対しての成果物は一切ありませんでした。この状況でなにも反論ができなかったです。そして気づきました確かに僕はvue.jsをちっとも勉強していないぞ！！



改めて、先程のスライドですが、確かに最初はVue.jsの勉強が目的でした。
そのために、高いモチベーションを保つのが必要と考えていたのです。
ただ、自分でも気づかないうちに目的が「高いモチベーション」にすり替わってしまったのです。そのため、モチベーション下がったら、モチベーションが下がってしまった上げようと考え、上がったら満足していたのです。



何でこうなってしまったのかを振り返ってみました。
１つ目は勉強会に参加して、勉強した気になってしまいました。
勉強会は基本的にスキルレベルが違う人が集まり、短い時間でのアウトプットです。そこで自分の興味あるものに関しては自分で家に帰ってから勉強していくものです。そこでvueの基本を全て学べるはずがないのに、自分はやった気になっていたのです。
２つ目はできる人の話を聞いたりして、何もしていない自分までも何だかできると思い込んでいました。
できると思いこむこと自体は良いことだと思います。ただ、今回の僕はそもそも何もしていないです。何もしていないのにできるはずがないです。
これに気づき、まずはVue.jsの勉強をしようと初めたのですが、当初の「現場に入るまでにvueを一通り学ぶという目的」を達成できず、最初のキャッチアップ期間は少し大変でした。。



色々話して来ましたが、今回僕が伝えたかったのは「最初の目的は何であったかを忘れないことが大切！！」だということです。
僕自信も最初の目的はVue.jsの勉強でした。怠けたい気持ちや努力したくない気持ちはなく、むしろ頑張ろう！！という思いで動いていました。ただ自分でも気づかないうちに目的がズレてしまい、違う方向に進んでいました。
この気づかないうちにというのは本当に怖く、また頑張っている人ほど陥りやすいのではないのかとも思います。
このようにならないためにも適度に目的はなんだっけ？と振り返ることが大切なんだと気づきました。
あと、Vue.jsを勉強するのに高いモチーベーションが必要なのではなく、結局はやるかやらないかで、モチベーションは必要なかったのだと気づきました。何だかモチベーションという幻想に取り憑かれてしまったのかもしれません。



はい。これで終わりです。ご清聴ありがとうございました。


振返り

発表前までは不安な気持ちでしたが、終わるとやはり楽しかった
発表している時に、頷いたり、こちらを見てくれたりすると凄く安心しました
前の人のLTに被せるような内容も入れると和む（今回は筋トレLTの内容を入れました）
レイアウトはテンプレートでとりあえず充分
意外と時間が超えるのでもっとコンパクトにしないといけない
話し方や身振り手振りは練習しないといけない
想像できると実践できるは全然違う


最後に
正直、LTの資料作成や発表を考えるのに時間は掛かりますが、それ以上に自分の考えを整理できるいいアウトプットになりました。
発表を聞くときはちゃんと相槌を入れて、資料だけではなく発表者を見ようと思うようになりました。
次の機会があれば、またLTをしたいなと思います。
LTの機会頂いたプロラボさんのもくもく会は雰囲気もよく、始めてLTする人もオススメだと思います。
長くなりましたが最後まで読んでいただき、ありがとうございました。
",False,https://qiita.com//turmericN/items/6f13667fc34269acd88c
"

概要
初参加で開始直後までウキウキしていたラズパイもくもく会で無線LANドライバ黙々させちまったので、環境再構築をもくもくやるだけで1日が終わりそうだったから声掛けた人とペアプロしてたら楽しかったよっていう感想文。

そもそもWindowsユーザにプログラはミング環境構築させるの難しくない？

横で聞いていて立ち止まっていたように感じたポイント

SSHってなんぞ
エディタの設定難しくない？


SSH接続ってなんですか
Windowsユーザでコマンドプロンプト使ってたり「Win10上でUbuntuが動くようになったぞわーい」って言っていた人、PCユーザ全体から見たらごく少数だと思うんです。
なんとなくMacという人も含めた大多数の人類はターミナルとかCUIとか触れることなく一生を終えるのではないでしょうか。

TeraTerm入れてー。xxxx入れてー。
Linux(apt系列やyumとか）やMac(brew)と違って、Chocolateyみたいなものなんぞ普通のWindowsユーザは知らないのが現状です。
コマンド叩いたらインストールできるなんて恵まれた環境はゲームというオアシス以外はサバンナで生活しているWindows使いには想像もできない世界です。condaも似たようなものです。
究極統合開発環境Excel様がいればIDEもDBも何もかも手に入って設計も書ければプレゼン資料だって作れて救われるし、Winの方が（Macより）安いし（ハードウェアメーカー）サポートあるし。
VLOOKUP使える程度でウィザード級名乗れる世界もあるんやぞあっち側。1
それにコマンドプロンプトやPowershellの説明から接続プロトコルだなんだ言うより五感に訴えかける方が多分だけどわかりやすい。
彼と僕には半日も時間がないんです。
今、目の前で何かわかりやすいことが起きた方が刺激になる＝ラズパイ楽しい→〇〇できるんじゃない？　と私が知らない世界を見せていただけるという当時の下心を書き連ねたのが以下のポエムです。

プログラムを書くなら何使う？
ご本人はもくもく会に来る前からとりあえず程度にatom入れてました。
でもSSH接続もよく分からない状態でラズパイ触るなら、リモートから　実行→即→結果　って環境の方が面倒が少なくてきっと刺激が強いよね。
ってことでatomからリモートで自分でできる気がする範囲でほとんど設定せず実行する方法を調べてもらったんですけど、どうもいい感じのが見つからない様子。
私が見つけたプラグインもなんか古くなっていたか設定がよく分かんなかったかしていたみたいで思い通りになっていなかったっぽい。
本人が調べても設定方法がよく分からないものをやれっていうのは無理があります。
何か、何か手間のない私に負担がない方法はないか。
彼は.acアカウントのメールアドレス持ちなのでアカデミックライセンスでPyCharm Professional2 を入れることも考えましたけど、設定でいろいろ時間食うのも嫌だなと思いました。
登録で何分使うか覚えてないから、PyCharmから抜け出せなくなるのは今回はパスね。
今回は幸運なことに、ご本人が心理学専攻の院生で実験でJupyter notebookを使っている以外はPythonどころかプログラムの経験がほとんどないという恵まれた星の下に生まれた人でした。
ローカルでJupyter立ち上げて計算機にしてグラフ描いたりしてたのね。
OKOK。経験者じゃん。イイヨイイヨー。
エディタとしてRaspberryPiで立ち上げたサーバで動かしているJupyter notebookを母艦PCから操作していただくことに決めました。

RaspbianでJupyter notebookを使う方法
画面左上のラズベリーアイコン→「設定」→「Rappberry Piの設定」から
SSH（と多分リモートGPIOも）を有効にしときましょう。
Jupyterをラズパイ本体から起動させるだけならSSH無効でもいいかもしらん。
そうだとしたらラズパイ本体から直接GPIOへメッセージ送れるんだからリモートGPIOもいらんのやろか。
それはそれとして。
端末から
$ sudo apt install jupyter-notebook
$ jupyter notebook --generate-config

viでもいいんだけど、Winユーザを想定しているので操作が比較的わかりやすいnanoを使いますね。3
$ sudo nano ~/.jupyter/jupyter_notebook_config.py

144行目あたりの
## The IP address the notebook server will listen on. 直下にある
#c.NotebookApp.ip = 'localhost` を
c.NotebookApp.ip = '0.0.0.0'　に変更

これで初期設定ならポート8888から、Jupyterを通じてRaspberyPiのGPIOも含めてLAN内からコントロールできます。
Raspbian上のPythonをそのまま使うため、gpiozeroなどを別途インストールする必要はありません。4
Jupyter notebookのセルに
!pip freeze

と入力するとインストール済みのパッケージ一覧が出てくるので、gpiozeroなどが入っているのを確認してください。
その他、ターミナルでなにかやりたいことがあればnotebook上のセルに ""!"" 始まりで実行すると直接叩けるのでWin機（コード打つ方のマシン）で端末触ることはなくなります。
pipで何か入れたければ
!pip install spam

これをセルに入力したらspamパッケージが入ります。
その他のコマンドも""!""から始めて入力したら全部実行できます。
TeraTermなんていらなかったんや。

初心者の躓きポイント
ネットワークの仕組みだコマンドだなんだよりもっと大切なことがあると思うんです。
詰まったら質問しない限り自分の中に答えはない。
自分がわからない状態にいることを他人にわかってもらえることってほとんどないので、検索しても分からなかったら質問しないと一生わかんないと思う。
私この件でたくさん叱られたので、前に進みたいなら質問しましょう。5
他の人に質問するのが怖いなら、会社の後輩ならとりあえず私に質問振ってと言って私が他の人に質問を投げた後ドヤ顔で回答するようにしています。
後日でいいならエンジニア仲間に質問投げまくってますし、自分でも喋っていて疑問になったところは友人知人に相談しまくってます。

非エンジニアがラズパイを触るにあたってJupyter notebookが優れている点のおさらい

環境構築が楽


RaspberyPi上でjupyterの設定ファイルを生成→1行書き換えだけでPython実行環境もエディタも手に入る


デバッグが楽


notebookそのものの利点だけど、セル毎の実行結果が返ってくるので何がおかしいのか分かってもらえやすい



もとからnotebookに慣れていたからってアドバンテージがでかすぎるという理由は大いにある。

所感
Lチカなんてしょーもない。
きっと皆さん何度も何度も何度も何度も見てきたことでしょう。
何度も何度も何度も何度も思ってきたことでしょう。
そんな目をした人を見てきましたし、Qiitaでも最近クズ記事ゴミ記事は作るなって話題が出ていますもんね。
私も素人レシピだのワイドショーな「〇〇さんは××だそうです。今後が気になりますね」だののゴミ記事は検索結果から消えてほしいと思いながらここでゴミのようなポエムを書き綴っています。
でもねでもね。

ただただLEDを光らせることに成功した

time.sleepを使って明滅させることに成功した
関数を作って明滅させる時間を変化させることに成功した

randomを使って明滅時間に幅を持たせることに成功した
2色LEDを使って赤・緑・橙を不規則に明滅させることに成功した

自分で悩んで考えて、それが思い通りの結果を返してきた。
こんなに楽しくて嬉しくてワクワクしたことって人生の中で何回あるんでしょうか。
「こんにちは、世界！！」
初めてデジタルワールドに没入したときの気持ちも忘れて死んだ魚のような目をして打ち込むあの合言葉みたいな僕と違って、いろんなことを成功したのを目の当たりにしたら心駆動開発やってる人間としてはお気持ちをシェアしたくなるってものです。
帰るときにハロウィンパーティーか何かの帰りな人々に出くわしたのですが、彼が言っていた「クリスマスの電灯みたいなの作れそう」という心駆動開発でパリピも沸き立つものが作れそうな気がしませんか。したよ私は。
将来的にはロボットを作りたいとのことで秋葉原歩きながらどんな部品があってどんなセンサーがあってどんなことができそうか。
という話をしたら色々考えていたので心駆動開発が捗ってくれたらうれしいなと思う次第です。

おわりに
こっちの方が楽だよっていう最適解があれば教えてください。
彼じゃなく僕の生活水準が向上します。
以上、何卒よろしくお願いいたします。

おわれない
Jupyter notebookがラズパイの電源入れたら自動起動するように設定しておくべきでしたね。
毎回コマンド叩きたくないし、そのためだけにSSHつないでもらうのもなんだかな。

おわらせない
Lチカだけじゃ終われないでしょ。
何ができるかじゃなくて、何がしたいかでラズパイ触っていたいです僕も。

はじまり
やりたいことが現実味を帯びてくるときの目っていいよね。
ワクワクしますね。
ゾクゾクしますね。

はじまった
自分のラズパイ再セットアップ終わらせてコードも入れなおした。
床でゴロゴロしてから僕のもくもく会がはじまる。




事務方のおじさんが進化したいなら「闇の関数おじさん」か「闇のVBAおじさん」の二択でした。Excelは非エンジニアでも簡単に直接触れるDBという優位点だけは揺るぎません。 ↩


大学発行のacドメインなメールアドレス持ちはJetBrains社のIntelliJとかPyCharmとか無料で使えるから、卒業以降も買い続ける沼にどっぷりと浸かるためにも今から使っとこう。 ↩


その後、別の人と食事しながら「Lispって面白いんですよ」と宗教勧誘していました。 ↩


venv使うより新しいラズパイ買う方がいいよね。って場面が多い気がする。初心者だし。金で解決できる苦労は金で解決しろ。 ↩


私は「10分調べて分からなければ周りに質問することを考えはじめろ」と言われたのですが、30分考えるというお話を伺いました。偉いなって思いました（こなみ ↩



",False,https://qiita.com//GuitarBuilderClass/items/e6c9c140f97161dbe2f7
"

この記事の目的
プログラミングを始めて間もない人に私と同じようになってほしくない、今同じような状況にいる人の参考になればと思います。
内容も短いので私の経験を反面教師にしてください。

公式サイト、動画学習サイトを見るだけ
コードを見るだけで手を動かさないと本当に痛い目にあいます。
もちろん無駄ではありませんが、何がどのように動いているのか、何故動いたのか、何がそれを動かしたのかが見ただけでは何も理解できません。
入門者向けの記事、動画だとどの言語も同じような内容ばかり(条件文、繰り返し処理など）なのでわかった気になってしまいます。
これが1番怖い事でわかった気になってしまっているから次々レベルの高い内容に目を通してしまい、もちろんその内容もほとんど見るだけなので使えるようになんてなりません。
例えば野球がしたくてYouTubeを見てフォームを真似してバットを振ってみる人と、動画を見るだけの人では身につくスピードが明らかに違います。
そして振ってる間に何か気づくことが必ずあります。
プログラミングも同じように読むだけではなく参考にしたものと同じものを作る、コードを書くことで何が理解できていないかはっきりし、理解できなかったことが体に身につく瞬間があります。
泥臭く、手を動かし続けましょう。
",False,https://qiita.com//Reo777/items/b82912a87420ce4c4095
"『エリック・エヴァンスのドメイン駆動設計』（以後、DDD本）を読んで思ったことのメモ。

オブジェクト指向はモデリングのパラダイムである
DDD本を読んで自分の中ですごく腑に落ちたのは、「オブジェクト指向はプログラミングのパラダイムではなく、モデリングのパラダイムである」ということ。
「オブジェクト指向のメリットは、現実をそのまま表現できること」と言われることがある。
これは、オブジェクト指向のモデリングパラダイムとしての側面を捉えた表現だ。
オブジェクト指向に限らず、リレーショナルモデルでも統計モデルでも、モデルというのは「現実世界の物事を、ある捉え方で表現したもの」みたいな定義だ。
現実世界の物事を「属性（フィールド）」と「振る舞い（メソッド）」の塊としてモデル化のがオブジェクト指向モデルだ。

なぜモデル化するのか
「現実をそのまま表現する」と複雑すぎるので、オブジェクト指向モデルだったりリレーショナルモデルだったりで表現して整理するのだ。
モデル化せずにコンピュータの考え方でアプリケーションを作ろうとすると、ここはこういう分岐でこういうときはこうなって...とめちゃくちゃ複雑になる。
だから、現実世界をモデル化できない構造化プログラミングではなく、現実世界をモデル化できるオブジェクト指向プログラミングが良いとされるのだ。

様々なモデリング手法の中で、なぜオブジェクト指向なのか
複雑すぎる現実を単純化するために何らかのモデリングが必要なのだが、その中でオブジェクト指向が選ばれるの理由は以下の2つだ。

オブジェクト指向モデルは、エンジニアでなくても理解できる
オブジェクト指向モデルには、プログラミング言語で実装が与えられた


オブジェクト指向モデルは、エンジニアでなくても理解できる
オブジェクト指向以外にも、リレーショナル、関数型、論理型など、様々なモデリング手法がある。
そんな中、オブジェクト指向だけは理解のハードルがめちゃくちゃ低い。
クラス図を見ながら「こういう役割のやつがいて、こんなふうに指示を出すんですよ」とか、ITの知識がなくても会話が成り立ちうる。
エンジニアと周りの人を繋ぐ共通言語となり、認識齟齬を減らしうるから、オブジェクト指向が良いとされるのだ。

オブジェクト指向モデルには、プログラミング言語で実装が与えられた
いくらモデルが分かりやすくても、そのまま実装に落とせなければ、実装時にエンジニアの思い込みが入ってしまう。
オブジェクト指向の素晴らしいところは、モデルとして図に描いたりして議論した内容を、そのまま実装に落とせるところだ。
モデルをそのまま実装に落とすことで、こんなときはきっとこう動くんだろうなーとかも推測しやすくなる。

つまり...
オブジェクト指向モデルが選ばれる理由は以下の2つだ

だれでも理解できるモデルだから、エンジニアと周りの人の認識齟齬が生じにくい
そのまま実装に落とせるため、要件と実装の乖離が減る


オブジェクト指向とは何か
オブジェクト指向モデルとは、現実世界の物事を「属性（フィールド）」と「振る舞い（メソッド）」の塊である「オブジェクト」のやり取りで表現するモデルである。
難しいのは、物理的に実体のある物だけでなく、ポリシーのような概念もオブジェクトとして捉えるからだ。

マイクロサービスもオブジェクト指向だ
オブジェクト指向はプログラミングではなくモデリングのパラダイムだと理解すると、あることに気付く。
「あれ？マイクロサービスもオブジェクト指向モデルなのでは？」
各サービスに「属性（DB）」を持たせて隠蔽し、「振る舞い（API）」によってやり取りするのは、まさにオブジェクト指向だ。
確かに「マイクロサービス オブジェクト指向」とググると、そういう話がたくさん出てくる。
やはり、オブジェクト指向はプログラミングパラダイムではなく、モデリングのパラダイムなのだ。

マイクロサービスのヒントをオブジェクト指向から得る
マイクロサービス・アーキテクチャには、オブジェクト指向と似たデザインパターンがある。
例えば、Proxyパターンとサイドカー・パターン、FacadeパターンとBackend For Frontendは似ている。
他にも探せばたくさんありそうな気がしてくる。
マイクロサービスを構築するときは、オブジェクト指向の文脈を思い出してみるとヒントになるかもしれない。
",False,https://qiita.com//os1ma/items/363acab598e063b7d3ab
"最近「オブジェクト指向とは何ぞや」を書くのが流行ってるらしいので、自分なりの理解を書いてみる。一言でまとめると、
オブジェクト指向でコードを書くときは、「神」になるべきである。

オブジェクト指向言語＝創世記
まずは、何も言わずに次のドキュメントを見てほしい。
1 はじめに神は天と地とを創造された。
2 地は形なく、むなしく、やみが淵のおもてにあり、神の霊が水のおもてをおおっていた。
（中略）
11 神はまた言われた、「地は青草と、種をもつ草と、種類にしたがって種のある実を結ぶ果樹とを地の上にはえさせよ」。そのようになった。 

出典：wikisource
これはある「神」というプログラマが「世界」というプロジェクトを実装したときのドキュメントで、「創世記」という名前で知られている。
これこそ「オブジェクト指向」の典型だと思う。なぜなら、このドキュメントからコードを書き起こすことができるからだ。

創世記を実装してみる（？）
とはいえ、このままだと「は？意味わかんないんだけど」「飛ばし記事乙」「ゴミ記事を増やしてんじゃねえ」となるので、もう少し解説する。

ヘッダ
ここでのポイントは「xxとはこういうものだ」という表現にある。そのxxがクラスと呼ばれる。
たとえば、項番1,2を実装するとしたら次のようになる。まず、出てくるもの、つまり実装すべきクラスを整理する。ここでは天（heaven）と地（earth）を実装すると書いてある。つまり、C++で言うとヘッダでクラスを定義することにあたる。

委譲
「地は（中略）やみが淵のおもてにあり」ということから、地はやみ（dark）というメンバ関数を持っていることがわかる。このようにクラスが他のクラスのメンバを持つことを集約（コンポジション）とか、特にそのメンバの機能を用いて元のクラスの実装を簡単にしているような場合は委譲1 と言ったりする。
以上をまとめると、例えば地の実装は次のようになる。
class earth{
   dark m_dark;
};


メンバ関数、メソッドの実装
「地」の実装では、変数だけがまとめられている。これは別にオブジェクト指向でない言語での構造体でも実装できる。
11番を実装しよう。
神はまた言われた、「地は青草と、種をもつ草と、種類にしたがって種のある実を結ぶ果樹とを地の上にはえさせよ」。そのようになった。 

ここで果樹を実装することを考えると、「実を結ぶ」という機能が果樹に備わっている、ということがわかる。これを実装すると、
class fruitTree{
   fruit m_fruit;
   void bearFruit(void);
}

となる。

「もの」を宣言していく作業こそ「オブジェクト指向」
オブジェクト指向とは何ぞやという話に戻ろう。
オブジェクト指向では、

こういう「もの」があって
それはこういう変数を持っていて
こういう機能がある

ということを設計していく。つまり、プログラムの中の世界を作り上げていくということである。これが「神になれ」という所以である。すべてのプログラマは自身のコードの中では造物主である。

じゃあどうやって神になるか

根本的に、抽象化力を磨く。
鍛錬を積む。何度も世界を作って壊す。
うまい神の世界を見てみる。うまい神と一緒に世界を作ってみる。
デザインパターンを勉強してみる。


参考文献

Getter/Setterは悪だ。以上。https://www.kaitoy.xyz/2015/07/22/getters-setters-evil/






C#では委譲というと単にメンバを持つということではなく、また別の機能を指すので注意（本質的には同じなのかもしれないが）。 ↩



",False,https://qiita.com//Bluepost59/items/0248f9a60ba42eb2d1af
"英語でよくある、いいこと言ってそうに見えるけどなんかプッとくる名言みたいなのが好きで集めていた。職場での息抜きにそういう一文をslackに貼り付けるのはよくやっていて、面白いからまとめてウェブサイトにした。
エンジニアの名言メーゲン
https://www.meigen.ga/

バックグラウンドにきれいな景色の写真を入れて無駄に「いいこと言ってそう」な雰囲気にした。
名言は人気順ということでツイート数順に上から並べている。もし気に入ったのがあって、ツイッターで共有していただければ順位が上がりますよ、と。

無料だから.gaでドメインを取った。「名言. ガー」とでも覚えてください。
https://www.meigen.ga/
ボタンひとつで英語版に切り替わるので英語の勉強にもどうぞ。（学習効果の保証は一切無し）

ほんとただの趣味でしかないんだけど、集めているのでもし面白いのをご存知でしたらサイトのフォームから入れてお知らせいただければ嬉しい。一応ご説明させていただくと、松下幸之助語録みたいな真面目にありがたい感じのザ・名言みたいなのは募集してなくて「いいこと言ってる風でプっとくる」ってのがポイントです。

使った技術
ほとんど以前に作った個人開発アプリの使い回しなので実装は一瞬で終わった。

Ruby on Rails
Netlify
React + Redux
GraphQL
heroku

これ以上にこのウェブアプリに意味は無い。普段から「収益性を考えてないウェブアプリなんかやめてしまえー」とか言ってるわりにこのアプリに収益を得る戦略もなにもない。目的はただ、いいこと言ってそうなエンジニアの名言を集めたいだけ。
別にいいだろ、モヤモヤしている時にスッとさせるにはなんでもいいから手を動かして作って公開するんだよ。
ということで最後はリーナス・トーバルズさんの名言メーゲン。

",False,https://qiita.com//jabba/items/0d4e730be7f23c21c6f9
"随分前にこんなこと書きました、要件が定義できないことの説明の草案。
色々学びなおして、色々と考えが変わってきたので、現段階の整理。

観察することが世界に与える影響
観察をすることは世界に影響をあたえることになります。
これは、システム思考で有名な考え方です。
観察、つまりは、リサーチを重ねれば重ねるほど、その世界に影響が出てくるというものです。
観察の結果出来上がったサービスを作って、それを世界にリリースした時、さらにその世界には変化が現れます。
その世界は、もう、観察前の世界、観察していた世界、とは大きく変わっている世界になると思います。

要件定義やシステム思考が役に立たない世界
この理屈は、システム開発において要件定義が役に立たないのと一緒です。
デザイン思考でリサーチを重ねても答えが出ないのと一緒です。
新しいサービス未導入の世界、新しいサービスを使うとどうなるかわからない世界、新しいゲームも、新しい音楽もない、そんな状態で、どういうサービスが欲しいか？どんなゲームがやりたいか？どんな曲が聴きたいか？ それを訪ねられても答えを持ってる人（素人）はいません。

この辺りはAppleのスティーブジョブズやフォードあたりで良く言われる事例ですね

どういうサービスが欲しいかを聞けば聞くほど、その聞かれた人にも変化が出てくるのです。
どういう課題があるか聞けば聞くほど、その聞かれた人にも変化が出てくるのです。

経験あると思いますが、改まって聞かれると、あることないこと言ってしまうものです。
人間はそれほど言語だけで物事を感じたりしていないので、言語化する時点で考えが変化していきます。

こうなると本当は欲しくもなかった機能まで欲しいと言ってきました。
本当は課題と思ってないことまで課題と言ってきます。
そうやって作られてリリースされたサービスは、本来の課題解決のためのサービスではなく、ただ、""その人が欲しい"" と言っただけのサービスになってしまうのだと思います。

言語化しない観察は？ なんとかリサーチやえすのぐらふぃーとか？
遠くから、観察対象にばれないようにカメラをまわせば、それは観察対象に影響を与えないのでしょうか？
その人たちと一緒のように暮らせば影響を与えないのでしょうか？
そんなことありません。
最初に言った通り、観察をすることは、その観察対象に影響を与えることなのだと思います。
それには様々な要因が考えられます。

シュレーディンガーの猫のような理屈もあるでしょう。
観察することによってその対象が決まるという理屈です。
深淵を覗き込んだ時には、深淵にも覗かれているということを忘れてはいけないのです。

とてもとてもシンプルに言えば、認識している「世界」は自分の中にのみあるからです。
カメラを回せば、カメラで撮れなかった全てが削り落とされます。 音は？ 温度は？ 今のセンサでは捕捉できない第六感的な何かは？ 観察する前は無限にあったはずの情報が観察することで、観察できる情報だけに特定されて、それ以外が削り落とされます。
観察は、取捨選択をすることなのです。
結局、観察してるふりをして、自分に都合の良い欲しい情報を得ているだけです。

では、どうすればよいのか？
世の中にある優れたプロダクトやサービスは、XX思考やXXプロセスの結果生まれてきたわけではないのです。
後付けで、そんな思考やプロセスを加えることはできますが、そのプロダクトやサービスを作った彼ら・彼女らの絶え間ない努力、知識と経験に裏付けられた勘によって作られたものです。
結局は、とてもシンプルに、自分（自分たち）の専門性を鍛えて、審美眼を鍛えて、その専門性をもって「価値」を作り上げていくしかないのだと思います。

その鍛錬の過程として、過去のノウハウである XX思考 や XXプロセス を学ぶのは大事なことだとは思います。
それらを学ぶことによって、学ばない場合に比べて、成功の確率は上がると思いますので。

作り上げた「価値」が本当に誰かの役に立ったかどうかは、それはその「価値」が世の中に受け入れられたかを静かに見守るしかないのだと思います。
ダメだったら、またやり直せばいいのだと思います。
やらなければいけないことは、

日々鍛錬すること
自分に謙虚に、世の中の批評を受け、失敗に気付き、日々改善を続けること

おそらくはシンプルにそういうことなのだと思いました。
",False,https://qiita.com//567000/items/fe8fc5a771107aa1fa02
"

デザインって何ですか......？
「デザイナー」というと、「グラフィカルなUIなどをデザインする人」って意味にとられますよね？
でも、ソフトウェアエンジニアだって、システムのデザインをします。
そんな「デザイン」という、あまりにも曖昧な言葉が世の中にあります。
白黒はっきりつけたがる技術屋にとってはあまりにも気持ちの悪い言葉なのでちょっと整理してみました。

それなりにリサーチした結果ではありますが、「あれ」「それ」みたいな単語が様々な意味を持つように、「デザイン」みたいな一般用語は文脈によってさまざまな意味を持ってしまいます。なのでこれが必ずしもすべての状況においての正解ではないないです。ただ考え方の拠り所にはなるかと思います。


図解
みなさん大好きUMLでの整理です。
わたしはオブジェクト指向が大好きなので、情報アーキテクチャ的な分類をするときもOOに頼ることが多いです。


目的による大別
「問題解決」と「問題提議」の2種類に大別されます。
（上図の左側のツリーです）
世の中の問題を解決することを目的とした活動は「デザイン」と呼ばれます。
なにも枕詞をつけないで「デザイン」というと、今であれば「ビジュアルデザイン」いわゆるUIデザインやグラフィカルなデザインを指すことが多いです。ただ、ITなどの技術を使って問題解決を目指す「システムデザイン」というものも存在します。
対して、世の中に問題提議をする、問いを投げかけることを目的とした活動は、一般的に「アート」と呼ばれることが多いです。

目標達成のための思考法・アプローチ・手法による大別
次に、目的は何であれ、手法・思考法・アプローチとしてのデザインも存在します。
こちらも「問題解決」と「問題提議」の2種類に大別されます。
（上図の右側のツリーです）
対面している問題に対して、その問題を明らかにすることで解決に近づける、そのために、ユーザインタビューやヒューリスティックな調査を行う手法として、デザイン思考やHCD（人間中心設計）があります。
データを中心としたサイエンティフィックな手法としてはデータ分析・統計・ロジカルシンキングのようなものがあります。
逆に、対面している問題に対して、問題自体を新たに投げかけることで解決に近づける 手法・アプローチもあり、スペキュラティブデザインなどがあります。

「デザイン」を整理して見えてきた、目的 と 手法 は別であるという気づき
例えば、「交通渋滞」という明らかに目に見える課題があったとして、その交通渋滞を解決するためにどのようなアプローチをとるか？は自由に取捨選択が可能です。

例. 交通渋滞 - デザイン的な目的とアプローチ
エンジニアリング的なデザインの場合はこんな感じです。
目的： 交通渋滞を解決したい
手法： 交通量を調査して道幅や信号を調整する

ビジュアル的なデザインの例ではこうなるでしょうか。
目的： 交通渋滞を解決したい
手法： 交通標識を見やすくする、街路樹などによる心理的なイライラの解消


例. 交通渋滞 - アート的な目的とアプローチ
「交通渋滞を解決するとはどういうことだ？」
「交通渋滞とは何だ？」
「そもそもなぜ我々は移動するのか？」
そういうことを考えさせることによって、交通渋滞を根本から問いなおします。ひょっとしたら人間にとって「交通」なんて必要がなく、そもそも交通渋滞なんて不要なものだったということに気付くかもしれません。
目的： 交通渋滞とは何か？という問題提議
手法： 問題についての気づきを与えるためのアートイベント、アート展示


例. 交通渋滞 - デザイン（問題解決）目的に対してのアート的なアプローチ
上の例２つの組み合わせなので省略しますが、実現したいのは「問題解決」であったとしても、そのアプローチとして「問題提議」を行うこともあり得ます。
わかりやすい例では、ダブルダイヤモンドは、「問題解決」のために、改めての「問題提議」を行い、そこから「問題解決」を行っていきます。

まとめ
デザイン、アート、サイエンス、色々な言い方がありますが、デザインという言葉を使わずにロールと結び付けることで以下のような説明ができそうです。

一般的な、いわゆるエンジニアは、「エンジニアリングによる問題解決」を目的として、「Logical thinkingに代表されるデータ中心で論理的」なアプローチを行うロールです。
一般的な、いわゆるデザイナは、「（ヴィジュアル）デザインによる問題解決」を目的として、「デザイン思考に代表されるユーザ中心」なアプローチを行うロールです。
一般的な、いわゆるアーティストは、「問題提議」を目的として、「自分中心、コンセプト中心」なアプローチを行うロールです。

曖昧な言葉を曖昧なままに使うのはやはり気持ち悪いもので、整理してみましたが、このように分けていくと意外とシンプルな構造でした。
まとめてみて思ったのは、別に「エンジニアだからビジュアルデザインはやらない」とかではなく、逆に、「エンジニアリング的な思考で考えているけれど、アート的な発想がかけているのでは？」と言った気づきを得られる１つの軸として、今回の整理は越境をするための大きな指針になるのでは？という気づきでした。

ボタンの二度押しの抑止をするのにシステム的な仕組みを入れるのもよいですが、「２回押したら地獄に落ちる」とおどろおどろしく書いておくのも良いですし、「なぜ２回押すのか？」という問題提議からボタンを押さない仕組みを考えるのも良いと思います。

",False,https://qiita.com//567000/items/52c9519ab37e69b9ca48
"

Opening
今回はAgile Software Developmentについてまとめてみました。
正直なところ、Agileの考え方には賛同できても、実践することに対してはマイナスイメージしか無いので書くつもりはありませんでした。
AgileやCCPMは考え方には賛同できるのですが、いざ実践しようとなると誰かしらの邪念が混じったり、ひとまず教科書通り（大嘘）にやってみようと本質を無視して細かいところにこだわったりと、上手くいかないケースが多いと思っています。
えらいひとがAgileやろうと言ってきたときに「これこれこういう理由でAgileは適さないんじゃないですか？」くらいは発言できるように知識をつけておくと、辛さを回避できるかもしれません。

Appendices
そのうち書き足していくつもりです。

【新人向け】これだけは知っておいて欲しい基礎知識
【新人向け】これだけは知っておいて欲しい基礎知識　Linux編
【新人向け】これだけは知っておいて欲しい基礎知識　Shell編
【新人向け】これだけは知っておいて欲しい基礎知識　DB編
【新人向け】これだけは知っておいて欲しい基礎知識　アプリケーション編
【新人向け】これだけは知っておいて欲しい基礎知識　プログラミング編
【新人向け】これだけは知っておいて欲しい基礎知識　バージョン管理編


Body

Agileとは？
まずはAgileの概要について

アジャイル開発とは？今さら聞けない開発手法のメリット・デメリット - 発注ラウンジ


アジャイル（Agile）とは、直訳すると「素早い」「機敏な」「頭の回転が速い」という意味です。アジャイル開発は、システムやソフトウェア開発におけるプロジェクト開発手法のひとつで、大きな単位でシステムを区切ることなく、小単位で実装とテストを繰り返して開発を進めていきます。従来の開発手法に比べて開発期間が短縮されるため、アジャイル（素早い）と呼ばれています。


Manifesto for Agile Software Development
宣言の引用は公式サイトからですが、宣言のそれぞれの内容については下記リンク先の解説が参考になります。

アジャイルソフトウェア開発宣言について - ライトパスのブログ


プロセスやツールよりも個人と対話を、
包括的なドキュメントよりも動くソフトウェアを、
契約交渉よりも顧客との協調を、
計画に従うことよりも変化への対応を、
価値とする。すなわち、左記のことがらに価値があることを
認めながらも、私たちは右記のことがらにより価値をおく。 


Twelve Principles of Agile Software
原則のそれぞれの内容については（以下略

アジャイルソフトウェアの12の原則 - ライトパスのブログ


1.顧客満足を最優先し、価値のあるソフトウェアを早く継続的に提供します。
2.要求の変更はたとえ開発の後期であっても歓迎します。変化を味方につけることによって、お客様の競争力を引き上げます。
3.動くソフトウェアを、2-3週間から2-3ヶ月というできるだけ短い時間間隔でリリースします。
4.ビジネス側の人と開発者は、プロジェクトを通して日々一緒に働かなければなりません。
5.意欲に満ちた人々を集めてプロジェクトを構成します。環境と支援を与え仕事が無事終わるまで彼らを信頼します。
6.情報を伝えるもっとも効率的で効果的な方法はフェイス・トゥ・フェイスで話をすることです。
7.動くソフトウェアこそが進捗の最も重要な尺度です。
8.アジャイル･プロセスは持続可能な開発を促進します。一定のペースを継続的に維持できるようにしなければなりません。
9.技術的卓越性と優れた設計に対する不断の注意が機敏さを高めます。
10.シンプルさ（ムダなく作れる量を最大限にすること）が本質です。
11.最良のアーキテクチャ・要求・設計は、自己組織的なチームから生み出されます。
12.チームがもっと効率を高めることができるかを定期的に振り返り、それに基づいて自分たちのやり方を最適に調整します。


XP / Extreme Programming
狭義のAgileは思想として語られていることが多いと思いますが、XPとScrumはそれぞれAgileの思想を実現するための手法と言えます。

XP (エクストリームプログラミング) - Qiita


アジャイル開発が流行る中で、一番初めに流行りが来た手法です。
最近ではすっかりスクラムの方が流行っていますが、スクラムよりも組織論のよりも開発チームに近いので、最初に導入したり、考え方を理解するのには向いているかもしれません。
基本的にアジャイル開発なので、スクラムの手法の中にも似たようなものも多くあります。


Scrum Project Management
Agile（スクラム）のプロセスやメンバーの役割については下記の記事に簡潔にまとまっています。
Waterfall modelと比較した図もあるのでIT素人のユーザーさんにも説明しやすいですね。

アジャイル開発 ～顧客を巻き込みチーム一丸となってプロジェクトを推進する～ - NEC
前編
後編



結局Agileでやると何が嬉しいの？
個人的に考えるAgileの良いところ悪いところ



QCD
Good
Bad




Quality
ユーザーが本当に欲しかったものを実現できる
Policy無しにユーザー要望を汲み取ると神機能が生まれて保守性が死ぬ


Cost
Waterfallほど手戻りの影響が大きくない
タスク管理を細かくやったり頻繁に振り返りをするので管理工数が取られる


Delivery
細かい単位でリリースするので初回稼働まで早い業務の変化への対応が早い
同じ規模のシステムを全部作り終えるならWaterfallの方が早いかもね



Agileが失敗したり辛かったりする一番の理由は、人によってAgileに求めるものの認識が異なっていて、各人が無茶な要求をしてくるからかもしれない。
「Agileでやるんだから品質は高いんでしょ？」と言う人もいれば、「Agileって素早いって意味なんだから工数は少なくて良いよね。」とか「Agileなんだからもっと早くリリースしてよ。」と言うユーザーもいる。
基本的にQCDはトレードオフなので全部改善されるなんて夢物語はないのだが、えらいひとほどAgileに期待して現場との温度差が生まれて…という辛さがある。Agileの素早いというのはGoodにあげた点であって、最終的な工数・開発期間はそう短くならないと思っていた方が無難です。
…つい感情が入ってしまいました。
というわけで、Agileを適用するのであれば、QCDの何を重視してAgileを適用するのかえらいひと含めて認識を共有しましょう。
以下追記
最近読んだ英語の記事で、「技術基盤の判断ミスによる影響が大きい」という意見にも共感するところがありました。
「とりあえずAgileやってみよう」というプロジェクトだと、あまり準備されないままスクラム開発が始まったりすると思いますが、Agileを適用するのであればSprint 0で要件定義や技術基盤の検討は念入りにやっておくべきだと思います。

Agileを成功させるために必要なもの
個人的に、一番大きいのは経営陣がAgileとITを理解し、環境づくりに取り組むことだと思っています。
デンソーの例は羨ましいですね。

アジャイルを無責任に広めるのはもうやめよう - フロントライン通信


アジャイル、特にスクラムを成功させるには、下記のような条件が満たされている必要があります。
プロダクトオーナー（開発の価値判断を決める責任者）とスクラムマスター（進行役）が高いレベルのスキルと責任感を持っていること
メンバーのスキルと意欲が高いこと
チームが常に一つの場所に集まっていること
経営陣がアジャイル開発の価値を理解していて組織がそれに合わせて構成されていること


業務担当者の人的資源をシステム開発に投入しない経営側 - わけモブ


開発プロジェクトの度に思うが発注元は業務担当者にシステム開発を協力させる気がなく、業務担当者の仕事を一切減らすことがなくシステム開発中も業務担当者に同じ仕事をフルタイムでやらせている。
当然システム開発に協力している時間はなく、忙しい時間の合間に渋々協力することになる。
理想を言えばシステム開発期間中は業務担当者の時間を空けてシステム開発に協力してもらう必要がある。
システム開発中の業務担当者が業務をやっているのはおかしいのである。
代替要員を用意するか、せめて業務の時間を半分に減らすべきである。残り半分の時間をシステム開発に当てて欲しい。
このようなことは経営側が指示しなければできないことである。
つまり発注元の経営側がシステム開発の常識を把握していなければ正しいチーム編成はできないのだ。


生き残りを賭けた「デンソー流アジャイル」成功の秘訣―“人間力”を核とした組織力 - マイナビニュース


「日本ではアジャイル開発がうまくできていない、という状況を聞きますが、取り組むための環境ができていないことが原因のように感じています。デンソーはスタートが遅れていたからこそ、環境や役割など必要な要因を準備できました。これが成功している理由かもしれないですね」と佐藤氏。


その他
Agileに限らずドキュメント腐る問題はシステム開発について回ります。
Agileであっても残すべきドキュメントは作成しておかないと誰かが苦しむことになるので、ドキュメントのメンテナンスプロセスについては押さえておくべきでしょう。

我々はいかにシステム開発におけるドキュメント腐る問題と戦えば良いのか - Medium


サボらず整合性を取れよと一笑するのは、開発現場の苦しみをあまり知らない人でしょう。そもそもソースコードとドキュメントの整合性を取ることは非常に大変な作業です。また、常にハードな締め切りの中、課題に対してどういう解決方法が一番スマートか周辺システムを含めて調査し、解決案に対しての影響度範囲と費用対効果をバランスした上で技術判断し、コードを修正し、ユニットテストからステージング環境まで含めた品質チェックを行い、そしてなるべくユーザと既存システムに影響を与えないようにプロダクション環境にデプロイするのです。場合によってはそれぞれのフェーズでステークホルダーに説明が求められますし、政治やお金の問題で別のアプローチへの突然の変更もありるでしょう（もちろん、しばしばスケジュールの期限は変わらずに）。
システム開発は常にトレードオフの世界です。金銭的なコストと、ユーザに直接影響を与える部分は大体の場面において、優先度高だと思います。逆に開発ドキュメントは直接的に今すぐ影響を与えるわけではないので、もし機能改修とドキュメント保守のどちらかを選択しなければならない時、リーダーが開発を優先しドキュメントを保守後回しにする判断をすると思います。分かる、仕方ない。


最後に
言うまでもないと思いますが、Agileはあくまで手段にすぎません。
形骸化したプロセスを標的に語られることが多いAgileですが、Agileのプロセスをなんの考えもなしに適用すれば形骸化したプロセスと同じ轍を踏むことになります。
結局は、自分たちの置かれている状況をよく理解し、一つ一つプロセスを改善していくことが、辛くないシステム開発への近道なのではないかと思います。
ですので、Agileの考え方には敬意を払いつつも、Agileに固執しないプロセス改善ができると良いですね。

Closing
以上です。
",False,https://qiita.com//tomPlain/items/31848772e6d176228952
"

ワイ、会社やめたってよ
ワイ「上司くんが嫌い過ぎて会社やめてもうたわ」
ワイ「あんなプログラミングのことを何も分かってへんようなやつが指示をする立場におるから、プロジェクトが炎上すんねん。」
ワイ「クライアントにまともに質問もできひん。だから仕様がなんとなくしかエンジニアに伝わってけぇへんねん」
ワイ「何や既存の仕様を踏襲って。そればっかやん。」
ワイ「その既存の仕様の仕様書をよこせっちゅうねん」
ワイ「表面上は見えないような仕様はどないして踏襲しろっちゅうねん・・・いてまうど！」

就職活動をしないといけない
ワイ「とにかく仕事探さんとな・・・」
ワイ「いうてもワイ、学歴もなくて職歴もクソやから、書類審査で落とされんねん・・・」
ワイ「ポテンシャルはめちゃくちゃあるんやけど、アホな企業どもがワイの力を見抜けへんねん」
ワイ「あー、なんかプログラミングのテストを受けて、その結果をもとに企業はんにアピールできる就活サイトとかないんかな・・・！」
ワイ「・・・ググってみよか」
ワイ「プログラミング、テスト、転職・・・と、これで検索や！」

あった
ワイ「コーディング転職サイトquizaやと！？」
ワイ「なるほどな。プログラミングのクイズを解いて、自分のランクを上げていって、そうすると企業はんからスカウトが来るわけやな。」
ワイ「さっそく問題を解いてみるで！！！」

レベル1：サイコロ転がし

サイコロを転がして、コマが何マス進んだかを答える問題です。

ワイ「おお、面白そうやないか」

あなたはサイコロとコマを使って友人と遊んでいます。

ワイ「なるほど、ヒマなんやな」
ワイ「ワイとおんなじや」

サイコロを机に置いた状態で、友人の指示に従ってサイコロを前後に転がし、転がした際に出た目のぶんだけコマを進めます。
指示通りサイコロを転がし終わった後、コマが元の位置から何マス進んだかを答えてください。

　ワイ「なるほどなるほど」

友人からの指示は「前、後、前、前、後、後」といった形で渡されます。
「前に転がす」とはサイコロを前方に90度回転させる事を指します。
「後ろに転がす」とはサイコロを後方に90度回転させる事を指します。
前後に1回転がすたびに、出ている目のぶんだけコマを進めてください。

ワイ「なるほどな、サイコロを前に90度転がして、6の目が上になってたらコマが6マス進むってことやな」

最後に、進んだマスの合計値を出力してください。

ワイ「指示通りにサイコロを回転させて、その都度コマを進めて、合計何マス進んだかを解答すんねやな」
ワイ「やったろうやないかい、試験開始！と」

コードを書いてみる
ワイ「おお、ブラウザ上にテキストエディタが出てきたで。ここでコードを書いていくんやな」
ワイ「言語はJavaScriptを選択や！」
ワイ「なんや、デフォルトでコードが1行書いてあるな」
const shiji = [""前"", ""前"", ""後"", ""後"", ""前"", ""後""];

ワイ「なるほどな。これが友人くんからの指示や」
ワイ「前・前・後ろ・後ろ・前・後ろ、の順でサイコロを回転させて、そのたびにマスを進めていけばええねん」
ワイ「そんで、最初はどの面が上を向いてんねやろ。仕様によると・・・」

＜条件＞
初期状態ではサイコロの目は以下の状態になっているものとします。
「1が上、6が下、2が前、5が後ろ、3が左、4が右」

ワイ「なるほど。ほんならまずはサイコロの目の初期状態を変数に入れて、それをいじっていく感じで行こか」
let saikoro_eyes = {
    ""上"": 1,
    ""下"": 6,
    ""前"": 2,
    ""後"": 5,
    ""左"": 3,
    ""右"": 4
};

ワイ「サイコロの目の情報が入った配列やから、変数名はsaikoro_eyesや！」
ワイ「何マス進んだかを覚えておく変数も作らんとな。初期値はゼロや」

script.js
let masu = 0;


ワイ「サイコロを前後に回転させると、サイコロの目の位置が変わるわけやから、そのための関数が必要やな」
ワイ「”前”と”後”に対応した関数が実行されれば良いわけやから、前回転と後ろ回転の2つの関数を持った連想配列を作れば良さそうやな」
const kaiten_functions = {
    ""前"": saikoro_eyes => {
        // サイコロの目の状態を引数として受け取って
        // 前回転の処理をして
        // 回転後のサイコロの目の状態を返す。
    },
    ""後"": saikoro_eyes => {
        //後ろ回転バージョン
    }
};

ワイ「前回転の処理はどないな感じや」
ワイ「サイコロが前に90度回転すると・・・」
ワイ「回転後に前に来るのは、もともと上にあった面やな」
const new_saikoro_eyes = {};
//回転後の目の状態を格納する連想配列を作成。

new_saikoro_eyes[""前""] = saikoro_eyes[""上""];
//上にあった面の数値を、回転後の「前」に格納。

ワイ「ほんで、回転後に上に来るのは、もともと後ろにあった面やな」
new_saikoro_eyes[""上""] = saikoro_eyes[""後""];

ワイ「ほんで、後ろに来るのは下にあった面」
ワイ「下に来るのは前にあった面」
ワイ「左と右の面は変わらずや。前後の回転だけやからな」
ワイ「前回転の関数ができたで！」
const kaiten_functions = {
    ""前"": saikoro_eyes => {
        const new_saikoro_eyes = {};
        new_saikoro_eyes[""前""] = saikoro_eyes[""上""];
        new_saikoro_eyes[""上""] = saikoro_eyes[""後""];
        new_saikoro_eyes[""後""] = saikoro_eyes[""下""];
        new_saikoro_eyes[""下""] = saikoro_eyes[""前""];
        new_saikoro_eyes[""左""] = saikoro_eyes[""左""];
        new_saikoro_eyes[""右""] = saikoro_eyes[""右""];
        return new_saikoro_eyes;
    },
    ""後"": saikoro_eyes => {
        //後ろ回転バージョン
    }
};

ワイ「後ろ回転は、その逆やから・・・こうやな！」
const kaiten_functions = {
    ""前"": saikoro_eyes => {
        const new_saikoro_eyes = {};
        new_saikoro_eyes[""前""] = saikoro_eyes[""上""];
        new_saikoro_eyes[""上""] = saikoro_eyes[""後""];
        new_saikoro_eyes[""後""] = saikoro_eyes[""下""];
        new_saikoro_eyes[""下""] = saikoro_eyes[""前""];
        new_saikoro_eyes[""左""] = saikoro_eyes[""左""];
        new_saikoro_eyes[""右""] = saikoro_eyes[""右""];
        return new_saikoro_eyes;
    },
    ""後"": saikoro_eyes => {
        const new_saikoro_eyes = {};
        new_saikoro_eyes[""後""] = saikoro_eyes[""上""];
        new_saikoro_eyes[""下""] = saikoro_eyes[""後""];
        new_saikoro_eyes[""前""] = saikoro_eyes[""下""];
        new_saikoro_eyes[""上""] = saikoro_eyes[""前""];
        new_saikoro_eyes[""左""] = saikoro_eyes[""左""];
        new_saikoro_eyes[""右""] = saikoro_eyes[""右""];
        return new_saikoro_eyes;
    }
};

ワイ「サイコロ回転関数ができたで！」
ワイ「あとは指示内容が入った配列、つまりshijiをforEachメソッドで回してサイコロの面を更新していけばエエねん」
ワイ「サイコロを動かすたびに変数masuに数値を加算していくのも忘れずに、やな！」
shiji.forEach(houkou => {
    saikoro_eyes = kaiten_functions[houkou](saikoro_eyes);
    //サイコロの目の状態を更新。
    //引数 houkou には""前""か""後""が入っていて、
    //前回転か後ろ回転の関数が実行される。

    masu += saikoro_eyes[""上""];
    //回転後に出ている目（サイコロの上の面）の数値をmasuに加算。
});

ワイ「最後に、合計で何マス進んだかを出力や！！！」
console.log(masu);

ワイ「どや！」

結果

結果：正解です

ワイ「よっしゃ、次はレベル2に行ったるで！！！」

レベル2：サイコロ転がし（複数編）

「レベル1:サイコロ転がし」の複数編です。
サイコロが5個に増えます。
サイコロの目の初期状態は、5個それぞれ異なります。
指示も5個分あたえられます。
コマも5つになります。
指示通りにサイコロたちを回転させ、それぞれのコマが何マス進んだか、
5回出力してください。

ワイ「！？」
ワイ「思わずドラゴンボールみたいな声出してもうたわ」
ワイ「面倒臭そうやな・・・とにかく、試験開始！と」

やってみる
ワイ「お、今回もデフォルトでなんかコードが入力されとるで」
const saikoro_infos = [
    {
        eyes: {""上"": 1, ""下"": 2, ""前"": 3, ""後"": 4, ""左"": 5, ""右"": 6},
        shiji: [""前"", ""前"", ""後"", ""後"", ""前"", ""後""]
    },
    {
        eyes: {""上"": 2, ""下"": 3, ""前"": 4, ""後"": 5, ""左"": 6, ""右"": 1},
        shiji: [""後"", ""前"", ""後""]
    },
    {
        eyes: {""上"": 3, ""下"": 4, ""前"": 5, ""後"": 6, ""左"": 1, ""右"": 2},
        shiji: [""後"", ""後"", ""前"", ""後""]
    },
    {
        eyes: {""上"": 4, ""下"": 5, ""前"": 6, ""後"": 1, ""左"": 2, ""右"": 3},
        shiji: [""前"", ""後"", ""前"", ""後"", ""前""]
    },
    {
        eyes: {""上"": 5, ""下"": 6, ""前"": 1, ""後"": 2, ""左"": 3, ""右"": 4},
        shiji: [""前"", ""前"", ""前"", ""後"", ""後"", ""前"", ""後""]
    }
];

ワイ「おお、今回は5個分のサイコロの目と、それぞれどう回転させるかの指示の情報がsaikoro_infosいう定数に入ってんねやな」
ワイ「えーと、それぞれのサイコロを指示通りに回転させて・・・そのとおりに、それぞれのコマを進めていくには・・・」
ワイ「めんどくさ・・・」
ワイ「わけわからんくなってきたわ・・・」

現実逃避
ワイ「もっとこう、それぞれのコマ君たちが自分で指示書を読んで、サイコロを動かして、出た目のぶんだけ進んでってくれたらええのにな・・・」
ワイ「コマ君たちは自分が今何マス目におるか、自分のサイコロが今どっちを向いてるか、みたいな情報を記憶してくれてんねん」
ワイ「ほんで、サイコロを振る、とか、進む、とかいう技をもってんねん」
ワイ「そういった便利なコマオブジェクトを5個作ってくれるオブジェクトメーカーみたいなんがあったらええのにな・・・」
ワイ「・・・」
ワイ「そんな便利な機能あるわけないか・・・」

クラスを使おう
ワイ「！！」
ワイ「そうや、クラスや！」
ワイ「たしか、クラスを使えばオブジェクトの持つべき機能を定義して、そこからおんなじ種類のオブジェクトを何個も量産できんねや！」

どんなクラスを作ろう
ワイ「ワイのイメージでは、コマ君が5体おんねや」
ワイ「コマ君たちはそれぞれ1個ずつサイコロと指示書をもってんねや」
ワイ「ほんで、指示書のとおりにサイコロを回転させて、出た目のぶんだけ毎回マスを進むんや」

Komaクラスを作ってみる
ワイ「ほんならまず、コマのクラスを作るで」
ワイ「・・・初めてやから、どう作ってええか分からへん・・・」
ワイ「ワイは、何を作りたいんや・・・コマ君を作って、どんな事をさせたいんや・・・」

まずコマ君をどう動かしたいかを考えてみる。
ワイ「コマ君を5体生成して、それぞれ自分の指示書を見ながらサイコロを振って、進んでって欲しいねや」
ワイ「イメージだけでコードにするとこんな感じや」
//サイコロと方向指示の情報を元に、5体のコマ君が入った配列を生成。
const komas = saikoro_infos.map(saikoro_info => new Koma(saikoro_info));

//コマ君たちをforEachで回す。
komas.forEach(koma => {
    //コマ君は指示書に書いてあるぶんだけ、
    koma.shiji.forEach(houkou => {
        koma.saikoro_kaiten(houkou);
        //サイコロを回転させ、

        koma.susumu();
        //マスを進む。
    });
    console.log(koma.genzai_masu());
    //進み終わったら、現在のマスを出力。
});

ワイ「こんな感じでコマ君を使いたいんや」　

Komaクラスを作ってみる（再）
ワイ「コマ君にどう動いてもらいたいかを考えたら、少しは要件が分かってきたで・・・」
ワイ「クラスにはプロパティとメソッドがあんねや」
ワイ「プロパティいうんはコマ君についての情報や状態、つまりコマ君が持ってるもんを格納するためのもんや」
ワイ「コマくんが持ってるのは・・・」

サイコロ
指示書
何マス目にいるかの情報

ワイ「こんな感じやな」
ワイ「あとメソッドも作らなあかん」
ワイ「メソッドいうんは、コマ君のもってる技や」
ワイ「今回必要な技は・・・」

サイコロを回転させる
出た目の文だけマスを進む
いま何マス目にいるかを答える

ワイ「こんな感じやな」
ワイ「それをコードにすると・・・」
class Koma{
    //プロパティはコンストラクタの中で設定する。
    constructor(saikoro_info){
        this.shiji = saikoro_info.shiji;
        //方向指示の情報をshijiというプロパティに格納。

        this.saikoro = ""サイコロの情報（デフォルトの目の状態とか）"";
        //サイコロの情報をsaikoroプロパティに格納。

        this.masu = 0;
        //何マス目にいるかを保存しておくプロパティ。
    }

    //サイコロを回転させるメソッド
    saikoro_kaiten(houkou){
        this.saikoro.kaiten(houkou);
        //自身の持つサイコロに「回転せえや！」と指示。
    }
    susumu(){
        this.masu += this.saikoro.get_eye();
        //自身の持つサイコロに「いま出てる目を教えてくれや！」と指示し、
        //教えてもらった数の分だけマスを進む。
    }
    output_masu(){
        console.log(this.masu);
        //現在のマスの数値を出力。
    }
}

ワイ「こんな感じか・・・なんや、サイコロはどうやってプロパティにすればええねん」
ワイ「サイコロは、目の情報を持ってて、あと前回転と後ろ回転ができんねん・・・」
ワイ「！！」
ワイ「サイコロも、自分に関するデータと、技みたいなもんを持ってんねや・・・」
ワイ「自分に関するデータ＝プロパティ、技＝メソッドや！」
ワイ「サイコロもクラスにして、サイコロオブジェクトを生成すればええねや！」

Saikoroクラスを作ってみる
ワイ「Saikoroクラスは・・・」
saikoro = new Saikoro(saikoro_info.eyes);

ワイ「こないな感じで、サイコロの目の情報を渡したら、回転機能を備えたサイコロオブジェクトが返ってくる・・・」
ワイ「そんな感じにしたいねや」
ワイ「ということは・・・」
class Saikoro{
    constructor(saikoro_eyes){
        this.eyes = saikoro_eyes;
        //引数でもらった「サイの目のデフォルト位置」を記憶。

        this.func_names = {
            ""前"": ""mae_kaiten"",
            ""後"": ""ushiro_kaiten""
        };
        //前と後、それぞれの関数が実行されるように
        //関数名の連想配列を作成。
    }

    //サイコロ自身を回転させるメソッド
    kaiten(houkou){
        const func_name = this.func_names[houkou];
        //houkou（方向）が前ならmae_kaiten、
        //後ろだったらushiro_kaitenがfunc_nameにセットされる。

        this[func_name]();
        //mae_kaitenまたはushiro_kaitenメソッドを実行。
    }

    //前回転メソッド
    mae_kaiten(){
        const new_saikoro_eyes = {};
        new_saikoro_eyes[""前""] = this.eyes[""上""];
        new_saikoro_eyes[""上""] = this.eyes[""後""];
        new_saikoro_eyes[""後""] = this.eyes[""下""];
        new_saikoro_eyes[""下""] = this.eyes[""前""];
        new_saikoro_eyes[""左""] = this.eyes[""左""];
        new_saikoro_eyes[""右""] = this.eyes[""右""];
        this.eyes = new_saikoro_eyes;
    }

    //後ろ回転メソッド
    ushiro_kaiten(){
        const new_saikoro_eyes = {};
        new_saikoro_eyes[""後""] = this.eyes[""上""];
        new_saikoro_eyes[""下""] = this.eyes[""後""];
        new_saikoro_eyes[""前""] = this.eyes[""下""];
        new_saikoro_eyes[""上""] = this.eyes[""前""];
        new_saikoro_eyes[""左""] = this.eyes[""左""];
        new_saikoro_eyes[""右""] = this.eyes[""右""];
        this.eyes = new_saikoro_eyes;
    }

    //いま出ている目を教えてくれるメソッド
    get_eye(){
        return this.eyes[""上""];
    }
}

ワイ「Saikoroクラスはこないな感じや！」

Komaクラスに戻る
ワイ「Komaクラスのコンストラクタの中で、そのコマ君の持つサイコロの情報を入れとる部分があったな」
this.saikoro = ""サイコロの情報（デフォルトの目の状態とか）"";

ワイ「このsaikoroいうプロパティに、Saikoroクラスのインスタンスを」
ワイ「ぶち込んでやったらええねん」
this.saikoro = new Saikoro(saikoro_info.eyes);
//サイコロオブジェクトを生成して、saikoroプロパティに格納。

　ワイ「こうや！」

提出コード
const saikoro_infos = [
    {
        eyes: {""上"": 1, ""下"": 2, ""前"": 3, ""後"": 4, ""左"": 5, ""右"": 6},
        shiji: [""前"", ""前"", ""後"", ""後"", ""前"", ""後""]
    },
    {
        eyes: {""上"": 2, ""下"": 3, ""前"": 4, ""後"": 5, ""左"": 6, ""右"": 1},
        shiji: [""後"", ""前"", ""後""]
    },
    {
        eyes: {""上"": 3, ""下"": 4, ""前"": 5, ""後"": 6, ""左"": 1, ""右"": 2},
        shiji: [""後"", ""後"", ""前"", ""後""]
    },
    {
        eyes: {""上"": 4, ""下"": 5, ""前"": 6, ""後"": 1, ""左"": 2, ""右"": 3},
        shiji: [""前"", ""後"", ""前"", ""後"", ""前""]
    },
    {
        eyes: {""上"": 5, ""下"": 6, ""前"": 1, ""後"": 2, ""左"": 3, ""右"": 4},
        shiji: [""前"", ""前"", ""前"", ""後"", ""後"", ""前"", ""後""]
    }
];

class Saikoro{
    constructor(saikoro_eyes){
        this.eyes = saikoro_eyes;
        //引数でもらったサイの目のデフォルト位置を記憶。

        this.func_names = {
            ""前"": ""mae_kaiten"",
            ""後"": ""ushiro_kaiten""
        };
        //前と後、それぞれの関数が実行されるように
        //関数名の連想配列を作成。
    }

    //サイコロ自身を回転させるメソッド
    kaiten(houkou){
        const func_name = this.func_names[houkou];
        //houkou（方向）が前ならmae_kaiten、
        //後ろだったらushiro_kaitenがfunc_nameにセットされる。

        this[func_name]();
        //mae_kaitenまたはushiro_kaitenメソッドを実行。
    }

    //前回転メソッド
    mae_kaiten(){
        const new_saikoro_eyes = {};
        new_saikoro_eyes[""前""] = this.eyes[""上""];
        new_saikoro_eyes[""上""] = this.eyes[""後""];
        new_saikoro_eyes[""後""] = this.eyes[""下""];
        new_saikoro_eyes[""下""] = this.eyes[""前""];
        new_saikoro_eyes[""左""] = this.eyes[""左""];
        new_saikoro_eyes[""右""] = this.eyes[""右""];
        this.eyes = new_saikoro_eyes;
    }

    //後ろ回転メソッド
    ushiro_kaiten(){
        const new_saikoro_eyes = {};
        new_saikoro_eyes[""後""] = this.eyes[""上""];
        new_saikoro_eyes[""下""] = this.eyes[""後""];
        new_saikoro_eyes[""前""] = this.eyes[""下""];
        new_saikoro_eyes[""上""] = this.eyes[""前""];
        new_saikoro_eyes[""左""] = this.eyes[""左""];
        new_saikoro_eyes[""右""] = this.eyes[""右""];
        this.eyes = new_saikoro_eyes;
    }

    //いま出ている目を教えてくれるメソッド
    get_eye(){
        return this.eyes[""上""];
    }
}

class Koma{
    constructor(saikoro_info){
        this.shiji = saikoro_info.shiji;
        //方向指示の情報をshijiプロパティに格納。

        this.saikoro = new Saikoro(saikoro_info.eyes);
        //サイコロオブジェクトを生成して、saikoroプロパティに格納。

        this.masu = 0;
        //何マス目にいるかを保存しておくプロパティ。
    }

    //サイコロを回転させるメソッド
    saikoro_kaiten(houkou){
        this.saikoro.kaiten(houkou);
        //自身の持つサイコロに「回転せえや！」と指示。
    }
    susumu(){
        this.masu += this.saikoro.get_eye();
        //自身の持つサイコロに「今の目を教えてぇな！」と指示し、
        //教えてもらった数の分だけマスを進む。
    }
    genzai_masu(){
        return this.masu;
        //現在のマスの数値を返す。
    }
}

//サイコロと方向指示の情報を元に、5体のコマ君が入った配列を生成。
const komas = saikoro_infos.map(saikoro_info => new Koma(saikoro_info));

//コマ君たちをforEachで回す。
komas.forEach(koma => {
    //コマ君は指示書に書いてあるぶんだけ、
    koma.shiji.forEach(houkou => {
        koma.saikoro_kaiten(houkou);
        //サイコロを回転させ、

        koma.susumu();
        //マスを進む。
    });
    console.log(koma.genzai_masu());
    //進み終わったら、現在のマスを出力。
});


結果

結果：正解です

ワイ「よっしゃ！おおきにやで！」

晩酌タイム
ワイ「もう15時やないか・・・酒飲む時間やで」
ワイ「レベル2の問題、ホンマむずかしかったなぁ・・・」
ワイ「サイコロが5個になったときは死のうかと思ったわ・・・」
ワイ「サイコロの目の情報も5倍、どう回転させるかの指示も5倍、マスの位置の管理も5倍」
ワイ「こんなん、普通に配列とかで管理してたら、訳わからんくなるで」
ワイ「クラス様様やで」

クラスを見直した
ワイ「クラスって、書き方だけは勉強してたんやけど今まで何に使ってええか分からへんかったんや」
ワイ「人間クラスから生成した太郎オブジェクトが、自己紹介メソッドで」
ワイ「こんにちは、僕は太郎です！」
ワイ「って挨拶できる感じなのは知ってたんやけど」
ワイ「太郎があいさつして何がうれしいねん！」
ワイ「太郎は何を便利にしてくれんねん！」
ワイ「と思ってたんや」
ワイ「でも、便利なもんやったんやな」
ワイ「そもそもクラスって誰が考えたんやろ・・・」
ワイ「ググってみよか・・・」

クラスとオブジェクトの歴史
ワイ「クラスとオブジェクトっていう機能は、もともとSimulaっていう言語で初めて実装されたんか」
ワイ「Simulaはその名の通り、現実世界をシミュレート（simulate）するために作られた言語なんか」
ワイ「せやから、色んな物体の状態・情報・特性・・・」
ワイ「その物体がどんな動きをするか・・・」
ワイ「そういう事を定義できるクラス構文が生まれたんやな」
ワイ「ほー、そんときはまだオブジェクト指向いう言葉すら無かったんか」
ワイ「Simulaハンパないな」

なぜクラスを使うと分かりやすいか
ワイ「もともと現実世界をシミュレートするために作られた構文だけあって、人間が世界を認識するときの考え方に基づいてんねんな」
ワイ「せやから、人間にとって理解しやすい書き方になんねん」
ワイ「サイコロたちの目を格納する配列、それぞれのコマの位置を格納するための配列・・・ってそれぞれの情報ごとに管理するんやなくて、」
ワイ「コマ君が、サイコロを持ってんねん」
ワイ「自分のための指示書も持ってんねん」
ワイ「そんで、サイコロを回転させて、マスを進んでくれんねん」
ワイ「そういったイメージを、割とそのままの形でコードに出来んねん」
ワイ「せやから、そのイメージを頭の中にずっと置いておく労力が減るねん」
ワイ「ワイのようなメモリの少ない脳みそでもプログラミングが捗るねんな」

クラスを使うとコードも読みやすく
ワイ「せやからコード自体も読みやすくなるな」
ワイ「コマに関する情報や動作はKomaクラスの中に書かれる」
ワイ「サイコロに関する情報や動作はSaikoroクラスの中に書かれる」
ワイ「コードが知らんうちにグループ分けされるんや」
ワイ「ワイのようなザコーダーやと、自分の書いたコードでも」
ワイ「コマの位置を格納した配列はどれやったけ〜？」
ワイ「ってなんねんけど、」
ワイ「Komaクラスの中にまとまってれば安心や」

ガンガン使っていこう・・・？
ワイ「せっかくクラス構文の使い道っぽいのに気づくことができたんやから、どんどん使っていくで！」
ワイ「でも普段Webサイト作る時に、どんな場面でJavaScriptのクラスを使うたらええんやろな・・・」
ワイ「ブラウザにはDOMっちゅう機能があって、ページ内のタグ1たちをJSで操作できるようにしてくれてんねん」
ワイ「だから自分でクラスを考えなくても、タグたちを好きなように操作できんねん」
ワイ「タグたちを、いろんなプロパティやメソッドを持ったオブジェクトとして使えんねん」
ワイ「DOMもオブジェクト指向に基づいて作られてたんやな・・・」
ワイ「Document Object Modelいうくらいやもんな」
ワイ「ほんならワイがクラス書く場面ないやん」
ワイ「ないやん・・・」




正しくは「要素」ですね。 ↩



",False,https://qiita.com//jzmstrjp/items/a8c7c89ca45ce5d1635f
"なんかPC漁ってたらスクショが出てきた。

多分検索ボックスに""><s>aaaaa</s>を突っ込んだだけ。
他にも50音検索とランキングページとかでXSSができたみたい(汚いメモ書きしか残ってない)。
XSSフィルタを通過できるDOM Based XSSもあったぽい。
なんかに使えないかなと思ってる間に漫画村は閉鎖された。
",False,https://qiita.com//u48/items/aa7c35782fcd1a6620ff
"2018年7月ごろまでココナラにあった反射型のXSS。
スクショとかは撮っていなかった。

https://coconala.com/categories/191?layout=""><script>alert(1)</script>
https://coconala.com/categories/191?pro_priority_display=""><script>alert(1)</script>

6月にXSSを見つけて、他のココナラについての問い合わせと一緒にサポートに連絡した。
XSSについては関連部署に伝えるとのことだったが、報告してから丸1ヶ月間XSSはそのまま残っていた。
サポートにXSSを伝えても無駄だと思ったので、Twitterでココナラの取締役を見つけてDMでXSSを報告した。
たしかそれから1週間くらいでXSSは直ったと思う。
ついでにココナラのクソな仕様について不満を書いたらそっちは無視された。
",False,https://qiita.com//u48/items/bddb0ffe77691800fceb
"女子向けアプリを作るとうことで勉強を兼ねて記事を10本執筆してみました。ようやく終わった。
目標としていた期間よりもだいぶ伸びでしまいましたが、幾つか学びはあったかなと思います。

学んだこと
・うまくいっているアプリをトレースする（デザインを真似してみる？）ことで気づきが得られた
→例えば食べログ。
今まで行った、行きたいという項目はただ単にユーザがお店を見つけ易いようにするだけだと思っていました。ただ、この機能はおそらく飲食店に向けに広告の前後でどれだけ効果があったか（行った、行きたいと思う人が増えたか）を計測するためにつけているように思えます。もちろん私の推測ですが。
・デザイン全般の超初歩的な知識を得た
→ノンデザイナーズ・デザインブックの知識
・Sketchちょっと使えるようになった
→これが実務的な面では大きかったかも。今後の開発に向けてXcodeとの連携方法を見ていきたい。
・女性ニーズの細かさと雑多さ
→女子向けのアプリはファッション、グルメとか自分からしたら関連性がよくわからないものが一緒に並べられている。また口コミも同世代の意見を参考にしたいというのは、女性の方がライフスタイルが変わりやすく、参考になる情報が変化しやすいからなのかなとも思った。
",False,https://qiita.com//GETYAMAME/items/40807ba437161c7b10cb
"

この記事について
大学の研究室の後輩に向けて書いたドキュメントに加筆したものです。プログラミング超初心者の人が急に巨大なプログラム（数万行～）のデバッグをやらされることになったとき1に手引きになればと思い、公開しました。

背景となる問題意識
あまりにポエムなのでブログにまとめてます。
http://bluepost69-tech.hatenablog.com/entry/2017/11/16/191631
非情報系ではプログラミングの授業でデバッグのやり方を教えないので、問題意識を持っています。

想定している環境

SSHでCUIしか使えない

実際の環境としては、SSH接続した先でfortran+MPIコードをデバッグするという形です。リモートで編集、実行まで行うのでIDEは使えないという状況です。スパコンでシミュレーションをやる業界なら割とよくあると思います。

大原則：プログラムの気持ちになる
まず「何が起こっているのか」「どこで止まっているのか」を調べる。

エラーメッセージをよく読む
デバッグオプションをつけてコンパイルしてみる
適当なところにwrite文を挿入して実行してみる。


怪しい変数はwrite文で出す
write文の内容が出力された箇所までは計算が終わっている。



（write文は適宜言語ごとに標準出力に出力するコマンドに置き換えてください。Cだとputsやprintf、pythonだとprintなど）
特にエラーメッセージを読まない人は本当に多いです。「No such file」みたいな読めばわかるエラーを人に聞くのは恥ずべき事だと思いましょう。
Fortran runtime error: Cannot open file 'hoga.txt'

「先輩！なんか意味不明なエラーが出るんですけど！」
「cannotは中学1年生で習う英語だよ？中学校からやり直したら？」

おおまかな場所を見つけたら、プログラムの気持ちになるですよ！

実行する計算を追ってみる。
要所要所で変数をwrite文で出力する。

write文はプログラムに自身の状態を出力させる強力なツール。（ただしログが大きくなるのでデバッグが終わったら削除しておく。そのために変更箇所には目印をコメントしておくとよい。）

バグったな？と思ったら...代表的なバグとその対処法

計算が回らない


コンパイルエラー
実行時エラー


計算は回るが終わらない


デッドロック
無限ループ


計算結果が明らかにおかしい


計算が回らない

コンパイルエラー
コンパイル時にエラーが出る場合、コンパイラが直しきれない明らかな文法誤りがある。
一か所の誤りがコンパイルされないために芋づる式に誤りでない場所も指定されることがあるので、エラーメッセージをよく読んで原因を見極める。たいてい最初の方に根本原因がある。
読み切れないときは
gfortran smp.f90 > compile.log
のようにリダイレクトしたり、
gfortran smp.f90 | less
とlessにパイプして読む。

実行時エラー
原則としてコンパイルエラー同様、エラーメッセージをよく読む。ただしOSからのシグナルの場合、あまりあてにならないこともある。
SIG_SEGVが出力される場合、セグメンテーションフォールト(Segmentation fault）と呼ばれるバグに該当する。これは許可されていないメモリを参照した場合に出るエラーで、fortranの場合はほとんどが配列の引数の間違いに起因する。（1～99がallocateされた配列x(1:99)でx(100)と指定するなど）。そんな馬鹿なことはないだろうと思うかもしれませんが、変数を添え字に入れてたりすると変数が意図しない値になったりするのはよくあるんですよ。
また、
・0での割り算を実行しようとした
・平方根sqrtの中身がマイナスになった
といった場合もSIG_SEGVになることがある。

計算は回るが終わらない
デッドロックと無限ループが代表的。並列数を1にしてバグが治ればデッドロック。

デッドロック
（fortranでは）MPI特有の現象。プロセスが1対1で通信する場合、受け取る側がMPI_RECVを実行し送る側がMPI_SENDを実行しなければならないが、交互に通信する場合で両者がMPI_RECV状態になり計算が止まってしまうことがある。
対策としてはMPI_SENDRECVに書き換えるか、順番を入れ替える。これについては別記事で書きたい。

無限ループ
原則として無限ループで条件を満たした時exitするようなコードは組まない。exitが何らかの理由で実行されなかった場合や条件が満たされなかったとき、デバッグがややこしくなる。
やむを得ずwhileループを使う場合などは、上限を指定したループで条件を満たさないときにexitする形に変える。

smp_ng01.f90
!これはダメな例
do while(flag)
! 処理
end do



smp_ok01.f90
do i=1,N_LOOP_MAX
! 処理
   if(.NOT. flag) exit
   if(i== N_LOOP_MAX)
      ! まともな言語なら例外を投げるべき
      write(6,*) ""Error: loop not ended!""
   end if
end do



計算結果が明らかにおかしい
計算過程を一つ一つ見る必要がある。

変数が思い通りの結果になっているか（0やNaNなど変な値が代入されていないか）
関数の引数がまちがっていないか（タイプミスで似た名前で宣言された別の変数が入っていることがある）

チェックしたい変数をwrite文で出力してみる。

デバッグに使うツール

grep
変数やサブルーチンがどのモジュールに書かれているのかを調べるのに使う。
例：fortran90ファイルの中で""hoge""がどのファイルに書かれているかを調べる
grep -n ""hoge"" *.f90

emacsの検索機能
C-sで開いているファイルで検索をかけることができる。

diff
2つのファイルを比較してどこが異なるかを出力する。作業結果をまとめるとき、バージョン間での比較に便利。
例：A.f90とB.f90の変更箇所を調べる
diff A.f90 B.f90

最後に
このレベルのデバッグをろくに教えもしないで、配属間もない学生にコード管理をやらせる研究室はもれなくブラックなので、覚悟しましょう2。




信じられないことに、理系の理論シミュレーション系だと人不足でしばしばこういうことがあります。 ↩


特にB4ならその研究室に進学することは考え直したほうがいいです。 ↩



",False,https://qiita.com//Bluepost59/items/545ace5e7a63934a22cf
"

あたりまえ
ライブラリを入れるというのは闇雲に行うべきではなく、
入れようとしているものが本当にこれなのかを問うて下さい。


問おう。

1つ目
「今、自分は何を解決したいですか？」

2つ目
「その(ライブラリ|フレームワーク)は、何を解決するために存在していますか？」

それだけ？
2つだけ問いかける。ほんとこれだけ。これだけなのに、
ブチ込むライブラリと自分が本当に解決したいこととのミスマッチを防ぎ、
プロジェクトが重厚になるのを遅らせる事ができます。
自分でコード書けば済んだり、もっと小さな単一の問題を解決するライブラリで済んだり。

あとがき
単発のクローラーを作るためだけのためになぜかDjango(フルスタックWebフレームワーク)を
入れようとしている人がいて、ちょっとそれはズレてるよ…となったので。
短くてコードもない記事ですが、起こしました。
",False,https://qiita.com//yuukive/items/ec97be44777b9c0201b1
"

Qiita運営に望む事

ユーザーブロック機能の実装
記事に対する「いいね」だけでなく「低評価」も作成するべき


レーティングが低い記事はno followとするべき


ユーザーごとにサブドメイン化
通報に敏感になるべき

玉石混在で記事の量が増えすぎてサイト内を探す場合、情報フィルターでも掛けないと不正確かつ、未検証で理解の浅い「クソ記事」を読まされ時間が無駄。また、浅い理解で低品質な記事を量産するユーザーは傾向が見受けられるため、ブロックする機能を要望する。低品質な記事にはNOを明示できるようにするべき。また大量に非ブロックを受けたものに対し、警告を行い行動を改めるように促すべきだ。
Qiitaはそろそろ、自身のサイトで量産される記事が検索結果を汚染してネット検索の効率性を下げている事実を認めるべき。理想とかけ離れた状態になっているので自浄作用を働かせる仕組みを導入するべき。
また運営は通報に対する対処が遅い・不透明・基準不明であり、技術情報のインフラを担うつもりならも少し対応をまともにした方が良いと思う。いつまでも自分たちがベンチャーだとか思ってるなら大きな間違いである。
SNS的用をを含むインフラとして機能するならTwitter・Facebook程度のフィルタリング機能はしかるべきなのに今の今まで何も対策されていないことに非常に疑問を感じると共に責任を果たしていないと思う。

Qiitaユーザーに望む事

釣りタイトルをつけない
引用や参考資料は必ず明示する
ちょっと理解した程度ですぐ記事を書かない
同じ内容で少し違うだけの記事は書かない
互助会でいいねしない
ポエムが多すぎ、ブログでやりなさい

昨今、ゴミ記事エントリが人気だったり、侍エンジニア塾を叩いて喜んでるみたいだけど、Qiitaは全く人の事言えない立場にある事を理解するべき。一部に非常に質の高い記事がある事は認めるが、Qiitaは総じて低品質であり記事の質の低さは底が抜けてる。
また、完全に丸パクリの転載記事に引用元も明記しないで少し手直ししただけでアップしなおすとか本当に辞めて頂きたい。
あとQiitaは、「はてブ」では無いため互助会でいいね等の行為は一切控えてほしい。ノイズにしかならないしQiitaのブランド（？）を完全に既存してる。内容と技術でいいねを判断したほうがよい。
",False,https://qiita.com//CriticismQiita/items/96eae77f52441828da51
"

モチベのおはなし
ここのQiitaを読んでいる、または記事を投稿している方たちのほとんどはどこかの会社に勤める サラリーマン だと思っています。
わたしもその会社勤めのサラリーマンなのですが、お仕事は プログラムを書いたり 、 テストをしたり 、 時にはインフラをいじったり 、 仕様書を作成したり 等、多岐にわたっていると思います。
全てのタスクが楽しいなんて人は珍しく、ほとんどの人はやりたくないタスク、好きなタスクがあると思います。 
どのタスクも楽しくない人は転職か独立をしましょう。
私は今年の夏の終わりに転職をして新しい環境での生活をスタートしたのですが、ふと モチベ の重要性について考えることがあったので、記事にしたいと思います。
技術のことは書かないので興味のない方はブラウザバックしてくださいね。
では書き綴ります。

お仕事としてのモチベ
導入にも書きましたが皆さんはお仕事に対してのモチベーションはどうでしょう？？
モチベーションが高く保てるのは仕事の環境が変わった時、新規プロジェクトが始まった時などありますが、常にモチベーションを高く保つのは　至難の業 です。
そこで私が意識を保つために行っていることを書き出してみたいと思います。

お仕事に直結していない本を読んでみる。
海外のブログを読んでみる。
個人開発をして個人用のプロダクトを持ってみる。
こうして記事を書いてみる。
起業する案について考えてみる。

こんな感じでしょうか。
まず1つ目から説明していきます。

お仕事に直結しない本を読んでみる
おいおい、余裕あるな。。と思われたあなた！！
余裕は自分で作るものです。私には自然には生まれませんでした。
本を読むことはだれでも簡単にできます。
皆さんもこの業界に入ったころは本を読んで勉強をしたことでしょう。
初心忘るべからず ですよ！！！
ポイントは お仕事に直結しない という部分です。
例えば

普段はバックエンドの業務を行っている方なんかはデザインの勉強をしてみるとか
インフラを専門でやっていてShellは書けるけどWebの開発は疎かになっている方とは

業務では生きないから勉強しない！なんてもったいないです！！！ 
業務で取り扱わない範囲を少し知っているだけで視野は広くなると信じて私は勉強をしています笑
次に2つ目は

海外のブログを読んでみる
これはおすすめできます。
おすすめは英語で読むことです。
英語力をつけるために読んでいましたが、意外と読めるものです。
おすすめは

Reddit
TechCrunch
The Verge

このへんでしょうか。
鉄板だと思いますが、読みやすいと思います。
是非、英語も一緒に勉強してしまいましょう。
3つ目は

個人開発のプロダクトを持ってみる
これは私も行っています。
私はRailsで作っていますが、自分の得意な言語、好きな言語、勉強中の言語、FW、なんでもいいと思います。
自分の好き勝手にできる環境は楽しい ですよ。
納期もセキュリティもくそもありません。
(自分に返ってきますが笑)
また、 試したいことを簡単に試すことができます 。
これは、非常に大切なポイントです。
休みにゲームをするよりも私は楽しく感じています笑。
さて4つ目は

記事を書くこと
最近Qiitaで話題になっていた記事で 「くそ記事を書くな」 、とか 「記事を書くことを習慣化しろ」 とかを見かけましたが、
私の個人的な意見としては 記事は書くべき です。
アインシュタインの名言を引用しますが
If you can’t explain it to a six year old, you don’t understand it yourself. 
6歳の子供に説明できなければ、理解したとは言えない 。という意味です。
私は 記事にかけないような理解では真に理解できているとは言わない 。
といい感じに解釈してみました。
つまり、 アウトプット していくことで、 理解が深まる と信じて私は記事を投稿しています。
別に読んでもらうことがすべてではありません。
自分用に記事を書く、思いを書きなぐる、なんでもいいです。
私も、技術の記事を書いて編集リクエストをいただいたことがあります。
このように、間違った知識を正してくれる優しい方がたくさんいますので記事はたくさん書いていくべきです。
くそ記事と良い記事を見分けるのは個人の情報リテラシの問題ですし、記事が多くなってしまって困るのは運営の方なので何も気にする必要はありません。
(運営の方、ごめんなさい)
さいごに

起業する案について考えてみる
これは単純に考えているだけで楽しいです。
アイデアは技術の知識量にもよりますが必ずしも依存しない と思います。
なので、アイデアがたくさん浮かんでくるのは良いことです。
是非、たくさんのアイデアを共有してみてください。

最後に
と、こんな感じで書き出してみましたがいかがでしょうか？
拙い文章ですが読んでいただきありがとうございます。
私自身、 平凡なプログラマ だと思っていますが。
平凡からスキルアップしていくためには何らかのアクションが必要だと思い、行動しております。
皆さんは普段、スキルアップのためにどのようなことをしていますでしょうか？ 
そんな問いを投げかけられたら私は幸いです。

最後に宣伝ですが、先にも書きましたが、私は個人開発をしております。
Railsできるよ！って方、またはやってみたいという方、是非、一緒にやりましょう笑
私もRuby, Rails歴は浅く初心者なので共に成長しましょう！！
Slack等で連絡取れたら喜びますです笑 
では皆さん、良いコーディングライフを！！

参考

http://iyashitour.com/archives/21439
「ゴミ記事は書くな」はゴミ記事を増やすことにつながるから避けるべきでは？
日本人ITエンジニアの90％に記事を書いてほしくない
エンジニアは全員技術記事を書くことを習慣化した方がいいぞ

",False,https://qiita.com//yoru722/items/6668f0720820da654ec7
"

プログラミング言語とソフトの関係性について
　僕は趣味でいろいろな言語に触れている。それぞれにいろいろな考えがあり、作った人の考えがどうやって言語化されているのかに興味があるからだ。Pythonのガッチリとした設計思想やRubyの柔軟性を重視した設計、そしてLispという関数型の元祖と呼ばれる言語。他にもいろいろと触って、調べてみると人間がどうやって世界を記述しているかが興味深く見れる。
　だが、どれだけ設計のよい言語であっても流行らないこともある。それの最たる例がLispであろう。Lispはできた当時から、優れた言語であると尖った技術者たちから太鼓判を押されているにもかかわらず、知名度はとても低い。（これは、経営者や一般的な技術者などの認知度だ。）
　なぜ、このような事態が起こっているのであろうか？
　僕はソフトととの関係が深いと考えている。これを考える時に必要となってくるのが、日本のゲームハードとソフトの関係を読み解いてくいくと関係性がわかってくるだろう。

セガと任天堂を例にとって
　よくわかるのは、初期のゲーム市場においてのゲームハード戦争だろう。セガがSG-1000と任天堂のファミコンの戦いがあるだろう。SC-3000の設計は簡単に言うと「超高性能ハード」を作ることだった。それに対してファミコンは「安くていいもの」という設計だった。それに加えて、ファミコンには「マリオ」というキラータイトルがあり、この戦争の軍配は任天堂に上がった。
　このことから、考えられることは必ずしもカタログスペックや実際のマシンパワーの差が戦力の決定的な差でないと言うことがわかる。

では、なぜこうなったのか。
　考えられることは、利用者がすべて「ヲタクでない人」を相手にしているからだ。これは、一般人からしてみたら、「メモリーが10G！」「ハードディスクが大容量！」「今までにない速さが実施できます！」といってもピンとこない。これは車を一般人に売るのにスポーツカーを買わせるようなものだ。
　それなら、遊べるソフトがある場所にお客が流れる。そして、ファミコンは特殊なCPUを使っていて、開発者が理解しづらい設計だった。これを正確に読めたのは何を隠そう前任天堂社長「岩田聡」社長です！つまり、ファミコンのCPUの挙動を正確に読めるのはそのくらいの大天才でない限り、理解をするのが無理なハードだった。（あの頃のハードに高級言語が存在していなかったのも起因しているが）
　この話を一般人に話したとしてもトリビアの一つとして片付けられてしまい、購買意欲を削がれることもない。
　このように、「ゆりかごから墓場まで」よろしく、「ハードからソフトまで」というのがユーザが求めているものである。

これをプログラミング言語に置き換えると
　この構造はプログラミング言語にも言えることだ。というのも、プログラミング言語をハードに置き換えて考えてみるとなぜその言語が流行ったのかがわかる。
　この例を上げるのに最たるものはRubyだろう。Rubyは非常に優れた言語だ。柔軟性が高く、さまざまな書き方ができる。だが、これだけでは一般人からは「だからなに？」と言われてしまう。そこに来たのが「Ruby on Rails」だ。これをさえつかっておけば、ウェブ全般の作業がRailsの規約どおりに書くことによってなんでも書ける。
　Pythonにも同じことが言える。Pythonは人工知能、機械学習で有名で駅前のそこそこの書店でも本が扱われているほどに有名になっている。
　このように、キラータイトル、使用用途が明確になっているものは有名になりやすくなっていることがわかる。
　一方、Lispはというと、人工知能にも使えてすごい！という評価はあるが、キラータイトルがないのが現状だ。僕の知る限りでは、Lispは宇宙やロボットに進出をしていて局地仕様の場所に重宝をされているという印象がある。だが、一般人の認知度は低い。以前、就活をしたときにLispが好きです、といったら「Lispってなに？」と言われました。
　つまるところ、みんなハードのことや言語のことなどは見てなく、重要なのは「それが何につかえるのか？」というところなのだ。

では、用途不明のものがすべて不必要なのだろうか？
　もちろんそんなことはない。むしろ重要なほどだ。一見して不必要に見えたとしてもそれはダイヤが原石の時点でどれだけ美しいかを測定するようなものだ。
　ここでも、一例を申し上げましょう。Grimoire.jsというウェブGLのフレームワークだ。これは、WebGLというブラウザ上で使う３Dのフレームワークであり、簡単にウェブGLを使うことができる仕様になっている。これができたのは一昨年くらいのころで、最初に登壇で見せてもらったときにはすごい威力だった。しかも、この作品は国からも一目置かれている存在になっている。
　だが、まだどう流通させていくのかがわかっていない。というのも、仮にブラウザ上で３Dを使えたとしても、一般的に扱われている３Dゲームをしようとするとブラウザ自体が持たないのだ。携帯機器しかわからなかったが、マックスの容量が５MBしかなかった。この容量でネイティアプリのゲームのようにしたら、クラッシュをしてしまう。
　だが、使い方によっては新しい表現の仕方も見えてくるだろう。例えば、リアルタイム通信とガベージコレクションを利用して描画ごとにリアルタイム通信をしておいて、いらないデータを捨てながら描画をすればもしかしたら、ゲームができるかもしれない。そうでなくても、新しいウェブのあり方を発見することができるかもしれない。
　言語だっておんなじだ。まだ使い方がわからないと言って切り捨ててしまうのはもったいない。僕はLispが好きで、よく書いている。だが、あまり仕事で使われているかと言われるとそうではない。仕事はまだ少ない。だが、Lispの表現方法を模索していけば何かがあるかもしれない。可能性を排除してしまっては新しい表現などできるわけがない。

参考
セガ SG-1000：
https://ja.wikipedia.org/wiki/SG-1000
ファミコン：https://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A1%E3%83%9F%E3%83%AA%E3%83%BC%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF#%E9%AB%98%E3%81%84%E3%82%B3%E3%82%B9%E3%83%88%E3%83%91%E3%83%95%E3%82%A9%E3%83%BC%E3%83%9E%E3%83%B3%E3%82%B9
Grimoire.js:
https://grimoire.gl/
任天堂ファミコン開発秘話について：
https://trendy.nikkeibp.co.jp/article/special/20081002/1019327/
CPU 6502:
https://ja.wikipedia.org/wiki/MOS_6502
CPU Z80:
https://ja.wikipedia.org/wiki/Z80

参考文献
熱血！アセンブラ入門（アマゾン）：
https://www.amazon.co.jp/%E5%A4%A7%E7%86%B1%E8%A1%80-%E3%82%A2%E3%82%BB%E3%83%B3%E3%83%96%E3%83%A9%E5%85%A5%E9%96%80-%E5%9D%82%E4%BA%95%E5%BC%98%E4%BA%AE/dp/4798051543/ref=sr_1_1?ie=UTF8&qid=1540100963&sr=8-1&keywords=%E7%86%B1%E8%A1%80+%E3%82%A2%E3%82%BB%E3%83%B3%E3%83%96%E3%83%A9%E5%85%A5%E9%96%80
",False,https://qiita.com//kumagai_games/items/778e658fd8d0d952dcfd
"あまりに当たり前過ぎて最近の優秀な若手エンジニアに伝えるのが気恥ずかしいのですが、タイピングスキルを磨くと色々良いことがあるよというポエムです。タイピングスキルを磨くことのメリットの紹介が主な内容で、具体的な学習方法についてはあまり触れません。

はじめに：タイピングスキルはいつ陳腐化するのか
本題に入る前に、タイピングというスキルがこの先どれぐらい長持ちするかを考えたいと思います。

キーボードの次のインターフェイス
Amazon EchoやGoogle Homeの一般家庭への普及などに伴い、VUI（音声ユーザーインターフェイス）という言葉を耳にする機会が増えました。1
更には、脳波でコンピューターを操作する BCI（ブレインコンピューターインターフェイス）という技術の研究も進んでいるようです。2
テクノロジーは目まぐるしく進化しているため、声や脳波でパソコンを操作して仕事する時代がいつかは訪れるかもしれません。3

私たちはいつまでキーボードを使って仕事をするのか
しかしながら、2018年現在、私の周りのほとんどのエンジニアは、キーボードをタイプしてパソコンを操作し仕事をしています。私が社会人になった約10年前もそうでしたし、初めてパソコンに触れた20年前も同様でした。
これから20年後にどうなっているかは分かりませんが、個人的な感覚として、向こう10年はキーボードが主流のままである可能性は高いように感じます。
ですので、キーボードを素早く正確にタイプする技術、すなわちタイピングスキルを磨くことは10年スパンで考えると決して損にならないはずです。

タイピングスキルを磨くメリット
ここから本題に入ります。

プログラミングに投下できる時間を増やせる
質の高いプログラムを書くためには、まとまった集中できる時間が必要です。ただ、多くの職業エンジニアは、プログラミング以外の業務にも時間を割かなければなりません。例えば、ミーティングの議事録作成や、顧客とのメールのやりとり、様々な報告書の作成などです。
タイピングスキルの向上による最大のメリットは、これらの業務を素早く終わらせて、プログラミングに投下できる時間を増やせるという点です。自己啓発書などを読んで仕事効率化の知識を身に付けることも大事ですが、タイピングスキルを上げる方が、より即効性があると思います。

プログラミングにおける「木炭」が手に入る
タイピングスキルの向上は、プログラミングという行為の質を高めることにも良い影響を与えると思います。タイピングが速い人が必ずしも質の高いプログラムを書けるという訳ではありませんが、ポール・グレアムの名著「ハッカーと画家」の第0章「メイド・イン・USA」には以下のような一節があります。

コードは、ピラミッドみたいに、慎重な計画をしてから苦労して組み立てていくものじゃない。一気に集中して素早く手を動かしながら、常に気を変えていく、木炭スケッチみたいなものだ。

シンプルで、手に馴染み、スケッチに適した道具として「木炭」が挙げられています。タイピングスキルを磨くことは、プログラミングにおける「木炭」を手に入れるための近道だと思います。

タイピングスキルを磨くことのコスパの良さ
人の記憶メカニズムは、陳述記憶と非陳述記憶の二種類に分類できます。陳述記憶とはイメージや言語として内容を想起できるもので、プログラミング言語に関する知識などが該当します。
一方、非陳述記憶の代表として手続き記憶というものがあります。Wikipediaにはこのように書かれています。

手続き記憶は簡単には言葉で説明できないことが多く、意識しなくとも使うことができる。いわゆる『体が覚えている状態』である

陳述記憶はある程度時間が経つと復習しない限りは忘れてしまうのですが、手続き記憶は一度習得してしまえば滅多なことでは忘れないという特徴があります。自転車や水泳などが手続き記憶の例として挙げられることが多いですが、タイピングも手続き記憶に該当します。
つまり、タイピングは習得までにある程度の学習時間が必要なものの、一度習得すればずっと使えるということです。これは非常にコスパが良いです。

どれ位のタイピングスキルがあれば良いのか
以下の3点を満たしていれば十分だと思います。

それなりに正確にタイプできる（誤字の頻度が低い）
それなりのスピードでタイプできる
キーボードを見ないでタイピングできる

「それなり」とぼかしているのは、絶対的な基準がある訳ではないからです。参考までに私自身を例にしますと、久しぶりにe-typingの腕試しレベルチェックを何回かやってみたところ、スコア300点台前半という結果でした。これぐらいのスコアで、職場でタイピングが速い方と言われる＆会議の全発言を書き起こす類の議事録（いわゆる逐語録）について会議中に半分以上は書き起こせるといったレベル感になります。
個人的な感覚としては、200点台後半以上のスコアであれば業務で困ることは少ないのではないかと思います。

どうやってタイピングスキルを身に付ければ良いか
無料・有料を問わず世の中には沢山のタイピング教材がありますので、楽しんで続けられそうな教材を選ぶのが良いと思います。タイピングに苦手意識がある方は、ホームポジションを意識して短い単語からステップアップしていく教材を使うのが、遠回りに見えて実は近道になると思います。
それでは、良いエンジニアライフを！




Alexaのページが分かりやすいです: 音声ユーザーインターフェース (VUI) とは _ Amazon Alexa _ アレクサ ↩


BCIが良く分かる良記事です: 【初級編; 2017年版】脳波で機械を操る！ブレインコンピュータインタフェース(BCI_BMI)って何？ - Qiita ↩


パソコンに代わる何かが台頭している可能性もあります ↩



",False,https://qiita.com//nakazax/items/aae8a5860f1ebed37e8f
"

はじめに～HRTは本当に大事。でも意識しすぎて失敗することも
最近バズった記事からHRTに関わる記事やコメントを見る機会が増え、HRTが大事と改めて意識しました。
それとともに、HRTを実践するのは難しいなと改めて感じたのでまとめてみました。
まずはHRTについてQiitaガイドラインから

HRTとは、書籍『Team Geek――Googleのギークたちはいかにしてチームを作るのか』で紹介されている「Humility（謙虚）」、「Respect（尊敬）」、「Trust（信頼）」を示す言葉です。書籍では“あらゆる人間関係の衝突は、謙虚・尊敬・信頼の欠如によるものだ”と述べられています。
Qiitaを利用する際には、このHRTを意識するよう心がけましょう。記事は記事を書いた本人ではありません。

とても素晴らしい言葉で、自分の中でこうありたいと思う理想でもあるので心に刻み付けています。
しかし、意識していても中々うまくいかないことも。そんな私の今まで体験してきたチーム開発やチーム間交渉、会議の中でのHRTに関わる失敗談をまとめたいと思います。

謙虚にし過ぎて相手の増長を生み
尊敬し過ぎて支配者を生み
信頼し過ぎてプレッシャーを生む


「Humility（謙虚）」が過度になり「Hostility（敵意）」を増長させる
謙虚に、誠意を持って、場を収めようとしても、一定数の敵意を持った方の声が大きくて収まらない場面もあります。
お互い謙虚になって大人の対応をというのが理想なのですが、「相手が攻めてこない！」とばかりに相手が徹底攻勢を見せることも。
例えばコードレビューの場、とあるコードの書き方で議論に。例えばこんなコードがあったとして、
char *bytecode=buffer;
//bytecodeを4byteづつ抜き出して中身をチェックしたい
for(int i=0;i<buffer_length;i+=4) {
    checkbyte(bytecode+i);
}

好みによってはこんな書き方をする人もいると思います。個人的にはどのアドレスを見ているかがわかりやすい上が好きだけど、こういっちゃなんですがどっちでもいいです。
//bytecodeを4byteづつ抜き出して中身をチェックしたい。とりあえずbuffer_lengthは4byte区切りだと思ってください。
for(char *current = bytecode;current != bytecode + buffer_length; current += 4) {
    checkbyte(current);
}

ここに対してAさん「(上の書き方があまり好きじゃないから)何故その書き方にしたの!?理由は?」と指摘がはじまる。
もし私がレビュー参加者ならAさんに指摘理由を聞き、Aさんとレビューイ双方が納得いく形を模索したいところですが、Aさんのこだわりが強いと
「何故私のやり方では駄目なのか！」と譲らず、結局Aさんの意見が通るという形になることが多々あります。
この例はコードの書き方なので対した問題ではないですが、以下のような責任や役割分担が絡む場面でこだわりを発揮されると収めどころにいつも困ってしまう。

会社間でのレビュー
仕様、要件面での調整

変に無理は通さず収めようとして or 無理なものは無理と伝えたやり方が下手で、火に油を注いだりしたことも多数。うーん、難しい

「Respect（尊敬）」が過度になり「Ruler（支配者）」を生む
相手を尊敬し、敬意をもって接するのは素晴らしいことです。
ただ現場によっては経験の長い人等特定の人への尊敬が行き過ぎて、「まずはこの人の意見を聞いてみないと不安」みたいなご意見番が出来上がるなんてことを見かけることがあります。
また、経験のある技術者であればそれ相応にこだわりを持っていたりしますよね。
結果、こだわりのある技術者の意見を通さないと次に進めないという構図が出来上がり、「どうすればあの人の意見に合うだろう」ということばかり考えるような、現場の支配者による統治が生まれてしまうなんてことも。
特定の人だけでなく全員に対して尊敬を払い、対等に意見を言い合える現場を作っていきたいものです。うーん、難しい

「Trust（信頼）」が過度になり「Tired（疲れた）」
これは私個人が今一番直したいところでもあるんですが、後輩や同僚に対して相手を信頼しすぎて
相手への信頼⇒期待⇒期待が過度に⇒プレッシャー
へと変異してしまい、結果(私の信頼感から生まれた)プレッシャーで相手を追い込み成長を阻害してしまうことがあります。やってしまったと後で振り返れるシーンがあったのはまだいいですが、多分気付いていない部分で意図せず人を傷つけてしまったことが沢山あるんだろうなと予想できます。
もう本当に後悔しかない。うーん、難しい

どうすればよかったのか
答えは何年生きても出ないと思いますが、自分なりの反省点をまとめます。

前提: 優しい世界になるのは全員の意識共有が必要
HRTが通用しない方も一定数います。またこちらがHRTを実践している！と思っているだけで相手にそれが伝わっていなければ意味がありません。
常に優しい世界で世界が回っている、回せているわけではないので、その前提はきちんと認識しておかないと。

「Humility（謙虚）」、時には「Heroic（勇敢）」に
声の強い方に対して譲れる部分は譲ればいいです。ですがここは譲れないという部分に対しては、自分を「Trust（信頼）」し、毅然とした態度を取る。
譲れない場面については「Heroic（勇敢）」に立ち向かう。
「謙虚に振る舞い場を収める」と「譲れない部分に対して勇敢に立ち向かう」のバランスをとってふるまうことが出来れば、全員にとってよりよいプロジェクト進行が出来るはず！

「Respect（尊敬）」、ある程度は「Reasonable（合理的）」に
尊敬・敬意は特定の人に対して贈るのではなく、全員に贈るべきものだと思います。また、みんなが尊敬する相手が常に正しいことを言っているとも限らない。
自分や他者の意見も「Trust（信頼）」して、またもし自分がRespectされる立場だったとしても「Humility（謙虚）」を忘れずに、その場その場で「Reasonable（合理的）」な判断を心がければ、皆が対等なプロジェクトになるはず！

「Trust（信頼）」、偏らないよう「Thoughtfulness（思いやり）」を
相手に信頼を寄せすぎて疲れさせてしまわない為には？まずそのような状況にならないよう「Thoughtfulness（思いやり）」を持って接する。
負担がかかっているのであれば周囲がサポートできるよう状況を「Transmit（伝え）」、周りが負担を「Take over（肩代わり）」出来るようにする。
そうですよね、支えあったり刺激を受けながら開発出来るからこそ、大変ながらもチーム開発は面白いんですよ！

モチベーション低下なら「Trust（信頼）」を「Transmit（伝える）」のもいいかも
先ほどのアンサーは「過度な期待・プレッシャー」⇒「負荷向上」⇒「Tired」というケースに対してのものですが、
「過度な期待・プレッシャー」⇒「モチベーションの低下」⇒「Tired」というケースならきちんと「Transmit（伝える）」のがいいかもしれません。
私が勝手に相手を信頼して、勝手に相手に対して期待を込めても、それが相手に伝わらなければ意味がないです。
「Transmit（伝える）」ことを意識し、「tactical（戦術的）」な仕事の振り方を考えた上で行動に移す。
相手のモチベーションを保つためにどうするべきかをしっかり考えた上で行動に移す必要がありますね。

最後に～HRT、3つで1つ・全員にシェア出来れば失敗は減らせるかも
一通りまとめて感じたことは、HRTそれぞれが単独で存在しているわけではなく、3つが地続きで繋がっている。各ポリシーの裏で別のポリシーも保つ。
また、全てのことを周りに対しても自分に対しても実践する、思いやりを持った環境を作れるよう周りと考えをシェアすることが出来れば、どれかが行き過ぎて失敗するということは減らせる気がしました。
技術者が働く上で技術的な悩みは自分の中でどうとでもなるけど、人間関係の悩みはどれだけ技術や経験を積んでも必ず付きまといます。
今後も同じような失敗に出会うことは沢山あると思うけど、参加したプロジェクトがHRTの浸透した場所になるよう、出来ることはやっていこう！
また、私自身を振り返るきっかけを作ってくださった皆さまありがとうございます。
さあ、ポエムはこのくらいにして、マイナーでもいいから技術記事を投稿するぞ！

参考(この記事を書こうと思ったきっかけ)
Qiitaの記事には絶対コメントを付けてはいけない (わけがない)
その他考えさせられるところのあったいくつかの自分 or 他の方のコメント(リンクは割愛させていただきます)
※2018/10/23
@taptappun さんのコメントを受けて「Trust（信頼）」⇒「Tired（疲れた）」の対策を「Thoughtfulness（思いやり）」に変更。
「Transmit（伝える）」は1例に移動。（アドバイスありがとうございます）


2018/10/21 追記
色々なコメントを受けてどうしても書きたいことが出てきたので追記

「Respect（尊敬）」、「Trust（信頼）」がどうしてもできない相手に対して
記事の前提でふれたように、どうやってもHRTが通用しない方、やっぱりいます。
どうやっても悪意で返してくる、どうやっても自分が「Respect（尊敬）」、「Trust（信頼）」出来ない人。
そういった人に対しても常にHRTの姿勢で対面し続けよう、そうすればいつかは改善してくれる！
なんていうのは正直理想論にしかすぎません。
じゃあどうしようというのも答えが出ませんが、一例として思ったことを挙げさせてもらいます。

課題を分離し、自分の課題としてのHRTを実践しませんか？
嫌われる勇気というアドラー心理学について書かれた本があります。
その中に出てくる課題の分離。簡単に説明するとこんな感じです。

課題には「自分の課題」、「他者の課題」があり、「自分に対する課題を解決出来るのは自分のみ」
課題を上記に分離し、「自分の課題は自分で決断を下す」、「他者がどう他者の課題に対応したかは介入しない」(出来るのは勧めることだけ)

HRTの件に対して課題を分離していくと、以下のようになりますかね(太字は自分の課題)


自分はHRTを持って相手に接する。

自分は周りにもHRTを持ってもらいたいと行動を起こす。
結果一部の人は明らかに「Respect（尊敬）」、「Trust（信頼）」を持たない振る舞いを続ける (相手の課題)

「Respect（尊敬）」、「Trust（信頼）」を持たない一部の相手に対してどうするかを選択する。

この1, 2, 4といった課題に対しては、自分でどうするか選択することが可能です。
例えばこれで4に対する選択として「一部の相手と距離を置く」という選択をしたとしても、1の自分がHRTを持っているということに矛盾はないはずです。
人類総HRT化計画は無理でも、自分の課題に対してHRTの実践を選択することならはじめられるのではないでしょうか？

""「Respect（尊敬）」、「Trust（信頼）」に値しない""人とは？
この記事でもいただきましたし、他の記事でも見かけたことがあるこの言葉。
「Respect（尊敬）」、「Trust（信頼）」に値しない人。一体どのような人のことを指すのでしょうか？

HRTをどうやっても持たない人は値しない？ ⇒それはそう判断してもしょうがないかも
私の中では上で書いた""明らかに相手に「Respect（尊敬）」、「Trust（信頼）」を持たない振る舞いを続ける""人が該当するのかなと考えています。
攻撃的で「どうしてもこの人の敵意には耐えられない、無理」となる人。
そういう方に対しては、距離を置く選択を取らないと自分が壊れてしまいますし、無理に「Respect（尊敬）」、「Trust（信頼）」しろとは思いません。
それが相手の選択結果ですしもうしょうがないです。

自分より知識や経験のない人は値しない？ ⇒それは違うよ
ただ、中には自分より知識や経験のない人を""「Respect（尊敬）」、「Trust（信頼）」に値しない""とみているのかな？と思う場面があったりします。
(特に経験の浅い方の書く記事に対するコメントのような場面)
こういった経験、知識の有無を""「Respect（尊敬）」、「Trust（信頼）」に値しない""としている人がいるとしたら、それに私は同意できません。
経験が無い若手に対しても相手の考え方や態度、生活スタイルなどなんでも参考になるし、経験の有無だけで勝手にマウントを取るやり方はHumility（謙虚）に欠ける態度ではないかと思います。
恐らくですが、QiitaガイドラインにHRTが掲げられているのも、こういった姿勢を取られる方への注意が含まれているのではないかと思います。

私も実はやらかしている…
これは記事を書くきっかけになった1つである、自分がコメントでやらかした体験談でもあります。
私も(恐らく)自分より経験の浅いQiita投稿者に対して、初記事から自分と違う観点が興味深かったため、よくコメントしていました。
しかしその時のコメントを今思い返すと「Respect（尊敬）」を欠いており、結果その投稿者は記事を削除してしまったということがありました。
記事を削除されて思い返して後悔しても時すでに遅し。
あの時投稿していた方がこの記事を見ているかわかりませんが、あの時は私の意見、考えを押し付けてしまい申し訳ありませんでした。

参考
嫌われる勇気(amazonへのリンク)
新人プログラマ応援タグをつけた理由も追記:サーチライト/筋肉少女帯より抜粋

俺の歌とペンが
サーチライトになるんだ
俺みたいになるなよ
考えてやるのさ
俺みたいになるなよ　俺みたいになるなよ　俺みたいにはなるなよ 俺みたいには なるなよ

",False,https://qiita.com//developer-kikikaikai/items/9e08fe0f8ee0eebaccab
"

はじめに
これはポエム。
クラス指向についてのまとめ。
状態不変性を維持する為、「メソッドのみの静的クラス」にしか適応できません。
プロパティは用意してもいいですが、get専用でなければいけません。

クラス指向
名前の通り、「クラス(分類)」を軸にした構想や表現のことです。
この構想を実際に動くコードに落とし込むのことがクラス指向プログラミングです。
クラス指向には2つの構想、表現があります。

分類、種類(type,kind)
階級、階層(rank,hierarchy)

一般的にはこの2つのバランスを考慮しながら物事を整理し、
構想や表現を構築する事が、クラス指向の目的となります。

まとめ

クラス指向は、物事を分類したり、階級をつけて整理する考え方
上記によって整理された構想を、PCで動くコードで表現する事をクラス指向プログラミングと言う

非常にシンプルですね。シンプルイズベストです。

「分類」
クラス指向の一つ「分類」に慣れましょう。
練習をしてみましょう。

にんじん
豚肉
牛乳

これを「液体、個体、気体」で分類してみましょう。

固体:にんじん、豚肉
液体:牛乳

次に、「動物から採れるもの、植物から採れるもの」で分類してみましょう。

動物から採れるもの:豚肉、牛乳
植物から採れるもの:にんじん

次に、「おいしい、まずい」で分類してみましょう。
・・・・・・・・・・・・
・・・・・・
はい、くだらないですね。
練習はここまでにしましょう。

まとめ

分類の仕方は無限にある

無用な分類もあれば、有用な分類もある
応用：プログラミング上で包含関係にあるコードは、個別のクラス毎に記述することになる

非常に当たり前と言った感じです。
しかし、有用な分類を見つけるには、より高い知識と知能と運と根気が必要になります。
有用な分類を見つけられば、後はそれにそって物事を分類するだけです。

「階級」
無限にある分類の中に、特別にされている分類があります。
「階級」という分類です。
物事の順序や部分集合、親子関係を分ける際に有用な分類です。

さいごに
よりよい「有用な分類」を見つけるには、そのドメイン(案件)に関する幅広く、深い知識が必要です。
そういう意味では、途中で別の分類方法を思いつくかもしれません。
そうなった際にも困らないよう、コードの置換技術や、クラス替えの自動化方法は身に着けておきましょう。
はじめにも書いた通り、これは「メソッドのみの静的クラス」にのみ有用な概念です。
動的クラスでは別途「オブジェクト指向」という別の構想が必要になり、扱う概念の範囲が別になることに注意してください。
",False,https://qiita.com//ueruku/items/b05673b243b43d1cfd1f
"今更ですが技術書典5お疲れ様です。今回の会場、めっちゃ大きかったですね。（前回比おおよそ3倍らしいです）なのにやっぱりすし詰め状態で笑いました。
まあそれでも前回技術書典4はすし詰めどころか ハンバーグこねてる域 だったので、大きな会場取ってくれて運営マジ感謝って感じです。
とまあ、技術書典ってめちゃくちゃ混むんですよね。
混むんですけど、それは単純に人が多い以上に（コミケと比較して） 流れが遅い というのが大きい気がします。
いやもうほんと、人が流れないんですよあの空間。なんでかなーって考えてみた結果いくつか要因が浮かんできたんですけど、殆ど「技術書典ならしょうがないか……」な結論だったので共有。
運営等々を非難したいわけじゃないです。ただ「あれ？俺浮いてね？」って思ったので自分なりに考えてまとめただけです。運営等々を非難したいわけじゃあないんです。 大事なことなので2回言いました。

めっちゃお話してる
いやもうサークル前に人がたまるんですよ。めっちゃ立ち読みするしめっちゃお話してる。
コミケで大手はしごするマンにはなんというか色々別世界ですね。後ろで人がつっかえちゃうし、自分自身そんなゆっくりしてたら獲物が完売しかねないので……。
ただ技術書典の基本理念的なあれとしては「技術を中心としたコミュニケーション、頒布による普及活動」とあります（技術季報 vol.4より引用）。
サークル参加者向けアンケートにも「一般参加者と話す時間は十分でしたか？」の項目があるように サークルさんとの会話はむしろめがっさ推奨されているようです。
でもコミケ経験者とかならわかる人がいると思うんですけど、 サークルさんの前を長時間占拠するのってものすごい迷惑かけてそうでめちゃくちゃ怖いんですよね……。

半端な値段と電子決済
800円とか1200円みたいな絶妙に困る金額結構多いんですよ。個人的には小銭出すのが手間取るのでやめてほしくて。
お互いお釣りとか面倒じゃないですか？？？とか。もう600円にするぐらいなら1000円でいいよ！！！とか、かなり思うんですけど。
で、この半端な値段をある程度救済する仕組みとして技術書典公式の後払いアプリがあります。（サークルさんによってはPixivPAYも）
でもこれ、正直遅いんですよ。
QRコードの読み取りがもたつくんですよね。（この辺はサークルさんがどこにQRコード置いてるかにもよりますが）あと当日の回線状況にも左右されるのが悲しいところです。
当日QRコードで払う人を横で見てて、「ああ、これ1000円1枚出せば秒で終わるのに」みたいなのはかなりありました。
でも金額って、売り上げに関わる非常にデリケートな部分じゃないですか。安すぎて儲けが出ない、あるいは高すぎて買ってくれず赤字になったらどうしようもないですよね。
なので サークルさんはサークルさんの意思を貫いてください。あなたの創作が末永く続くことが最優先です。

「うるせえ買い専のクソオタクが！黙ってろ！！！」←わかる～～～！！！
はい。所詮買い専のクソオタクです。 サークルさんはサークルさんの、運営は運営の都合を最優先してください。 買う本や、イベントそのものがつぶれてしまってはどうしようもないのです。
それはそれとしてあの混雑は非常にしんどいのは確かです。
午後過ぎに入場してゆったり回ろうかなとも考えたりはしたんですけど、この手の本は個人的に紙媒体至上主義なのでDLカードしか残ってないなんてのは正直避けたい……。
（ちなみに私が紙媒体を買う一番大きなポイントが「人に貸すのが（精神的に）楽」な点です。データだと「これ渡したら再頒布になっちゃわないか？」みたいなあれそれを気にしちゃうので……）
　　
　　
でもまあ、食わず嫌いしないで一回午後ぶらっと回ってみるの、アリかもなぁ。それもまた一つの技術書典なんだろうなあ。
……いっそ、私も本書いてみようかな？
",False,https://qiita.com//sisi_maExtend/items/5ee451b20c5b7d9bace0
"

1. はじめに
「プログラミング宗教戦争」は, いつもどこかで勃発しています.
もちろんこの Qiita 上でもいざこざは絶えません.  
近しい記憶でいけば, 「オブジェクト指向が5000%理解できる記事」や「ゴミ記事書くな記事」にまつわる争いがありました.  
果たしてこの「プログラミング宗教戦争」は 悪 なのでしょうか？




2. 「オブジェクト指向が5000%理解できる記事」から学ぶ宗教戦争
昨今のプログラミングといえば, オブジェクト指向 で書かれることが多いですが, その概念は非常に難解な上に捉え方が幾通りもあり, プログラマたちを日夜苦しめています.  
そんな殺伐とした中, 当該記事が投稿されたわけです.
しかし, その内容は万人に受け入れられるものではありませんでした.  
ある人にとっての「オブジェクト指向」と, また別なある人にとっての「オブジェクト指向」は全くの別モノだったわけです.
その結果, コメント欄で ""宗教戦争"" が勃発しました.
そして次第にコメント欄の枠を超えて, 記事 vs 記事 での大戦争となったわけです.  
あたりまえですが, 人によって考え方はさまざまです.
だからこそ, この記事は大変盛り上がったわけです.
おのおのが考える ""理想のオブジェクト指向像"" を大いに語らいあいました.



さて, ここで冷静になってコメント欄や関連記事を眺めてみましょう.
今まで「こうだ！」と思って疑わなかったことを, 「否定」ないし「肯定」されていませんでしたか？  
もしかすると, 今まで知らなかったような知識や知見を得られた人もいるのではないのでしょうか？  
当該記事のコメント欄の中でも「宗教戦争」という言葉がでてきていましたが, この「宗教戦争」があったおかげで自身の知識に対して何らかの刺激があったかと思います.  
こういった白熱した 戦争 が勃発したおかげで新たな発見が生まれたわけですね.
それだけでもこの 戦争 が勃発した意味があったのではないでしょうか.




3. 建設的な ""宗教戦争"" をしよう！
すべての宗教戦争が 悪 だとは私はまったく考えていません.
むしろ宗教戦争には 良い面 がたくさんあると思っています.  
ここで断っておきますが, 何も皆さんに 宗教戦争 をしまくって欲しいわけではありません.
建設的な宗教戦争をしましょうという提案にすぎません.  
「宗教戦争なのに建設的とはコレいかに？」と思われるかもしれません.  
しかし, たしかに「建設的な宗教戦争」は存在しています.  
それは, お互いを罵倒しあうものではなく, 対象の「何がダメ」かを指摘し,
自案の「何がよい」のかを展開するのです.  
「お前の意見はクソ. 俺の意見が至高.」
これではいけません.  
対象を dis るにはそれなりの理由があるはずです.
まずはそれを指摘し, それに勝る案を披露してあげればよいのです.  
そうすることによって相手にとってもプラスになり,
また, 自分の意見をさらに上回る素敵な考え方に突っ込まれるかもしれません.  
しかし, それもまた自身の知識や技術力の糧となると考えれば, 良き出会いだと言えるでしょう.  
そして, ""建設的な宗教戦争"" は すなわち　ディスカッション と同義なのです.  
相手の意見を聞き, 自分の意見とすり合わせ, さらに良いものを作り上げていく上で ディスカス は必要不可欠な存在です.
ディスカスを通じてのみ成長できると言っても過言ではないでしょう.  
みなさんも, 「建設的な宗教戦争」ライフを楽しんでいきましょう.




4. おわりに
コメント欄で「宗教戦争するな」的なコメントをよく見ます.
そこでの宗教戦争の内容が, いわゆる「無価値な宗教戦争」であったならば止めにかかるのはありだと思います.  
ただ「建設的な宗教戦争」であるならば, さらに活発にしても良いのでなかろうかと思います.  
そして, この記事もまた, 私個人の意見であり, 全体に対して強要するようなものでもありません.
この記事自体が「宗教戦争」の火種なのかもしれません.  
が, それでまた知見が広がるのであるならば, それはそれで有意義なものだったのでしょう.  
",False,https://qiita.com//Midori_co583826/items/b1f282677c52ecb037bd
"

はじめに
2018年10月15日現在9つのAWS認定が公開されていますが、約2年半掛けて全ての認定を取得しました。これからAWS認定取得を目指される方向けに、各AWS認定の難易度やあまりお金を掛けない勉強法、受験に際してのTipsなどを紹介したいと思います。

経歴
エンジニア歴10年目、AWS歴は5年目です。以下は簡単なAWS歴です。

2012年: SIerで半年ほどAWSを用いた検証プロジェクトに従事


VPC/EC2/RDS/S3などベーシックなサービスが中心


2015年〜2018年現在: 事業会社でサーバーサイドエンジニアをしており、普段から業務でAWSを利用


20〜30のAWSサービスを利用
自分たちで利用するAWSサービスを選定でき、新しいサービスが出たらすぐに試せるという恵まれた環境




受験履歴
以下の順番で受験しました。スコアはまちまちですが、いずれも一発合格でした。



AWS認定
取得日
スコア




ソリューションアーキテクト - アソシエイト
2016年3月19日
81%


ソリューションアーキテクト - プロフェッショナル
2017年6月1日
72%


デベロッパー - アソシエイト
2018年2月2日
85%


クラウドプラクティショナー
2018年9月14日
910


SysOpsアドミニストレーター - アソシエイト
2018年9月15日
80%


DevOpsエンジニア - プロフェッショナル
2018年9月29日
77%


ビッグデータ - 専門知識
2018年10月5日
78%


セキュリティ - 専門知識
2018年10月6日
787


高度なネットワーキング - 専門知識
2018年10月6日
68%




各AWS認定の難易度
完全に主観によるものですが、難しい順に並べるとこのようになると思います。



順位
認定




1位
ソリューションアーキテクト - プロフェッショナル


2位
高度なネットワーキング - 専門知識


3位タイ
DevOpsエンジニア - プロフェッショナル


3位タイ
ビッグデータ - 専門知識


3位タイ
セキュリティ - 専門知識


6位
ソリューションアーキテクト - アソシエイト


7位タイ
デベロッパー - アソシエイト


7位タイ
SysOpsアドミニストレーター - アソシエイト


9位
クラウドプラクティショナー



まず9位の「クラウドプラクティショナー」ですが、2018年に新設されたAWSのエントリーレベルの認定です。AWSの基本的な概念や、AWSのベーシックなサービスの特長、請求・アカウントマネジメント・料金モデルなどに関する理解が問われます。エンジニアだけでなくマネージャーや営業など様々なポジションを対象とした認定であるため、問われる知識の範囲は意外に広いです。ただ、知っていれば解ける問題が多いため、この順位にしています。
7位の「デベロッパー - アソシエイト」「SysOpsアドミニストレーター - アソシエイト」は、AWSを用いたシステムの開発や運用を行うエンジニアをターゲットにした認定です。それぞれで出題されるAWSサービスの傾向は若干異なりますが、両試験で似たような問題が多分に含まれており、難易度は大きく変わらないため、同率7位にしました。
6位の「ソリューションアーキテクト - アソシエイト」は、AWSを用いたシステムの設計を行うエンジニアをターゲットにした認定です。アソシエイトレベルの中では最も広範な知識が必要であり、クラウドプラクティショナー及び他2つのアソシエイトレベルの内容をある程度押さえておく必要があります。逆に言うと、この認定に合格できる実力があれば、他のアソシエイト認定もほぼ問題なく合格できると思います。
3位タイの「DevOpsエンジニア - プロフェッショナル」「ビッグデータ - 専門知識」「セキュリティ - 専門知識」は、受験される人の得意あるいは不得意分野によって難易度の感じ方が変わると思います。例えば普段からEMRやRedshift等のサービスに触れる機会が多い方の場合、「ビッグデータ - 専門知識」は取り組み易いと思います。出題傾向は認定によって異なりますが、いずれの認定もAWSの各サービスへの深い理解が求められます。
2位の「高度なネットワーキング - 専門知識」は、AWS DirectConnectに関する問題が非常に多いのが特徴です。他にもBGPやVPNなどのネットワーク系の知識も問われます。ソフトウェアエンジニアやサーバー寄りのインフラエンジニアにとっては学習量が多いため、他の専門知識レベルの認定に比べると難易度が高く感じると思います。
1位の「ソリューションアーキテクト - プロフェッショナル」ですが、AWS認定の中で最も幅広いIT関連の知識が問われます。各種AWSサービスは当然のこと、ネットワークやデータベース、はたまたWindows ServerやActive Directoryなど様々な知識が必要です。また問題文が全体的に長く、5行以上の問題も少なくありません。

基本的な学習方法
諸々のハードルをクリアできるのであれば、AWS公式のクラスルームトレーニング（集合研修）を受講するのが最も効果的だと思います。ハードルというのは、予定の確保・料金・場所の3点です。
ソリューションアーキテクト - アソシエイト向けの研修である Architecting on AWS を例にすると、期間は3日間、料金は税抜で21万円、そして開催場所は東京に集中しています。名古屋・大阪・福岡などでも開催されていますが、東京に比べるとかなり少ないです。
全ての人が上記のハードルをクリアできる訳ではないので、本記事ではあまりお金を掛けない＆居住地に依存しない学習方法を記載したいと思います。
具体的には、オンライントレーニングや書籍を中心にインプットしAWS公式のサンプル問題と模擬試験を完璧に理解するまで復習するという方法を中心に据えます。
私はAWS公式のクラスルームトレーニングは一切受講せず、この方法で全ての認定を取得しました。
全くお金を掛けずに勉強することも可能ですが効率的では無いため、必要なところで課金するスタイルが良いと思います。

AWS認定の全体像を理解する
各AWS認定の概要やキャプチャ付きの試験申し込み手順、試験準備のノウハウなどが詰まったAWS公式のSlideShare資料です。こちらを一読することでAWS認定の全体像をつかめると思います。
20180606 AWS Black Belt Online Seminar AWS 認定取得に向けて

学習に当たって準備しておくもの

Amazonアカウント
模擬試験はAWSトレーニング及び認定ポータル（以降「AWS認定ポータル」）から申し込みます。サインインはAmazonアカウントで行いますので、アカウントがない場合は作成しておきましょう。AmazonアカウントとAWSアカウントは別ものなので注意。AWS認定ポータルのURLは以下です。
https://www.aws.training/certification

メモアプリ
模擬試験は1回の申し込みで1回しか受験できません。復習できないのは勿体ないのでキャプチャを取りながら解くことをオススメします。エクセルでも良いですが、なるべくなら色々なWeb記事をスクラップできるメモアプリを使うと学習が捗ると思います。以下はアプリの例です。

Dropbox Paper
Evernote
Microsoft OneNote


AWSアカウント
模擬試験が効果的とはいえ、実際に手を動かせる環境があった方が学習が捗ります。AWSアカウントはなるべく早めに作っておきましょう。
https://aws.amazon.com/jp/register-flow/

AWSサービスの技術仕様の調べ方
AWSはサービスのアップデートが早いため、個人ブログなどに掲載された情報が古くなってしまっていることがままあります。ですので基本的には、AWS公式の資料であるAWSクラウドサービス活用資料集と開発者ガイドを中心に確認するのが良いと思います。

AWSクラウドサービス活用資料集 (Black Belt)
AWS認定の勉強で最もお世話になる資料集、通称Black Beltです。AWSの中の人（ソリューションアーキテクト）が各AWSサービスについて分かりやすくまとめてくれています。何か新しいAWSサービスを学習する際には、最初にBlack Beltに目を通すのが効率的だと思います。以下のページにリンクがまとまっていますのでブックマークを推奨します。
AWSクラウドサービス活用資料集

開発者ガイド
各AWSサービスの技術仕様の詳細が書かれたドキュメントです。Black Beltよりも細かい内容を知りたい場合などに都度参照します。以下一部サービスのリンクを記載します。実際はもっと多くのドキュメントがありますが書ききれないので都度Googleで検索するのが良いと思います。

Amazon VPC とは_ - Amazon Virtual Private Cloud
Amazon EC2 とは - Amazon Elastic Compute Cloud
Amazon S3 とは何ですか？ - Amazon Simple Storage Service


AWS公式サンプル問題と模擬試験の活用方法
認定資格向けの日本語書籍は2018年10月15日時点で1冊しか無いため、AWS公式のサンプル問題と模擬試験を主要な学習教材として用いるのが現実的な戦略になると思います。以下はサンプル問題と模擬試験のオススメの活用方法です。



#
やること




1
問題のキャプチャを取ってメモアプリに貼り付ける (模擬試験の場合のみ)


2
何も見ずに自力で問題を解く。あとで見返せるように回答を記録する


3
全ての問題を解くと合否と分野別の正解率が出るので記録する (模擬試験の場合のみ)


4
1問ずつ見返していき、良く分かっていない用語や概念などを調べる。その際、役に立ちそうな記述があったらメモアプリに貼り付ける。問題文と選択肢が完全に理解できたら改めて回答する


5
4を全問題分繰り返す



4が学習のキモの部分で、問題の長さによりますが1問当たり5分〜30分程度掛かると思います。1日で全てやり切るのはしんどいので、数日に分けて学習を進めるのが良いと思います。
理解が浅いサービスについてはBlack Beltを通読する、細かい技術仕様については開発者ガイドの該当箇所をチェックするというやり方が効率的だと思います。
模擬試験の復習が終わった頃には、メモアプリの中に自分のオリジナルの参考書が出来ているはずです。

学習リソース: クラウドプラクティショナー & アソシエイト編
ここからは各認定で役に立ちそうな学習リソースについて記載します。あくまで一例ですので、色々な方の受験記に目を通されるのをオススメします。

インプット

オンライントレーニング: AWS Cloud Practitioner Essentials
https://www.aws.training/learningobject/curriculum?id=17954
AWS公式のオンライントレーニングです。無料でAWS認定ポータルから視聴できます。まずはこちらを視聴して、AWSの世界観に慣れていきましょう。

オンライントレーニング: 手を動かしながら2週間で学ぶ AWS 基本から応用まで
https://www.udemy.com/aws-14days/
オンライン学習サイトのUdemyで公開されているトレーニングです。定価は¥15,600ですが頻繁にセールをやっていて、千円台〜三千円代で購入できることが多いです。AWSアカウントを作って各種サービスを実際に触ってみるという内容で、非常に分かりやすくオススメです。

書籍: 合格対策 AWS認定ソリューションアーキテクト - アソシエイト
https://www.amazon.co.jp/合格対策-AWS認定ソリューションアーキテクト-－アソシエイト-大塚康徳-ebook/dp/B01LZ2CBKB
おそらく日本語で市販されている唯一のAWS認定対策本です。2016年発売とちょっと時間が経っているのですが、AWSのベーシックなサービスを中心に分かりやすくまとまっており、AWS初学者にはオススメです。

オンライン資料: AWSサポートの紹介 / AWS Trusted Advisorの紹介
https://aws.amazon.com/jp/premiumsupport/compare-plans/
AWSのサポートプランの比較がまとまっていますので目を通しておきましょう。
「AWSサポートの紹介」と「AWS Trusted Advisorの紹介」のBlack Beltの動画が2つ埋め込まれていますので、そちらも目を通しておきましょう。

アウトプット

AWS公式サンプル問題

クラウドプラクティショナー (英語/10問)
ソリューションアーキテクト - アソシエイト (日本語/9問)
デベロッパー - アソシエイト (日本語/10問)
SysOpsアドミニストレーター - アソシエイト (日本語/9問)

各認定のページで無料のサンプル問題が公開されています。クラウドプラクティショナーのみ英語です。英語が苦手な方はGoogle翻訳などを活用しましょう。難しい場合はスキップして、日本語版が提供されている模擬試験に取り組むのも良いと思います。

AWS公式模擬試験
AWS認定ポータルから申し込みます。前述の「AWS公式サンプル問題と模擬試験の活用方法」を参考にしつつ、完璧に理解できるまでやり込みましょう。



認定
料金 (税別)
言語




クラウドプラクティショナー
2,000円
日本語、英語など


ソリューションアーキテクト - アソシエイト
2,000円
日本語、英語など


デベロッパー - アソシエイト
2,000円
日本語、英語など


SysOpsアドミニストレーター - アソシエイト
2,000円
日本語、英語など




学習リソース: プロフェッショナル & 専門知識編

インプット
日本語で有用なリソースが中々見当たらないのが正直なところです。ただ、アソシエイトレベルを制覇した方であれば、模擬試験を中心とした学習でも十分合格が狙えると思います。英語に苦手意識が無い方は、UdemyやLinux Academyなどのオンライン学習サイトで評判が良いコンテンツを探してみるのも良いと思います。

アウトプット

AWS公式サンプル問題

ソリューションアーキテクト – プロフェッショナル (英語/6問)
DevOps エンジニア – プロフェッショナル (英語/10問)
ビッグデータ - 専門知識 (英語/10問)
高度なネットワーキング – 専門知識 (英語/10問)
セキュリティ – 専門知識 (英語/10問)


AWS公式模擬試験



認定
料金 (税別)
言語




ソリューションアーキテクト – プロフェッショナル
4,000円
日本語、英語


DevOps エンジニア – プロフェッショナル
2,000円
日本語、英語


セキュリティ – 専門知識
4,000円
日本語、英語など



「ビッグデータ - 専門知識」と「高度なネットワーキング - 専門知識」は2018年10月15日時点でAWSから模擬試験が提供されていません。おそらく近いうちに提供が開始されると思いますが、これらの認定については簡単に別のQiita記事にまとめていますのでよろしければご覧ください。

AWS認定ビッグデータ - 専門知識に合格しました - Qiita
AWS認定高度なネットワーキング – 専門知識に合格 & AWS認定9冠を達成しました - Qiita


受験時のTips
ここからは受験に際してのTips集です。

受験時のリモート監視
テストセンターによってはパソコンにカメラが付いており、受験時の様子を逐一リモートから監視されます。リモート監視ありの場合、以下のようなことに気を遣う必要があります。

リモート監視の人とのコミュニケーションは英語でのチャットが基本（日本語が出来る人も一部いました）
試験を開始する前にカメラに向かって本人確認書類を提示する必要がある
備え付けの防音ヘッドホンを使おうとするとチャットで「NGです」と言われる
手を口元に持って行くとチャットで「机の上に手を戻しなさい」と言われる

ただ、いくつかの認定試験受験時にはリモート監視がありませんでした。おそらくですが、リモート監視に対応しているテストセンターとそうでないところがあるのかもしれません。気になる方はテストセンターに問い合わせてみると良いかもしれません。

受験時のシステムフリーズ
受験時にシステムがフリーズして次に進めなくなることが結構な頻度であります。全くフリーズしない時もありましたが、多い時は1回の試験で3回フリーズしました。フリーズの間は制限時間は減らないようなので慌てる必要はありません。テストセンターの人を呼んで対応してもらいましょう。

日本語訳が不自然な場合がある
まだ日本語版が公開されてから日が浅い「ビッグデータ - 専門知識」と「セキュリティ - 専門知識」の受験時に、誤訳と思われる記述が2〜3問ありました。以下は例です。



英語
AWS認定での日本語訳
望ましい日本語訳




EC2 Workers
EC2作業者
EC2ワーカー


Config Rule
設定ルール
Config Rule


company's website
社内Webサイト
会社のWebサイト



幸いなことに、専門知識試験では画面上部のリストボックスで英語と日本語を切り替えることができました。もし正解の選択肢が無いと思った場合は、一度英語に切り替えて文章を確認、それから日本語に戻して回答というステップを踏むのが良いと思います。
全ての試験で言語の切り替えが出来る訳ではないのですが、日本語版が公開されてから時間が経っている試験では不自然な日本語訳は少なかったと思います。おそらくAWSサイドで細かくアップデートしているのだと思います。

要件と優先事項を押さえる
プロフェッショナルレベルや専門知識レベルの認定試験は問題文が5行以上あることも珍しくありません。問題文の中から要件をしっかり読み解くことが重要です。ベストプラクティスと思しき選択肢でも要件を満たしていない場合は不正解、一見泥臭くても要件を満たした選択肢があればそちらが正解です。
要件を満たす選択肢が2つ以上ある場合は優先事項に着目して絞り込みます。優先事項というのはコスト効率や堅牢性、デリバリーまでのスピードなどです。

最後に
長々と書きましたが、実際にAWSのサービスに触れてみるのが知識の定着に繋がると思います。無料枠を活用してガンガン触りましょう。それと色々な方の受験記に目を通されるのもオススメします！

「AWS 認定 ソリューションアーキテクト」の検索結果 - Qiita
「AWS 認定 デベロッパー」の検索結果 - Qiita
「AWS 認定 SysOps」の検索結果 - Qiita
「AWS 認定 クラウドプラクティショナー」の検索結果 - Qiita
「AWS 認定 プロフェッショナル」の検索結果 - Qiita
「AWS 認定 専門知識」の検索結果 - Qiita

",False,https://qiita.com//nakazax/items/20458e146d3d9f2aa615
"

1. はじめに
先日書いた「思考実験：AQUAフレームワークの品質保証兵站への応用」では品質や品質保証そのものにあまり触れていなかったので日ごろ考えていることをメモがてら書いてみます。

2. 品質
「品質」でググるとわかるようにいろいろな人がいろいろな定義をしていてSQuBOK Guide(ソフトウェア品質知識体系ガイド)ではアリストテレスまでさかのぼってさまざまな品質の定義を紹介しています。ソフトウェアエンジニアでSQuBOKを読んだことのない方は読まれることをお勧めします。
品質には多面性があり唯一の正解というものはないのですが筆者はワインバーグの「品質とは誰かにとっての価値である1」というのが好きです。
お金を出して商品を買ってくれたユーザや無料版を無料で使用しているユーザと比べると「誰か」というのはとっても広いですね。でも商品を買う前のまだユーザになっていない段階でレビューを見て良さそうだとかイマイチっぽいとか、品質、つまるところ対価を払って得られる価値が購入するという行為の判断材料の一つになっていることは誰しも経験があるかと思います。そして、何に価値を感じるかは人それぞれでもあります。

3. 品質が良いとは
「品質が良い」というのは以下の2つにブレークダウンできます。

価値がある、優れている
価値を損ねるものがない、少ない

ソフトウェアの品質が良いというとバグゼロやバグが少ないことを思い浮かべるかもしれませんが価値があること、優れていることも大事です。

4. 品質保証
品質保証はQA、Quality Assuranceとも呼ばれます。保証を意味する英語としてはassuranceのほかにwarranteeやguaranteeもありますが以下のassuranceの解説を読んでなるほどこれが品質保証、QAかと納得しました。

製品が品質基準に合致しているかを、設計・試作・製造・検査の全ての工程で確認する仕組みがあり、それを実行していることを保証する
出典：Limited warranty - 英借文ドットコム

ソフトウェアには製造工程がないため筆者は以下のようにアレンジしています。

製品が品質基準に合致しているかを、仕様設計・実装・テストの全ての工程で確認する仕組みがあり、それを実行していることを保証する

ここで「全ての工程」とあるように品質保証は全工程、全員参加の取り組みであり品質保証の担当者だけで行うものではありません。

4.1 品質基準
品質を満たしていること(価値を備えていることや、価値を損ねるものが少ないこと)を判断するためのさまざまな目標、基準があります。

品質にはお客様視点、設計視点、テスト視点、経営視点などがある
品質モデルとして例えばISO25000や狩野モデルがある
基準の作り方の一つにGQM(Goal, Question, Metric)がある
ソフトウェアの品質には定量化の難しいものもある


4.2 工程
仕様設計の工程、プログラム実装の工程、テストの工程をいわゆるV字モデルで配置し、併せてそれぞれのスコープの一例を枠線で図示します。V字の左側で価値を作り右側で価値を損ねるものを除去します。3章で述べたようにソフトウェアの品質にとって左側、右側の両方とも大事な工程です。


4.3 確認する仕組み
確認する仕組みの例を以下に示します。



工程
例




仕様設計
・成果物のレビュー


実装
・成果物のレビュー・静的解析・動的解析・単体テスト


テスト
・テスト内容(なにを)のレビュー・テスト方法(どうやって)のレビュー・テスト対象のチェックとテスト(結合テスト、システムテスト、受入れテスト)



このほかにも例えばWindows Insider Programのプレビュー版やmacOSのパブリックベータ版といった正式リリース前のビルドを有志の方々にテストしてもらい価値の確認や不具合の検出を行う方法もあります。

4.4 実行
例えば不具合を見つけたら不具合票を起票し修正を行って再テストしてクローズしたり、出荷判定の判定会を開催して議事録を残したり、品質基準をクリアするよう日々さまざまな取り組みを実行します。

4.5 保証する
保証するにはその裏付け(エビデンス)が必要です。
設計部門と品質保証部門で組織が分かれている場合は品質保証部門が設計部門にエビデンスのデータを要求します。一方、設計部門に品質保証の機能を備える場合は設計担当、品証担当の誰もが生データ含めこれらのデータにアクセスでき、エビデンスを集めるのは品証担当が行います。
エビデンスを集約、整理し、しかるべき立場の人がリリース可能と判定すると晴れてリリースとなります。

5. SQAの役割
JaSST'17 Tokyoの招待講演で講師の奈良さんが「QAの役割はQMSを回すこと2」と説明されていたのがシンプルかつ深みがあり筆者は好きです。
QMSを回すとはプロセス警察(エビデンスを要求したりプロセス通りに実行しているかを監視する)のことではなく、価値を最大化し価値を損ねるものを最小化するためにソフトウェア開発の全工程に対してよりよく回るようにしたりうまく回っていないところにテコ入れすることといったほうがしっくりくると思います。

5.1 SQAのスコープ
前出のV字モデルにSQAのスコープをピンクの枠線で図示したものを以下に示します。見ての通りSQAのスコープはソフトウェア開発の全工程です。


5.2 SQAの活動
SQA担当の主な活動を以下に例示します。仕様設計担当、実装担当、テスト担当は主に目先のプロダクトにフォーカスしているのに対してSQA担当は目先だけでなく将来に備えた活動も行います。なお、レベルや粒度はばらばらで必ずしも筆者が行っているものとは限りません。

品質保証戦略の立案、見直し
品質のモニタリングとコントロール


メトリクスの決定、見直し
メトリクス集計技術の開発
メトリクス集計


プロダクト品質
プロセス品質


メトリクスに基づいたコントロール


QMSの改善


できていることをよりよくできるようにする(強みの強化)
できていないことの阻害要因を除去する(弱みの克服)


品質保証技術の開発


デバッグをやりやすい環境の構築
不具合摘出のフロントローディング
ツールの内製
ツールの試食、導入


テックシェルフ(知識、ノウハウの蓄積と展開)
不具合管理


参考：組込みソフトウェア開発における品質向上の勧め[バグ管理手法編]

不具合分析


欠陥エンジニアリング




再発防止、未然防止


ソフトウェアFTA
ソフトウェアFMEA
ソフトウェアHAZOP


品質の評価(品質保証レベルのテスト)
品質保証活動のリソース計画と執行

このほか仕様設計、実装、テストで手薄なところのテコ入れとして以下のようなタスクを行うことがあります。

レビュー
デバッグのヘルプ(不具合の再現手順を探したり)
テスト、修正された不具合の再テスト


5.3 SQA＝テスト？
SQAはテストをすることもありますがそれは上で挙げたように役割の一部でありSQA担当でテストされている方は以下のいずれかと思います。

品質保証レベルのテストを行っている
いろいろなSQAのタスクを片っ端から自動化して捻出した工数をテストに充てている
SQAチームの層が厚くてメンバーをテストにアサインできる
テストすることをSQAと呼んでいる


5.4 参考書、資料など

5.4.1 書籍
前出のSQuBOK Guide(ソフトウェア品質知識体系ガイド)のほか、アマゾンで""ソフトウェア品質""で検索したり、「よく一緒に購入されている商品」「この商品をチェックした人はこんな商品もチェックしています」をたどるといろいろ見つかります。
また、コミックマーケットや技術書典などの同人誌即売会でソフトウェアテストやソフトウェア品質保証を扱っている同人誌が頒布されることがあります。

5.4.2 Web
Web上にもさまざまな資料がありますがここでは2018年9月～10月にかけてASTER(NPO法人ソフトウェアテスト技術振興協会)からリリースされたテストおよびテスト設計の資料を以下に示します。

ASTER 技術セミナー:「ASTERセミナー標準テキスト」公開について
テスト設計コンテスト'19 - U-30クラス チュートリアル
テスト設計コンテスト'19 - OPENクラス チュートリアル


5.4.3 シンポジウム、カンファレンス
ソフトウェアテストやソフトウェア品質を冠した国内のシンポジウムとして以下の2つがあります。

ソフトウェアテストシンポジウム(JaSST)
ソフトウェア品質シンポジウム(SQiPシンポジウム)

上記のほかさまざまなシンポジウムやカンファレンス、勉強会などがあり貴重なナレッジを公開されている方々には感謝しかありません。

6. おわりに
こうして眺めてみるとSQAの活動は多岐にわたることがわかります。
なお、プロダクトやプロジェクト、SQAの規模、成熟度、優先することなどは各社各様で、優先度をつけてやることを絞ったりここに挙げていないこともやったりなど活動内容や活動方法は現場ごとにさまざまと思います。ここに書かれていることが必ずしも皆さんの環境に当てはまるとは限りませんが何かの参考になれば幸いです。




ソフトウェア品質 - Wikipedia ↩


JaSST Tokyoのまとめ(8ページ目) ↩



",False,https://qiita.com//pbjpkas/items/8064523ea5170adf5678
"

はじめに
これはポエム。
適当に文献漁ったので、まとめ。

オブジェクト指向
観測対象となるモノの状態を、机上で表現する方法の１つ。
この概念自体に動作を要求するものではない。
「クラス(種別、分類)」という概念も(あれば便利だけど)必要ない。

背景
群を形成するシステムに対して、観測調査や再現調査を行いたい。
既にシステムを構築する以下の2つに対して十分な理解が得られている。

要素


システムを構築するモノ
Yes観測対象
いっぱいある
種類もいっぱいある
環境と他の要素との相互作用によってシステムが表出する


環境


要素に対して作用するモノ全般
No観測対象
簡単な弾道計算であれば古典的な物理法則や射出器等



システムをコンピュータ上でシミュレートする場合、
シミュレーション環境とシミュレーションモデルを
コンピュータでのコードに落とし込む必要がある。
「ライフゲーム」のような極度に単調な要素と環境しか持たない
システムならまだメタ的な落とし込みは可能だが、
相手はもうちょっと複雑だったりしてた。

性質の明記
環境に配置された観測対象となる要素(=オブジェクト)の性質を表す事。
表す性質は以下の２つ。これらを合わせて「マルチプルビュー」とか呼ぶ。

静的ビュー


要素の性質

クラスでいうプロパティ


動的ビュー


作用によって表出する要素の性質

クラスでいうメソッド



上記性質を元に、「創発」を起こす性質を持つことがある。
これを「創発特性」と言う。

創発特性
主に相互作用や環境作用による、「それ単体で表出しない性質や状態」の事。
創発特性に関しては、Wikipadiaに「創発」の項にある「ライフゲーム」を参照すると理解しやすい。
Wikipadia - 創発

オブジェクト指向とマルチプルビューの関係
オブジェクト指向におけるマルチプルビューでないドキュメントおよび構想は、
以下の通り「観測に満たない/観測不可能」となる。

静的ビューのみ


ただの値群
相互作用を起さないため、創発特性を持たない
状態を持つが、観測対象ではない(正確には、これに状態を上書きした要素が観測対象)
どちらかというと、設定や状態保存に使う、環境に位置するドキュメント
それ静的なデータだよね


動的ビューのみ


ただのメソッド群、静的クラス
相互作用を起さないため、創発特性を持たない
状態を持たないため、観測不可能
どちらかというと環境からの作用メソッドに使う、環境に位置するドキュメント
それ静的なクラス指向だよね



オブジェクト指向の大前提である「机上における観測対象の表現方法」という点では、
対象がマルチプルビューであればオブジェクト指向と言える。
ただし、「インスタンスが作成不可能」「無意味な静的ビュー/動的ビュー」等の
観測可能性や観測意義が危ぶまれる場合は、
オブジェクト指向ではなく、インターフェイス指向やクラス指向等、
他の指向に準ずる考えられる。

ご利用方法
基本的にオブジェクト指向は表現の方法のため、
実用という面では「理解しやすい」以外のモノはない。
また、コンピュータに読み込ませて動かすことも考慮していない。
ただの設計書。マルチプルビュー。
ここから必要になるのは、オブジェクト指向で書かれたドキュメントを
プログラムとして組み込むオブジェクト指向プログラミングを行うことです。
ここで便利なシンタックス(構文表現)がでてきました。
クラスやらインスタンスやらカプセル化やら継承やらなんやらが出てきます。
これを使うとオブジェクト指向プログラミングが楽になります。
しかし、これらはあくまでもオブジェクト指向で書かれたドキュメントを、
プログラムのコードに簡潔に落とし込むのに使ったシンタックスの1つで、
これそのものや、これを使用したコードがオブジェクト指向というわけではないです。
(javascriptもES2015より前ではclassがなかったので、functionを利用してたりする。)

オブジェクト指向：表現方法および、マルチプルビューに則ったドキュメントや構想のこと
オブジェクト指向プログラミング：上記ドキュメントや構想を元にプログラミングする


言語がサポートしているシンタックスは関係なし
あるシンタックスを用いることでより簡潔にコードに落とし込めるようになるだけ




クラス指向プログラミング
クラス(分類)をプログラミング(コードに落とし込む)すること全般を言います。
親クラスとか継承とかあります。
これも便利なシンタックスがありまして、classですね。
ここで大事なのは、どのようにクラス(分類)分けするかは、
完全にそのシステム依存というところです。
（is has 議論とかいうオブジェクト指向全く関係ない話もここら辺）
ここら辺は現在大変にバズっており、
クラス指向に関する作法(is has 分類の切り口 等)や、
クラス指向プログラミングの実用記事もあるので省略。

Q. オブジェクト指向とクラス指向は何が違うのか
A. 指向(関心)が違う。
ただし、クラス指向も地味に2種類あり、
純粋なクラス指向(機能の分類、静的クラス)と、
オブジェクト指向の拡張としてのクラス指向(動的クラス)がある。
現行ではオブジェクト指向の拡張の方が主流。

オブジェクト指向：インスタンスやシステムの状態に関心
クラス指向(静的)：システムから見た機能の分類に関心
クラス指向(動的)：オブジェクトの分類、親子関係、カプセル化、継承等に関心


Q. オブジェクト指向とクラス指向は共存可能か
A. 可能。ただしオブジェクト指向優先という条件がある。
クラス指向優先での機能分類を行うと、
オブジェクト指向における機能の単純化が阻まれる事が容易に起こるため、
基本的にはオブジェクト指向を元に、分類分け(クラス指向)をすることになる。

オブジェクト指向：インスタンスレベルのクラスとなる部分
クラス指向：インスタンスを含むクラス周り全般の分類

ちなみに静的なクラス指向(機能の分類)に失敗すると、神クラスが出来上がる。

他にもいろいろな指向

インターフェイス指向
オブジェクト指向の拡張としてのクラス指向に対して、
さらに制約をかけるものとして扱われる。
契約と言われている。
流儀としては、クラス指向。

メッセージ指向
オブジェクト同士の相互作用に関心を寄せたもの。
オブジェクト自体は、オブジェクト指向によって作成される事前提であり、
その上でメッセージ指向でオブジェクト同士を結びつけるという発想。
このオブジェクトの相互作用におけるやりとりを「メッセージ」と表している。
オブジェクトに関しては利用しているだけで、
あくまでもメッセージをどうするかに関心がある。
本質的にはオブジェクト指向ではなく、切り分けが必要である。

おわりに
オブジェクトを作っているのか、
機能の分類をしているのか、
オブジェクトの分類をしているのか、
オブジェクトの動作の契約をしているのか、
オブジェクト同士のメッセージ処理周りを作っているのか、
そんなこと考えても、旨味なんてなかった。
",False,https://qiita.com//ueruku/items/b3ef4182954bc3708e7f
"

この記事はなに？
「設計」という言葉について、概念的に自分の中で安心感を覚えるために行った調べ物と思考プロセスを記した記事です。
ソフトウェアエンジニアをやっていると、「設計」という言葉と無関係ではいられません。
私も例に漏れることなく、ほぼ毎日この言葉にふれながら業務にあたっています。
しかしながら、「設計」という言葉に対して何かぼんやりというか、不安定な感じを覚えるというか、そういった感情がつきまとっていました。
基本設計やら外部設計やら概要設計やらといった言葉が飛び交い、人それぞれが「設計」の捉え方も違う。
自分が普段行っている活動は「設計」なのかどうなのかも怪しい。
こうした状態の中、「設計」という言葉を咀嚼し、腑に落ちるような形で他の言葉で置き換えたい、と思った次第です。

偉人たちは「設計」についてなんと言っているか？
設計に対する不安感を取り除くために、知の巨匠たちは設計とは何であると言っているかを調べてみました。
そうすると、一番初めにたどり着いたのがスティーブ・ジョブズの以下の言葉です。

Design is not just what it looks like and feels like. Design is how it works.
デザインとは、単なる視覚や感覚のことではない。デザインとは、どう機能させるかだ。

ほぼ同じ意味でこうも言っていました。

Design is a funny word. Some people think design means how it looks. But of course, if you dig deeper, it’s really how it works.
デザインとは面白い言葉だ。デザインは「物がどのような形に見えるか」ということだと思うひとがいるが、しかし、深く掘りさげると、デザインとは「物がどのように機能するか」という意味になる。

ジョブズ以外の偉人が「設計」に対してどのように言っているかも調べたのですが、「設計はこのようにすべきだ」というものが多く、「設計とは」という部分を説明しているものは見つけられませんでした。
もしよろしければ、コメントにてお教えいただければと思います。

辞書は「設計」を何と説明しているか？
こちらの英英辞書サイトで調べてみました。
すると、いくつかの定義がある中で、次の3つの定義がIT業界で使われる「設計」のヒントになるように思われました。

５. an arrangement scheme
配置の計画
６. something intended as a guide for making something else
何か他の物を作るためのガイドとして意図された何か
７. an anticipated outcome that is intended or that guides your planned actions
意図している、または計画した行動を導く、予期された結果

また、これらの中で気になった「scheme」も調べてみると、次のように説明されていました。

５. a group of independent but interrelated elements comprising a unified whole
統一された全体を構成する、個々に独立しているが互いに関係を持っている要素のグループ

これらを組み合わせてみると、意訳も混ざってしまいますが、次のようになるのではないかと思います。
設計とは、
①独立しながらも互いに関係を持っている要素のグループを配置するものであり、
②作る際のガイドとなるものであり、
③期待した振る舞いを導くものである。
④そしてそれは（偶然の産物ではなく）意図して生み出されたものである。
言い回しを変えると、
①設計は、独立しながらも互いに関係を持っている要素のグループを配置するものである。
②設計は、作る際のガイドとなるものである。
③設計は、期待した振る舞いを導くものである。
④設計は、（偶然の産物ではなく）意図して生み出されたものである。
ということになりそうです。
④の意味を料理のレシピに当てはめてみると、食材や調味料の投入順は、あみだくじ等でランダムに決められたものではなく、帰納法やら演繹法やらなんでもいいので、「この順番ならうまくいく」というある一定の根拠や意図がなければ、それは設計とは呼べないということになります。
また、先のジョブズの提示した設計の定義は、①と③の定義に関係がありそうです。

これらを踏まえた上で設計の定義はどうなる？
ジョブズの設計の定義、辞書での設計の定義、そしてソフトウェアエンジニアとして活動している私の知識と経験から、ソフトウェア業界における「設計」を以下のように私は言語化しました。
「設計（Design）とは、ある規則（Rule, Pattern, Principle）に基づいて、対象（Object）の複雑性を理解しやすい部品（Component）に分解し、部品ごとに最適な解決策（Solution）を選択して、全体として動く（Work）ように組み合わせること（Integration）」
この設計の定義は、個人的には非常に納得のいくものになっています。
ソフトウェア業界における設計者は、まずはデザインルール、デザインパターン、設計原則について知っておく必要があります。
そして状況に応じた適切な設計規則を選択し、複雑性を理解可能なコンポーネント単位に分解します。
さらに分解したコンポーネントごとに最適なソリューションを当てはめるために、技術要素やライブラリ等の知見も持ち合わせている必要があります。
そうして最後にコンポーネントを集結させて、システム全体として課題を解決できるものになっていることを確認します。
ここでいう「複雑性」とは、システムの複雑性であり、現実世界の複雑性であり、業務の複雑性であり、人間の認知活動の複雑性です。
またそもそもとして、「設計」を行う目的は、拡張性・頑健性・保守性・セキュリティ・モジュール性などが高いソフトウェアを構築することです。
こうしたソフトウェアを構築することは、価値を迅速（agility）かつ安定的（stability）に世に送り出すことにつながります。
この「分解し、部分ごとに検証し、再度組み合わせる」というのは、まさに「科学的手法」に他なりません。

最後に
私個人としては、
「設計（Design）とは、ある規則（Rule, Pattern, Principle）に基づいて、対象（Object）の複雑性を理解しやすい部品（Component）に分解し、部品ごとに最適な解決策（Solution）を選択して、全体として動く（Work）ように組み合わせること（Integration）」
という定義に今は非常に納得感を持っています。
しかしながら、あくまでも「私個人は」であり、「今は」です。
これ以上に「設計」をうまく表す言葉が見つかれば、私もそちらに乗り換えます。
したがいまして、もしよろしければ記事をご覧くださったみなさんが「設計」を言語化するとなればどのようになるか、コメントにてお教えいただければと思っております。
ジョブズの項でも記載しましたが、「この人は設計をこのように定義していた」というのも、ぜひご教授ください。
「設計」に対する多様な考えが集まれば集まるほど、弁証法でいうところのアウフヘーベンにより、さらなる高みへと進むことができるようになります。
西欧では議論の場で反対の意見がたくさん出てくることは、「議論が割れる」というふうにネガティブに捉えられるものではなく、アウフヘーベンにより高い段階へと進むことのできる「建設的な議論」の証だと捉えられているようです。

参考

非機能要求とアーキテクチャの関係分析の
事例研究結果のご紹介
「悪い設計」の定義について考えたことはあるか
ソフトウェア設計
設計とは何か
設計とは何だろうか
大規模Webアプリケーションにおける複雑性とアーキテクチャ設計に関する一考察
Webアプリケーションフレームワーク導入時に考慮すべき22の観点
『エリック・エヴァンスのドメイン駆動設計 ソフトウェアの核心にある複雑さに立ち向かう』
『実践ドメイン駆動設計 エリック・エヴァンスが確立した理論を実際の設計に応用する』
『Clean Architecture 達人に学ぶソフトウェアの構造と設計』
『プリンシプル オブ プログラミング 3年目までに身につけたい 一生役立つ101の原理原則』

",False,https://qiita.com//_rema_lp/items/d74a3cba72b9707b0103
"

前置き
この記事は「ネットワークのこと教えてくださーい」とやってきた学生君に対してアドリブで対応した記録になります。不正確な部分、あえて詳細に触れていない部分、また間違ってる部分などがあると思います。間違っている部分についてはご指摘いただけると幸いです。

はじまり
学「澤田さ～ん、今お時間よろしいですか」
私「いーよ」
学「ネットワークについて教えてほしいのですが」
私（アバウトに来たな）「ネットワークって具体的に何を？」
学「先輩が話していることを理解したくて、ネットワークをどう学べばいいのかと」
学「例えば、サーバってなんですか？コンピュータと違うのですか？」
学「メールってどうなってるんですか？クライアントって、僕のマシンでメール書いてますけど、これはクライアントですよね？この書いたメールがどうやって相手に届いているんですか？」
私「なるほど、じゃあ一から説明することにしますか」

ネットワーク階層モデルの話
私「確か基本情報処理技術者の資格持ってるって言ってなかった？当然ネットワークも範囲に入ってるはずだけど」
IPAのページ検索
私「サーバはコンピュータであり、情報の提供者、ここでいうIPAね。で、それを受け取って表示する側がクライアント、具体的に言うとこのブラウザ」
私「クライアントからサーバに向けて、これくださいというリクエストが送られて、サーバから、はいどうぞとレスポンスが返される。これが基本」
私「で、どっちから話すか、上からでいいか。具体的にどのような形式でリクエストを送り、どのような形式でレスポンスが返されるかが決まっている。それがプロトコルというもの。HTTPのPはprotocolのP」
私「で、このサーバのコンピュータではいろんなプログラムが動いているわけだけど、・・・あーやっぱり下から説明した方がいいな」
私「情報処理技術者試験の勉強してたときにOSI参照モデルっての出てこなかった？」
学「ありました」
私「今ネットワークについて説明しようとする際に、全部まとめてごちゃっと説明しようとするとすごい説明しにくいのね。それを切り分けて説明するために階層構造が作られているというわけ」

物理層～データリンク層（イーサネット）

物理的な接続
私「そこにケーブルが下がってるわけだけど、通信をするためにはともかくつながっていないといけないわけね。まあ今どきは無線もあるわけだけど」
私「で、ともかくつなぐことができたら0,1が送れると。例えば電圧が高いのを1、低いのを0みたいに。これが物理層」

非常に雑なイーサネットの説明
私「で、0,1は送れるようになったわけだけどそれだと使いにくいからある程度まとまった単位でやり取りをするようにしたわけで、現実世界だとハガキを送るみたいなもん。層によってハガキの名前が違うのだけど、イーサネットだとフレームと言う」
私「さっきは線がつながっていれば通信できると言ったけど、実際にはネットワーク内にコンピュータが何台もあるから誰に送るかを決めないといけない。誰に送るかというのがアドレス」
学「172なんとかってやつですか？」
私「それはIPアドレス。イーサネットだとMACアドレスと言う」
学「MACアドレスってコンピュータに一つなんですか？」
私「いや、ネットワークカードに一つ。だからネットワークカードを2つ挿してたら2つMACアドレスがある。このポート1つにMACアドレス1つな感じ」
学「僕のマシンはポートないんですけど」
私「それって無線だよね。まあ無線のモジュールに対してMACアドレスが付いてる」
※通信全体の話をしたかったので、MACアドレスの形式、アドレス長などの話はざっくり省略です

ネットワーク層（IP）
私「データリンク層で通信はできるようになったわけだけど、このIPAのサーバ、そこら辺にありそう？」
学「ないですね」
私「そう、IPAのサーバは別ネットワークにある。つまり、IPAのネットワークまでリクエストを届けないといけない。それをやってるのがネットワーク層、TCP/IPで言うとIP。ここで使われるのがIPアドレス」
学「あー、だからインターネットって言うんですね」
私「別ネットワークに届けるってどうやってるのとか色々話すと長くなるのでざっくり省略する。ともかく相手までは届くようになったわけだ」
※同様に、IPアドレスの話等はざっくり省略です

トランスポート層（TCP）
私「相手まで届くようになったので、やっと話が戻ってくるわけだけど、サーバコンピュータで動いてるどのプログラムに届けるか決めなければいけない。これを扱うのがトランスポート層」
学「どうやってるんですか」
私「ああ、言い忘れてた。ポート番号で識別する。HTTPだと80ね」
学「プロキシだと8080ですよね」
私「まあ基本的に自由に使っていいのだけど、自由過ぎるとつなぎ先がわからなくなるからこのサービスにはこの番号ってのが決まってる」
※TCPのハンドシェイク等については以下同文

アプリケーション層（HTTP）
私「ここまでで送る先のプログラムが決まったので、後は個別のアプリケーションの処理になる」
学「セッション層とプレゼンテーション層はどうなってるんですか？」
私「OSI参照モデルだとあるけど、現実の世界だとぶっちゃけそんなきれいに分けれないからセッション層～アプリケーション層がごちゃっとなってる」
学「アプリケーション層のプロトコルっていくつあるんですか」
私「むっちゃある」

HTTPは何をしているのか
学「HTTPってHTMLやり取りするプロトコルですよね」
私「まあHTML以外も送受信できるけど」
学「HTMLの解析結果が送られてくるんですか？」
私「ああそういうこと、HTTPではくださいといったもの、index.htmlくださいとリクエストしたら、index.htmlの内容が送られてくる。それを読み込んで表示するのはクライアント側（ブラウザ）の仕事」

トンネリング？
学「ここまではわかりました。ところで先輩はマシンAで取った画像をマシンBで見れないとおっしゃってるのですが」
私「それだけだと何がいけないのかわからないけど、ネットワークプログラムがちゃんとできてないのか、さっきは難しくなるから略したけど、ファイアウォールがあってIPもしくはTCPレベルではねられているのか」
学「トンネリングがなんとかっておっしゃってたんですけど」
私「そっちの方。ふーむ、その話に進むと一段難しくなるわけだけど、具体的に何がしたいかにもよるし」

次回予告？
というわけでネットワークの基礎について対話形式（学生君の相づちや簡単な質問は省いているので大体私がしゃべっていますが）で説明してきました。
結局のところ学生君の聞きたかったのはややこしくなるから省いたプライベートネットワークとその接続のことのようです（まあそれを理解するための前提となる基本的な通信について説明したわけですが）。おそらく次週もやってくると思うのでそれを反映した各層の深掘りについて記事を上げる、かもしれません。
",False,https://qiita.com//junjis0203/items/cb4ee3eeea7a0cdc0ac2
"

「愛の反対は無関心である」
マザーテレサが言ったとか言わなかったとか噂される伝説の言葉。

コメントを 付けちゃいけない 理由など
あるのだろうか ないのだろうか。
だけど現状を見ると、まるで、そんな理由が存在するかのように振る舞っているように思える。
it behaves like ... xxx
なんのオブジェクト指向なんだ。なんのダックタイピングなんだ。
そう感じることがある。

「僕らはもっと、Qiitaのコメント欄を使って良いはずだ」
こちらが本当のタイトルだ。
たとえば優れた技術記事に対してだって「ただの感想」「受けた印象」を書いたって良いじゃないか。
なぜいけないんだ。それが世界を悪くするとでもいうのか。

いいね問題 〜ドアと切手と花束と〜
「いいね」に比べて、Qiitaのコメント欄は使われなさすぎる気がする。
これはなんとなく「ドアだけ叩いて、そのまま立ち去って行く大勢の人達」というような感じを受ける。ドアは叩かれただけ、それは人がいたということで、嬉しいことなのだけれど、ドアを開くとそこにはもう誰もいない。
旅人たちは、直接花束を渡して「ありがとう」と伝えることはないのだ。ただ、ドアに「いいね」と書かれた、小奇麗な切手を貼っていくだけだ。(その昔は、この切手には「ストック」と書かれていた)

アンバランスな関係 〜カロリーとビタミン〜

記事に誤りがあると、コメントでの指摘や編集リクエストが飛んでくる。


Qiitaの世界のセーフティネット。いつもお世話になっております。
これ自体は良いことだと思っている。自分の考え間違えも訂正してもらえる。


Qiitaガイドラインに違反すると思われる記事には、一瞬でコメントがつく。


ここでいう「違反」には個人的解釈も含まれる。
個人的には「エンジニアに役立つことなら、なんでも書けば良いじゃないだろうか」と思っている。
Qiitaガイドラインには HRT原則も書かれているはずなのだが、こちらはあまり振り返られない。HRTに関しては後述する。


ユーザーが感情的になりやすい記事には、無数のコメントがついて議論がおこなわれる。これは人間心理手はあると思うけれど。


例: 「ゴミ記事が多すぎる問題」に対する私見 - Qiita

「燃えやすい記事」やのコメントは多いが、他のごく平凡な記事、ちょっといい記事にコメントが付く例は奇跡的に少ない。


飛び抜けて優れた記事には、数百個、数千個のいいねがついてバズったりするが、コメント数は意外に少なかったりする。例えば1000いいねに対して0個ということもある。

カロリーは十分だけれど、ビタミンは足りていない。
カレーのルーはあるけれど、ライスがない。
なんだか、そんな感じがしないだろうか。

なぜ僕らはコメント欄に感想を書かないのだろう
たとえば、コメントを書こうかどうかと迷ったあげく、やめてしまった時の、自分の心理を見返してみる。

「技術共有サービスなので、何か技術的な知見や、情報を書かなければけない」とハードルを上げてしまう。
「間違ったことを書いてしまうのではないか」という恐れ。
「取るに足らないことを書いてしまうのではないか」という遠慮。
「周りと外れたことを書いてしまうのではないか」というQiita世間体。エンジニア世間体。
「いいね」機能があるので、あえて「いいね」をコメントにする必要がないんじゃないかと思ってしまう。
短い文章で感謝を表そうとするが、短いがゆえに内容のなさを感じたり、小さなニュアンスに困ったり、失礼にならないか迷ったり、考えすぎて結局コメントをやめてしまう。

上のようなことを気にして、何度心に思ったことを形にせずに、そのまま記事を立ち去ったことだろう。何度ドアに切手だけを貼って、花束を渡さずに枯らしてしまったことだろうか。

自分で自分に反論するバリエーション

Qiitaは技術共有サービスだが、コメント欄にまで必ず技術情報を書かなければいけないわけではない。
たとえ取るに足らないことを書いてしまったとしても、世間は自分のことなんかまるで気にしない。考え過ぎは自意識過剰から起こる。
周りから外れたことを書いてしまったとして、それが何だというのだろう。周りの99人が「悪い」と言っている記事なら、僕らは「悪い」と言わなければいけないのだろうか。
確かにQiitaには「いいね機能」があるが、自分自身の手でキーボードを打って伝えることに、意味と手応えがある。


ユーザーインターフェイス的にも、いいねは単純に数がカウントされていく画面の一部領域に過ぎないが、コメントは「縦幅」をとる、厚みのある存在だ。
「いいね」は固定された三文字の日本語だが、コメント欄にかく文章は、たとえそれが三文字の「いいな」であったとしても、無限の日本語の組み合わせから、選び合わされたものだという部分で、意味がある。


短い文章はたしかにニュアンスを伝えるのは難しいが、3秒迷っている間に書いて、コメントを送ってしまった方が良い。


Qiitaのガイドラインには HRT もあるんだぜ
コメント欄で「ありがとう」「良かった」を伝えることは、Qiitaにも承認されている行為だ。(と僕は憲法解釈をする)

記事を書いた人に対する尊敬の思いを表現として含めることが大切です。
HRTとは、書籍『Team Geek――Googleのギークたちはいかにしてチームを作るのか』で紹介されている「Humility（謙虚）」、「Respect（尊敬）」、「Trust（信頼）」を示す言葉です。



ソーシャルにも心理的安全性を
仕事や開発チームにだけじゃなくて、ソーシャルサービスにも「心理的安全性」ってやつが必要なんじゃないだろうか。
それは感謝を伝えたり、お互いを尊敬し合うことによって生まれるんじゃないだろうｋ．
Webサービスというと、システムやユーザーインターフェイスの問題が取り上げられがちだけれど、それよりも「文化」というものが深く依経しているのではないだろうか。つまり、人間の問題だ。

コメントと雑談効果
ところで僕は、雑談から仕事は生まれる、雑談からひらめきは生まれると思っている。雑談効果だ。
もっと気軽にコメントを付けたりすることで、技術情報を書くことはもっと楽しくなるし、エンジニアリングも楽しくなるし、人と話しているうちに新しい着想も湧くだろうし、悪いことはひとつもなくて、良いことは起こる、そんな良い状態が作れるんじゃないだろうか。
実際にこの「ソーシャルにも心理的安全性を」という項目だって、コメント欄でもらったコメントからひらめきを得て、新しく追加したものだ。
もしコメント欄でのやり取りがなければ、生涯決して思いつかなかったか、思いつくのもずっと先になっていたかもしれない。

「こんなコメントの付け方はいかが」の具体例
最近の自分のコメント履歴から3つを取り上げてみる。
フリーランスエンジニアの単価を決める - Qiita

ペアプログラミングして気がついた新人プログラマの成長を加速する３つの習慣 - Qiita

[Vim] 全行ヤンクにggyG使うのはやめよう - Qiita


たとえば自分が記事を書いた時のことを、逆に考えてみよう
自分が書いた記事にポジティブな反応がついたり、もしくはフラットな感想がついたら、どう思うだろうか。僕なら嬉しい。少なくともプラスであって絶対にマイナスではない。
「コメントが付くと嬉しい」という話は、Qiitaの他の記事でもどこかで見聞きしたような気がする。まさか世界中で僕ひとりだけの感覚ではないだろう。
僕らは「いいね」機能などのお手軽なソーシャル文化に毒されていないだろうか。もっと昔ながらの、Web黎明期のような「自分のコメント」があっても良いんじゃないだろうか。僕ら人類は、昔からずっとそうやって「コメント」を活用してきたのだから。

具体例
こういうのが嬉しいんだよ。




ところでQiitaには「コメント一覧」というものもある
実はたった今気づいた。
自分だけが見られるのではなく、publicに誰でも見られるみたいだ。
この一覧を「特に感動した記事」「心に残った記事」として、「ストックの上位版」として使ってみてはいかがだろうか。

https://qiita.com/YumaInaura/comments

いいねだけじゃダメなの？
　良いと思う。

いいねと思ったときに、いいねを付けるのをやめない。
なにかコメントが浮かんだ時に、コメントを付けるのを止めない。


ポジティブなものであればなおさら、ためらう必要はない。
ただし、ネガティブなものは立ち止まって考えてみよう。



「止めないこと」が大事だと思う。

はてなブックマークやTwitterでコメントする
全然ありだ。
いいねよりも近くて、Qiitaへの直接のコメントよりも遠い。絶妙な距離感を持てることだろう。
(余談。人間はデジタル空間にさえ「距離感」を感じられるのだから、本当に不思議だ)
ただQiitaの直接のコメントよりも、著者には届きにくいところはネックではある。
僕自身も、記事を書いてから2年ぐらいを経て、ようやく自分の記事についてはてぶコメントを見つけたという例がある。

次回続編

「なぜQiitaにポエムを書いてはいけないのか」
「あえて給与を気にせず働く”楽しさ5”のエンジニアになろう」
「”ずぼらヨガ""健康法 〜システムエンジニア編〜」

の三本立てにご期待下さい！

この記事は、ポエムタグの提供でお送りしました
ここまで書いて、その逃げ方？
だがこれは「自分の勝手だから、放っておいてほしい」というニュアンスではないのだ。
「ちょっとエモいこと言いたいんだけど、聞いてくれる？」というニュアンスでのポエムタグ。
理想的世界への情熱と言えば大げさな、ほんのちょっとしたビジョンを感じながら書いたわけだから。
(ここはひとつ。「ニューポエム」とかいうエモダサいタグでも作ってみようか。嘘だけど)

Thanks to
この記事はこの記事に寄せられた皆さんのスポンサーにより新しい着想が浮かび、編集してさらに良いものになりました。
(本文の「マザーテレサ」「心理的安全性」などについては、あとからリメイクされたものです)

まだQiitaでコメントしたことがない人へ
この記事に何かコメントを付けて、「弾み」にしていただければ。
(2018/10/20 追記)

あわせて読みたい？


大事な言葉・HRT～「Humility（謙虚）」、「Respect（尊敬）」、「Trust（信頼）」。ああなんて難しい - Qiita

HRTを見直そう！


拝啓 本当は Qiita を書きたいのに、まだ迷っているあなたへ。 - Qiita

枯れ果てた開発者の心のバケツ - Qiita

OSSとは直接関係ないが、心理的には同種の話。



",False,https://qiita.com//YumaInaura/items/5532cb4eea013b2f4a4b
"

README.md
ゴミ記事界隈の急先鋒たる「侍エンジニア塾」の記事をGoogleの検索結果から除外する方法です。
Chorome限定で「ゴシップサイトブロッカー」という拡張機能で検索結果からサイトを削除します。
侍エンジニア塾の記事をGoogleの検索結果から除外する
https://takachan.hatenablog.com/entry/2018/10/13/002107
勿論、このサイトに限らず、任意のサイトを自由にブロックできます。最近「Personal Blocklist」が機能しなくなり少々困っていました。しかし、代替機能を開発されている方がいらしたのでそのご紹介も兼ねています。

こういったWebジャンクは積極的に通報しましょう
外注のライターに内容が薄い記事を大量生産してSEOをハックするWebスパムの手口なので、Googleの「スパムレポートを送る」を使って通報しても良いと思います。
",False,https://qiita.com//Taka/items/aac598da094ded3de4f0
"

概要
そんな状況来ないのが一番。ではありますが時が流れる以上、廃れてしまうときが来てしまう。
廃れないまでも、運用する人の都合でサービスを続けられなくなってしまう時もある。そんなときは

運用していたサービスを別のチームに任る事になる
サービスを別の方々に引き渡すことになる

ということになりますね。
たまたまそういうサービスに立ち会いまして、色々知見がたまりました。
上手くできないこともありましたし、反省も兼ねて次はこうしたいなぁ。という気持ちも込めてまとめてみました。

Step0 まずは引き渡しあとのフォローする範囲や引き継ぐ人を明確にしよう

引き渡しあとのフォロー範囲
引き継ぎ元の方々の手を離れるわけですから、そりゃすぐには今まで通りの運用は無理です。
もともと５分で対応できていたことが、引き継ぎ先の人たちだと４０分かかるようになったとか。
そのとことチクチク引き渡し先に突かれても互いに不幸なので、こういう不具合や、こういう作業はいつまではコチラで引き受けます。サポートも〇月〇日までは引き受けます。
と、きちんと決めましょう。

引き渡す人と引き継ぐ人を決めよう。
ここがふんわりしたまま引き継ぎ作業が進むと大変です。
引き渡す側は普段使っているマニュアル、デプロイフロー、インフラ内容、アプリケーション運用方法などをスムーズに引き渡せるように準備しよう。
引き継ぐ側は不安点などがあれば予め列挙し、渡された資料は早めに一読し、サポート期間内にできるだけ触っておくとサポート期間が終了後でもかなりさわれる状態になることが出来ると思います。
伝えることはめちゃくちゃあると思うので、誰が渡して、誰がもらうのか。これはすぐに決めるべきです。

Step1 開発環境・インフラを引き継ごう
何を引き継ぐべきかもう少し深ぼってみたいと思います。

インフラを引き継ごう
サーバー周りに関しては構成図などの資料を引き渡し、現状稼働している内容のものを伝えましょう。
AWSなどクラウド管理しているなら、閲覧権限のみのアカウントでもさくっと作って中を見てもらいましょう。百問は一見にしかずです。
もしクラウドで管理してるとなると構成内容によってはお金もガッツリかかることになります。サーバーのコスト感も伝えて、最初はテスト環境などは止めてほしいとか要望があればその時点で聞いておくのもいいと思います。

開発環境を引き継ごう
Chef、itamae等のプロビジョニングツールの使い方。もしくはdockerを使った開発環境の構築の方法ですね。
ここは万が一詰まると手ほどきが大変なので、もし対面で話す機会があればそのときに構築だけはやってしまうといいですね。
早い段階から環境に触れることで、不安も打ち消せるし、引き継ぎ期間ならサポートを受けながら勉強できるし、勉強してもらえれば此方の負担も減るしWIN-WINです。
なる早でやりましょう。

Step2 デプロイ方法を引き継ごう
可能なら簡単な修正を実際に行ってもらって、本番へデプロイする手順を実施してもらいましょう。
勿論、ドキュメントに残して渡すことも大事ですが、サポートなしの状態でデプロイは万が一が怖いし不安も感じると思います。
互いに事故防止をする意味でもやっておきましょう。

Step3 過去起こった障害や、監視ツールを引き継ごう

過去の障害対応
これは車輪の再開発防止だけでなく、相手側に「既にバグとして顕在していたやつかどうか」の判断を素早くしてもらうためにも大切です。
知っていたのに黙っていた。なんてのは最悪です。引き継ぐ人は疑念を抱きますし、都度念入りに確認しなきゃいけないという状態にもなってしまいます。
不幸なすれ違いを回避するためにもやっときましょう。

監視ツール系
エラー検知や、ログ監視、死活監視など普段使っているツールが有ると思います。
ただ、現在使っているツールと相手側の使っているツールとは違う場合がありますので、早めに打ち合わせて乗り換えていかないと大変です。
可能なら、引き継ぎ側がサポートしている間にさくっと対応してしまえるならそのほうが後のトラブル防止になったりします。

Step4 ドキュメント・マニュアルや復旧方法例を引き継ごう

ドキュメント・マニュアル
最低限Step3までやっとけば、引き継ぐ側が色々触れるようになり始め、わからない点を質問したり、疑問点をまとめたりしてくれると思います。
その回答と並行して、システムを構築するにあたってバラけているドキュメントなど整理したりしましょう。
もし既にまとめてあるならStep1とかでやってしまってもいいのですが、おそらくこのタイミングでもまたドキュメントの手直しが発生すると思います。
なぜなら、内情がわかっている側がまとめたマニュアルだと、引き継ぐ側が本当に知りたかった項目というのが漏れたりするからです。
どうせ追記する可能性があるなら、綺麗にまとめたりするのはこのタイミングでも問題ないです。

復旧方法例
サポート期間終了後になんかよくわからんがシステムがダウンした！！
という場合に、引き継ぎ側がすぐに原因を特定できればいいのですが、そうじゃない可能性もあります。
そうならないために、ロールバックの方法であったり、また1からシステムを構築する復旧方法は用意しておくのが良いです。AWSならAMIイメージを用意して、立ち上げるだけでとりあえずは動くようになるとかですね。

Step5 ツールのアカウントやサービスのソースコード。issue・bugを引き継ごう

アカウント
AWSやgithub、監視ツール系などなど、もしそのままスライドして使い続けるサービスがあれば

いつ引き継ぐのか
ツールを利用するための費用は引き継いだ後どう支払いするか
引き継ぎ側の誰がツールを管理するのか
パスワードなど重要情報はどうやって渡すのか
そもそも引き続き使うのか。いらないなら解約するのか。

など早めに相談して、手はずを整えておきましょう。お金が掛っているサービス系は結構引き継ぎが面倒なので注意が必要です。

ソースコード
git管理していれば割とかもしれないです。
が、例えば

リポジトリを新しくして、過去のissueやpull requestなどは引き継がないようにしたい
GithubからBitbucketにしたい

など、そういう要望があると思います。
早めに確認して、どのタイミングで移行するのがベストか決めておくと良いですね。

issue・bug
過去の障害対応について引き継ぐのと似ていますね。
既にわかっている範囲でbugやissueなどは引き継ぎましょう。
あとからbugじゃないか！と怒られてもお互いに不幸です。

Step6　よくある作業や、イレギュラー対応などを引き継ごう
人の手で定期的に行っている作業があれば忘れずに引き継ごう。自動化出来ない何かしらの理由が存在しているわけだし、仕方ない。
あとは、イレギュラー的な対応なんてのは存在していませんか？ 頻度は少ないけど、たまに起こってしまう対応も忘れず伝えましょう。

暗黙知の排除、発掘
長年運用しているサービスだと、気づかぬうちに暗黙知としてしまっている事があると思います。
これを気に、「あ、誰にも伝えてないけどこういう事やってた」っていうのは徹底して排除しよう。
ずっとやっていると気づきにくいかもしれないですが、この辺りは意識して伝えるようにしましょう。

Step7 保守する上でwikiとかtipsがあれば引き継ごう
マニュアルやドキュメントではないが、よくつかう技術を伝えておくのもいいと思います。

このシステムを使う上でちょっと特殊な技術
開発していればぶつかるであろうわかりにくい部分
知っていれば楽に開発が進む部分

そういうのも伝えましょう。感謝はされど嫌がられることはないと思います。

Step8 懸念点を引き継ごう

こういうアイディアがあって、こうしたら良くなるけど時間がなくてやれなかった
顕在化していないバグだが、システム的にこう舵切ったらヤバイ箇所ある

など、相手に予め教えとくだけでも幸せになれます。もし、こうなっていたら。というのは長年サービスを使っていて感じる「勘」のようなものは割と大切だと思います。

終わり
ぼくが引き継ぐにあたって行って良かった、行えなかった反省が以上になります。
勿論これ以外にも細かいところで諸々動いたりはしましたが、大枠はこんな感じです。
こんなに時間取れねーよ！とか、他にもやることあるだろう！とかあると思いますが、そのへんはよしなに改変して使ってもらったり、コメントしてもらえると助かります。
それでは、お役に立てばなんとやらです。
",False,https://qiita.com//mmusasabi/items/790f4892e66f00807984
"

できるエンジニア
のどがカラカラで死にそうな相手に、すぐに水道水を提供する

できないエンジニア
のどがカラカラで死にそうな相手に、南アルプスまで天然水を汲みに行って、コーヒー豆を焙煎してアイスコーヒーを作って提供する
",False,https://qiita.com//kingpanda/items/bf78eb49be3a3b08ad73
"

TL;DR
色々ともてはやされる「オブジェクト指向」です。
オブジェクト指向を使うことで色々な問題が解決されることがあります。
例えば、privateによって保守性を上げることが出来ます。 継承 や ポリモーフィズム によって、または Stateパターン によって条件分岐の複雑性を減らしたりできます。
ただ、こういう考え方自体は、別にオブジェクト指向じゃなくても存在します。
ただオブジェクト指向の方が得意なのです。
というのも実は少しおかしな話で、というかむしろ逆で、「オブジェクト指向」と呼ばれるものがなかった時代のスーパーエンジニア達が考え作りだしてきたスーパーテクニックを、一般大衆が使えるようにまとめあげたものが「オブジェクト指向」なのです。
なので！！
すごくて当然です。
以上

実例

C言語でのスコープの妙技
C言語では、publicやprivateという言語的な概念はありませんでした。
ただ、C言語時代のエンジニアは、2つのグローバルスコープを使って、それを実現していました。
・ファイルを1オブジェクトと考える
・ファイル内のグローバル変数 … ファイル内からはアクセス可能、他ファイルからはアクセス不可 ≒ Privateなメンバ的な使い方
・ファイルをまたがったグローバル変数 … どのファイルからでもアクセス可能 ≒ Publicなメンバ的な使い方

です。
これによって実装のしやすさを維持しつつ保守性をあげてました。

関数ポインタによるポリモーフィズム
これも黄金パターンですが、C言語では関数ポイントとして関数のポインタを変数で持つことが出来ます。
なので構造体に関数ポインタを持たせることで、変数と関数≒メンバとメソッドをひとまとめにすることは普通に行われていました。
また、関数ポインタを、配列にいれて、イベントが発生するたびに配列の添え字を操作して、処理を切り替えていくというのもドライバ界隈では普通に行われていたことです。関数ポインタの型さえ一緒であれば呼び出しが可能なので、アドレスを入れ替えることで、呼び出し元には手を加えずに、別の処理を呼び出すことができました、
インタフェースが一緒であれば、リンクする .so を変えることで処理を変えるということもできました。
これはいわゆるポリモーフィズムです。

つまり
ただ、これらはC言語の本来の使い方ではなく（わけではないですが……）、その有効性が明らかであるのにかかわらず、初学者からみるとかなりトリッキーで難易度が高いものでした。
こういう過去の偉人たちのノウハウをまとめて、それを「新たな言語」のレベルまで「新たなパラダイム」にまで持っていったものが「オブジェクト指向」です。
それは、すごくて当然なのです。
実際わたしのまわりの、こういうのを普通にやっていた先輩エンジニア達は、オブジェクト指向が出てきたときにも「別にいままでやってたことだよね？」という涼しい顔をしていました。。。そこまでの道のりを、ぎゅっと縮めてくれたのがオブジェクト指向だと思って、十分にそのメリットを活用していただければと思います。
",False,https://qiita.com//567000/items/f4f8e75172c15b2cafa3
"ゴミ記事が多すぎるなんてのがまぁいろいろと火種になっている今日このごろ、あの議論をするのは基本的に強い人達で、初心者たちは端っこで縮こまっている(か、そもそもこの話題を知らない)ので、初心者からポエムを書いてみようという、ゴミ記事です。
当方、別にゴミ記事を書くことを推奨しているわけではなく、ゴミ記事を寛大な心で見てほしいという心持ちです。

誰でも最初は初心者です。
これはあまりに当然なことだと思うんですが、初心者のうちには全てのことがとてもむずかしいです。Hello worldすら何か見ないとかけないなんてこともまぁあります。そして初心者にとって難易度や、コミュニティ全体のことを考えて記事を書くことなんてまぁ無理です。自分の足跡として、もしくは書いてみたいという好奇心とか、モチベーションみたいなもので初心者は記事を書くわけです。そこでゴミ記事を書くなと言われてしまうと、単純に辛いです。ゴミ記事の定義もはっきりしないまま否定されることは、理由もない否定に近いですから。かといって、ゴミ記事のボーダーなんて決めようがないですし。
エンジニアはPCとかに向かい合うのも仕事かもしれませんが、記事を読んでいるときに向かい合っているのは人でもあります。少しは記事の向こうの人についても考えていただきたい…
上級者の立場から一方的なゴミ記事認定をすることはパワハラに近い性質を帯びている気もしなくもないです。

日本の技術記事はゴミばっか？
If you seriously worry about engineers' level in Japan, it's more reasonable to teach beginners or writers of bad articles to make them better authors than only to damn. If you don't, it's better for you to use only foreign community.
If you are not well accustomed to English communication, I guess there are a lot of ""good English articles for beginners"". Of course, some of them are not useful for those who are fluent in English!
This may sounds offensive, but I want to say that people who are complaining ""I have to search in English"" are lack of English skill. English is a part of engineering skill because almost all the cutting-edge technologies are developed by English. So, whether Japanese articles contain a lot of useless ones has not to be a problem for top engineers.
I never think that I am good enough at English. There must be many mistakes in this short sentences. 
I just want to say that no one is perfect. I hope to be a person who can respect, find good point of, and sometimes　give　advice　(not deny) to others. I wish everyone in a community to be such a person, and also hope continuous development of engineers' community and technology itself.
すでに様々な方がお気持ちを表明されていて、言いたいことはかなり言い尽くされているので結構短くなってしまいました。
",False,https://qiita.com//mosamosa/items/acf8966bf0269bead08d
"最近、端的にいうと「クソ記事を書くな」という投稿が話題になっている。
日本人ITエンジニアの90％に記事を書いてほしくない
私としてはもっと気軽に記事が書けるようになることが結果としてゴミ記事が減ることにつながると考えているため、それについてまとめていきたいと思う。

ゴミ記事が多い理由と、その解決策
まずゴミ記事が目に入ることは、ゴミ記事の絶対量が多いことと全く関係がない。

この世には検索でヒットする数よりはるかに多い記事が存在する
Googleはユーザの行動を元に、どの記事を表示するかを決めている

したがって検索で良い記事をヒットさせるためには、

良い記事が評価されていない
良い記事を書ける人が少ない
良い記事を書ける人が書かない

これらの問題を解決する必要がある。

良い記事が評価されるために　
今はほとんど統計で回っていて、数が多いほうが正解となる。
したがって良い記事かどうかは、「その記事を評価する読者の数」が重要になってきやすい。
そしてプログラミングブームの影響もあり「初心者」の母数が今増えてきている。
そのためか、初心者エンジニア向けの記事が評価されやすくなっているように感じる。
特に「未経験営業からプログラミング勉強始めました」といったような人々も増えており、「ディレクトリという単語を知らない」「コピペという単語を知らない」というレベルの人も多い。
そのため専門用語を使わず噛み砕いた表現を使う記事の方がより多くの人に支持され、結果として上位表示されやすくなっているように見える。
こういった背景から、「日本語の記事はレベルが低い」という感覚を覚える人がいると推測する。
日本人ITエンジニアの90％に記事を書いてほしくない

解決策１：皆がどんどん記事を書く
結局のところ良い記事を評価させるためには読者のレベルを底上げし、良い記事を良いと思う人を増やす必要がある。
そのための手段は色々あると思うが、1つとして「記事を書く」ことが成長に繋がることは間違い無く、少なくともそれを否定するべきではないだろう。

解決策２：良い記事を評価する
良い記事として評価することは、個人でもできる。
たとえば「いいねをつける」「SNSやBlogで拡散する」などだ。
これによりQiitaやGoogleでの評価が高まり、人々の目につく確率が高くなる。

良い記事を書ける人を増やすために
例えば100人の村に、良い記事を書ける人が10人いたとする。
そこで90人のゴミ記事を書く人が書くのを辞めたとしても、良い記事は増えない。
これを日本語圏全体に広げて考えると、日本語圏全体で良い記事が書ける人を増やす必要があるということになる。

解決策１：皆がどんどん記事を書く
文章を書く能力がつくのももちろん、書くことで知識が体系化し理解が深まる側面もある。
したがって皆がどんどん記事を書くことが「良い記事を書ける人を増やす」ということにつながっていくわけだ。
これについては他記事でまとめられているため、詳細は省く。
エンジニアは全員技術記事を書くことを習慣化した方がいいぞ

解決策２：間違った記事があったら訂正してあげる
間違った記事があったときにそれを訂正してあげることも重要だ。
その記事を見て間違った理解をする人が減るのはもちろん、記事を書いた人の理解も深まるからだ。
弁証法的に議論が進んでいくことで、より良い答えを導き出せるようになることが重要である。
「空気を読む人」が海外で評価されない、実はとても哲学的な理由

良い記事が書ける人に書いてもらうために
良い記事が書ける人が書かない理由は色々あるが、日本語圏全体の心理的安全性が低下すると書く意欲がなくなってしまうのではないだろうか。
どんな記事であっても、いざ書こうとすると何時間もかかってしまったりする。
それを世の中に出したときに、仮に「こんな記事を書くな」という空気感があると、記事を書くという意欲が失われてしまってもおかしくない。
「外にアウトプットをだす」という行為は心理的ハードルが高いため、そういう経験があると内に閉じてしまいがちになってしまうと推測する。

「いい会社にしても、それが外に伝わらないんです！！」本当に？多くの場合、それって社内の人がぬるま湯にいて、社外を見てない外に発表したりしないくらい自信がなく、オープンマインドがない。いい人がいる会社がいい会社じゃない。常に学習してラーニングゾーンにいる会社がいい会社では？ pic.twitter.com/mWLkbDHMxd— 広木 大地/ エンジニアリング組織論への招待 (@hiroki_daichi) 2018年10月4日

このツイートは社内の話だが、これは社外との相対的な話でもあるだろう。
仮に日本語圏全体が心理的安全性と責任の高い環境なのであれば、日本語圏全体がラーニングゾーンにいき、スキルレベルの底上げにも繋がってくるかもしれない。

解決策１：敬意を持つ
少なくとも「記事を書く」という時点で、基本的には誰かのために時間をかけて行なっている行為である。
良い記事には感謝をするべきだし、意見が対立したとしても安易に悪意を向けるべきではないんじゃないだろうか。
（もちろん批判するなということではない）

解決策２：良い記事を評価する
良いものを良いと評価することで、良いものを生み出せる人のモチベーションにもつながってくる。
そのため「いいねをつける」「SNSやBlogで拡散する」などで評価することは、結果として良い記事を増やすことにもつながってくるだろう。

おわりに
ということで、結局のところシンプルにゴミ記事が多いなら、日本語圏全体の技術力を底上げする必要があるんじゃないの？という話だと思う。
そのためには個人個人が成長していく必要があり、「ゴミ記事を書くな」という空気感は形成するべきではないんじゃなかろうか。

とまあ長々と書いてきたけど、経験上「自分用メモ」みたいな内容でも役立ったことはなんどもあるので、みなさんどんどん記事を書いてくれると助かります。
",False,https://qiita.com//dorarep/items/cda32733e5763d998af9
"

きっかけ
@shibukawa さんの オブジェクト指向と20年戦ってわかったこと を拝読した際に気づいた事です。

ビジネスの根幹をITで動かしていく時代が来たら、ドメインエキスパートは利用者ではなくて、その会社のシステム部門の中にしかない、という時代が来たよ、という話を聞きました。システムを作ってしまったら、細部まで把握している人はコードを書いた人しかいなくなりますよね。

腑に落ちたのと同時に「エンジニアの未来は明るい」と思いました。

How engineers win over others in business?
理論で組み立てていく姿勢と、その理論を確認して理解できる知能が、曖昧で漠然としたセンスを凌駕する時代が来ます。
現在のソフトウェアエンジニアにとって恐らくは最大級の悩みであり、同時に腕の見せ所でもあるのは曖昧で漠然としていて頻繁に矛盾している利用者側の要求を上手く抽出し、整合性を保ちながら破綻しないようにシステムに落とし込む部分だと思います。
それは昔からあった業務のひとつかもしれませんが、機械は空気読んでくれない、指示した通りにしか動かず、意図しない結果が出た時は指示が間違っていたのだと受け止めざるを得ないが故に、コンピューターの発明以後に急激に進展している分野なはずです。1
この要求の把握がある程度上手くできるようになると、次第に利用者側よりもエンジニア側のほうがビジネスに対してより正確で効果的なアプローチを発見しやすくなると思うのです。
即ち、将来的にはビジネスで勝てるのはエンジニア集団だけになり、エンジニアリングを否定して個人のセンスだけに頼る集団を凌駕するようになるはずです。
ビジネスで勝ちたいなら、まずは自身がエンジニアになる事。それがどうしてもできないなら、最上のエンジニア集団を確保してご機嫌をとりながら味方でいてもらい続ける事、それがビジネスの基本的な常識になる時代が必ずくるのです。
あまりにもビジネス側が無能で、見合う待遇がなされていないと感じるなら、エンジニアはいつでも自分でビジネスを興せばいいし、そこに勝機はあるのです。2

だからプログラミング教育
故に、ソフトウェアエンジニアリングの基礎であるプログラミング教育が義務教育に組み込まれる事は当然に理に適っています。
同時に、されど日本国の義務教育だと思ってしまうのは、極めて個人的にですが義務教育に対しては苦い経験の比重が圧倒的に勝っているからでしょうか。3
プログラミングはとても面白いし、その先にあるエンジニアリングはもっと面白く、極めて機械的になるところから始まったはずなのに、何故かどんどんと人間という泥臭い存在の面白さを気づかせてくれてリスペクトできるようになる所に繋がっていきます。
それを個人の興味や関心、その学習曲線を無視して、義務だからやらなくちゃいけない、落ちこぼれた場合どころか抜きんでた場合ですらも周囲に歩調を合わせる事を強要されるつまらない物にしてしまわないように、現場の先生方や教育委員会的な組織が頑張ってくれると信じたいですね。
先生方、宜しくお願いしますよ!!




対人間の場合に、この点で甘えてしまっている人が多いと感じませんか？ ↩


同時に、想定もしていなかったような泥臭くて人間臭い業務をやってくれている人がいる事をリスペクトするいい機会になるかもしれません。 ↩


「だから海外の教育は素晴らしい」には残念ながらなりません。日本より素晴らしい所があるのと同時に日本にはない彼らだけの奥深い問題が同時に存在しているでしょう。良い部分を取り入れる事ができないか検討する姿勢は大事だと思いますが、心酔して盲信するのは中二病患者の症状の一つだと思います。 ↩



",False,https://qiita.com//a_k_/items/82b9df063061350b7de3
"

有史以来、人々は「こうあるべき」で戦争を続けてきた
この記事は、「こうあるべき」をやめようという記事であると同時に、新たなる「こうあるべき」を人々に押し付ける新たな火種である。
参考までに、オブジェクト指向を巡っては過去にも何度か盛り上がったことがあり、前回の盛り上がりはオブジェクト指向と10年戦ってわかったことだったかと記憶している。かの有名なmatzは、このように言った。

オブジェクト指向と30年付き合ってわかったこと: そんなものは実在しない。— Yukihiro Matsumoto (@yukihiro_matz) May 10, 2016

今回はオブジェクト指向が5000%理解できる記事を皮切りに、オブジェクト指向が0.05%も理解できない記事、【オブジェクト指向】自然言語を基準に設計することの問題点など、数多くの記事が出てきている。オブジェクト指向がn%理解できる記事まとめには、より多くの記事がまとめられている。

オブジェクト指向は便利だから普及した
さて、本題に入ろう。
果たして、オブジェクト指向は考え方として正しかったから普及したのだろうか。単に、便利だったから普及したのでは?
いくら正しくても、便利じゃなかったら、みんな面倒くさがって使わないよ。

「どうあるべきか」「現実はどうであるか」よりも「どう使えたら便利か」
よくある説明の「Animalを継承したDogでpochi.say()したら""ワンワン""」の何がマズいか。
そのように書くことで得られるメリットがさっぱり分からないのがマズい。
犬が動物なのは知ってるが、犬が動物だったら動物を継承しないといけないのか。わざわざそんなことを考えて一体、自分の抱えている問題をどう解決してくれるのか。それがさっぱり伝わってこないのがマズい。
オブジェクト指向を便利に使おう、という観点でいうと。「犬が動物だから動物を継承しないといけない」のではなく、「動物を継承して犬を作ると、動物に共通するものを定義しなおす必要がなくなる。また、犬を他の動物と同じように扱えるから、もしそうしたいなら便利」という話になる。
よくある説明をさらに読み続けると pochi.say() の下の行に tama.say() で""にゃーん""と書かれているかもしれない。
けれど、プログラミングを始めたばかりの人が、その説明を読んで、便利さに気づけることは少ないだろう。
一方で、プログラミング経験がある程度ある人なら、インタフェースが共通である嬉しさ、つまり 何らかの動物.say() で、どんな動物でも似たような動作をするという嬉しさに気づくだろう。また、静的型付けな言語を使ってる人なら、Animalを取る関数を書いときゃDogにもCatにも使えることや、Animalの入る配列を用意すりゃDogもCatも入ることにも気づくだろう。

C言語でもオブジェクト指向っぽい実装はありうる
唐突だが、C言語の話をしよう。C++の間違いではない。C言語の話だ。
オブジェクト指向の話になると、オブジェクト指向を取り入れた言語の構文の話になりがちなので、あえてそこから離れるためだ。
オブジェクト指向じゃない言語のはずの、C言語でもオブジェクト指向っぽい実装はある。
darknetは、Cで書かれたニューラルネットワークのプロジェクトだ。ディープラーニングに少し詳しい人なら、リアルタイム物体認識のYOLOというものを聞いたことがあると思う。このプロジェクトはYOLOの作者が開発しており、YOLOの実装もdarknetでされている。
ソースのファイルに、なんたら_layer.cってファイル名が多いのが気になる。
$ ls *layer.c
activation_layer.c     crnn_layer.c             iseg_layer.c      maxpool_layer.c        shortcut_layer.c
avgpool_layer.c        crop_layer.c             l2norm_layer.c    normalization_layer.c  softmax_layer.c
batchnorm_layer.c      deconvolutional_layer.c  layer.c           region_layer.c         upsample_layer.c
connected_layer.c      detection_layer.c        local_layer.c     reorg_layer.c          yolo_layer.c
convolutional_layer.c  dropout_layer.c          logistic_layer.c  rnn_layer.c
cost_layer.c           gru_layer.c              lstm_layer.c      route_layer.c

layer.cというファイルと、なんたら_layer.cというたくさんのファイルがある。
ニューラルネットワークは、何段もの、いくつかの種類の ""layer"" を積み重ねて構成される。いろんなネットワークが作れるように、いろんなレイヤーを用意しているっぽい。
例えばmaxpoolレイヤーがどうなっているのか見てみる。Cでは、maxpool_layer.cで定義された外部から呼び出せる関数はmaxpool_layer.hで宣言される習わしなので、それを見てみる。

maxpool_layer.h(一部抜粋)
typedef layer maxpool_layer;

image get_maxpool_image(maxpool_layer l);
maxpool_layer make_maxpool_layer(int batch, int h, int w, int c, int size, int stride, int padding);
void resize_maxpool_layer(maxpool_layer *l, int w, int h);
void forward_maxpool_layer(const maxpool_layer l, network net);
void backward_maxpool_layer(const maxpool_layer l, network net);


ふむ。ではconvolutional_layer.hは?

convolutional_layer.h(一部抜粋)
convolutional_layer make_convolutional_layer(int batch, int h, int w, int c, int n, int groups, int size, int stride, int padding, ACTIVATION activation, int batch_normalize, int binary, int xnor, int adam);
void resize_convolutional_layer(convolutional_layer *layer, int w, int h);
void forward_convolutional_layer(const convolutional_layer layer, network net);
void update_convolutional_layer(convolutional_layer layer, update_args a);
image *visualize_convolutional_layer(convolutional_layer layer, char *window, image *prev_weights);


なんとなく似ている。
さて。darknetでは、設定ファイルによってネットワークの形を作ることができ、各レイヤーはnet.layers[]配列に入ることになっている。

parse_network_cfg@parser.c
    while(n){
        params.index = count;
        fprintf(stderr, ""%5d "", count);
        s = (section *)n->val;
        options = s->options;
        layer l = {0};
        LAYER_TYPE lt = string_to_layer_type(s->type);
        if(lt == CONVOLUTIONAL){
            l = parse_convolutional(options, params);
        }else if(lt == DECONVOLUTIONAL){
            l = parse_deconvolutional(options, params);
        }else if(lt == LOCAL){
            l = parse_local(options, params);
        }else if(lt == ACTIVE){
        // 略
    }


設定ファイルの中身に応じて、各レイヤーをパースし、layer型の構造体にしていく。
オブジェクト指向言語だと、それぞれのレイヤーは、そのレイヤーに応じてlayer型を継承した型にすると思う。Cでは継承の概念がないので、Darknetでは、全部layer型となっている。layer型はどのレイヤーが来ても困らないように、かなり多数のメンバ変数を持たせている。恐らく、子レイヤーで新たなメンバ変数が必要になったら、layerに新たに付け足すか、別の層のために用意したものを使い回すのであろう。
なお、Cの言語仕様としては継承の概念はないが、Cの黒魔術により、余分にメモリを確保してlayerの後ろを使うこともできる。(実際、そのようにしてCで継承のようなことをしているソースも見たことがある)
では、メソッドはどうしているのか。大変いい質問だ。
オブジェクト指向言語風にいうコンストラクタにあたる、make_なんたら_layer()に答えがある。
l.forward, l.backwardは、メソッドのように使われているが、どうやって作っているのか見てみる。

make_maxpool_layer
maxpool_layer make_maxpool_layer(int batch, int h, int w, int c, int size, int stride, int padding)
{
    maxpool_layer l = {0};
    l.type = MAXPOOL;
    l.batch = batch;
    l.h = h;
    // 略
    l.forward = forward_maxpool_layer;
    l.backward = backward_maxpool_layer;


forward_maxpool_layer, backward_maxpool_layerはヘッダで定義されていた関数だ。
つまりこれは、関数ポインタ！
Cには、最近の言語と違って「関数はオブジェクト」なんて概念は無いが、関数もメモリ上に置かれるものである以上は、アドレス(単なる数値と思っていい)を持ち、そのアドレスを指定することで関数を呼び出すことができる。
つまり、変数の中に関数(のアドレス)を入れることができ、呼び出すことができる。
こうすることで、まるでCがオブジェクト指向言語だったかのように、配列に入れたnet.layers[]の各要素について、l.forward()を呼び出しができる。

forward_network@network.c
    for(i = 0; i < net.n; ++i){
        net.index = i;
        layer l = net.layers[i];
        // 略
        l.forward(l, net);
        // 略
    }


l.forward()が指す関数は、レイヤーの種類ごとに異なる。第一引数にl自身が入れられているのは、Pythonでいうところのselfにあたると思ってほしい。
言語としてはオブジェクト指向をサポートしていないので、こういうのを自分で書かないといけないのがつらいところだ。

オブジェクト指向は便利だ
今の御時世、なぜ作者がC言語を使ったのかは私は知らない。
けれど、なぜこのようなオブジェクト指向的な書き方をしたのかは、想像がつく。
オブジェクト指向的な書き方は、便利なのだ。
つまり、コード量を減らし、コードを把握しやすくできる。
似たものを似たような操作方法で操ることができる。
各レイヤーの違いを意識する必要がない場面では、違いについて一切考えなくてもプログラミングできる。
オブジェクト指向的な書き方は、たとえオブジェクト指向的でない言語を使って書いていても、自然に選択肢に入るような、便利な書き方なのである。

あ！　こんなところにもオブジェクト指向が！
ファイルを開く操作を考えてみよう。

ファイルを開いて書く
FILE *fp = fopen(""/path/to/file"", ""w"");
fputs(""test"", fp);
fclose(fp);



Pythonではこうなる
f = open(""/path/to/file"", ""w"")
f.write(""test"")
f.close()


最近は妙に、Pythonの細かい文法に関するマサカリコメントをよく頂く気がするので、念の為に書いておくが、Pythonではwithを使って書いた方が閉じ忘れがなくていいという話は、今はする必要がない。
重要なのは「ファイルって何だ?」という話だ。
ハードディスクなりSSDなりに保存されているバイト列、とでも言うのが適切なのだろうか。
いや、もしかしたらネットワークドライブかもしれない。
Linuxには、procファイルシステムというものがあり、そこには実行中のプロセスに関する情報が書かれたファイルがある。それって、ディスクに保存されてるの??
FUSE (Filesystem in Userspace)というものもある。これは仮想ファイルシステムを作るためのライブラリで、ファイルが開かれたとき、書き込みがされたとき、などの動作を定義すると、自分オリジナルのファイルシステムが作れるライブラリである。
C書けないよって人はここにPythonで動くサンプルが載ってる。
ファイルの開き方、書き方は、皆が知っているとおり、共通した動作だ。
けれど、このような共通のインタフェースの裏では、見知らぬ仕組みが適切に選択され、適切に動いている。どんなファイルシステムか、デバイスは何か、あるいはprocファイルシステムやFUSEのように、ソフトウェア的なものかもしれない。
「ファイル」という仕組みは、オブジェクト指向でよく言われる概念をきれいに実装している。
「ファイル」は、コンピュータの世界で最も成功したオブジェクト指向だと私は思っている。
そして、オブジェクト指向は、便利で分かりやすいインタフェースを作ろうとすると自然に現れる選択肢であると。

この記事で何を伝えたかったのか
オブジェクト指向は素晴らしい。よいものだ。
なぜ素晴らしいのか。なぜよいのか。
確かに、思想が優れているという一面もあるのかもしれない。
けれど私は「利便性」に着目したい。
オブジェクト指向自体を正しく使うのではなく、オブジェクト指向を使うことによって、書くコード自体がシンプルで分かりやすく綺麗になる。
オブジェクト指向を使わされる、オブジェクト指向に使われるのではなく、オブジェクト指向をうまく使うことで、よりよいコードを、より効率的に書ける。
私はそういうエンジニアでありたいし、そういうエンジニアが増えてほしい。
オブジェクト指向自体の「あるべき論」も、私は好きだ。私は宗教戦争大好き人間だ。
けれど、「こうあるべき」よりも「利便性」に着目してオブジェクト指向を見つめ直すと新たな世界が見えてくるかもしれない。
そういう思いから、新たな宗派を立ち上げてみた。
",False,https://qiita.com//gyu-don/items/09db0a298136debfe757
"

きっかけ
あるプログラミング言語のある機能に本当に悩まされている。1
自身の不勉強故もあるが、それだけでなくチームワークにおいては他の人のやり方を受け入れるしかない局面が多い事も苦悩に繋がっていると思う。
やはりこういう局面を想定すると、誰もが自分勝手にコードを書いていい自由はチームワークでは採用すべきではないと思う。
他の人のやり方を無制限に受け入れ続ける事はできないので、それを一定の範囲に縛らざるを得ないのだ。

功罪両面を捉えて、破綻から逃れて現実を生き抜く為の、とある暫定的なソリューション
無論、規約で自由を縛る事は、規約制定側の、極めて狭いであろう想定範囲に物事を縛り付ける事だから、いつでも規約は見直し続けなければいけない。
規約制定作業には、悪意を持たない関係者なら誰もが参加できるべきであろうし、
そのように複数の異なる意見が衝突する事が容易に想像できる状態でも、
一つの規約を確定させる為には、何らかの意思決定機構が必要だ。
このように決められる規約は暫定的なものにしか成り得ないだろう。
恒久的で変更不可能な規約を制定し、それを尊寿し続ける事は、未知の状況に際した時に破滅が不可避である。
実際にそのようなものを制定して、それでも未知の状況に対応するならば、「恒久的で変更不可能」とした事を翻す事になる。
我々に与えられた時間は有限で、無限に想定する事は事実上不可能だし、
それでも現実では未知の状況を含めた想定外の状況が起こる。
ならば、「この規約は恒久的で変更不可能」と定義する事は賢明に将来を生きようとする者の選択ではないはずだ。
その選択は実在しない安定した世界、もし実在させるなら時が止まって生きているとは言えない世界向けの選択になる。
破滅を回避しつつ現実を生きる為には、暫定的な規約を定義し、同時にその規約を保守し続ける機構も定義しなくてはいけないだろう。
この暫定的な規約とそれを保守する機構、改定の頻度、参加者の誰にどんな権限があるべきあるいはあるべきではないのかについては、
唯一の大域的最適解は、これもまた現実的に与えられた時間で求められる物事ではないように見える。
それでも、だから考えないのではなく、これもまた暫定的に成らざるを得ない事を承知の上で、
せめて局所的最適解でも目指すつもりで決めなければ、混沌の中にある破滅から逃げ続けられるとは言えないだろう。

結論
現実世界でチームワークをするなら、良い規約になる事を目指して規約を定義せよ。
それでも定義された規約は決して究極の物にはなり得ないのだから、規約を改定をする機構も定義せよ。
その機構でさえも決して究極の形にはなり得ないのだから、その機構自体に、機構の自己改定機能を内在させよ。
全ては現実世界で破綻から逃れつつ未知の状況に対応していく為に。

追加の観点

最初にやる事
この規約の定義と改定の機構を持つ事は簡単ではないだろうし、全てを最初から想定することはできないのだからまずはたたき台を作って始めてみて、現実のフィードバックを得ながら改善していく事がよいように思える。

規約の大きさ
規約が大きくなれば、覚えきれない、探しきれないという状態がすぐにくる。
こうなれば規約を守りたいと思っても守れない事になる。
これもまたバランスだから不要になった規約は随時捨てていく必要があるだろう。
規約を探しやすくするツールなども、あればまた有用だろう。

規約の改定の頻度
多すぎても少なすぎても問題があるだろうが、これも組織のサイズや性質、その他の状況に依存するので、いい具合の所を模索し続けていく必要があるはずだ。

意思決定機構
これは現実の組織では割と簡単な問題で、実際には機構の外側になるだろうが組織の代表が最終にして至高の意思決定者になる。
つまり全てはその人にどんでん返しされ得る儚い機構なのである事を理解していなければ、実際にその時が来た時につらすぎるだろう。
それでも、やはり機構の内側に最終にして最高の意思決定者を、実際にはそうなのではなくても形式的には定義したほうがよいと思う。
その人は自身のセンスが世界一だと己惚れるなら独裁者にもなれるが、謙虚さが欠片でも残っているならば、その人の下に合議制のようなシステムを構築するべきだと思う。
この時、「全ての人は平等であるべきだ」などという綺麗事は恐らく偏った思想の一つになって、矛盾して解決困難な状況を多く生み出す要因になると思う。
全てのステークホルダーはその特性から分類して、何ができて何が許されないのか決めていかなければいけない。

複雑すぎる機構への対応
ここまで書いた事だけでも既にとても複雑で容易には管理できない。
意思決定機構も定義改定機構も多重の階層化が必要になるだろう。
そしてそれは、途中に挟まる各層毎に、それより下の層をより縛る事なのだから、
上にいけばいくほど謙虚で真摯に洞察し続けるセンスと努力が求められる事だ。
馬鹿と無能は上層に置いてはいけないし、そうなってしまった時にいち早く気づいて、いち早く取り除く事は、より上層にしかできない極めて重要な仕事のひとつである。
上層になればなるほど、より謙虚に、より真摯になるべきであり、
自身が下層にいて、上層がそうではない、最上層もそうではなかったと結論した場合には、ただ立ち去り、それ以降も何も言わない事が一番賢い選択肢に思える。
上層に状況を伝える努力は必要だが、それが単なる闘争に変質してしまった時、勝機はない事の方が一般的だ。
ある程度永い眼でみて、状況を受け入れられないか、あるいは今は無理でも受け入れられるように努力して成長できないかは検討する価値があるが、何事にも限界はある。
本当に無理な状況であった場合はその努力の先にあるのは成長ではなく破滅のほうだ。
しっかり見据えて、限界を超えつつあるときは勇気ある撤退を選べる事も、ある種の知性のひとつと言える。

オチと記事名の回収
記事名につけた「しないと酷い結果に繋がる」というのは、このようにこの職場を辞めるべきなのかまで悩む人が出てくるという話である。

実際にコーディング規約を制定する際に参考になりそうな記事


ベストなコーディング規約の作り方
どんな項目について、どのように検討すべきなのか簡素かつ詳細なまとめがあります。





この件については具体的に挙げてしまうと差し障りもあるだろうし、上手くパターンとして抽出できればこれに限らない汎用的な知見になると思うので語る事は後の別の機会に譲る。 ↩



",False,https://qiita.com//a_k_/items/dd55671c547969b3e597
"少し時間が経ってしまいましたが、Sentencepiceというニューラル言語処理向けのトークナイザ・脱トークナイザを公開しました。MeCabやKyTeaといった単語分割ソフトウエアとは趣旨や目的が異なるソフトウェアですので、少し丁寧にSentencepieceの背景、応用、実験結果等をお話したいと思います。

サブワード
ニューラル言語処理の中心となる要素技術にLSTM (RNN)があります。テキスト(トークン列)を低次元のベクトルに符号化したり、ベクトルからテキストを復号化したり、その応用範囲は多岐にわたります。ニューラル機械翻訳 (NMT) は、LSTMによる符号化・復号化を組み合わせて翻訳を行います。
NMTのアーキテクチャは従来法と大きく異なりますが、入出力はこれまでと同様、なにかしらのトークン列です。どのような列でもよいのですが、慣習的に単語列が使われてきました。多くの人が何も考えずに、MeCab(+neologd)で分割した結果をLSTMに食わせているのではないでしょうか。しかし単語をそのまま扱うのは実用上の問題点があります。RNNによるテキスト生成では、語彙サイズに依存した計算量が必要となるめ、大規模な語彙を扱えません。高頻度語彙のみに限定することで計算量の問題は回避できますが、低頻度語が捨てられてしまいます。
この問題を解決する手法の一つがSentencepieceの土台ともなったサブワードです。1
サブワードのアイデアは非常に簡単です。要は、低頻度語は文字や部分文字列にフォールバックしましょうというだけです。具体的には、事前にテキストを単語分割し、各単語の頻度を求めておきます。このときに、高頻度の単語は1単語として扱い、低頻度語はより短い単位に分割します。最終的なトークン数が事前に与えられたサイズ(通常数千から数万以下)になるように分割を進めていきます。
Byte Pair Encoding (BPE) は、テキストの圧縮率を目的関数にして、貪欲的に分割を決定していくサブワード分割アルゴリズムです。BPEはもともとデータ圧縮の分野で提案された手法ですが、NMTのトークン化に適用されその効果が報告されています。
サブワードにより実質未知語がなくなります。どんな低頻度語でも最終的には文字のならびにフォールバックされます。また、数千から数万程度の語彙サイズになるよう分割が行われるため処理速度が向上します。さらに、テキストの圧縮率をベースに最適化を行うため、テキスト生成時のステップ数もそれほど増加しません。仮に「文字」で分割してしまうと、語彙サイズは減りますがステップ数の増加が問題になります。サブワードは語彙サイズとステップ数のバランスをうまくとる効果があります。

サブワードからSentencepieceへ
サブワードはシンプルなアイデアにもかかわらずその効果は想像以上です。しかし、単語列からスタートしているのが気になります。英語等のヨーロッパ言語の場合、単語の同定は容易ですが、日本語・中国語にサブワードを適用しようとすると、事前にMeCabやKyTea等で分割しないといけません。この事前の分割処理をなんとか駆逐できないかと思って作ったのが Sentencepieceです。
Sentencepieceは、単語列からスタートするのではなく、生文から直接分割を学習します。アイデアはそれだけですが、以下のような拡張を加えています。


BPEの高速化: Sennrichらが単語リストからBPEを学習した理由に計算量があります。ナイーブに実装すると計算量はO(n^2)(nはテキスト長)になるため、単語分割による探索空間の削減は必須でした。Sentencepieceでは O(n log n)のアルゴリズムを採用しており、大規模な生文から直接学習・分割が可能です。

言語モデルベースの分割:  BPEとは別に、言語モデルベースの分割手法も実装しています。大雑把な比較として、BPEはトークン数を目的関数にする、辞書に基づく圧縮、言語モデルは尤度を最大化するエントロピー圧縮とみなせます。どちらもテキストを圧縮するという意味では同じです。最終的な分割結果を見ると、BPEの貪欲法によるエラーが散見され、言語モデルのほうが正しい分割をしているようです。2


End to End 処理に向けたテキストの可逆分割: これは後述します。

以下に分割例を示します。SentencePieceでは、低頻度の固有名詞は文字単位に分割されますが、高頻度の機能語列(〜により、〜された)は1トークンになっていることがわかります。結果、分割数をそれほど変えることなく、語彙サイズを1/10 (8k) にまで圧縮しています。



分割手法
分割
分割数




SentencePiece (8k)
本 山 は 、 足利義 満 により 建立 された 京都 の 相 国 寺 。
15


KyTea
本山 は 、 足利 義満 に よ り 建立 さ れ た 京都 の 相国 寺 。
17


MeCab
本山 は 、 足利 義満 により 建立 さ れ た 京都 の 相国寺 。
14


MeCab-neologd
本山 は 、 足利義満 により 建立 さ れ た 京都 の 相国寺 。
12




テキストの可逆分割
「脱トークン化」とは、トークン列からもとの文を復元する処理のことを指します。形式的には、トークン化(単語分割)の逆変換に相当し、ユーザに提示する文字列を生成する重要な処理です。この脱トークン化、単純そうに見えてトークン化と同様、言語依存処理の塊です。
例えば ”Hello World.” という文は [Hello] [World] [.] の3つのトークンに分割できます。脱トークン化はこの逆変換、すなわち3トークンからもとの文を生成する処理ですが、[Hello] と [World] の間にはスペースを入れ、[World] と [.] は結合するといったように処理を分けなければなりません。言語が変わるとどうでしょう？ 日本語の [こんにちは] [世界] [。] の場合、スペースは不要です。
スペースを入れるか入れないかという判断は何を拠り所にすればよいのでしょう？ これぐらいなら、ちょっとしたルールで何とかなる... いやいやそんなに甘くはありません。

[I] [said] [""] [Hello] [""]  →  I said ""Hello""
いわゆるダムクォート。開き・閉じでスペースの位置が変わる。
[20] [""] [display] → 20"" display 
"" が単位として使われると左に結合する。
[ABC] [-] [DEF]  →  ABC - DEF or  ABC-DEF?
記号列はもはやもとの表記にスペースがあったかわからない。

[나] [는] [학생] [입니다] → 나는 학생입니다
韓国語は原則分かち書きをするが、品詞によって振る舞いが変わる。


これまで、脱トークン化は、言語と文字種を手がかりに、目を覆いたくなるような複雑なルールで実装されてきました。最先端なNLPをやってるのに、こういうところは前世紀のテクノロジーに未だに依存している現実にバランス感覚のなさを感じます。個人的には真っ先に駆逐したい対象でもあります。
脱トークン化が複雑にならざるを得ない理由は、トークン化の際に、スペースの有無情報を一切落としていることにあります。
Sentencepieceは、スペースも通常のトークンとして扱います。具体的には、NFKC等の言語非依存の単純なテキスト正規化処理を行ったあと、便宜的にスペースをメタ文字(""▁"" (U+2581)) に置き換えます。
Hello▁World.
このあと、BPEや言語モデルに基づく教師なし分割を行います。例えば以下のように分割されたとします。
[Hello] [▁Wor] [ld] [.]
トークン列に元のスペースの情報が残っているため、脱トークン化は以下の操作で曖昧性なく行えます。
detokenized = ''.join(pieces).replace('_', ' ')
トークン化と脱トークン化が対称性を持ち、可逆変換になることで様々な利点が生まれます。まず、言語依存の処理がなく、データから教師なしで分割を学習するため、解析・生成を含めて完全な End-to-End 処理が可能になります。また、システムが出力したトークン列を情報を落とすことなくそっくりそのまま別の処理の入力として再利用できます。多言語処理も複数言語のコーパスを混ぜて単一のSentencepieceモデルを学習すれば、言語ごとに処理を切り替える必要がありません。

実験
従来の単語分割手法(MeCab,MeCab+nelogd,KyTea)とSentencepieceを機械翻訳を応用に比較してみました。
結果はこちらをご覧ください
Sentencepieceはたった8000の語彙サイズ(しかも英語と日本語で共有)にもかかわらず、従来の単語分割手法を凌ぐBLEUスコアが達成できています。さらに興味深いことに、一文あたりのトークン数はSentencepieceを使ってもそれほど大きく変わっていません。Sentencepieceはテキストを少ないパラメータで表現し、結果として翻訳精度の向上に貢献したのではないかと考えます。
MeCab+neologd は、他の分割手法に比べてBLUEスコアが著しく低いことがわかります。neologdは、恣意性の高い基準で語彙が決定されているため、統計的圧縮を基礎とするSentencepieceとは対極にある単語分割手法です。少なくともNMTでは、統計的に一貫性のある分割手法を使ったほうがよさそうです。

おわりに
Sentencepieceに限ったことではありませんが、応用に即した単語分割を使うべきだとこれまで主張してきました。意味処理ならJUMAN,  音声がからむと unidic,  情報抽出だと neologd といった塩梅です。
ニューラル言語処理にもそれに適した分割手法を選ぶ必要があります。厄介なことに、これまで良いと考えられてきた基準「文法的に正しい分割」「語彙のサイズ」は役に立たないどころか時として逆効果のようです。単語分割に限らず、ニューラル言語処理はこれまでの常識が通用しなかったり、覆されるような結果が出ることが日常茶飯事です。
ニューラルネットワークは、これまで人手でチューンされてきた特徴抽出・前処理の自動化に大きく貢献してきました。トークン化も一種の特徴抽出とみなせば、下手に外部知識を使うよりは、データから直接学ばせたほうが End-to-End としての性能向上が期待できます。Sentencepieceを使ってみたくなったでしょうか？




他にもSoftmaxをサンプリング等の手法を組み合わせて高速化する手法もあります。 ↩


文法的に正しいことが翻訳に常に有効とは限りません。 ↩



",True,https://qiita.com/taku910/items/7e52f1e58d0ea6e7859c
"

はじめに
固有表現抽出は、テキストに出現する人名や地名などの固有名詞や、日付や時間などの数値表現を抽出する技術です。固有表現抽出は、質問応答システム、対話システム、情報抽出といった自然言語処理を用いた応用アプリケーションの要素技術としても使われています。
今回は機械学習技術を使って固有表現抽出器を作ってみます。
※注意事項
理論的な話は一切出てきません。理論を知りたい方は他を当たってください。

対象読者

固有表現抽出を少しは知っている方
固有表現抽出器を作ってみたい方
Pythonコードを読める方


固有表現抽出とは？
ここでは、固有表現抽出の概要と方法について説明します。

概要
固有表現抽出は、テキストに出現する人名や地名などの固有名詞や、日付や時間などの数値表現を抽出する技術です。具体例を見てみましょう。以下の文から固有表現を抽出してみます。
太郎は5月18日の朝9時に花子に会いに行った。

上記の文に含まれる固有表現を抽出すると人名として太郎と花子、日付として5月18日、時間として朝9時が抽出できます。
上記の例では固有表現のクラスとして人名、日付、時間が抽出されました。一般的には以下の8つのクラス (Information Retrieval and Extraction Exercise (IREX) の固有表現抽出タスクにおける定義) がよく使われます。



クラス
例




ART 固有物名　
ノーベル文学賞、Windows7


LOC 地名
アメリカ、千葉県


ORG 組織
自民党、NHK            　


PSN 人名
安倍晋三、メルケル     　　


DAT 日付
1月29日、2016/01/29 　　


TIM 時間
午後三時、10:30


MNY 金額
241円、8ドル           　


PNT 割合
10％、3割           　　




方法
固有表現抽出を行う方法として、形態素解析済みの文に対してラベル付けを行う方法があります。以下は「太郎は5月18日の朝9時に・・・」という文を形態素解析してからラベル付けを行った例です。

B-XXX、I-XXX というラベルがこれらの文字列が固有表現であることを表現しています。B-XXX は固有表現文字列の始まり、I-XXX は固有表現文字列が続いていることを意味しています。XXX 部分にはORG、PSN などの固有表現クラスが入ります。固有表現でない部分には O というラベルが付与されます。
ラベル付けは規則を用いて行うこともできますが、今回は機械学習技術を用いて行います。つまり、あらかじめラベル付けされた学習データからモデルを作成し、そのモデルを用いてラベルの付いていない文にラベル付けを行います。具体的にはCRFというアルゴリズムを用いて学習を行っていきます。
それでは実際に手を動かして行きましょう。

インストール
まずは必要なPythonモジュールをインストールするところから始めます。ターミナルで以下のコマンドを実行してモジュールをインストールしてください。CRFのライブラリとしてCRFsuiteをインストールしています。
pip install numpy
pip install scipy
pip install sklearn
pip install python-crfsuite

インストールしたら必要なモジュールをimportします。以下のコードを実行してください。
from itertools import chain
import pycrfsuite
import sklearn
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelBinarizer


固有表現抽出器を構築するのに使用するデータ
CRFは教師あり学習なので教師データがタグ付けされたデータが必要です。今回はあらかじめタグ付けしたデータを用意しておきました。こちらからダウンロードしてください。ファイル名は「hironsan.txt」です。
それでは、まずはダウンロードしたデータを読み込むためのクラスを定義します。
import codecs

class CorpusReader(object):

    def __init__(self, path):
        with codecs.open(path, encoding='utf-8') as f:
            sent = []
            sents = []
            for line in f:
                if line == '\n':
                    sents.append(sent)
                    sent = []
                    continue
                morph_info = line.strip().split('\t')
                sent.append(morph_info)
        train_num = int(len(sents) * 0.9)
        self.__train_sents = sents[:train_num]
        self.__test_sents = sents[train_num:]

    def iob_sents(self, name):
        if name == 'train':
            return self.__train_sents
        elif name == 'test':
            return self.__test_sents
        else:
            return None

次に、作成したクラスを用いてダウンロードしたデータを読み込みます。学習データ数が450文、テストデータ数が50文となっています。
c = CorpusReader('hironsan.txt')
train_sents = c.iob_sents('train')
test_sents = c.iob_sents('test')

読み込んだデータの形式は以下のようになっています。形態素解析器「MeCab」で形態素解析を行った後にIOB2タグを付けています。データは一文ごとに分かれており、一文は複数の形態素情報の集まりから構成されます。
>>> train_sents[0]
[['2005', '名詞', '数', '*', '*', '*', '*', '*', 'B-DAT'],
 ['年', '名詞', '接尾', '助数詞', '*', '*', '*', '年', 'ネン', 'ネン', 'I-DAT'],
 ['7', '名詞', '数', '*', '*', '*', '*', '*', 'I-DAT'],
 ['月', '名詞', '一般', '*', '*', '*', '*', '月', 'ツキ', 'ツキ', 'I-DAT'],
 ['14', '名詞', '数', '*', '*', '*', '*', '*', 'I-DAT'],
 ['日', '名詞', '接尾', '助数詞', '*', '*', '*', '日', 'ニチ', 'ニチ', 'I-DAT'],
 ['、', '記号', '読点', '*', '*', '*', '*', '、', '、', '、', 'O'],
...
]

それでは次に、固有表現抽出に使う素性について説明していきます。

使用する素性
ここでは、使用する素性の概要を説明してから、それをコーディングしていきます。

概要
次に、使用する素性について説明します。今回は、前後２文字の単語、品詞細分類、文字種、固有表現タグを使います。以下にこれらの素性を用いた場合の例を示します。枠で囲まれた部分が使用する素性です。

文字種の分類は以下のようになっています。全部で７種類あります。



文字種タグ
説明




ZSPACE
空白


ZDIGIT
アラビア数字


ZLLET
英字小文字


ZULET
英字大文字


HIRAG
ひらがな


KATAK
カタカナ


OTHER
その他



素性として用いる文字種は単語に含まれるすべての文字種を結合したものです。例えば「多い」という単語は漢字とひらがなを含んでいます。ひらがなの文字種タグはHIRAGであり、漢字の文字種タグはOTHERです。そのため、「多い」という単語の文字種は「HIRAG-OTHER」になります。

素性抽出のコーディング

文字種の判定
文字種の判定については以下のようなコードになります。文字列に含まれるすべての文字種を-(ハイフン)で結合しています。
def is_hiragana(ch):
    return 0x3040 <= ord(ch) <= 0x309F

def is_katakana(ch):
    return 0x30A0 <= ord(ch) <= 0x30FF

def get_character_type(ch):
    if ch.isspace():
        return 'ZSPACE'
    elif ch.isdigit():
        return 'ZDIGIT'
    elif ch.islower():
        return 'ZLLET'
    elif ch.isupper():
        return 'ZULET'
    elif is_hiragana(ch):
        return 'HIRAG'
    elif is_katakana(ch):
        return 'KATAK'
    else:
        return 'OTHER'

def get_character_types(string):
    character_types = map(get_character_type, string)
    character_types_str = '-'.join(sorted(set(character_types)))

    return character_types_str


品詞細分類の抽出
形態素情報から品詞細分類を抽出するコードは以下のようになります。
def extract_pos_with_subtype(morph):
    idx = morph.index('*')

    return '-'.join(morph[1:idx])


文からの素性抽出
以上をふまえて、各単語に対して素性抽出をするコードを書くと以下のようになります。少し冗長ですがわかると思います。
def word2features(sent, i):
    word = sent[i][0]
    chtype = get_character_types(sent[i][0])
    postag = extract_pos_with_subtype(sent[i])
    features = [
        'bias',
        'word=' + word,
        'type=' + chtype,
        'postag=' + postag,
    ]
    if i >= 2:
        word2 = sent[i-2][0]
        chtype2 = get_character_types(sent[i-2][0])
        postag2 = extract_pos_with_subtype(sent[i-2])
        iobtag2 = sent[i-2][-1]
        features.extend([
            '-2:word=' + word2,
            '-2:type=' + chtype2,
            '-2:postag=' + postag2,
            '-2:iobtag=' + iobtag2,
        ])
    else:
        features.append('BOS')

    if i >= 1:
        word1 = sent[i-1][0]
        chtype1 = get_character_types(sent[i-1][0])
        postag1 = extract_pos_with_subtype(sent[i-1])
        iobtag1 = sent[i-1][-1]
        features.extend([
            '-1:word=' + word1,
            '-1:type=' + chtype1,
            '-1:postag=' + postag1,
            '-1:iobtag=' + iobtag1,
        ])
    else:
        features.append('BOS')

    if i < len(sent)-1:
        word1 = sent[i+1][0]
        chtype1 = get_character_types(sent[i+1][0])
        postag1 = extract_pos_with_subtype(sent[i+1])
        features.extend([
            '+1:word=' + word1,
            '+1:type=' + chtype1,
            '+1:postag=' + postag1,
        ])
    else:
        features.append('EOS')

    if i < len(sent)-2:
        word2 = sent[i+2][0]
        chtype2 = get_character_types(sent[i+2][0])
        postag2 = extract_pos_with_subtype(sent[i+2])
        features.extend([
            '+2:word=' + word2,
            '+2:type=' + chtype2,
            '+2:postag=' + postag2,
        ])
    else:
        features.append('EOS')

    return features


def sent2features(sent):
    return [word2features(sent, i) for i in range(len(sent))]


def sent2labels(sent):
    return [morph[-1] for morph in sent]


def sent2tokens(sent):
    return [morph[0] for morph in sent]

sent2featuresで文から素性を抽出します。実際に抽出される素性は以下のようになります。
>>> sent2features(train_sents[0])[0]
['bias',
 'word=2005',
 'type=ZDIGIT',
 'postag=名詞-数',
 'BOS',
 'BOS',
 '+1:word=年',
 '+1:type=OTHER',
 '+1:postag=名詞-接尾-助数詞',
 '+2:word=7',
 '+2:type=ZDIGIT',
 '+2:postag=名詞-数']

データから素性が抽出できることがわかりました。後で使用するために、データから学習データとテストデータ用の素性とラベルを抽出しておきます。
X_train = [sent2features(s) for s in train_sents]
y_train = [sent2labels(s) for s in train_sents]

X_test = [sent2features(s) for s in test_sents]
y_test = [sent2labels(s) for s in test_sents]


モデルの学習
モデルを学習するために、pycrfsuite.Trainerオブジェクトを作成し、学習データを読み込ませた後、trainメソッドを呼び出します。まずは、Trainerオブジェクトの作成と学習データの読み込みを行います。
trainer = pycrfsuite.Trainer(verbose=False)

for xseq, yseq in zip(X_train, y_train):
    trainer.append(xseq, yseq)

次に学習パラメータを設定します。本来は開発用データを用いて決めるべきですが、今回は固定値としておきます。
trainer.set_params({
    'c1': 1.0,   # coefficient for L1 penalty
    'c2': 1e-3,  # coefficient for L2 penalty
    'max_iterations': 50,  # stop earlier

    # include transitions that are possible, but not observed
    'feature.possible_transitions': True
})

それでは準備が整ったのでモデルを学習させます。ファイル名を指定して、trainメソッドを実行してください。
trainer.train('model.crfsuite')

実行が終わると、指定したファイル名のファイルが作成されます。この中に、学習したモデルが格納されています。

テストデータの予測
学習したモデルを使用するためには、pycrfsuite.Taggerオブジェクトを作成し、学習したモデルを読み込み、tagメソッドを使用します。まずは、Taggerオブジェクトの作成と学習済みモデルの読み込みを行います。
tagger = pycrfsuite.Tagger()
tagger.open('model.crfsuite')

それでは、実際に文に対してタグ付けをしてみます。
example_sent = test_sents[0]
print(' '.join(sent2tokens(example_sent)))

print(""Predicted:"", ' '.join(tagger.tag(sent2features(example_sent))))
print(""Correct:  "", ' '.join(sent2labels(example_sent)))

以下のような結果が得られるはずです。Predictedは作成したモデルを用いて予想したタグ列で、Correctは正解のタグ列です。今回の文の場合は、モデルの予想結果と正解データが一致していました。
昨年 10 月 に は 、 34 人 が 、 今回 の 現場 に 近い エジプト の タバ で 爆発 事件 の ため 死亡 し て いる 。
Predicted: B-DAT I-DAT I-DAT O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O
Correct:   B-DAT I-DAT I-DAT O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O

固有表現抽出器の構築はこれで終了しました。

モデルの評価
モデルを作成しましたが、これだけでは良いのか悪いのかわかりません。そのため、作成したモデルについては評価を行うことが重要です。それでは作成したモデルについて評価していきましょう。評価は、適合率、再現率、F値で行います。以下が評価を行うコードです。
def bio_classification_report(y_true, y_pred):
    lb = LabelBinarizer()
    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))
    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))

    tagset = set(lb.classes_) - {'O'}
    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])
    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}

    return classification_report(
        y_true_combined,
        y_pred_combined,
        labels = [class_indices[cls] for cls in tagset],
        target_names = tagset,
    )

評価に使うためのテストデータ集合内の文に対してタグ付けします。
y_pred = [tagger.tag(xseq) for xseq in X_test]

学習したモデルを用いてタグ付けしたデータと、正解データを評価用の関数に渡して結果を表示します。各カテゴリについて、適合率、再現率、F値、タグ数を表示しています。
>>> print(bio_classification_report(y_test, y_pred))
             precision    recall  f1-score   support

      B-ART       1.00      0.89      0.94         9
      I-ART       0.92      1.00      0.96        12
      B-DAT       1.00      1.00      1.00        12
      I-DAT       1.00      1.00      1.00        22
      B-LOC       1.00      0.95      0.97        55
      I-LOC       0.94      0.94      0.94        17
      B-ORG       0.75      0.86      0.80        14
      I-ORG       1.00      0.90      0.95        10
      B-PSN       0.00      0.00      0.00         3
      B-TIM       1.00      0.71      0.83         7
      I-TIM       1.00      0.81      0.90        16

avg / total       0.95      0.91      0.93       177

ちょっと結果が良すぎる気もしますが、用いたデータに同じような文が含まれていたのでしょう。
※注意
UndefinedMetricWarningが出るかもしれません。予測したサンプルに存在しないラベルに対して適合率などが定義できないからのようです。用意できたデータ数が少ないので・・・

おわりに
今回は、Pythonのライブラリであるcrfsuiteを使うことで簡単に固有表現抽出器を作ることができました。タグ付けにはIREXの定義に基づいて8種類の固有表現を付けました。しかし、実際に使うにはIREXの定義は粗いことが多いです。そのため、固有表現抽出を何かのタスクに使う場合には、タスクに応じて必要なタグを付けたデータを用意する必要があります。
また、より良い素性やモデルのパラメータを探してみると良いと思います。

参考

python-crfsuite Documentation
Support Vector Machineを用いた日本語固有表現抽出
日本語固有表現抽出におけるわかち書き問題の解決
IREXによる固有表現の定義

",True,https://qiita.com/Hironsan/items/326b66711eb4196aa9d4
"Deep learningのライブラリとして有名なTensorFlowには英仏翻訳のチュートリアルがありますが、AWSのGPUインスタンスを使って英日翻訳で試してみたので紹介します。

環境

TensorFlow 0.9
AWS g2.2xlarge instance
Ubuntu Server 14.04
Python 3.5

TensorFlowをAWSのGPUインスタンスで動かす方法については、以下の記事の通りに行いました。
Setting up TensorFlow 0.9 with Python 3.5 on AWS GPU-instance
初期のTensorFlowはAWSとの相性が悪くパッチを当てる必要があると言われていましたが、バージョン0.9ではすんなりとインストールできました。
注意点として、GPUインスタンスの使用にはAWS側に申請して1日ほど待つ必要があったのと、そのままだとディスクサイズが足りなくなったので32GBに増やした点があります。

データ
機械翻訳モデルの訓練には対訳コーパスが必要となります。グラムさんによる日本語対訳データのリストには無料のデータも載っていますが、量が足りなかったりドメインが偏っていたりして使いづらかったので、英辞郎のデータを約2000円で購入して使いました。英辞郎のデータには単語の辞書も含まれていますが、今回は例文データ（約60万文）のみを以下の前処理を行ったうえで使いました。

【出典】や〈俗〉などのタグの除去：スクリプト

MeCab 0.996とunidic 2.1.2を用いた単語分割（英語はbasic_tokenizerでトークナイズされます）
訓練データ99%とテストデータ1%のランダム分割

前処理後のコーパスは以下のようになりました。

英語

The ""expert on international politics"" is Sophia University Prof. Kuniko Inoguchi, former ambassador to the U.N. Conference on Disarmament in Geneva.


日本語

「 国際 政治 の エキスパート 」 は 、 駐 ジュネーブ 軍縮 会議 代表 部 の 前 大使 で ある 猪口 邦子 上智 大 教授 の こと だ 。

これらのファイルをAWSインスタンスにコピーして以下のようにリネームしておきます。
% ls eijiro-data
giga-fren.release2.en
giga-fren.release2.fr
newstest2013.en
newstest2013.fr

なお、英辞郎のデータで訓練した機械翻訳モデルの再配布は不可とのことなので、今回の記事には訓練済みのモデルは含まれていません。

訓練
機械翻訳モデルを訓練するためには、TensorFlow 0.9のチュートリアルで使われているスクリプトtranslate.pyをダウンロードしてきて、対訳コーパスのあるディレクトリを指定して実行します。
なお、ニューロンの数とレイヤー数はデフォルトの1024と3だとg2.2xlargeのインスタンスではGPUのメモリに収まらなかったので512と2に設定してあります。語彙サイズは英語・日本語ともにデフォルトの40,000語です。チュートリアルによると、アテンション付きの翻訳モデルBahdanau+ 2015が使われているようです。
% wget https://raw.githubusercontent.com/tensorflow/tensorflow/r0.9/tensorflow/models/rnn/translate/translate.py
% mkdir eijiro-model
% python translate.py --data_dir eijiro-data/ \
    --train_dir eijiro-model/ \
    --size=512 --num_layers=2

実行して3日ほど待つと340kステップが終了するので、それなりの翻訳ができるようになります。モデルの精度を表すパープレキシティ（低いほどよい）は3.3程度まで下がりました。
global step 353200 learning rate 0.0090 step-time 0.62 perplexity 3.32
  eval: bucket 0 perplexity 5.23
  eval: bucket 1 perplexity 5.00
  eval: bucket 2 perplexity 6.16
  eval: bucket 3 perplexity 7.34

入力ディレクトリには訓練データとテストデータのほかに、語彙ファイルと単語のID列が保存されています。
% ls eijiro-data/
giga-fren.release2.en
giga-fren.release2.fr
giga-fren.release2.ids40000.en
giga-fren.release2.ids40000.fr
newstest2013.en
newstest2013.fr
newstest2013.ids40000.en
newstest2013.ids40000.fr
vocab40000.en
vocab40000.fr

出力ディレクトリにはニューラルネットのパラメーターが保存されています。
% ls eijiro-model/
checkpoint
translate.ckpt-260600
translate.ckpt-260600.meta
（中略）
translate.ckpt-353000
translate.ckpt-353000.meta

必要なお金は、データを変えたりしながら3回ほど試したので全部で$152でした。

翻訳
訓練したモデルを使って翻訳するには、--decodeオプションを指定してtranslate.pyを実行します。注意点として、訓練に使ったのと同じパラメータを指定しないと行列のサイズが合わずにエラーになります。
% python translate.py --decode \
    --data_dir eijiro-data \
    --train_dir eijiro-model \
    --size=512 --num_layers=2
Reading model parameters from eijiro-model/translate.ckpt-352000
> Hi, how are you?
こんにちは 、 元気 ？
> I am a software engineer.
私 は エンジニア の エンジニア です 。
> My name is Yoh.
私 は _UNK と いう 名前 です 。
> This sentence is machine translated.
この 文 は 、 翻訳 文 を 翻訳 し ます 。

なお、一文の翻訳は軽い処理なのでGPUのないマシンでCPUだけで実行しても大丈夫です。

評価
機械翻訳の評価はテストデータ中の参照訳（翻訳家による翻訳）を用いるBLEUという評価指標がよく使われています。BLEUを計算するスクリプトはコロナ社の機械翻訳（自然言語処理シリーズ）を参考にして実装しました。
BLEU: automatic evaluation for machine translation

% bleu.py eijiro-data/newstest2013.sys.fr =(head -22 eijiro-data/newstest2013.fr)
BLEU: 0.0886173562228986
Brevity Penalty: 0.8911913833655019
1-gram precision: 0.4384858044164038
2-gram precision: 0.14124293785310735
3-gram precision: 0.06432748538011696
4-gram precision: 0.024539877300613498

TensorFlowのスクリプトのバグで翻訳できない文があったためテストデータをすべて使うことはできていませんが、4-gramまでの一致率を見るBLEU-4は8%という結果になりました。

参照訳との比較
vimdiffコマンドを使って機械翻訳と参照訳を比較しました。左が機械翻訳、右が参照訳です。


感想
翻訳精度はまだ高いとは言いがたいものの、これだけ単純なモデルを訓練するだけでそれなりの翻訳ができるのはびっくりしました。ニューラル以前の機械翻訳ではアライメントと翻訳モデルと言語モデルで別々の目的関数を最適化したうえで組み合わせたりしていたので、単一の目的関数で機械翻訳を実現できたという意義が一番大きいのではないかと思いました。
一方でニューラル翻訳は重要な単語を翻訳しそびれたり、同じ単語を繰り返し翻訳したりする傾向があるので改善の余地があると感じました。フレーズベース翻訳ならフレーズテーブルに入っているもの以外は出てこないという安心感があったので、このあたりをどう対処していくかが実用的には重要になってきそうな予感がしました。
",True,https://qiita.com/yoh_okuno/items/50e7dccfb6ebfa8a7238
"この記事では、PRML第6章で述べられている、ガウス過程回帰を実装します。
対応するjupyter notebookは筆者のgithubリポジトリにあります。連載全体の方針や、PRMLの他のアルゴリズムの実装については、連載のまとめページをご覧いただければと思います。

1 理論のおさらい
まず、PRMLで述べられている記号や式をざっくり復習しましょう。ちょっと記号をPRMLから変えてありますが、PRMLの中身を理解している方は、ここを飛ばして2節に進んでしまって大丈夫です。
途中計算1がややいかつい章ですが、ガウス過程回帰は本質的には「ベイジアン線型回帰の一種で、priorとしてガウス過程を用いたもの」と捉えると分かりやすいかと思います。

1.1 設定

$N \in \mathbb{N}$ : 訓練データの個数
$d \in \mathbb{N}$ : 入力の次元
訓練データのうち、入力の部分を$x_0, x_1, \dots , x_{N-1} \in \mathbb{R}^d$、ラベル(出力、目的値)を$t_0, t_1, \dots, t_{N-1} \in \mathbb{R}$とします。今回は回帰問題を考えるので、$t_i$は実数値ですね。また、ラベルをまとめて$t := (t_0, \dots, t_{N-1})^T \in \mathbb{R}^N$を表すことにします。


1.2 ガウス過程
先ほども述べたように、ガウス過程回帰では、ガウス過程をベイジアン線型回帰の事前分布として用います。そこで、回帰問題を考える前に、まずガウス過程そのものについてざっと見ておきましょう。

1.2.1 定義
ガウス過程とは、ある集合$X$上の実数値函数上に値をとる確率変数で、次の条件を満たすものをさします2：
ある$\mu : X \rightarrow \mathbb{R}$と$k : X \times X \rightarrow \mathbb{R}$が存在し、任意の$N \in \mathbb{N}$と任意の$x_0, \dots, x_{N-1} \in X$に対し、ガウス過程で得られる函数$y : X \rightarrow \mathbb{R}$の$x_0, \dots, x_{N-1}$における値$y(x_0), \dots, y(x_{N-1})$が確率密度
$$
\begin{align}
    p\left(y(x_0), y(x_1), \dots, y(x_{N-1}) \middle| \mu,k \right) 
    = \mathcal{N}\left((y(x_0), y(x_1), \dots, y(x_{N-1}) )^T \middle| M, K \right), 
\end{align}
$$
に従う。ただし、
$$
\begin{align}
    \mathcal{N} &: \mbox{ ガウス分布の確率密度} \\
    M &:= \left(\mu(x_0), \mu(x_1), \dots, \mu(x_{N-1}) \right)^T \ \ \in \mathbb{R}^N \\
    K &:= \left( K_{i,j} \right)_{i,j = 0, \dots, N-1}, \ \ K_{i,j} = k(x_i, x_j), 
    \mbox{ $K$は正定値 }
\end{align}
$$
ここで出てきた函数$k$をカーネル函数、行列$K$をカーネル行列(またはグラム行列)と呼びます。

1.2.2 平均と分散
上記の確率密度函数より、
$$
\begin{align}
    & \mathbb{E}[y(x_n)] = M_n \\
    & \mathbb{E}\left[\left(y(x_n)-M_n \right) \left( y(x_m) - M_m\right) \right] = K_{n,m}
\end{align}
$$
が従います。つまり、$\mu$は平均、$k$は共分散を与えるわけです。応用上は、$M = 0$とすることが多いとのことですので、この記事でも以後は$\mu$は常に0を与える函数と仮定します。
この定義は全くもって数学的に厳密なものではありません。ただ、実用上は有限個の点しか考えないので、上記の定義で問題なくガウス過程を利用できるかと思います3。

1.3 ガウス過程回帰
いよいよ、ガウス過程回帰です。既に聞き飽きたかと思いますが、ガウス過程回帰は本質的には、「ガウス過程を事前分布に用いたベイジアン線型回帰」です。ガウス過程の確率密度関数は与えてあるので、この節では尤度を与え、これらから予測分布を導出します。また、3章のベイジアン線型回帰でエビデンス近似を行ったように、エビデンス(周辺尤度)を最大化するハイパーパラメーターを自動で決める方法も述べます。

1.3.1 尤度
入力$x_n$とガウス過程で決まる函数$y$が与えられたとき、出力(ラベル)$t_n$は次の確率密度に従うとします。
$$
\begin{align}
    p\left(t_n \middle| y(x_n) \right) = \mathcal{N}\left(t_n \middle| y(x_n), \beta^{-1} \right), 
\end{align}
$$
つまり、$t_n$は$y(x_n)$にガウスノイズが加わったものとなります。
ノイズが独立と仮定すると、次のモデルが得られます。
$$
\begin{align}
    p \left( t\middle| y \right) = \mathcal{N} \left( t \middle| y, \beta^{-1} I_N \right) , 
\end{align}
$$
ただし、記法を簡単にするために、$y := (y(x_0), y(x_1), \dots, y(x_{N-1}))^T$としました4。また、$I_N$は$N$次元単位行列です。

1.3.2 予測分布
事前分布と尤度を与えたので、ここから予測分布を求めることができます。
正確には：$N$点の訓練データ$(x_0, t_0), (x_1, t_1), \dots, (x_{N-1},t_{N-1})$が与えられたとき、新しい入力$\xi$に対する出力$\tau$の値の分布を求めます5。
$$
\begin{align}
    t &:= (t_0, t_1, \dots, t_{N-1})^T \\
    t' &:= (t_0, t_1, \dots, t_{N-1}, \tau)^T
\end{align}
$$
と表すと、今求めたい確率(密度)は、$p(\tau| t )$と表せます(入力$x_0, x_1, \dots, x_{N-1}, \xi$はいったん固定されているとするので、ここでは省略します。)。
この確率を求めるために、まず同時分布$p(t')$を求めます。次に、$t$について条件づけを行い、$p(\tau| t )$を求めます。
まず同時分布は以下の通りです(PRML (6.61), (6.64)。これらの式はPRML(2.115)から導出できます。)：
$$
\begin{align}
    p(t') &= \mathcal{N}(t'| 0, C'), \\
    C_{N+1} &= 
    \begin{pmatrix}
        C & \kappa \\
        \kappa^T & k(\xi, \xi) + \beta^{-1}
    \end{pmatrix}, \\
    \kappa &\in \mathbb{R}^{N}, \ \ \kappa_n = k(\xi, x_n) \\
    C &:= K + \beta^{-1} I_{N}
\end{align}
$$
これより、条件つき確率は次のようになります(PRML (6.67), (6.68)。これは、PRML(2.81), (2.82)から導出できます。)：
$$
\begin{align}
    p(\tau | t ) &= \mathcal{N} (\tau | m(\xi), \sigma^2(\xi)), \\
    m(\xi) &:= \kappa^T C^{-1} t, \\
    \sigma^2(\xi) &:= k(\xi, \xi) + \beta^{-1} - \kappa^T C^{-1} \kappa
\end{align}
$$

1.4 ハイパーパラメーター最適化
ここまでは、カーネル函数と$\beta$は与えられたものと仮定してきました。
しかし、実際の応用では、適切な$\beta$とカーネル函数を上手く選ぶ必要があります。これを手で行うのは困難ですが、この節ではこれをデータから自動で行う手法について述べます。具体的には、周辺尤度を最大化するハイパーパラメーターを選ぶことにします。なお、ここでは「ハイパーパラメーター」は、カーネルに含まれるパラメーターと$\beta$を併せたものを指すとします。PRMLの本の方には明確には述べられていませんが、ここでは$\beta$も同時に最適化します。

1.4.1 周辺尤度
カーネルのパラメーターを$\theta$と表すことにします。 
周辺尤度$p(t|\beta, \theta)$は
$$
\begin{align}
    & p(\boldsymbol{t}|\beta, \theta) = \mathcal{N}(\boldsymbol{t} | 0, C) \\
    & C = K + \beta^{-1} I_N 
\end{align}
$$
と表せます(PRML(6.61), (6.62)式)。
従って、対数尤度函数は
$$
\begin{align}
    \log p(t|\beta, \theta) = -\frac{1}{2} \log ({\rm det} C) 
        -\frac{1}{2} t^T C^{-1} t -\frac{N}{2} \log (2\pi)
\end{align}
$$
と表せます。
これを$\beta$と$\theta$の函数とみなし、最大化します。

1.4.2 勾配
対数尤度の最大化にあたって、その勾配の情報も利用することにします。
カーネルパラメーターについての勾配は、
$$
\begin{align}
    \frac{\partial}{\partial \theta_i} \log p(t|\beta, \theta) 
    = -\frac{1}{2} \mathrm{Tr} \left(C^{-1} \frac{\partial C}{\partial \theta_i} \right) + \frac{1}{2} t^T C^{-1} \frac{\partial C}{\partial \theta_i} C^{-1} t
\end{align}
$$
で与えられます。$\beta$についての勾配も全く同様です。
$C$の定義より
$$
\begin{align}
    \frac{\partial C}{\partial \theta_i} &= \frac{\partial K}{\partial \theta_i} \\
    \frac{\partial C}{\partial \beta} &= - \frac{1}{\beta^2} I_N
\end{align}
$$
となります。後のコードでは、この表式を用います。

2 数式からコードへ
ここまで述べてきた数式を、コードに翻訳していきます。
まず、カーネル函数を表すクラスを定義します。これを利用して、ガウス過程回帰を行うクラスを定義します。
これらを別々に定義することにより、様々なカーネル函数を用いたガウス過程回帰を容易に行うことができます6。

2.1 カーネル函数

2.1.1 概要
scikit-learnでのkernelの書き方(こちらの公式ドキュメントを参照)に倣い、ここではMyKernelクラスを、以下のデータ属性とメソッドを持つクラスとして定義します(正確には、後で定義するGPRegressionクラスがカーネル函数として利用できるクラスは、以下のデータ属性とメソッドを持つと仮定します。): 

属性



theta : 1次元numpy配列で、カーネル函数のパラメーターを表します。

bounds: リスト(2次元リスト)。 パラメーターの下限と上限を表します。具体的には、次のような形とします：boundsの第i要素はbounds[i] = [a_i, b_i]と書け、a_iとb_i はどちらも実数で、それぞれi番目のパラメーターの下限と上限を表すとします。


メソッド



__call__ : カーネル行列を計算するメソッドとします。具体的には、(n_x, d) numpy配列Xと(n_y, d)numpy配列Yが与えられたとき、MyKernel(X,Y)は(n_x, n_y) numpy配列を返し、MyKernel(X,Y)[i_x, i_y] = $k($X[i_x], Y[i_y]$)$となるとします。また、__call__メソッドは引数として、eval_gradientを受け取るとします。eval_gradient = Trueのとき、カーネル行列に加えて、その勾配も返すとします。

diag : カーネル行列の対角成分を返します。カーネル行列全体を計算してからその対角成分を抜き出すこともできますが、はじめから対角成分のみを計算するほうが効率が良いため、__call__とは別にこのメソッドを定めることにします7。




2.1.2 カーネル函数の具体形
ここでは、カーネル函数の具体的な形としては、PRML(6.63)式のものを用います：
$$
\begin{align}
    k_{\theta}(x, y) = \theta_0 \exp\left( -\frac{\theta_1}{2} \| x - y \|^2 \right) + \theta_2 + \theta_3 x^T y
\end{align}
$$
ただし、カーネルのパラメーター$\theta_0, \theta_1, \theta_2, \theta_3$は非負とします8。
このカーネル函数に対して、その勾配は以下のようになります：
$$
\begin{align}
    \frac{\partial}{\partial \theta_0} k_{\theta}(x,y) &= \exp\left( -\frac{\theta_1}{2} \| x - y \|^2 \right) \\
    \frac{\partial}{\partial \theta_1} k_{\theta}(x,y) &= -\frac{\theta_0}{2} \| x - y \|^2 \exp\left( -\frac{\theta_1}{2} \| x - y \|^2\right) \\
    \frac{\partial}{\partial \theta_2} k_{\theta}(x,y) &= 1 \\
    \frac{\partial}{\partial \theta_3} k_{\theta}(x,y) &= x^T y
\end{align}
$$

2.1.3 コード
以上の設計と数式を元に、以下にMyKernelクラスを定義します：
class MyKernel:

    def __init__(self, theta, bounds=None):
        self.theta = theta
        self.bounds = bounds

    def __call__(self, X, Y, eval_gradient=False):
        '''
        This method calcualtes the kernel matrix for input points.

        Parameters
        ----------
        X, Y : 2-D numpy array
            numpy array representing input points. X[n, i] (resp. Y[n, i]) represents the i-th element of n-th point in X (resp Y).
        eval_gradientt : bool
            If True, the gradient of the kernel matrix w.r.t. to parameters are also returned.

        Returns
        ----------
        K : 2-D numpy array, shape = (len(X), len(Y))
            numpy array representing the kernel matrix. K[i, j] stands for k(X[i], Y[j])
        gradK : 3-D numpy array, shape = (len(self.theta), len(X), len(Y)), optional
            numpy array representing the gradient of kernel matrix. gradK[l, m, n] = derivative of K[m, n] w.r.t. self.theta[l]
            Returned only if return_std is True.
        '''

        tmp = np.reshape(np.sum(X**2,axis=1), (len(X), 1)) + np.sum(Y**2, axis=1)  -2 * (X @ Y.T)
        K = self.theta[0]*np.exp(-self.theta[1]/2*tmp) + self.theta[2] + self.theta[3]*(X @ Y.T)

        if not(eval_gradient):
            return K
        else:
            gradK = np.zeros((len(self.theta), len(X), len(Y)))
            gradK[0] = np.exp(-self.theta[1]/2*tmp)
            gradK[1] = -self.theta[0]/2*tmp*np.exp(-self.theta[1]/2*tmp)
            gradK[2] = np.ones((len(X), len(Y)))
            gradK[3] = X @ Y.T
            return K, gradK

    def diag(self, X):
        '''
        This method calculates the diagonal elements of the kernel matrix.

        Parameters
        ----------
        X : 2-D numpy array
            numpy array representing input points. X[n, i] represents the i-th element of n-th point in X.

        Returns
        ----------
        diagK : 1-D numpy array
            numpy array representing the diagonal elements of the kernel matrix. diagK[n] = K[n, n]
        '''
        diagK = self.theta[0] + self.theta[2] + self.theta[3]*np.sum(X**2, axis=1) 
        return diagK


2.2 ガウス過程回帰
この節では、ガウス過程回帰を行うGPRegressionクラスを実装します。

2.2.1 概要
GPRegressionには、以下のデータ属性を持たせることにします:


kernel : カーネル函数を表すオブジェクト。theta, bounds属性と、__call__、diag メソッドを持つものとします。 

beta : ノイズの精度パラメーター。

X_train : 訓練データの入力部分。予測を行う際に使用します。

t_train : 訓練データのラベル部分。予測を行う際に使用します。

Cinv : $C^{-1}$. 予測の際に用いるので、学習の際に計算して、保持するようにしておきます。

また、次のメソッドを持つものとします: 


fit : 学習を行うメソッド。Cinvを訓練データから計算し、保持します。引数のoptimize_hparamsがTrueのときは、まずハイパーパラメーターの最適化を行い、その後Cinvを計算します。

predict : 予測を行うメソッド。引数のreturn_stdがTrueのとき、予測標準偏差も返します。

また、ハイパーパラメーター最適化のために、コスト函数(と勾配)を返す函数cost_and_gradをGPRegressionとは別に定義しておきます。

2.2.2 学習(ハイパーパラメーター固定)
ハイパーパラメーターが固定のとき、学習で行うのは$C^{-1}$(Cinv)の計算だけです。
具体的には、
$$
\begin{align}
    & C = K + \beta^{-1} I_N \\
    & K := \left( K_{i,j} \right)_{i,j = 0, \dots, N-1}, \ \ K_{i,j} = k(x_i, x_j), 
\end{align}
$$
を実装するだけなので、これはそのまんまですね。

2.2.3 予測
入力$\xi$に対するラベル$\tau$の予測分布は、
$$
\begin{align}
    p(\tau | t ) &= \mathcal{N} (\tau | m(\xi), \sigma^2(\xi)), \\
    m(\xi) &:= \kappa^T C^{-1} t, \\
    \sigma^2(\xi) &:= k(\xi, \xi) + \beta^{-1} - \kappa^T C^{-1} \kappa \\
    \kappa &\in \mathbb{R}^N, \ \ \kappa_n := k(\xi, x_n)
\end{align}
$$
で与えられました。
これは1つの入力$\xi$に対する分布の表式ですが、実用上は複数の点$\xi_i$ ($i=0, \dots, N_{test}-1$)に対して、$m(\xi_i), \sigma(\xi_i)$を一括で計算することが多いかと思います9。そこで、ここではpredictをそのようなメソッドとして実装する準備をします。
まず、以下の配列を定義します。


mean : ($N_{test}$, ) numpy配列。mean[i] = $m(\xi_i)$

std : ($N_{test}$, ) numpy配列。std[i] = $\sigma(\xi_i)$

kappa : ($N_{test}, N$) numpy配列。kappa[i, j] = $k(\xi_i, x_j)$

すると、


mean[i] = $\sum_{j, k}$ kappa[i, j] $\left( C^{-1} \right)_{j, k} t_k$ 
(std[i])**2 = $k(\xi_i, \xi_i) + \beta^{-1} - \sum_{j, k}$ kappa[i, j] $\left(C^{-1} \right)_{j,k} $ kappa[i, k]


となります。predictメソッドの実装には、この表式を用います。

2.2.4 ハイパーパラメーター最適化
対数尤度とその勾配は
$$
\begin{align}
    \log p(t|\beta, \theta) &= -\frac{1}{2} \log ({\rm det} C) 
        -\frac{1}{2} t^T C^{-1} t -\frac{N}{2} \log (2\pi) \\
    \frac{\partial}{\partial \theta_i} \log p(t|\beta, \theta) 
    &= -\frac{1}{2} \mathrm{Tr} \left(C^{-1} \frac{\partial C}{\partial \theta_i} \right) + \frac{1}{2} t^T C^{-1} \frac{\partial C}{\partial \theta_i} C^{-1} t  \\
    C &= K + \beta^{-1} I_N
\end{align}
$$
で与えられます。
この節では、コスト函数(対数尤度の負符号を取ったもの)とその勾配を求める関数を定義しておきます。最適化の実行には、後でscipy.optimize.minimizeを用いるので、負符号をつけておきます。
少しややこしいですが、cost_and_gradの引数paramsの第0成分が$\beta$、それ以降の成分が$\theta$に対応しているとします。
なお、$\log(\mathrm{det}C)$の計算では、underflowを防ぐために一工夫を加えます。直接determinantを計算するのではなく、$C$の固有値を用います。具体的には、$\lambda_i$ $(i= 0, \dots, N-1)$を固有値として、
$$
\begin{align}
    \log(\mathrm{det}C) 
    = \log \left( \prod_{i=0}^{N-1} \lambda_i \right)
    = \sum_{i=0}^{N-1} \log \lambda_i
\end{align}
$$
のように計算します。固有値を求めるにあたっては、scipy.linalg.eighを用いました。
非常に小さい値のdetを計算してからlogを取るのではなく、logを取ってから和を取ることによって、underflowを防ぎます。
なお、この一工夫を行わずにnp.linalg.detを使って計算したときは、最適化が失敗するケースが多々ありました。)。


def cost_and_grad(beta, theta, kernel, X, t, return_grad=False):
    '''
    The method calculates cost function (negative of log marginal likelihood) and its gradient

    Parameters
    ----------
    beta : float
        noise parameter (precision)
    theta : 1-D numpy array
        1-D numpy array representing kernel parameteres
    kernel : kernel object
        An object representing kernel function
    X : 2-D numpy array
        Array representing input data, with X[n, i] being the i-th element of n-th point in X.
    t : 1-D numpy array
        Array representing label data.
    return_grad : bool
        If True, the function also returns the gradient of the cost function.

    Returns
    ----------
    val : float
        The value of the cost function
    grad : 1-D numpy array, optional
        Array representing the gradient of the cost function. Returned only if return_grad is True.
    '''
    kernel.theta = theta
    K, gradK = kernel(X, X, eval_gradient=True)
    C = K + 1.0/beta*np.identity(len(K))
    Cinv = np.linalg.inv(C)
    val = np.sum(np.log(eigh(C)[0])) + 0.5 * t @ Cinv @ t + 0.5*len(X)*np.log(2*np.pi)
    if not(return_grad):
        return val
    else:
        grad = np.zeros(len(theta)+1)
        grad[0] = -0.5*np.trace(Cinv)/(beta**2) + 0.5/(beta**2) * (t @ Cinv @ Cinv @ t)
        for cnt in range(len(theta)):
            grad[cnt+1] = 0.5 * np.trace(Cinv @ gradK[cnt]) - 0.5 * t @ Cinv @ gradK[cnt] @ Cinv @ t
        return val, grad



2.2.5 コード
ここまでのもろもろを組み合わせて、GPRegressionクラスを以下のように定義します。
class GPRegression:

    def __init__(self, kernel, beta):
        self.kernel = kernel
        self.beta = beta

    def fit(self, X, t, optimize_hparams=False):
        '''
        Parameters
        ----------
        X : 2-D numpy array
            Array representing training input data, with X[n, i] being the i-th element of n-th point in X.
        t : 1-D numpy array
            Array representing training label data.
        optimize_hparams : bool
            If True, optimization of hyperparameters (noise parameter and kernel parameters) is performed.
        '''
        self.X_train = X
        self.t_train = t
        if optimize_hparams:
            theta_old = np.copy(self.kernel.theta)
            bounds_full = np.concatenate(( [[0, None]], self.kernel.bounds ), axis=0)
            result = minimize(x0=np.append([self.beta], [self.kernel.theta]),
                              fun=lambda x : cost_and_grad(beta=x[0], theta=x[1:], kernel=self.kernel, X=self.X_train, t=self.t_train, return_grad=True), 
                              jac=True,
                              bounds=bounds_full)
            if not(result.success):
                print(result.message)
                self.kernel.theta = theta_old
            else:
                print(result.message)
                self.beta = result.x[0]
                self.kernel.theta = result.x[1:]
        self.Cinv = np.linalg.inv( self.kernel(self.X_train, self.X_train) + 1.0/self.beta*np.identity(len(self.X_train)) )


    def predict(self, X, return_std=False):
        '''       
        Parameters
        ----------
        X : 2-D numpy array
            Array representing test input data, with X[n, i] being the i-th element of n-th point in X.
        return_std : bool
            If True, predictive standard deviation is also returned.

        Returns
        ----------
        mean : 1-D numpy array
            Array representing predictive mean.
        std : 1-D numpy array, optional
            Array reprensenting predictive standard deviation, returned only if return_std is True.
        '''
        kappa = self.kernel(X, self.X_train)
        mean = kappa @ (self.Cinv @ self.t_train)
        if not(return_std):
            return mean
        else:
            std = np.sqrt( self.kernel.diag(X) + 1.0/self.beta - np.diag( kappa @ self.Cinv @ (kappa.T) ) )
            return mean, std


3 実験
では、ここまで書いてきたコードを動かします!
この記事では、$d=1$の例を扱います。

3.1 ガウス過程とカーネル函数
回帰を行う前にまず、ガウス過程でどのような函数が得られるか、特に、カーネル函数のパラメーターを変えたときにどのような影響が出るかを見てみましょう。
カーネル函数の形を再掲しておきます。
$$
\begin{align}
    k_{\theta}(x, y) = \theta_0 \exp\left( -\frac{\theta_1}{2} \| x - y \|^2 \right) + \theta_2 + \theta_3 x^T y
\end{align}
$$
各パラメーターの組に対して、5つのサンプル10を生成してプロットしたものが、以下の図になります。

直感的にも分かるように、$\theta_0$は振幅を、$\theta_1$は長さスケールを決めていることが見て取れます11。

3.2 データ
では、いよいよ回帰を行っていきましょう。使うのは、次のトイデータです。
$$
\begin{align}
    t &= f(x) + \varepsilon \
    f(x) &= \sin(2x) + 0.2 \sin x + 0.1x \\
    \varepsilon &\sim \mathcal{N}(0.0, 0.09)
\end{align}
$$
データ点の個数は50です。


3.3 回帰(ハイパーパラメーター固定)
まず、ハイパーパラメーターを固定してガウス過程回帰を行ってみましょう。
Thts = np.array([[1.0, 4.0, 0.0, 0.0],
                 [9.0, 4.0, 0.0, 0.0],
                 [1.0, 64.0, 0.0,0.0],
                 [1.0,0.25, 0.0, 0.0],
                 [1.0, 4.0, 10.0, 0.0],
                 [1.0, 4.0, 0.0, 5.0]
                ])

beta = 3.0

kernel = MyKernel(theta = np.array([0.0, 0.0, 0.0, 0.0]), bounds=[[0.0, None],[0.0, None],[0.0, None],[0.0, None]])
gpr = GPRegression(kernel=kernel, beta=beta)

fig = plt.figure(figsize=(18,10))
cnt = 0
while cnt < len(Thts):
    theta = Thts[cnt]
    kernel.theta = theta
    gpr.fit(X, t)
    mean, std = gpr.predict(Xtest, return_std=True)
    ax = fig.add_subplot(2, 3, cnt+1)
    plot_result(mean, std, ax)
    ax.set_title(f""beta={gpr.beta}, theta = ({theta[0]}, {theta[1]}, {theta[2]}, {theta[3]}) \n log_likelihod : {-cost_and_grad(beta=gpr.beta, theta=gpr.kernel.theta, kernel=gpr.kernel, X=X, t=t, return_grad=False)}"")
    cnt += 1
plt.show()

先ほどの節のハイパーパラメーターの組を用いて回帰を行った結果が以下の図です。$\beta=3.0$としてあります。


実線が予測平均、水色で彩色された領域が予測平均$\pm$予測標準偏差を表します。
点線が真の函数、点が訓練データを表します。

log_likelihoodとして書かれている数字は、対数周辺尤度の値を指します。

$\theta_1$の値が結果に大きな影響を与えていることが見て取れます。具体的には、$\theta_1$が大きい(長さスケールが小さい)と予測平均は細かく振動し、$\theta_1$が小さい(長さスケールが大きい)と予測平均はのっぺりしたものになります。これは直感的にも納得いく結果ですね。
対して、$\theta_0, \theta_2, \theta_3$が結果にどう影響しているかは、なんとも言いがたいというのが正直なところです。
また、$\beta$を大きくすると全体的に水色の領域が小さくなり、$\beta$を小さくすると水色の領域が大きくなります。これは、$\beta$の逆数がノイズの強さを表すことから納得できるかと思います。
$\beta=10.0$の図

$\beta=0.5$の図


3.4 ハイパーパラメーター最適化
先ほどは、ハイパーパラメーターを帰るとガウス過程回帰の結果が大幅に変わることを見てきました。この節では、それらのハイパーパラメーターを、「周辺尤度を最大化する」という基準で決めます。
kernel = MyKernel(theta = np.array([2.0, 1.0, 0.01, 0.02]), bounds=[[0.0, None],[0.0, None],[0.0, None],[0.0, None]])
gpr = GPRegression(kernel=kernel, beta=1.0)
gpr.fit(X, t, optimize_hparams=True)
mean, std = gpr.predict(Xtest, return_std=True)
print(f""beta = {gpr.beta}"")
print(f""theta={kernel.theta}"")
print(f""log_likelihod : {-cost_and_grad(beta=gpr.beta, theta=gpr.kernel.theta, kernel=gpr.kernel, X=X, t=t, return_grad=False)}"")
plot_result(mean, std)

結果はこちら

beta = 17.201507415481835
theta=[0.55294478 1.6059704  0.         0.        ]
log_likelihod : 40.25398887950302


さきほどより綺麗な結果になってますね! 具体的には：予測平均は真の値に近く、周辺尤度もさきほどの例(適当なハイパーパラメーターで計算したもの)に比べて大きくなっていることが分かります。

4 まとめ
この記事では、PRML第6章のガウス過程回帰を実装しました。
具体的には、カーネル函数を表すクラスMyKernelと、ガウス過程回帰を行うクラスGPRegressionを実装しました。後者には、周辺尤度最大化を基準に、ハイパーパラメーターを自動で選ぶ機能も持たせました。
また、トイデータを使って、ハイパーパラメーターの値を変えると回帰の結果がどのように変化するかを観察しました。ついで、周辺尤度最大化を用いることによって、より適切なハイパーパラメーターを自動で決めることができることを見ました。
PRML3章では特徴量の次元(基底関数の本数)$M$を、入力の次元$d$に対して指数的に大きくする必要がありましたが、ガウス過程回帰ではこのような問題は起きません。実際、カーネル函数の形によっては、$M= \infty$を扱うことができるのは、ガウス過程回帰の利点の1つと言えるでしょう。
一方、ガウス過程回帰では$C^{-1}$を計算するために$\mathcal{O}(N^3)$の計算量が必要になります。そのため、データ数が大きくなるとガウス過程回帰を直接適用するのは困難になります。
この問題に対処するために様々な近似手法が提案されているとのことです。機会があれば、これらの手法についても調査/理解/実装をしてみたいと考えています。




PRMLの内容をなぞるだけになってしまうので、この記事ではほぼ全て省略します。 ↩


この記事で与えるガウス過程の定義は、数学的には全く厳密ではありません。数学的に厳密な取り扱いについては、こちらのマスタケさんの記事が参考になるかと思います。 ↩


そもそもそんな確率過程が存在するのか、といった話については私はきちんとは理解できていません。気になる方は上記の記事をご覧いただければと思います。 ↩


上では$y$を函数の意味で用いていたので、これはabuse of notationですね。。。この後、函数としての$y$は登場しないので、文脈で区別はつくかと思います。 ↩


正確には、ここでノイズがi.i.d.という仮定を課しています。 ↩


この記事では一種類のカーネル函数しか用いませんが、同じ振る舞いをするクラスを自分で書けば、いろいろなカーネル函数を付け替えることができます。 ↩


カーネル行列全体を求めるためには$\mathcal{O}(N^2)$かかりますが、対角成分だけなら$\mathcal{O}(N)$で済みます。 ↩


加えて、$K$が正定値であるためには、$\theta_0, \theta_2, \theta_3$のうち少なくとも1つが正である必要があります。 ↩


ここで、$\tau_0, \dots, \tau_{N_{test}-1}$の同時分布については何も言ってないことに注意。 ↩


ここではデータ点ではなく、ガウス過程によって得られる1つの函数が1つのサンプルとなります。 ↩


残りのパラメーターの直感的な意味については、自信がありません。。。 ↩



",True,https://qiita.com/amber_kshz/items/e385e985fddf78d21f71
"

はじめに
Laravelでルーティングを行う際に便利な、Route::resourceを使用した際の注意点を書いておきます。

環境
Laravel 5.4

ルーティングのRoute::resource指定
Laravelでは、以下のようにルーティングにRoute::resouceを指定することで、CRUDルーティングを一度に行うことができます。以下が公式のドキュメントに載っていたルーティングの例と対応表になります。

/routes/web.php
Route::resource('photos', 'PhotoController');



また、以下のartisanコマンドによって、対応するコントローラとメソッドを自動生成してくれます。
$ php artisan make:controller PhotoController --resource


/app/Http/Controllers/PhotoController.php
//コマンド入力で自動的に作成される
namespace App\Http\Controllers;
class PhotoController extends Controller
{
    public function index()
    {
        //
    }

    public function create()
    {
        //
    }

    public function store(Request $request)
    {
        //
    }

    public function show($id)
    {
        //
    }

    public function edit($id)
    {
        //
    }

    public function update(Request $request, $id)
    {
        //
    }

    public function destroy($id)
    {
        //
    }
}



全部のメソッドは使わない場合
Route::resouceはCRUDのルーティングを一度に行えるのでとても便利なのですが、これらのメソッドを全て使うことはあまりないと思います。空のメソッドを用意しておいてもいいですが、できれば使わないメソッドは消しておきたいものです。例えば、以下のようなルーティングをしたとします。

/routes/web.php
Route::resource('hoge', 'HogeController');


今回show、updateメソッドを使わないとして、以下のようにメソッド自体消したとします。

/app/Http/Controllers/HogeController.php
//show、updateは削除する
namespace App\Http\Controllers;
class HogeController extends Controller
{
    public function index()
    {
        //
    }

    public function create()
    {
        //
    }

    public function store(Request $request)
    {
        //
    }

    //showを削除

    public function edit($id)
    {
        //
    }

    // updateを削除

    public function destroy($id)
    {
        //
    }
}


このとき、以下のようにブラウザのURL入力欄に直書きすると例外が出てしまいます。


showメソッドが無いと言われています。showメソッドを呼んだ覚えはないのですが、先程のresoucesを使ったときのルーティング一覧表を見ると、/hoge/{任意の文字列} を直書き(GET)した場合はshowが呼ばれてしまいます。

ルーティングを制限する
よってshowのルーティングを削除する必要があります。resourceを使ったときのルーティングを制限する方法は2つあります。

/routes/web.php
// show、updateのルーティングを削除

// onlyを使う方法
Route::resource('hoge', 'HogeController', ['only' => ['index', 'create', 'edit', 'store', 'destroy']]);

// exceptを使う方法
Route::resource('hoge', 'HogeController', ['except' => ['show', 'update']]);


ホワイトリストの方がソースも読みやすいと思うので、個人的にはonlyを使うことをおすすめします。
以上で、resoucesを使ったときにURLを直書きされた場合でも、例外(Whoops)が出ないようになりました。

備考: どうしてもWhoopsを出したくない場合
showメソッドを呼ばないようにして、これでもう大丈夫かと思いきや、先程とは違うエラー画面が発生しました。

これは、GETリクエスト(URL直書き)を許可していないURLにGETでアクセスしているため405エラーが出ています。この例外が出るのは意図している動きなので、拾うことにしました。
Laravelの例外は、以下のように/app/Exceptions/Handler.phpに記述して拾います。

/app/Exceptions/Handler.php

use Symfony\Component\HttpKernel\Exception\MethodNotAllowedHttpException;
use Illuminate\Foundation\Exceptions\Handler as ExceptionHandler;

class Handler extends ExceptionHandler
{
    public function render($request, Exception $excpetion)
    {
        if ($exceotion instanceof MethodNotAllowedHttpException) {
             //ここに処理を書く
        }
        return parent::render($request, $exception);
    }
}


これでエラー画面はでなくなりました。
※この方法はコメントでもいただいたとおり、MethodNotAllowedHttpExceptionを全て握り潰すので注意してください。

参考URL

Laravelのルーティング書き方まとめ
Laravel 5 - How do I handle MethodNotAllowedHttpException

",True,https://qiita.com/sympe/items/9297f41d5f7a9d91aa11
"Spring MVCのコントローラメソッドは、画面で入力されたパラメータ以外に、様々な情報を引数として受け取ることができます。

アノテーションを使用して受け取る
メソッドの引数にアノテーションを指定することで受け取ることができる情報がいくつかあります。

リクエストパラメータ
@RequestParamアノテーションを指定すると、URLに含まれるクエリパラメータや、メッセージボディーに含まれるポストパラメータを受け取ることができます。
@GetMapping(""/hello"")
public String hello(@RequestParam(""name"") String name) {

ブラウザから「/hello?name=world」のように、URLにクエリーパラメータを指定することで、コントローラメソッドで、指定された値を取得することができます。

required属性
リクエストパラメータは、デフォルトでは必須パラメータになります。パラメータの指定を任意にする場合、@RequestParamアノテーションのrequired属性にfalseを指定します。
@GetMapping(""/hello"")
public String hello(@RequestParam(name = ""name"", required = false) String name) {

リクエストパラメータが指定されなかった場合、引数の値はnullになります。Java 8以降であれば、Optionalで受け取ることも可能です。
@GetMapping(""/hello"")
public String hello(@RequestParam(name = ""name"", required = false) Optional<String> name) {


defaultValue属性
リクエストパラメータが指定されなかった時のデフォルト値を指定することができます。
@RequestParamアノテーションのdefaultValue属性にデフォルト値を指定することで、リクエストパラメータが指定されていない場合は、このデフォルト値が引数に設定されます。
@GetMapping(""/hello"")
public String hello(@RequestParam(name = ""name"", defaultValue = """") String name) {


URLパスパラメータ
@PathVariableアノテーションを指定すると、URLに含まれる動的なパラメータを受け取ることができます。
@GetMapping(""/hello/{name}"")
public String hello(@PathVariable(""name"") String name) {

URLマッピングで指定するURLに「{」と「}」で囲まれた部分がパラメータ名になり、@PathVariableアノテーションのvalue属性にパラメータ名を指定することで、URLの部分文字列を取得することができます。
受け取る変数は文字列型(String)である必要はありません。整数型、実数型を指定することもできます。
@GetMapping(""/hello/{age}"")
public String hello(@PathVariable(""age"") Integer age) {

日時型で受け取ることもできます。その場合は、@DateTimeFormatアノテーションを指定して、受け取る日時の文字列表現の形式を指定します。
@GetMapping(""/hello/{date}"")
public String hello(@PathVariable(""date"") @DateTimeFormat(iso = DateTimeFormat.ISO.DATE) java.util.Date date) {


required属性
URLパスパラメータは、デフォルトでは必須パラメータになります。パラメータの指定を任意にする場合、@PathVariableアノテーションのrequired属性にfalseを指定します。
@GetMapping(""/hello/{name}"")
public String hello(@PathVariable(name = ""name"", required = false) String name) {

ただし、これだけでは、パラメータが指定されていないURLは別のURLと認識され、404(Not Found)になってしまいます。URLマッピングにパラメータがないURLを追加して、パラメータの有無にかかわらず、コントローラのメソッドが呼び出されるようにします。
@GetMapping({ ""/hello"", ""/hello/{name}"" })
public String hello(@PathVariable(name = ""name"", required = false) String name) {

パラメータが指定されなかった場合、引数の値はnullになります。Java 8以降であれば、Optionalで受け取ることも可能です。
@GetMapping({ ""/hello"", ""/hello/{name}"" })
public String hello(@PathVariable(name = ""name"", required = false) Optional<String> name) {


URL行列パラメータ
URLパスパラメータ内に名前と値をペアとした行列パラメータを含めることができます。行列パラメータは「;」で区切り、複数指定することができます。
次の例は、行列パラメータとして「sex」と「age」を指定した例です。
http://localhost:8080/hello/scott;sex=male;age=25

URL行列パラメータを使用する場合、removeSemicolonContentプロパティーにfalseを設定する必要があります。
Spring Bootを使用している場合、WebMvcConfigurerAdapterを継承した設定クラスのconfigurePathMatchメソッドをオーバーライドして、removeSemicolonContentプロパティーにfalseを設定します。
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.config.annotation.PathMatchConfigurer;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;
import org.springframework.web.util.UrlPathHelper;

@Configuration
public class WebMvcConfigurer extends WebMvcConfigurerAdapter {
    @Override
    public void configurePathMatch(PathMatchConfigurer configurer) {
        UrlPathHelper urlPathHelper = new UrlPathHelper();
        urlPathHelper.setRemoveSemicolonContent(false);
        configurer.setUrlPathHelper(urlPathHelper);
    }
}

@MatrixVariableアノテーションを指定すると、URLパスパラーメータに含まれる行列パラメータを受け取ることができます。
@GetMapping(""/hello/{name}"")
public String hello(@MatrixVariable(""age"") Integer age) {

@MatrixVariableアノテーションのvalue属性に、URLパスパラメータに付随するURL行列パラメータ名を指定することで、行列パラメータの値を取得することができます。
受け取る変数は文字列型(String)である必要はありません。整数型、実数型を指定することもできます。

pathVar属性
複数のURLパスパラメータに、同名のURL行列パラメータ名が存在する場合、@MatrixVariableアノテーションのpathVar属性でURLパスパラメータの名前を指定します。
    @GetMapping(""/hello/{name}/{father}"")
    public String hello(
            @MatrixVariable(name = ""age"", pathVar = ""name"") Integer age,
            @MatrixVariable(name = ""age"", pathVar = ""father"") Integer fatherAge) {

次のURLに対してリクエストを発行します。
http://localhost:8080/hello/scott;age=25/tiger;age=50

コントローラーの引数「age」には「25」が、「fatherAge」には「50」が設定されます。

required属性
URL行列パラメータは、デフォルトでは必須パラメータになります。パラメータの指定を任意にする場合、@MatrixVariableアノテーションのrequired属性にfalseを指定します。
@GetMapping(""/hello/{name}"")
public String hello(@MatrixVariable(name = ""age"", required = false) Integer age) {

URL行列パラメータが指定されなかった場合、引数の値はnullになります。Java 8以降であれば、Optionalで受け取ることも可能です。
@GetMapping(""/hello/{name}"")
public String hello(@MatrixVariable(name = ""age"", required = false) Optional<Integer> age) {


defaultValue属性
URL行列パラメータが指定されなかった時のデフォルト値を指定することができます。
@MatrixVariableアノテーションのdefaultValue属性にデフォルト値を指定することで、URL行列パラメータが指定されていない場合は、このデフォルト値が引数に設定されます。
@GetMapping(""/hello/{name}"")
public String hello(@MatrixVariable(name = ""age"", defaultValue = 0) Optional<Integer> age) {


クッキーパラメータ
@CookieValueアノテーションを指定すると、クッキーパラメータを受け取ることができます。
@GetMapping(""/hello"")
public String hello(@CookieValue(""name"") String name) {

ブラウザのクッキーに値が設定されている状態でリクエストを発行すると、コントローラメソッドで、指定された値を取得することができます。

required属性
クッキーパラメータは、デフォルトでは必須パラメータになります。パラメータの指定を任意にする場合、@CookieValueアノテーションのrequired属性にfalseを指定します。
@GetMapping(""/hello"")
public String hello(@CookieValue(name = ""name"", required = false) String name) {

クッキーパラメータが指定されなかった場合、引数の値はnullになります。Java 8以降であれば、Optionalで受け取ることも可能です。
@GetMapping(""/hello"")
public String hello(@CookieValue(name = ""name"", required = false) Optional<String> name) {


defaultValue属性
クッキーパラメータが指定されなかった時のデフォルト値を指定することができます。
@CookieValueアノテーションのdefaultValue属性にデフォルト値を指定することで、クッキーパラメータが指定されていない場合は、このデフォルト値が引数に設定されます。
@GetMapping(""/hello"")
public String hello(@CookieValue(name = ""name"", defaultValue = """") String name) {


リクエストヘッダ
@RequestHeaderアノテーションを指定すると、リクエストヘッダに含まれている各項目を受け取ることができます。
@GetMapping(""/hello"")
public String hello(@RequestHeader(""User-Agent"") String userAgent) {

ブラウザからリクエストを発行することで、コントローラメソッドで、指定されたリクエストヘッダの項目を取得することができます。

required属性
指定したリクエストヘッダの項目は、デフォルトでは必須項目になります。リクエストヘッダの項目の有無を任意にする場合、@RequestHeaderアノテーションのrequired属性にfalseを指定します。
@GetMapping(""/hello"")
public String hello(@RequestHeader(name = ""User-Agent"", required = false) String userAgent) {

指定したリクエストヘッダの項目がなかった場合、引数の値はnullになります。Java 8以降であれば、Optionalで受け取ることも可能です。
@GetMapping(""/hello"")
public String hello(@RequestHeader(name = ""User-Agent"", required = false) Optional<userAgent> name) {


defaultValue属性
指定したリクエストヘッダの項目がなかった時のデフォルト値を指定することができます。
@RequestHeaderアノテーションのdefaultValue属性にデフォルト値を指定することで、指定したリクエストヘッダの項目がなかった場合は、このデフォルト値が引数に設定されます。
@GetMapping(""/hello"")
public String hello(@RequestHeader(name = ""User-Agent"", defaultValue = ""Mozilla/5.0"") String userAgent) {


リクエストボディー
@RequestBodyアノテーションを指定すると、リクエストボディーの内容をそのまま取得することができます。
@PostMapping(""/hello"")
public String hello(@RequestBody String body) {

リクエストボディーを含むリクエストを発行することで、コントローラメソッドで、リクエストボディーの内容を取得することができます。
次のHTMLファイルから、POSTリクエストを発行して、コントローラーでリクエストボディーを取得してみます。
<!DOCTYPE html>
<html>
<head>
<meta charset=""UTF-8"">
</head>
<body>
    <form action=""hello"" method=""post"">
        User: <input type=""text"" name=""user"" size=""20""><br />
        Password: <input type=""text"" name=""password"" size=""20""><br />
        <input type=""submit"" value=""Login"">
    </form>
</body>
</html>

入力欄「User」に「scott」、「password」に「tiger」を入力してリクエストを発行すると、コントローラーで@RequestBodyアノテーションを指定した引数の内容は、次のようになります。
user=scott&password=tiger


required属性
リクエストボディーは、デフォルトでは必須項目になります。リクエストボディーの有無を任意にする場合、@RequestBodyアノテーションのrequired属性にfalseを指定します。
@PostMapping(""/hello"")
public String hello(@RequestBody(required = false) String body) {

リクエストボディーがなかった場合、引数の値はnullになります。Java 8以降であれば、Optionalで受け取ることも可能です。
@PostMapping(""/hello"")
public String hello(@RequestBody(required = false) Optional<String> body) {


HTTPセッション属性
@SessionAttributeアノテーションを指定すると、HTTPセッションに保存されている属性値を取得することができます。
@GetMapping(""/hello"")
public String hello(@SessionAttribute(""age"") Integer age) {

HTTPセッションに指定した属性名が設定されている状態でリクエストを発行すると、コントローラメソッドで、指定された属性名の値を取得することができます。

required属性
HTTPセッションの属性は、デフォルトでは必須項目になります。HTTPセッションの属性の有無を任意にする場合、@SessionAttributeアノテーションのrequired属性にfalseを指定します。
@PostMapping(""/hello"")
public String hello(@SessionAttribute(name = ""age"", required = false) Integer age) {

HTTPセッションの属性がなかった場合、引数の値はnullになります。Java 8以降であれば、Optionalで受け取ることも可能です。
@PostMapping(""/hello"")
public String hello(@SessionAttribute(name = ""age"", required = false) Optional<Integer> age) {

",True,https://qiita.com/MizoguchiKenji/items/2a041f3a3eb13274e55c
"

はじめに
Capsule Network（CapsNet） は、ディープラーニング界のゴッドファーザーの一人、Geoffrey Hinton を中心に提案された新しいニューラルネットワークです。
この記事では、CapsNet の概要を説明するとともに、その PyTorch 実装と手書き数字の分類（MNIST）におけるテスト結果を紹介します。

実装はこちら → GitHub


Capsule Network (CapsNet)

CapsNet のモチベーション
近年、画像解析の中心技術といえばやはり畳み込みニューラルネットワーク（CNN）でしょう。CNN は、画像分類、物体検出、セマンティック・セグメンテーションなど、ビジョン系のタスクで新たな state-of-the-art を次々と打ち立ててきています。
CNN は、特徴マップの畳み込みを行う畳み込み層と、特徴マップの縮小を行うプーリング層の繰り返しです。CNN はプーリング層によって translation-invariance を獲得していますが、その代償として異なる特徴間の相対的な位置関係などを学習することが難しくなっています。CapsNet は CNN が抱えるこうした問題を解消するものとして提唱されました。

従来のニューラルネットワークとの違い
従来のニューラルネットワークにおいては、ネットワークの各ノード（ニューロン）は一つ下のレイヤに属するすべてのニューロンの出力を受け取り、それらに重みパラメータをかけて合計したものを出力します。つまり、各ニューロンの出力はスカラです。
それに対して、CapsNet においてはニューロンが ”Capsule” に置き換えられます。 Capsule も一つ下のレイヤに属する すべての Capsule の出力を受け取るという点では同じですが、それぞれの Capsule の出力はベクトルであるという点で、従来のニューロンとは異なっています。
下の図では、従来のニューロンと Capsule の違いをまとめています(画像は GitHub:CapsNet-Tensorflow より引用)。


Capsule が表現するもの
Capsule が出力するベクトルは、その大きさが「特徴」の存在確率を表し、その方向が「特徴」のプロパティを表しています。表現する特徴は Capsule ごとに異なります。例えば、人の顔を認識する CapsNet があるとすれば、ある Capsule は「目」という特徴を表現し、別の Capsule は「鼻」という特徴を表現するという具合です。
特徴のプロパティは、例えば、その特徴（物体）の方向やスケール、照明条件など、特徴の状態のことを言います。スカラを保持するニューロンからベクトルを保持する Capsule へと移行することによって、特徴の存在だけではなく、その特徴がどのような状態にあるのかを表現することができるようになります。
上でも述べたとおり、Capsule は一つ下のレイヤに属するすべての Capsule の出力を受け取るという点で従来のニューラルネットワークと共通しますが、CapsNet においては各特徴の状態を（出力ベクトルの方向として）明示的に表現した上で、より高次のレイヤの Capsule に伝達しています。これによって、低次の異なる特徴間の相対的な位置関係なども含めて、高次の特徴をより正確に学習することができると考えられます。
例えば、同じレイヤに属する異なる3つの 低次の Capsule は、それぞれ「鼻」、「口」、「目」の特徴を表現し、一つ上のレイヤに属する高次の Capsule は「顔」の特徴を表現するものとするとしましょう。低次のそれぞれの Capsule は「鼻」、「口」、「目」の方向やスケールをベクトルの方向として表現します。それらのベクトルを受け取った「顔」という特徴を表現する高次の Capsule は、それらの顔のパーツの相対的な位置関係などを手掛かりに、「顔」という物体の特徴を学習することができるという具合です（鼻はだいたい顔の真ん中、口は顔の下の方、目は顔の上の方に左右一つずつある、など）。

CapsNet をより詳しく
CapsNet では、このような異なる特徴間の階層構造（ヒエラルキー）を学習するために様々な工夫が施されています。論文以外にも、Mediumへのポストなどでも非常にわかりやすく解説がされています（ので、ここでは書きません）。

CapsNet の PyTorch 実装
コードは GitHub で公開しています。
timomernickさんによる実装からフォークさせていただきました。コードの大枠はそのままですが、squashの実装に誤りと思われる箇所があったので issue を出すとともに私のレポジトリで修正をしておきました。また、学習結果とテスト結果を TensorBoard で見られるようにしたり、学習パラメータをコマンドラインから指定できるようにしたり、クラス構造を（私なりに）理解しやすいように分割・統合したりと若干の修正を加えました。

手書き数字分類（MNIST）のテスト結果
以下、私の PyTorch 実装でのテスト結果です。論文でもベンチマークとして使われている手書き文字の分類データセット（MNIST）で CapsNet の学習とテストを行いました。
AWS の GPU ノードでとりあえず50エポックほど学習させてみました。最適化には論文どおりに Adam を使用しましたが、いろいろ試行錯誤した結果、この実装では学習率は0.01（論文では0.001）で精度が最も高くなりました。

分類結果
下図はテストデータにおける分類精度の推移です（横軸はイタレーション）。現状、最高精度で99.51%と、論文での精度（99.75%）には及びませんでした…。

下図は訓練データに対する損失の推移です（横軸は同じくイタレーション）。CapsNet には数字の分類を行うのとは別に、オートエンコーダーのように入力画像を復元するパスがあります。分類タスクと復元タスクそれぞれで損失を計算しており、その和が CapsNet 全体の損失となります。

図において、左が CapsNet の全体の損失、中央が分類タスクに対する損失、右が復元タスクに対する損失です。また、下の図はテストデータに対する損失の推移です。分類タスクに対する損失（中央）がやや過学習気味に見えます。


入力の復元結果
CapsNet による入力画像の復元結果も紹介します。ランダムに選んだテスト画像128枚に対して復元を行いました。学習が進むにつれて数字の特徴を捉えた復元ができている様子がわかります。


所感
論文ほどの精度はまだ再現できていないものの、CapsNet （と思しきもの）を実際に動かすことができました。コードと論文に違いがないかをもう一度確認し、論文の精度に近づけていきたいと思います。
CapsNet は、現状 MNIST のような比較的単純な画像での実証にとどまっており、物体検出やセマンティックセグメンテーションのようなより複雑なタスクに適用するためにはまだ工夫が必要になりそうです。しかし、CapsNet は従来のニューラルネットワークに代わるかもしれない新たなコンセプトを打ち出しており、今後、ビジョン系のタスクに膨大な応用や改善を生み出し得るものだと思います。Capsule 関連の新しい論文等、今後も注意して追って行きたいと思います。

追記
CapsNet の論文と実装が食い違っていたところを修正し、再度学習と評価を行った結果、テストデータに体する分類精度が99.67%まで向上しました。
変更点は以下の2つです。

論文に記載の方法に従って、トレーニングデータに対して data augmentation を行った
Routing において Capsule 間の結合重み係数 $c_{ij}$ の正規化処理を正しく修正した
2つ目の変更にともなって、学習係数を0.001まで小さくした
2つ目の変更にともなって、重み $W$ の初期化を一様分布からのサンプリングによって行うようにした

1つめ以外の変更点は、spikefairwayさんにGitHub上でご指摘いただきました。Routing プロセスは CapsNet の肝なのですが、ここが間違っていたことで大きく精度が低下していたようです。
分類タスクに対する損失がやや過学習気味だったのも改善されました。
学習を続ければもう少し精度が向上しそうです。

学習曲線




復元結果

",True,https://qiita.com/motokimura/items/cae9defed10cb5efeb62
"kubernetesでwebサーバをたてて、kubernetesクラスタの外部からアクセスする方法。
webサーバとしてjenkinsをたててみた。
Replication Controllerのサンプル
apiVersion: v1
kind: ReplicationController
metadata:
  name: jenkins
spec:
  replicas: 1
  selector:
    app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      volumes:
        - name: jendata
          hostPath:
             path: /jenkins_home
      containers:
      - name: myjenkins
        image: jenkins
        ports:
        - containerPort: 8080
        volumeMounts:
        - mountPath: /var/jenkins_home
          name: jendata

selectorでapp:webと名付ける。また、containerPortは8080に。この辺の設定が後で効いてくる。コンテナイメージはjenkinsでpullする。
また、Replication ControllerでPodがリカバリされた際に、基本VolumeはPodに紐づくので、せっかくPodが復活しても設定が全てクリアになっていたらあまり意味がない。
そこで、Volumeをローカルホストに規定し、Podのライフサイクルに依存せずデータを保持する。
コンテナ内の/var/jenkins_home配下を、ホストの/jenkins_homeにマウントする。
この時、ホスト側のディレクトリにchmod 777しておくことを忘れずに。適切な権限がない場合、Podが正しく起動できず（マウントできず）、kubectl get podコマンドのstatusでCrashLoopBackOffとなってしまう。
RCを起動してみる
kubectl create -f rc.yaml

$ kubectl get pod
NAME                   READY     STATUS    RESTARTS   AGE
jenkins-34f36          1/1       Running   0          45m
k8s-master-127.0.0.1   3/3       Running   0          3d

このままでは、起動したjenkinsはコンテナの外からはアクセスできない。コンテナと外部との接続を実現する方法がService。
Serviceのサンプル
{
    ""kind"": ""Service"",
    ""apiVersion"": ""v1"",
    ""metadata"": {
        ""name"": ""my-service""
    },
    ""spec"": {
        ""selector"": {
            ""app"": ""web""
        },
        ""ports"": [
            {
                ""protocol"": ""TCP"",
                ""port"": 80,
                ""targetPort"": 8080,
                ""nodePort"": 30000
            }
        ],
        ""type"" : ""NodePort""
    }
}

selectorで先ほどのReplicationControlerと同じapp:webとする。selectorで両者の対応付けをしているらしい。
type:NodePortとすることで、ホストのIP(local/global)と、kubernetesのコンテナのIPとの紐付けがされる。
※Typeは他にLoadBalancerがあるが、利用可否はCloudサービスに依存するらしい。
ports内の各要素は次の通り
・port： ローカル(kubernetesの外)からPodへ接続用ポート
・targetPort: コンテナ（Pod）内の規定ポート
・NodePort: ホストを経由して外部からPodへの接続用ポート
例では、jenkinsのpodで8080を待ち受けポートにしていたので、targetPortを8080にする。
外部からホストIP(local/global)に30000ポートで接続すると、Pod内の8080ポートでjenkinsに接続される流れ。
80ポートは、kubectl get svcで取得できるIP宛にアクセスすると、80:8080に変換されてjenkinsに接続できる。
Serviceを起動する
※podやrcと同じ
kubectl create -f service.json

接続先を確認
$ kubectl get svc
NAME         LABELS       SELECTOR   IP(S)        PORT(S)
my-service   <none>       app=web    10.0.0.191   80/TCP

このIPが外部からの接続先。
kubernetesで付与される。
試しに接続
curl http://10.0.0.191:80

応答が返ってくる。
Global IPを持つ場合は、http://[global ip]:30000に接続すると、外部ホストからjenkinsにアクセスできる。
※Firewall（AWSならセキュリティポリシー）で該当ポートを許可する必要あり。
試しにkubectl stop pod [pod名]で起動中のjenkinsを停止させてみると、replication controllerですぐに新しいjenkinsが起動され、Volumeをホストに指定していると、設定情報も引き継がれることがわかる。
参考
http://kubernetes.io/v1.1/docs/user-guide/services.html
",True,https://qiita.com/suzukihi724/items/5e16855c3941e629ff70
"前編 につづいて、書籍 マイクロサービスアーキテクチャ のまとめです。

要点

８章．監視

システムを複数のサービスに分割することで、個々のサービスの状態やサービス間のネットワークなど、監視すべき項目が増える。手動による監視では間に合わなくなるので、情報を自動的に収集し、運用監視に適したビューを提供する必要がある。
マイクロサービスでは、サービスのスケールによって監視対象のサーバなどが動的に増減する。監視の仕組みではそのような動的な監視対象の変化に自動的に追随できる必要がある。
監視はシステム全体で統合的に行う必要があるので、個々のサービスの独自ルールにするのではなく、システム全体でルールやツール、ライブラリなどを統一しておく。
ログやメトリックなどの収集した情報には、サービス開発者が必要なときにすぐにアクセスできるようにしておく。(誰かに依頼しないと入手できない、では遅い)

マイクロサービスでは、基本的にイベントベースの疎結合な連携を目指すので、イベントの監視が重要になる。イベントの監視機能を備えた汎用的なイベントルーティングシステムがあれば、システム全体のアーキテクチャを簡素化できる。

Riemann
Suro




ログの監視

複数のサーバに分散するログを収集する仕組みが必要。
例えば Logstash でログを収集して ElasticSearch に蓄積し、Kibana で運用に必要なビューやグラフを表示できる。
マイクロサービスによるシステムでは、ひとつのイベント(ユーザ操作や外部システムとの連携など)を基点に複数のサービスが連鎖的(しかも非同期に)に動作するため、あるイベントに関連する複数のログを紐付けて見ることができるようにする必要がある。
イベントの最初の入り口となるサービスで一意な 相関ID を発番して後続のサービスにも引き回し、各サービスでは相関IDをログに含めて出力する。


メトリックの監視

必要に応じてサービスをスケールさせるためにメトリクスを継続的に監視する必要がある。（CPU負荷やメモリ消費量、サービスの応答時間 など）
複数のサーバに分散して稼働しているサービスの場合、それらのサーバ全体の(=サービスとしての)メトリックと、個々のサーバのメトリックも見れるようにしておく。
大量のメトリックの収集では、直近の情報は細かく、古い情報は荒く記録するようにすることで、メトリックの保存量を節約する。
メトリック監視でも、Graphite のようなビジュアライズツールの利用を検討する。
インフラやミドルウェアのメトリック監視だけに頼るのではなく、サービス自身のプログラム内でメトリックを取得することでよりきめ細かで実用的なメトリックを収集すべき。
DropWizard Metrics のように、メトリックを収集するためのライブラリを利用できる。


システム全体の監視

システムの健全性を把握するには、個々のサービスやサーバのメトリック(低水準メトリック)からシステムが健全かどうかを判断よりも、システム全体に対してテスト用のトランザクションを送信することで健全性を判断する(セマンティック監視)ほうが、より優れた指標を入手できる。


連鎖的障害の検知


個々のサービス自身の健全性だけではなく、下流サービスも含めた健全性を監視して連鎖的障害が発生したら検知する必要がある。

下流サービスと リクエスト／レスポンス で連携する場合は、下流サービスから応答でその健全性を知ることができる。
下流サービスと イベントベース で連携する場合は、そもそも下流サービスを認知していないので、イベントを配信するキューの状態(メッセージが滞留していないかなど)から健全性を判断する必要がありそう。


リクエスト／レスポンス で連鎖的障害に対処するには、下流サービス呼び出しにサーキットブレーカー(下流サービス呼び出しが一定数失敗したら、復旧するまで処理しない。詳細は 11章）を仕込む。


９章．セキュリティ

ユーザの認証・認可
主にユーザインターフェースを通してアクセスするユーザを認証・認可する必要がある。

複数のサービスでユーザを認証・認可するためにシングルサインオンの仕組みが必要。

個々のサービスでユーザの認証・認可を実装するのではなく、ユーザインターフェースとサービスの間に入って認証・認可を透過的に行うゲートウェイを置く。

ゲートウェイでは、ユーザがそのサービスの呼び出すことができるかどうかの判断までとし、それ以上の制御は個々のサービスに任せる。




外部システムの認証・認可
サービスを呼び出す外部システムが信頼でき、許可される呼び出しかどうかを認証・認可する必要がある。



方式
説明
デメリット




境界内のすべてを許可
同じネットワーク境界内であれば、認証・認可しない。
ネットワーク内に侵入されると無抵抗になる。


ベーシック認証
HTTPSベーシック認証で認証・認可する。
呼び出し元のアカウント管理が必要。SSL化に伴い証明書管理が面倒、およびレスポンスのキャッシュが困難。



SAML／OpenID Connect

SAML や OpenID Connect などの標準的なシングルサインオンプロトコルを使って認証・認可する。
呼び出し元のアカウント管理が必要。実装が面倒。


証明書
クライアント証明書を使って認証・認可する。
証明書管理が面倒。



HMAC(Hash-based Message Auth Code)
あらかじめ交換しておいた鍵を使ってハッシュ化したリクエストを一緒に送信し、呼び出されたサービスでリクエストが信頼されるものであることを確認する。
サービス間での鍵交換・管理が面倒。実装が面倒。


APIキー
許可した呼び出し元にAPIキーを発行し、そのキーで認証・認可する。





自システム内での下流サービス呼び出しの認証・認可
ユーザインターフェースや外部システムから呼び出されたサービスが、さらに自システム内の他のサービスを呼び出す場合に認証・認可をどう扱うか検討が必要。


同じネットワーク境界内にあることを前提に 境界内のすべてを許可 することも可能。

下流サービス呼び出し時に、誰のために要求しているのか、もとの要求者の情報を下流サービスに連携しても良い。


下流サービスで同等の認証・認可を行うことにはデメリットもあるので、ケースバイケースで必要な認証・認可機構を組み込む。


情報の保護

従来のシステムと同様に、必要に応じて適切なデータ暗号化を行う。
機密情報はログにそのまま出力しない。
そもそも必要のない機密情報を保持しない。


１０章．コンウェイの法則とシステム設計

コンウェイの法則
システムを設計するあらゆる組織は、必ずその組織のコミュニケーション構造に倣った構造を持つ設計を生み出す。



より良いマイクロサービス(をベースとしたシステム)を生み出すには、組織を含めたチームビルディングも重要になる。


チーム

各サービスはひとつのチームによって所有され、チームはそのサービスについての要件・設計・実装・テスト・運用のすべてに権限と責任を負う。
こうすることでチームに強い当事者意識を持たせ、自律性が向上し、結果的にサービスのデリバリサイクルが速くなる。
マイクロサービスはDDDの境界づけられたコンテキスト によってビジネス視点で分割されるので、チームがサービスを所有すれば自然とビジネス視点・顧客視点を持つことができるようになり、より多くの価値ある機能をデリバリできるようになる。


メンバー

マイクロサービスでは、モノリシックなシステムと比較して、メンバーにより高いスキル(広範囲への考慮 や 新しい技術への適応 など)が求められる。
メンバーのスキルを踏まえて、マイクロサービス化を戦略的に進める必要がある。
メンバーが、サービスを開発するチームの一員として負わなければならない責任とその理由を理解し、自律的な成長を促すことが重要。


１１章．大規模なマイクロサービス

障害への対策

そもそも、どれだけ対策をしても障害を完全になくすことはできないので、費用対効果の薄い障害防止策にリソースを費やすよりも、いかに簡単に障害から復旧させられるかを考えたほうが、有効な障害対策になる。
非機能的な要件がどこまで許容できるかを把握し、やりすぎないように必要十分な対策を講じるようにする。




観点
内容




応答時間
ユーザから見たシステムの応答時間はどのくらいに収まるべきか。


可用性
システムの停止はどれくらい許容されるのか。


データの耐久性
ある程度のデータ欠損は許容されるのか、データはどれくらいの期間保全されなければならないのか。




マイクロサービスにおいては、あるサービスが停止してもシステム全体は稼働し続けることができろうに、安全に機能低下させてそこから回復させることを考える必要がある。（どのように機能低下させるかはビジネス的な判断になることが多い）
あるサービスで障害が発生した場合に、それが他のサービスにまで波及しないようにすることが重要。（主にリクエスト／レスポンスによるサービス連携で）
特に応答の遅延は下流のサービスに波及していって対応が困難になることがあるので、遅延しながら稼働し続けるよりは、停止（機能低下）させたほうが良い。




対策
説明
備考




タイムアウト
下流サービスの遅延の影響を受けないようにするため、すべての下流サービス呼び出しにタイムアウトを設ける。




サーキットブレーカー
下流サービスの呼び出しが一定回数失敗したらその呼び出しを停止（キューイングしておくか、機能低下させるか）する。下流サービスが復旧したかをチェックし、復旧したら自動的に再開する。

Hystrix・・・タイムアウトやサーキットブレーカー機能を持つJavaライブラリ


隔壁
ある下流サービスとの連携が他の下流サービスとの連携に影響しないように、接続プールなどのリソースを別にしておく。






障害を意図的に起こすことで、十分な障害対策ができているかを確認するとともに、チームが障害に対する学習の機会を得ることができる。（アンチフラジャイルな組織）


Chaos Monkey・・・Netflixが公開した、人工的に障害を発生させるツール。


サービスの振る舞いをできるだけ冪等にしておくことで、障害発生時にどこまで処理されたかを考えずに単純に再処理すれば良くなる。


スケーリング

システムのスケーリング

スケールアップ によって手っ取り早くシステムの性能を増強できるが、単一障害点を避けてシステムの回復性を高める意味では効果的ではない。
マイクロサービスアーキテクチャの利点を活かすためには、サービスごとにサーバを別にし、かつ同じサービスを複数のサーバで同時稼働させることが望ましい。
最初から大掛かりな仕掛けを構築するのではなく、システムの成長に合わせて適切な増強手段（必要であればサービスの書き直し）を講じていくほうが良い。
サービスの応答時間のしきい値や、サーバの障害の検知によって自動的にスケーリング（オートスケーリング）させることを検討する。


データベースのスケーリング


一般的にサービスのほとんどのデータベースアクセスは読み取りがほとんど。
読み取りのスケーリングでは、DBMSのレプリカ機能を利用できる。

ただし、レプリカの前にキャッシングを先に導入したほうが効果的。



書き込みのスケーリングでは、シャーディング（データの格納先を分散）する方法がある。

ただし、格納先を跨いだ処理は扱いにくくなる。


サービスごとにデータベースのスキーマを（同じインスタンス上で）分割するだけではそのインスタンスが単一障害点になってしまうので、サービスごとにデータベースインスタンス自体を別にするほうが望ましい。

DDDの CQRSパターン（Command-Query Responsibility Segregation）を使うことで、データベースのスケーリングに柔軟に対処することができる。


Command でデータベースの変更を同期的にも非同期的にも扱える。
データベースに限らず、要件に適した他のデータストアを組み合わせることもできる。




キャッシング
システム全体の性能を最適化するためにキャッシュを活用する。



キャッシュする場所
説明
備考




クライアント
サービスの利用側でデータをキャッシュする。利用側が自身でキャッシュをリフレッシュするタイミングを判断し、必要に応じてサーバを呼び出す。



プロキシ
クライアント-サーバ間に置くプロキシでデータをキャッシュする。クライアントからの要求に透過的にキャッシュを返し、サーバ側のデータが変更されたらキャッシュをリフレッシュする。

Squid、Varnishなど


サーバ
サービスの提供側でデータをキャッシュする。変更頻度の低いデータをメモリ上にキャッシュしておく。

Memcachedなど




サービス間の通信プロトコルとしてHTTPを採用している場合は、HTTP自体のキャッシング仕様を利用する。
更新処理では、データの変更要求をキャッシュしておいてまとめて下流サービスに流す（ライトビ ハインド キャッシュ）ことで更新処理をバッチ化して最適化できることがある。
また、キャッシュを永続化しておけば、下流サービスが停止から復帰したあとで永続化しておいたキャッシュから変更要求を送るような使い方もできる。
データの新鮮度が多少古くてもサービスが稼働し続けることのほうが価値がある場合は、キャッシュを使うことで下流サービスが停止していても情報を提供することができる。
キャッシュする場所が増えればデータの鮮度がわかりづらくなるので、キャッシュする場所はできるだけ少なく（1箇所だけが望ましい）する。


データの整合性

CAP定理
分散システムでは、整合性／可用性／分断耐性 の3つの特性のうち、2つしか同時に満たすことはできない。





CAP定理の特性
説明
犠牲にする場合




整合性（Consistency）
ある時点において、複数のノードが同じ結果を返す。

APシステム。結果整合性によって可用性と分断耐性を満たす。一時的に整合性が取れていないデータが見えてしまうことを許容しなければならない。


可用性（Availability）
あるノードで障害が発生しても、他のノードは稼働し続ける。

CPシステム。データの整合性を保証するために、応答時間低下やデータが不整合な場合はエラーとなることを許容しなければならない。


分断耐性（Partition tolerance）
一部ノード間で通信障害が発生しても、システム全体は稼働し続ける。

CAシステム。マイクロサービスアーキテクチャではありえない。




システム全体が CAP定理の APシステム または CAシステム のいずれかでなければならないわけではなく、サービスの要件に応じて使い分ける。
分散システムにおいてデータの整合性を保つことは困難であり、実際には結果整合性で運用できることが大半なので、APシステムが適切な選択になることが多い。


サービスの公開

サービスの検知
どのようなサービスが公開されていて、どのエンドポイントでアクセスすれば良いのかを検知したい場合がある。



方法
説明




DNS
単純に、エンドポイントのドメイン名に命名ルール（https://サービス名-環境.ベンダー/ など）を設ける。


サービスリポジトリ

ZooKeeper、Consul、Netflix Eureka などの、サービスリポジトリツールを利用する。


自作
自作（AWSであれば、タグを活用するなど）する。




サービスのドキュメント化
サービスが公開するAPI仕様をドキュメント化する。

JSONベースの RESTful APIであれば、Swagger などを使ってドキュメント化する。


人間味のあるレジストリ

実際に稼働しているサービスから情報（どんなサービスがあるのか、稼働状況はどうなのか、APIの仕様は、など）を自動的に収集して、人間が見やすい形にまとめることができれば、マイクロサービスを運用・管理する助けになる。


１２章．まとめ

マイクロサービスはどうあるべきか、と言う原則を理解する。
サービスは、技術的観点ではなく、ビジネス的観点（DDDの境界づけられたコンテキスト）で分割する。
テスト、デプロイなどは積極的に自動化する。
サービスは内部実装の詳細を隠蔽し、特定の技術要素に依存しないAPIを公開する。
サービスはチームに所有させる。
サービスを単独で本番デプロイできるようにする。
システムの可用性を得るには、連鎖的障害が発生しないような設計・実装が必要になる。
CAP定理の 整合性 と 可用性 のどちらを犠牲にするのが適切かを検討する。
セマンティック監視で、システム全体の状態を監視する。
ドメインに対する理解度が低い場合は、いきなりマイクロサービス化せずにいったんモノリシックに作って、徐々にサービス化していく。
ビジネスや技術の進歩にあわせて、サービスを徐々に進化（書き直し）させていく。

以上です。
",True,https://qiita.com/crossroad0201/items/2d115daeddbc884adfc5
"

キーワードマッチングを超えた知識を利用する価値
人間間の会話では""Twitter""や""Facebook""がSNSだなと分かって会話ができたり、""ヤマハ""と言われても前後の文脈で""ヤマハ""がバイクの""ヤマハ""かピアノの""ヤマハ""か分かります。
これは単語の背景に関連する知識情報を利用できているからです。
この単語を知識情報と繋げる手法として近年の自然言語処理ではエンティティリンキングという手法がよく用いられています。

コードを使ってすぐに確認したい方は下記でインストールしてください。
コード：
- https://github.com/SnowMasaya/WikiPedia_Entity_Vector_Get_Similarity_word
必要なデータ：
- 分析したいデータ
- Wikificatation
- 日本語 Wikipedia エンティティベクトル

ユースケース
これを実際に使用する場合に下記のようなユースケースが考えられます。
１：サジェスト
キーワード検索する際にその単語と関連する語が出てくれば検索が容易になり、ユーザーにとってもメリットができます。

２：対話インターフェース
対話で発せられる文章は短いため情報が少ないです。この少ない情報から高度な回答をするには単語だけでなく関連する知識へとリンクすることが必須になります。

３：Twitterからの情報抽出
Twitterのつぶやきも情報が少ないので、単純なキーワードを用いるだけで有用な情報抽出には使用しづらいのが現状です。キーワードを関連する知識と紐づければ、キーワードマッチングでは取得できなかった有用な情報を取得することにも繋がります。


エンティティリンキングとは
ACLという自然言語処理のトップのカファンレンスでも課題に上がるほど注目されている手法です。
単純なキーワードと知識を繋げる手法、繋げ方は詳細な情報を含んだリンクでも補足情報を含んだサマリ情報でも良いです。
重要な点は2点です。
１：テキストから重要と思われるキーワードのみ抽出
２：キーワードと関連する情報を繋げる

テキストから重要と思われるキーワードのみ抽出
一般的に重要と思われる単語はWikificatationを用いてキーワードマッチングすれば抽出可能ですが、単純に動作させたい場合はmecabで固有名詞のみ抽出すれば試すことは可能です。
本来はこのキーワードが有用かそうでないかの判断をする機械学習のモデルを挟む必要がありますが、今回紹介する記事では述べません。詳細を知りたい方は下記の資料をご覧ください
知識ベースを活用したエンティティリンキング 

キーワードと関連する情報を繋げる
単純にキーワードをWikiPediaまたはDBPediaとマッチングさせてマッチングしたリンク先の情報を使用するのも手法としてあります。
仮にマッチングするのがベクトル空間の場合、計算によって様々な操作が可能になり、用途が広がります。それを可能としているのが今回、紹介する日本語 Wikipedia エンティティベクトルです。
Word2Vecの発展させた手法なので、中身が分かれば応用が可能なので詳細を知りたい方は論文を読むことをお勧めします。読まなくてもベクトル計算後のデータはすでに用意してくれているので時間がない方はそちらを使ってもらっても問題ないと思います。
Wikipedia記事に対する拡張固有表現ラベルの多重付与

日本語 Wikipedia エンティティベクトルの作成方法
非常にシンプルで分かりやすく作られています。
１：WikiPediaのデータをmecabなどで単語に区切る
２：WikiPedia中のハイパーリンクが付与された単語をリンク先のタイトルに置き換え
３：WikiPedia中にハイパーリンク付きの単語が再度現れる場合はハイパーリンクが付いていないので、付いているものとみなし2と同様の処理を行う。
４：得られた単語群を元にWord2Vecで学習を行う。
これによって固有表現を単語として抽出し、 実世界のエンティティに関連づけること（エンティティリンキング）が可能になります。

日本語 Wikipedia エンティティベクトルを利用した実装例

上記がシステムの実装例になります。
wikipediaエンティティベクトルを使用して単語から関連する語を取得してみます。
コードはgithubに載せているので、重要な部分のみ引用します。
１：Twitterのつぶやきを収集（今回はりんなのデータを使用）
２：分かち書きを行う
３：Wikificationで固有表現を抽出
４：日本語 Wikipedia エンティティベクトルでベクトルを付与
５：コサイン類似度を計算し類似度が高いものを類義語として付与
コードは単純なので下記をご覧ください。
https://github.com/SnowMasaya/WikiPedia_Entity_Vector_Get_Similarity_word

高速化のための工夫

工夫１：OpenBlasを直接使用
コサイン類似度を計算していますが、計算コストが高いのでこの部分の高速化を試みます。
高速化のためにOpenBlasを使用している部分が難しいので解説を書いておきます。
Macでインストールする場合は下記のコマンドでインストールします。
brew install openblas 

下記でどのディレクトリにopenblasのライブラリがあるか明記しておきます。
[openblas]
libraries = openblas
library_dirs = /usr/local/opt/openblas/lib
include_dirs = /usr/local/opt/openblas/include

Cython型で計算しているため、効果は不明ですが、まず下記のコードでコサイン類似度を計算するベクトルがのメモリーのレイアウトがCのスタイルかどうかをチェックします。
このチェックの理由はBlasがベクトルのメモリーのレイアウトがCのスタイルだった場合コピー処理を行うのですが、Fortranのスタイルの場合にも同様の処理を行わないことで余計な処理を除き高速化するためです。
    def __force_forder(self, x):
        """"""
        Converts array x to fortran order Returns a tuple in the form (x is transposed)
        :param x(vector):
        :return:
        """"""
        if x.flags.c_contiguous:
            return (x.T, True)
        else:
            return (x, False)

次に下記のコードでベクトルの内積を計算しています。ベクトルの型をチェックした後でCの型の場合は変換処理があること明記しておくことでベクトルの型がFortranの場合は変換処理が必要ではなく、高速に計算が可能になります。
def __faster_dot(self, A, B):
        """"""
        Use blas libraries directory to perform dot product
        Reference:
            https://www.huyng.com/posts/faster-numpy-dot-product
            http://stackoverflow.com/questions/9478791/is-there-an-enhanced-numpy-scipy-dot-method
        :param A(mat): vector
        :param B(mat): vector
        :return:
        """"""
        A, trans_a = self.__force_forder(A)
        B, trans_b = self.__force_forder(B)

        return FB.dgemm(alpha=1.0, a=A, b=B, trans_a=trans_a, trans_b=trans_b)



工夫２：スレッドベースの並列分散処理
コサイン類似度の計算もボトルネックになりますが、WikiPediaEntityVectorの登録されている単語数も多いため同様の処理を何度も行うと時間が非常にかかります。
Pythonは基本的にシングルプロセスで動作するためスレッドベースでの並列処理を実装して高速化を試みました。
Queueを用いたProducer Consumerパターンを使用しています。今回のケースはConsumerの処理が重いため、Consumerに与えるスレッド数を多くして高速化を試みました。
コンシューマーのサイズを設定して、そのサイズ分だけスレッドを生成して動作を行っています。
for index in range(args.consumer_size):
        multi_thread_consumer_crawl_instance = threading.Thread(target=producerConsumer.consumer_run, name=consumer_name + str(index))
        multi_thread_consumer_crawl_instance.start()


結果の例
元の固有表現：[計算により導出された類義語群]
下記を見ると単純なキーワードマッチで取ることが難しいが関連性の高い単語が取れていることが分かります。
'秋田': ['長野', '福島', '高知', '岩手', '山形', '新潟', '青森', '熊本', '盛岡'], 

'百': ['百', '十', '千'],

'ゴジラ': ['ゴジラ_(1954年の映画)', 'ゴジラ_(架空の怪獣)', 'ガメラ'], 

'3': ['4', '6', '5', '0', '7', '8', '9', '2', '1'],

'赤': ['紫', '緑色', '緑', '朱色', '黒', '赤色', '青色', '白', '黄色', '藍色', '青']

'豚': ['牛', '羊', 'ヒツジ', 'ニワトリ', 'ヤギ', '鶏', '山羊', 'ブタ', 'ウシ'], 

'ゴルフ': ['ボウリング'], 

'竹': ['柳', '松']

'5': ['4', '6', '0', '7', '3', '8', '9', '2', '1'], 

'枝': ['茎', '葉', '枝は'],

'木': ['杉', '樫', '切り株', '松の木'],

'ふん': ['ぺん', 'ぎゅう'], 

'学生': ['生徒', '大学生'],

'餅': ['饅頭', '徳利', '赤飯', '玉子', '神酒', '粥', 'アズキ', '団子'],

'腰': ['臀部', '膝', '踵', '肩'], 

'髭': ['口髭', 'ひげ', '口ひげ', 'ヒゲ', '髭', '髪の毛', 'あごひげ'], 

'猫': ['小鳥', 'ネコ', '仔猫', 'ネズミ'], 

'中国': ['台湾', '朝鮮', '韓国', '中華人民共和国'],

'二つ': ['五つ', 'ふたつ', '２つ', '三つ'], 

'浴衣': ['浴衣', '普段着', '白無垢', '喪服', '着物', 'タキシード', '普段着', '白無垢', '喪服', '着物', 'タキシード'], 

'野球': ['ラグビー'],

'髪': ['頭髪', '黒髪', '長髪', '髭', '髪の毛', '前髪', '金髪', '髪型'],

'秋': ['秋', '夏', '春', '夏', '春'],

'奈良': ['和歌山']


注意点
固有表現はWikificationで表しているのでWikipediaに依存しています。
データの知識空間はWikiPediaに依存しています。
業界が特殊な場合やレアケースが多い場合は使用しない方がベターです。
日本語 Wikipedia エンティティベクトルはハイパーリンクの単語は""<<単語>>""で表されているので""<<>>""を除く処理が必要です。
メモリを多く消費します。
計算時間も非常に長いです。元の固有表現が192単語の場合はシングルプロセスでシングルスレッドで動作させると3時間程度かかりますが、固有表現ごとに同一の処理をしているため並列分散処理をすれば早くなります。

参考
知識ベースを活用したエンティティリンキング 
Yamada123, Ikuya, Hideaki Takeda, and Yoshiyasu Takefuji. ""Enhancing Named Entity Recognition in Twitter Messages Using Entity Linking."" ACL-IJCNLP 2015 (2015): 136.
Faster numpy dot product for multi-dimensional arrays
scipy.linalg.blas.ddot
numpy.ndarray.flags
Is there an “enhanced” numpy/scipy dot method?
models.word2vec – Deep learning with word2vec
",True,https://qiita.com/GushiSnow/items/d7c5a09b38d3ca650abd
"

Apache Spark on Kubernetes とは
　Apache Spark(以下、Spark)は、v2.3.0よりKubernetesをNativeサポートしました。これまで、管理者により事前に用意されたSpark/Hadoopと、それに割り当てられたコンピュートリソースを使って並列計算を実行していました。今回のNativeサポートでは、Kubernetesを管理する管理者がSpark用のPod(コンテナ)をあらかじめ準備する必要はありません。Kubernetes環境さえあれば、Spark用に用意されたコンピュートリソースを気にせずに実行することが可能となります。
　大まかな実行の流れとしては、SparkのユーザがSparkのCLI(spark-submitコマンド)を実行すると、KubernetesのAPI Serverを通じ、SchedulerにてSparkを実行するコンピュートリソース(Node)を定めます。次にNodeにSpark DriverのPod(Executorの管理を行うPod)と、計算を行うSpark ExecutorのPodがデプロイされ、並列計算を実行します。また、Sparkでの計算を完了する毎に、Podを削除/停止するため、計算を行っていない時に無駄なコンピュートリソースを消費することがありません。つまり、AWSやGCPなどの従量課金のパブリッククラウドを使う場合において、常に起動しているデーモンプロセスのようなPodが存在しないため、コンピュートリソース(CPU,メモリ)については実際に計算に使った分のみの課金となります。
　このように、Sparkのユーザは、spark-submitコマンドで指定するKubernetesのURLさえ知っていれば、多数のコンピュートリソースを使った並列計算環境を簡単に手にいれることが出来ます。これによって、自分が所有するコンピュートリソースで計算パワーが不足している場合に、手間と時間をかけず(お金だけで)解決できる並列計算環境が実現できます。

処理の流れ
Spark on Kubernetesの処理の流れを下図を使い順番に追っていきます。


(Step1)ユーザにて'spark-submit'コマンドを使い、計算の実行を指示します
(Step2) KubernetesのNode上にSpark DriverのPod(コンテナ)がデプロイされます
(Step3) SparkDriverがspark-submitで指定された数のExecutorのPodをデプロイし、各Executorにて計算が実行されます
(Step4) 計算が終了したExecutorのPodは自動で削除されます。全てのExecutorのPodが削除された後、最後にSpark DriverのPodが停止(Terminate)されます。計算結果は、Spark DriverのPodのログに出力されます。

Spark DriverのPodは自動で削除されないため、計算結果を確認後にkubectlコマンドを使い手動で削除します。

Spark v2.3.0での制限事項
Spark on Kubernetesは、まだまだ開発途上のプロジェクトです。
そのため、以下がFuture Workとなっています。

PySpark
R
Dynamic Executor Scaling
Local File Dependency Management
Spark Application Management
Job Queues and Resource Management

PythonやRでSpark on Kubernetesを試せるのは、少し先になりそうです。
詳細はこちらを参照ください。

動作検証
次に、実際にSpark on Kubernetesを動かし検証します。
動作検証の環境は、次の環境を使います。

Sparkのコンパイル/CLI実行環境: Ubuntu 16.04.4 LTS + Docker 18.03.1-ce
Kubernetes環境: Kubernetes v1.10.2　
(kubeadmを使いVirtual Box上に構築したMaster, WorkerNode x 2VMの合計3VMのクラスタ環境)
コンテナレポジトリ: Docker Hub
サンプルアプリ: Sparkに付属のSparkPi (Javaで書かれたπの計算プログラム)

OS, Docker, Kubernetes自身のセットアップおよびDocker Hubのユーザ作成については割愛します。

ダウンロード&コンパイル
SparkのダウンロードサイトからSpark 2.3.0をダウンロードします。
$ wget http://ftp.jaist.ac.jp/pub/apache/spark/spark-2.3.0/spark-2.3.0.tgz 

$ gzip -cd spark-2.3.0.tgz | tar xvf -
$ cd spark-2.3.0/

以下、spark-2.3.0のディレクトリで作業します。
次に、ダウンロードしたSparkのソースをコンパイルします(約45分)。
$ build/mvn -DskipTests clean package
<snip>
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [09:56 min]
[INFO] Spark Project Tags ................................. SUCCESS [05:12 min]
[INFO] Spark Project Sketch ............................... SUCCESS [  9.913 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 58.391 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 12.961 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 32.245 s]
[INFO] Spark Project Launcher ............................. SUCCESS [02:26 min]
[INFO] Spark Project Core ................................. SUCCESS [05:59 min]
[INFO] Spark Project ML Local Library ..................... SUCCESS [02:44 min]
[INFO] Spark Project GraphX ............................... SUCCESS [ 25.961 s]
[INFO] Spark Project Streaming ............................ SUCCESS [01:10 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [02:45 min]
[INFO] Spark Project SQL .................................. SUCCESS [05:33 min]
[INFO] Spark Project ML Library ........................... SUCCESS [02:18 min]
[INFO] Spark Project Tools ................................ SUCCESS [ 10.025 s]
[INFO] Spark Project Hive ................................. SUCCESS [02:19 min]
[INFO] Spark Project REPL ................................. SUCCESS [  6.327 s]
[INFO] Spark Project Assembly ............................. SUCCESS [  3.664 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 33.437 s]
[INFO] Spark Project External Flume ....................... SUCCESS [ 12.331 s]
[INFO] Spark Project External Flume Assembly .............. SUCCESS [  3.056 s]
[INFO] Spark Integration for Kafka 0.8 .................... SUCCESS [ 39.349 s]
[INFO] Kafka 0.10 Source for Structured Streaming ......... SUCCESS [ 33.933 s]
[INFO] Spark Project Examples ............................. SUCCESS [ 25.154 s]
[INFO] Spark Project External Kafka Assembly .............. SUCCESS [  4.582 s]
[INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [ 12.030 s]
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [  4.851 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 45:57 min
[INFO] Finished at: 2018-05-1T23:21:21+09:00
[INFO] Final Memory: 86M/861M
[INFO] ------------------------------------------------------------------------

BUILD SUCCESSが出力されればコンパイル成功です。
今回、筆者は、ソースファイルからコンパイルしましたが、Apache SparkのWebサイトからコンパイル済みのバイナリをダウンロードしても構いません。


Sparkのコンテナ作成＆レポジトリ登録
KubernetesにデプロイするSparkのコンテナイメージを作成します。
作成には、docker-image-tool.shを使います。
コンテナをBuildするためのDockerfileは、kubernetes/dockerfiles/sparkディレクトリ配下にあるので、必要に応じて自身のSparkのプログラムなどを追加してください。
$ sudo bin/docker-image-tool.sh -r ysakashita -t v2.3.0 build
<snap>
Successfully built f9cd85baa796
Successfully tagged ysakashita/spark:v2.3.0

$ sudo docker images
REPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE
ysakashita/spark                           v2.3.0              f9cd85baa796        2 minutes ago       350MB

次に、Buildしたコンテナをレポジトリに登録します。
$ sudo docker login
Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.
Username (ysakashita): ysakashita
Password: 

$ sudo bin/docker-image-tool.sh -r ysakashita -t v2.3.0 push
The push refers to a repository [docker.io/ysakashita/spark]
<snip>
v2.3.0: digest: sha256:3252c88b5527a97b9743824ae283c40e9d69b8587155ebd8c4d6f1b451d972f8 size: 2626

Docker Hubにログインし、実際に登録されているかを確認します。

登録されていれば、コンテナイメージの完了です。
次に、Spark on Kubernetesを実行するアカウントをKubernetesに作成します。
$ kubectl create serviceaccount spark

$ kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default

今回の検証では、defaultネームスペースを利用するので、defaultに権限を与えておきます。
以上で、検証の準備は完了です。
次は、いよいよ Spark on Kubernetesを使った並列計算を実行していきます。

Spark on Kubernetesを使った並列計算の実行
Spark on Kubernetesを使った並列計算を実行します。
実行には、spark-submitコマンドを使います。
$ kubectl cluster-info
Kubernetes master is running at https://192.168.0.23:6443
KubeDNS is running at https://192.168.0.23:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

$ bin/spark-submit \
    --master k8s://https://192.168.0.23:6443 \
    --deploy-mode cluster \
    --conf spark.executor.instances=3 \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
    --conf spark.kubernetes.container.image=ysakashita/spark:v2.3.0 \
    --class org.apache.spark.examples.SparkPi \
    --name spark-pi \
    local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar




Option
Description




--master
Kubernetes master(API Server)のアドレス


--deploy-mode
Spark on Kubernetesではclusterを指定


--conf spark.executor.instances
Spark Executorの数


--conf spark.kubernetes.authenticate.driver.serviceAccountName
Service Account名


--conf spark.kubernetes.container.image
Spark のコンテナイメージ


--class
実行するJavaアプリのクラス名


--name
実行するJavaアプリのプログラム名


<app jar>
事項するJavaアプリのjar ファイル名



その他のオプションについては、こちらをご参照ください。
spark-submitコマンドの実行結果にて、Exit codeが0(=正常終了)と表示されていれば、計算が正しく終了しています1。
         Container name: spark-kubernetes-driver
         Container image: ysakashita/spark:v2.3.0
         Container state: Terminated
         Exit code: 0

Spark DriverのPodのログをkubectl logsを使い計算結果を確認します。
$ kubectl get pods
NAME                                               READY     STATUS      RESTARTS   AGE
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-driver   0/1       Completed   0          29s

$ kubectl logs spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-driver |less
<snip>
2018-05-03 01:04:22 INFO  DAGScheduler:54 - Job 0 finished: reduce at SparkPi.scala:38, took 1.594295 s
Pi is roughly 3.144675723378617
<snip>

SparkPiの実行結果である3.144675723378617が出力されているのが確認できます。
ユーザが実行結果を確認できるように、Spark DriverのPodはReadyが0/1と停止している状態で削除されずに残り続けています。
実行結果が確認したら、Podを削除してください。
$ kubectl delete pods spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-driver


計算処理中のPodデプロイメントの検証
上記でspark-submitコマンドにて計算処理を実行している最中の、Kubernetes上のPodのデプロイメントについて見ていきます。
まずは、spark-submitコマンドを実行した直後です。
$ kubectl get pods
NAME                                               READY     STATUS    RESTARTS   AGE
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-driver   1/1       Running   0          4s

Spark DriverのPodがデプロイされています。
処理の流れの章で述べた(Step2)の状態です。

数秒後に再度Podの状態を確認してみます。
$ kubectl get pods
NAME                                               READY     STATUS    RESTARTS   AGE
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-driver   1/1       Running   0          14s
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-exec-1   1/1       Running   0          7s
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-exec-2   1/1       Running   0          7s
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-exec-3   1/1       Running   0          7s

すると、今度はSpark ExecutorのPodが3つデプロイされているのが確認できます。
処理の流れの章で述べた(Step3)の状態です。

今回はspark-submitコマンドでExecutorの数を3つと指定しているので、3Podsです。並列数を上げて計算したい人は、Spark Executorの数を増やしてみてください。
さらに、しばらくの間待ちます。
すると、各Spark Executorは、計算処理が完了すると自動的に削除されます。
$ kubectl get pods
NAME                                               READY     STATUS        RESTARTS   AGE
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-driver   0/1       Completed     0          19s
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-exec-1   0/1       Terminating   0          12s
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-exec-2   0/1       Terminating   0          12s
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-exec-3   0/1       Terminating   0          12s

$ kubectl get pods
NAME                                               READY     STATUS      RESTARTS   AGE
spark-pi-9efb8a5e1e3b38f789505ace52dac3fe-driver   0/1       Completed   0          29s

最後に、Spark DriverのPodがREADY 0/1になった状態で残っているのが確認できます。
処理の流れの章で述べた(Step4)の状態です。

このように、Sparkの並列計算の処理にあわせ、Kubernetes上にPodがデプロイ&削除されていきます。

クリーンアップ
作成したSparkのコンテナイメージをレポジトリから削除します。
Docker Hubにログインし、Settingsのタブを選択後、Delete RepositoryのDeleteボタンをクリックし、削除します。

ローカルのDockerからもコンテナイメージを削除します。
$ sudo docker images
REPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE
ysakashita/spark                           v2.3.0              f9cd85baa796        14 hours ago        350MB

$ sudo docker rmi f9cd85baa796
Untagged: ysakashita/spark:v2.3.0
Untagged: ysakashita/spark@sha256:3252c88b5527a97b9743824ae283c40e9d69b8587155ebd8c4d6f1b451d972f8
Deleted: sha256:f9cd85baa79655ccefe0d7b646b499cbf568acd064c77add7e1773dfe8e14eaa
<snip>

最後に、ダウンロード&コンパイルしたSparkを削除します。
$ cd ..
$ rm -rf spark-2.3.0


Know-How
今回の検証中に、Spark ExecutorのPodが、Pendingのまま実行されないという現象が発生しました。
$ kubectl get pods
NAME                                                READY     STATUS      RESTARTS   AGE
<snip>
spark-pi-8d89987d72ba32c2a89b99876ca81129-exec-3   0/1       Pending     0          10s
<snip>

そこで、失敗したPodをkubectl describeコマンドで確認してみると、
$ kubectl describe pods spark-pi-8d89987d72ba32c2a89b99876ca81129-exec-3
<snip>
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  12s (x6 over 27s)  default-scheduler  0/3 nodes are available: 1 Insufficient memory, 3 Insufficient cpu.

CPUとMemoryが不足しているため、Kubernetes SchedulerがPodを動作するNodeを割り当てられない状況になっているのがわかります。
Spark 2.3.0 の公式ドキュメントを確認してみると
We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor.
と記載にあるように、3CPU, Memory 4Gbytes以上必要そうです。
そこで、VirtualBoxのVMのCPU数とメモリを増やしてみたところ、無事にPodが実行されるようになりました。
以下のコマンドでは、VirtualBox上のVMに割り当てられたCPU数とMemoryを変更しています。
事前にKubernetesのVMを全てShutdownしておいた状態で、VirtualBoxの母艦のPCで実行してください。
$ VBoxManage list vms
""k8s"" {3c2bf51f-3344-431f-8a3e-5925e065389b}
""k8s-node1"" {82226c81-30ac-4292-9987-e401d9f41356}
""k8s-node2"" {e82c1953-1d60-4f3f-9a2b-282299249f78}

$ VBoxManage modifyvm k8s --memory 4096 --cpus 3
$ VBoxManage modifyvm k8s-node1 --memory 4096 --cpus 3
$ VBoxManage modifyvm k8s-node2 --memory 4096 --cpus 3

上記変更では、仮想CPU数が物理マシンに搭載しているCPU数を超えてしまう人が大半かと思います。
VirtualBoxのGUIでは、以下の警告メッセージが表示されたりしますが、仮想CPUのコンテキストスイッチ&割り込み処理で多少性能が落ちるだけで動作します。
性能を気にしない検証であれば無視してください。


感想
　Spark on Kubernetesを触って見た第一印象は、「Kubernetes」ユーザのためのSpark連携ではなく、「Spark」ユーザのためのKubernetes連携ということです。Kubernetesを利用しているユーザであれば、KubernetesのReplicasetを使わずに、独自にExecutorの数をコントロールしている点など、少し違和感を感じるかもしれません。
　SparkとKubernetesの両方を使いこなせるユーザは、世界的にも希有なのかもしれません。Sparkのユーザは、データサイエンティストのようなデータ分析に特化したスキルを持っている人達かと思います。そのようなユーザ向けには、従来のSparkでも利用していたspark-submitコマンドのみで、並列計算を指示できるのは、嬉しいかもしれません。--conf 以外の拡張では、--masterにk8s://の指定が出来るようになったのみで、他のオプションは変わりません。そのため、インフラやKubernetesを知らないSparkのユーザでも、敷居は低いかと思います。
　また、別の観点としては、コンテナイメージの作成&レポジトリ登録が、Sparkのユーザに可能なのか？という心配が生まれるかと思います。今回の検証では、サンプルプログラムを使ったため、Sparkのアプリ自体はコンテナに含めていましたが、コンテナに含めずに外部に置くことも可能です(Using Remote Dependencies参照)。これによりSparkのアプリを更新するたびに、コンテナイメージを再作成&リポジトリ登録する作業がなくなります。Kubernetesのクラスタを管理するインフラのスキルを持った管理者が、一度Spark on Kubernetesのコンテナイメージを作成&レポジトリ登録しておけば、Sparkユーザは、spark.kubernetes.container.imageを指定するだけでSpark on Kubernetesを利用できます。
　上述のように、Spark on Kubernetesは「Spark」ユーザ向けには、使い易いのではないかと思います。残念な点をあげるとすれば、Job Queuesがv2.3.0の時点では未サポートの点です。Sparkのユーザは、Intaractiveな計算よりは、バッチなどのJobを利用したケースが多いかと思います。この点については、Future Workにあがっているので、楽しみに待ちたいと思います。

参考情報

Running Spark on Kubernetes
apache-spark-on-k8s/spark


追記(2018/8)
HDFS on Kubernetesの検証の記事を公開しました。
これにより、Using Remote Dependenciesに記載されているspark-submitコマンドで指定できるspark.files(or --files)のデータ格納先HDFSもKubernetes上で管理することが可能となります。




Exit codeが0以外となった場合、コンピュートリソースが不足していることがありますので、KnowHowの節を参照し、Kubernetesのコンピュートリソースを増やしてみてください。 ↩



",True,https://qiita.com/ysakashita/items/1f87646ae804ba509d07
"Christopher Olah氏のブログ記事
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
の翻訳です。
翻訳の誤りなどあればご指摘お待ちしております。


リカレントニューラルネットワーク
人間は毎秒ゼロから思考を開始することはありません。このエッセイを読んでいる間、あなたは前の単語の理解に基づいて、各単語を理解します。すべてを捨てて、またゼロから思考を開始してはいません。あなたの思考は持続性を持っています。
従来のニューラルネットワークは、これを行うことができません、それは大きな欠点のように思えます。たとえば、映画の中の各時点でどのような種類の出来事が起こっているかを分類したいと想像してください。従来のニューラルネットワークが、映画の前の出来事についての推論を後のものに教えるためにどのように使用できるかは不明です。
リカレントニューラルネットワークは、この問題に対処します。それは内部にループを持ち、情報を持続させることができるネットワークです。

リカレントニューラルネットワークはループを持つ
上の図で、ニューラルネットワークのかけら、 $A$ は、入力 $x_t$ を見て、値 $h_t$ を出力します。ループは、情報をネットワークの１ステップから次のステップに渡すことを可能にします。
このようなループにより、リカレントニューラルネットワークは不可解なものに思われます。しかし、もう少し考えると、それが通常のニューラルネットワークとそれほど違いがないことが判ります。リカレントニューラルネットワークは、同じネットワークの複数のコピーであり、それぞれが後続のネットワークにメッセージを渡すと考えることができます。

展開されたリカレントニューラルネットワーク
この鎖状の性質は、リカレントニューラルネットワークが配列やリストに密に関連していることを明らかにします。それは、このようなデータに使用するための自然なアーキテクチャです。
そして、それは確かに使用されています！ここ数年、さまざまな問題にRNNが適用され、信じられないほどの成功がありました：音声認識、言語モデリング、翻訳、画像キャプション…リストは続きます。RNNにより達成することができる、驚くべき偉業に関する議論は、 Andrej Karpathy の優れたブログ記事、リカレントニューラルネットワークの理不尽な効力に託します。でも、それらは本当にかなり素晴らしいです。
これらの成功に欠かせないことに、「LSTM」の使用があります。LSTMは非常に特別な種類のリカレントニューラルネットワークであり、多くのタスクにおいて、標準バージョンよりもはるかに優れた働きをします。リカレントニューラルネットワークに基づくほぼすべてのエキサイティングな結果は、これを用いて達成されています。このエッセイが探求するのは、これらLSTMです。

長期依存性の問題
RNNのアピールの１つは、前のビデオ・フレームの使用が現在のフレームの理解を助けるように、前の情報を現在のタスクに関係づけることができるというアイデアです。RNNにこれができれば、RNNはとても役に立つでしょう。しかし、できるでしょうか？それは場合によります。
時おり、私たちは現在のタスクを実行するのに、最新の情報を見てする必要があります。例えば、言語モデルが、以前の単語に基づいて、次の単語の予測を行うと考えてください。「the clouds are in the sky,」の最後の単語を予測する場合、これ以外のコンテキストを必要としません、次の単語が sky になることはかなり明白です。このように関連する情報とそれを必要とする場所のギャップが小さい場合、RNNは過去の情報を利用することを学習することができます。

しかし、より多くのコンテキストを必要とする場合もあります。テキスト「 I grew up in France… I speak fluent French. 」の最後の単語の予測を試みると考えてみましょう。直近の情報は、次の単語がおそらく言語の名前であることを示唆していますが、どの言語か絞り込みたい場合、さらに後ろから、 France のコンテキストを必要とします。関連する情報とそれを必要とする場所のギャップが非常に大きくなることも十分あり得ます。
残念ながら、ギャップが大きくなるに従い、RNNは情報を関連づけて学習することができなくなります。

理論上、RNNはこのような「長期の依存性」を取り扱うことが十分できます。この形式の例題（toy problems）を解決するために、人が慎重にパラメータを選ぶことはできます。悲しいことに、実際には、RNNがそれを学習できるようにはならないようです。この問題は Hochreiter (1991) [ドイツ語] と  Bengio, et al. (1994) により徹底的に調査され、それが難しいいくつかのかなり基本的な理由が見つかりました。
ありがたいことに、LSTMにはこの問題がありません。

LSTMネットワーク
Long Short Term Memory ネットワークは、通常は「LSTM」と呼ばれ、長期的な依存関係を学習することのできる、RNNの特別な一種です。これらは Hochreiter & Schmidhuber（1997） により導入され、後続の研究1で多くの人々によって洗練され、広められました。それは多種多様な問題にものすごくよく動作し、現在では広く使用されています。
LSTMは長期の依存性の問題を回避するように明示的に設計されています。長時間の情報を記憶することは実質的にそのデフォルトの動作であり、学習するのに苦労はありません！
すべてのリカレントニューラルネットワークは、ニューラルネットワークのモジュールを繰り返す、鎖状をしています。標準のRNNでは、この繰り返しモジュールは、単一の tanh 層という、非常に単純な構造を持ちます。

標準RNNの繰り返しモジュールは単一の層を含む
LSTMもまたこの鎖のような構造を持ちますが、繰り返しモジュールは異なる構造を持ちます。単一のニューラルネットワーク層ではなく、非常に特別な方法で相互作用する、４つの層を持ちます。

LSTMの繰り返しモジュールは4つの相互作用する層を含む
詳細については心配しないでください。後に一歩一歩LSTMの図を見ていきます。今のところは、後で使用する表記を覚えておきましょう。

上の図で、それぞれの線は、ベクトル全体を、一つのノードの出力から他のノードの入力に運びます。ピンクの円は、ベクトルの加算のような、一点の操作を表し、黄色のボックスは、学習されるニューラルネットワークの層です。合流している線は連結を意味し、分岐している線は内容がコピーされ、そのコピーが別の場所に行くことを意味します。

LSTMの中心的アイデア
LSTMの鍵は、セル状態、図の上部を通る水平線です。
セル状態は一種のコンベア・ベルトのようなものです。それはいくつかのマイナーな線形相互作用のみを伴い、鎖全体をまっすぐに走ります。情報は不変で、それに沿って流れることは非常に簡単です。

LSTMは、セル状態に対し情報を削除したり追加する機能を持っています。この操作はゲートと呼ばれる構造によりしっかり制御されます。
ゲートは選択的に情報を通す方法です。これはシグモイド・ニューラルネット層と一点の乗算により構成されます。

シグモイド層は0から1までの数値を出力します。この数値は各コンポーネントをどの程度通すべきかを表します。0は「何も通さない」を、1は「全てを通す」を意味します！
LSTMは、セル状態を保護し、制御するために、このようなゲートを３つ持ちます。

ステップ・バイ・ステップLSTMウォークスルー
LSTMの最初のステップは、セル状態から捨てる情報を判定することです。この判定は「忘却ゲート層」と呼ばれるシグモイド層によって行われます。それは、 $h_{t-1}$ と $x_t$ を見て、セル状態 $C_{t-1}$ の中の各数値のために $0$ と $1$ の間の数値を出力します。 $1$ は「完全に維持する」を表し、 $0$ は「完全に取り除く」を表します。
では、前のすべての単語に基づいて次の単語を予測する、言語モデルの例に戻りましょう。このような問題では、正しい代名詞を使用するために、セル状態は現在の主語の性別を含むかもしれません。新しい主語を見るときには、古い主語の性別は忘れたいです。

次のステップは、セル状態で保存する新たな情報を判定することです。これには2つの部分があります。まず、「入力ゲート層」と呼ばれるシグモイド層は、どの値を更新するかを判定します。次に、 tanh 層は、セル状態に加えられる新たな候補値のベクトル $\tilde{C}_t$ を作成します。次のステップでは、状態を更新するために、これら2つを組み合わせます。
言語モデルの例では、忘れようとしている古いものを置き換えるために、セル状態に新たな主語の性別を追加したいです。

そして、古いセル状態 $C_{t-1}$ から新しいセル状態 $C_t$ に更新します。何をするべきかについては前のステップですでに判定しました。今、実際にそれをする必要があります。
古い状態に $f_t$ を掛け、さきほど忘れると判定されたものを忘れます。そして、 $i_t*\tilde{C}_t$ を加えます。これは、各状態値を更新すると決定した割合でスケーリングされた、新たな候補値です。
言語モデルの場合、前のステップで判定した通り、ここで実際に古い主語の性別に関する情報を落とし、新たな情報を加えます。

最後に、出力するものを判定する必要があります。この出力は、セル状態に基づいて行われますが、フィルタリングされたバージョンになります。まず、シグモイド層を実行します。この層は、セル状態のどの部分を出力するかを判定します。その後、判定された部分のみ出力するため、セル状態に（値を-1と1の間に圧縮するために） $tanh$ を適用し、それにシグモイド・ゲートの出力を掛けます。
言語モデルの例では、主語を見たとき、動詞が次に来る場合には、動詞に関連する情報を出力することを求められるかもしれません。例えば、主語が単数か複数かを出力するかもしれません。動詞が後につづく場合、どの活用形であるべきかわかるためです。


LSTMのバリエーション
これまで説明してきたのは、かなりノーマルなLSTMです。でも、すべてのLSTMが上記と同じではありません。実際には、LSTMを含むほぼすべての論文は、わずかに異なるバージョンを使用しているようです。違いは軽微なものですが、いくつかについて言及する価値があります。
Gers & Schmidhuber (2000) により導入された、一般的なLSTMのバリエーションの一つは、「のぞき穴の結合」を加えています。これは、ゲート層にセル状態を見させることを意味します。

上の図ではすべてのゲートにのぞき穴が追加されていますが、多くの論文では、いくつかにはのぞき穴を与え、他のものには与えません。
別のバリエーションでは、忘却ゲートと入力ゲートを組み合わせて使用します。何を忘れ、新しい情報を何に加えるべきかを別々に判定する代わりに、これらの判定を同時に行います。その場所に何かを入力するときのみ、忘却します。古いものを忘れたときのみ、状態に新しい値を入力します。

LSTMのもう少し劇的なバリエーションは、 Cho, et al. (2014) により導入された、 Gated Recurrent Unit 、あるいはGRUです。これは忘却ゲートと入力ゲートを単一の「更新ゲート」に組み合わせます。また、セル状態と隠れ状態をマージし、他のいくつかの変更を加えます。結果として得られるモデルは、標準的なLSTMモデルよりもシンプルであり、ますます一般的になってきています。

これらは、最も注目すべきLSTMのバリエーションのほんの一部です。他にも、 Yao, et al. (2015) による Depth Gated RNNs などがあります。また、長期の依存性に取り組むまったく異なるアプローチ、 Koutnik, et al. (2014) による Clockwork RNNs などもあります。
これらのバリエーションのうちどれがベストでしょうか？違いは重要でしょうか？ Greff, et al. (2015) は、ポピュラーなバリエーションのすばらしい比較を行い、それらすべてがほぼ同じだと結論づけました。 Jozefowicz, et al. (2015) は、１万以上のRNNのアーキテクチャをテストし、その一部は特定のタスクにおいてはLSTMよりも良いと結論づけました。

結論
さきほど、人々がRNNで達成した顕著な成果を述べました。基本的にこれらのすべてがLSTMを使用して達成されます。それはほとんどのタスクにおいて本当に多くの良い働きをします！
一連の方程式として書かれると、LSTMはかなり威圧的に見えます。このエッセイで一歩一歩見ていくことで、それがもう少し親しみやすくなっていれば幸いです。
LSTMは、RNNで達成することができるものにおける大きな一歩でした。以下の疑問は自然です：ほかに大きな一歩はありますか？研究者の間で共通の意見は次のとおりです：「はい！次の一歩があり、それはアテンションです！」。そのアイデアは、RNNのすべてのステップが、情報のいくつかの大きなコレクションから、見るために情報を摘まめるようにするというものです。たとえば、画像を説明するキャプションを作成するためにRNNを使用する場合、出力する単語ごとに、見るために画像の一部を摘まむかもしれません。実際、 Xu, et al. (2015) は、まさにこれを行いました、アテンションを知りたい場合、それは楽しい出発点かもしれません！アテンションを使用した、いくつかの本当にエキサイティングな結果があり、角を曲がればさらにたくさんあるように思われます…
アテンションはRNN研究の唯一のエキサイティングな糸ではありません。たとえば、 Kalchbrenner, et al. (2015) によるGrid LSTMは、非常に有望に思えます。生成モデルにRNNを使用した研究（ Gregor, et al. (2015) 、 Chung, et al. (2015) 、 Bayer & Osendorfer (2015) など）も、非常に興味深いと思われます。ここ数年は、リカレントニューラルネットワークにとってエキサイティングな時間でした。今後もさらにそうであることを約束します！

謝辞
LSTMをより良く理解する手助けをし、可視化についてコメントし、この記事にフィードバックしてくださった方々に感謝いたします。
有益なフィードバックをくださったGoogleの同僚、特に Oriol Vinyals 、 Greg Corrado 、 Jon Shlens 、 Luke Vilnis 、 Ilya Sutskever に非常に感謝しています。また、Dario Amodei 、 Jacob Steinhardt を含め、時間を割いて助けてくださった、多くの友人や同僚に感謝します。図について非常に考え深い対応のため、 Kyunghyun Cho には特に感謝しています。
この投稿以前、私は、ニューラルネットワークを教える2つのセミナー・シリーズの中で、LSTMを説明する練習をしました。忍耐づよく参加し、フィードバックしてくださったみなさまに感謝します。





原著者に加えて、多くの人がモダンLSTMに貢献しました。非包括的なリストは、次のとおりです：Felix Gers 、 Fred Cummins 、 Santiago Fernandez 、 Justin Bayer 、 Daan Wierstra 、 Julian Togelius 、 Faustian Gomez 、 Matteo Gagliolo 、 Alex Graves ↩



",True,https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca
"

PHPStorm上でPHPUnitを実施するメリットとは？
PHPで開発した関数やクラスを単体テストするためのフレームワークとして「PHPUnit」があります。
PHPUnitは、一度テストのクラスを作成しておけば、あとはテストを自動化して繰り返し実行できるようになるので、人力で動作検証するよりも実装ミスやエラーの発見がしやすくなります。
単体テストフレームワークの選択肢として、以前はSimpleTestが主流でしたがバージョンアップが遅かったり、実行速度も遅かったりして、PHPUnitへの乗り換えが進んでいます。
PHPUnitはテストフレームワークの中では導入コストが低いので、PHPアプリのテスト自動化を検討している人はまずはPHPUnitの導入をおすすめします。
そして、PHPStorm + Vagrant環境で開発している人は、わざわざVagrantゲストマシンにログインせずともPHPUnitをリモート実行することができるので、単体テストをさらに効率化できます。ぜひ、環境を作っておきましょう。

PHPStorm上でPHPUnit単体テストの実施例

利用するPHPStormの機能
PHPStorm + Vagranatでテスト自動化を行うには、以下2つの機能を利用します。

PHPStorm 7から導入された「Vagrant Support」（Vagrant操作をGUIから行う機能）
PhpStorm 8から導入された「Remote PHP interpreters」（リモートマシンのPHPアプリをSSH経由で実行する機能）

PHPStorm 10では、バージョン8の頃から設定手順が変わっているので、最新の設定手順を紹介したいと思います。

実施環境
■ホストマシン

Macbook Pro (Retina, 15-inch, Mid 2014) 
Mac OS X 10.11 El Capitan
PHPStorm 10.0.4
VirtualBox 5.0.20
Vagrant 1.7.4
Ruby 2.2.5p319 (2016-04-26 revision 54774) 

■Vagrantゲストマシン

CentOS 6.7  64bit
GuestAdditions 5.0.17
PHP 7.0.3

今回の手順では Vagrantユーザーを対象にしているため、VirtualBox + Vagrantのセットアップ手順は省略しています。
なのでもしVagrant環境がない人は、あらかじめホストマシン上に brew cask などでパッケージをインストールの上、以降の手順を進めてください。

はじめに PHPStorm の Vagrant Support を設定する
PHPStormの環境設定から Tools ＞ Vagrant を開きます。

それぞれの項目を入力します。
Vagrant executable:
/usr/local/bin/vagrant

instance folder:
/Users/[ユーザー名]/Documents/website_production/testsite.jp/vagrant

※自身のvagrantホームディレクトリを指定してください
Provider:
virtualbox

項目を3つとも入力したら［OK］ボタンをクリックします。

GUIからVagrantを起動してSSH接続をする
Vagrantゲストマシンを起動するには、メニュー ＞ Tools ＞ Vagrant ＞ up を選択します。

そうするとVagrantコンソールが開いてvagrant up が実行されます。

vagrant upが完了したのが確認できたら、VagrantゲストマシンにSSH接続をします。
メニュー ＞ Tools ＞ Start SSH session... を選択します。

SSH接続できた場合はターミナルが表示されます。

以上で、Vagrant Support機能は正しく設定できていることが確認できました。
以降は、Vagrantゲストマシンを起動したまま、PHPUnitのセットアップを進めていきます。

Vagrantゲストマシン上でPHPUnitをセットアップする
composerを使ってPHPUnitをインストールするので、もしまだ composer の実行環境がなければ先にインストールします。
$ sudo su -
$ cd ~/
$ curl -sS https://getcomposer.org/installer | php
$ mv composer.phar /usr/local/bin/composer
$ composer -v

それでは、PHPUnitをセットアップしていきます。
PHPUnitを使って自動テストをしたいPHPアプリのドキュメントルートに移動します。

composerパッケージの定義ファイル「composer.json」を設置する
composer.jsonを作成します。
$ cd /path/to/document-root
$ vim composer.json

composer.jsonの中身は下を記述します。
{
  ""require-dev"": {
    ""phpunit/phpunit"": ""5.3.*""
  }
}

phpunitのバージョン指定を ""5.3.*"" としましたが、最新の安定版バージョンはPHPUnit公式サイトで確認できます。

phpunitをインストールする
composer.jsonに記述したphpunitをインストールします。
$ composer install --dev

インストールされたphpunitは、<カレントディレクトリ>/vendor/binに配置されます。
phpunitコマンドが実行できることを確認します。
$ ./vendor/bin/phpunit --version

もし ./vendor/bin がなければ代わりに以下のパスにインストールされています。
$ ./vendor/phpunit/phpunit/phpunit --version


PHPUnitテスト用ディレクトリを作成する
テスト用ディレクトリを作成します。
利用しているフレームワークやCMSごとにテストディレクトリのパスが決まっていることがありますが、ここではドキュメントルート直下にtestsという名前で作成します。
$ mkdir tests

ブートストラップとなるファイルを設置します。
ちなみにブートストラップとは、テスト時に参照するフレームワークやCMSなどのAPIやクラスを読み込むためのものです。
$ vim tests/bootstrap.inc

たとえば、Drupal7用のモジュールをテストする場合、ブートストラップはこのようなコードになります。
<?php
/**
 * PHPUnit custom bootstrap for Drupal7
 * PHP version 7.x
 */
$_SERVER['HTTP_HOST']       = 'default';
$_SERVER['REMOTE_ADDR']     = '127.0.0.1';
$_SERVER['SERVER_SOFTWARE'] = null;
$_SERVER['REQUEST_METHOD']  = 'GET';
$_SERVER['QUERY_STRING']    = '';
$_SERVER['PHP_SELF']        = $_SERVER['REQUEST_URI'] = '/';
$_SERVER['HTTP_USER_AGENT'] = 'console';
define('DRUPAL_ROOT', '/path/to/document-root');
require_once(DRUPAL_ROOT . DIRECTORY_SEPARATOR . 'includes' . DIRECTORY_SEPARATOR . 'bootstrap.inc');
drupal_bootstrap(DRUPAL_BOOTSTRAP_FULL);

WordPressのプラグインをテストする場合は、このようなコードになります。
<?php
/**
 * PHPUnit custom bootstrap for WordPress
 * PHP version 7.x
 */
global $wp, $wp_query, $wp_the_query, $wp_rewrite, $wp_did_header,$current_user;
define('WP_USE_THEMES', false);
require_once ('/path/to/document-root/wp-blog-header.php');
require_once('/path/to/document-root/wp-load.php');
wp();
require_once( ABSPATH . WPINC . '/template-loader.php' );
require_once(ABSPATH . 'wp-admin/includes/admin.php');


PHPUnit設定ファイル「phpunit.xml」を設置する
ブートストラップの設置が終わったら、PHPUnitの設定ファイルを設置します。
設置場所は、ドキュメントルート直下に phpunit.xml というファイル名です。
$ vim phpunit.xml

phpunit.xmlの記述内容は下のとおりです。
<phpunit
  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:noNamespaceSchemaLocation=""http://schema.phpunit.de/5.3/phpunit.xsd""
  bootstrap=""tests/bootstrap.inc""
>
</phpunit>

bootstrap= のところには先ほど設置した tests/bootstrap.inc を指定します。
上の設定はもっとも簡潔な内容です。phpunit.xmlの詳しい書き方はPHPUnit公式マニュアルにまとまっています。
以上で、Vagrantゲストマシン上でのPHPUnitのセットアップは完了です。
ここでいったんVagrantゲストマシンをシャットダウンします。
シャットダウンは、PHPStormのメニュー ＞ Tools ＞ Vagrant ＞ Halt を選択すればOKです。


PHPStorm上でPHPUnitの環境設定をする
PHPStormからVagrantゲストマシンのPHPUnitをリモート実行できるように設定していきます。

Vagrantゲストマシン上のPHPを実行可能にする
PHPStormの環境設定 > Languages & Frameworks > PHP を開きます。
PHPの環境設定画面を開いたら、まずは「Interpreter:」の横の設定ボタン[･･･]をクリックします。

Interpreters画面を開いたら、左ペインから ”Remote PHP 7” を選択します。

右ペインの「Remote」にて、”Vagrant”にチェックを入れて、「Vagrant Instance Folder:」は、Vagrantのインスタンスフォルダのパスを指定します。
（ドキュメントルートのパスではないのでご注意ください）
「General」のところには自動的に値がセットされるので特に編集しません。
以上、編集が終わったら ［OK］ボタンをクリックします。
Interpreters画面を閉じてPHPの環境設定画面に戻ってきたら、カレントプロジェクトのPHPバージョンを設定します。

「PHP Language Level:」を設定します。
設定値は、VagrantゲストマシンのPHPと同じPHPバージョンにすればよいので、Vagrantゲストマシンの PHP 7.0.3 に合わせて 7 を指定します。
次に「Interpreter:」を指定します。
設定ボタンをクリックすると、PHPの選択画面が表示されます。
設定が終わったら、[OK]ボタンをクリックします。

Vagrantゲストマシン上のPHPUnitを実行可能にする
PHPStormの環境設定 > Languages & Frameworks > PHP > PHPUnit を開きます。
左ペインにデフォルトで”Local(current project)”が登録されていますが、そこはとくに触りません。

プラスボタン[+]をクリックして「PHPUnit By Remote Interpreter」を選択します。

Interpreterから、該当する Remote PHP 7 を選択して[OK]ボタンをクリックします。

右ペインに、新しいPHPUnitプロジェクト ”Remote PHP 7” が追加されます。

それぞれの項目を編集していきます。
PHPUnit library

Path to phpunit.phar: /path/to/document-root/vendor/bin/phpunit

Test Runner

Default configuration file: /path/to/document-root/phpunit.xml
Default bootstrap file: /path/to/document-root/tests/bootstrap.inc

編集が終わったら[OK]ボタンをクリックします。
以上により、PHPUnitをリモート実行する環境が整いました。

PHPUnitのテストファイルを設置する
とりあえず、動作検証のために適当なテストファイルを設置します。
テストファイルは以下の内容にしました。
<?php

class ExampleTest extends PHPUnit_Framework_TestCase {
  /**
   * Tests something to demonstrate that PHPUnit and Drupal are loaded
   */
  public function testSomething() {
    // check something simple
    $this->assertEquals(2, 2);
    // check something that depends on Drupal's libraries
    $this->assertEquals(drupal_strtolower('ASDF'), 'asdf');

  }

}

PHPUnitでテストクラスを設計する場合は、基本ルールとして PHPUnit_Framework_TestCase クラスを継承します。
その中に、pubulicメソッドを作成するのですが、クラス名は test*** という風にtestから始まる名前にします。
この命名ルールで作成されたpubulicメソッドは、PHPUnitを実行したときに自動実行されます。
その中にPHPUnitで利用可能なAssertメソッドを用いてテストコードを記述してきます。
詳しい設計方法は、公式ドキュメントが参考になります。

PHPUnit マニュアル – 第2章 PHPUnit 用のテストの書き方
PHPUnit マニュアル – 付録A アサーション


PHPUnitのコンフィギュレーションを設定する
PHPUnitのユニットテストで使用されるコンフィギュレーションを設定していきましょう。
PHPStormのメニュー > Run > Edit Configurations をクリックします。

プラスアイコン[+]をクリックして、新規コンフィギュレーションを作成します。

「Add New Configuration」ダイアログから「PHPUnit」をクリックします。

それではコンフィギュレーションを編集していきます。

「Name:」には、任意のテスト名を入力します。
Test Runner
「Test Scope:」は、4つのスコープから選ぶことになります。 「Directory」にチェックを入れた場合は、testsディレクトリ配下の全てのテストファイルをまとめて実行できるので、ひとまずこれを選んでおきましょう。 
「Directory:」には、ホストマシン上でのtestsディレクトリパスを入力します。
Command Line
「Interpreter options:」では、phpunitコマンドのパラメーターを指定可能ですがひとまず空白のままで大丈夫です。
「Custom working directory:」は、どこのディレクトリ上でphpunitコマンドを実行するのかを指定します。
空白のままならば、testsディレクトリ上でphpunitコマンド実行することになります。
僕の場合はドキュメントルート直下でphpunitコマンドが実行されるようにしたいので、ホストマシン上でのドキュメントルートのパスを入力しました。
編集が終わったら、[OK]ボタンをクリックして設定を保存します。
あとはテストを実施するのみです。
PHPStormのメニュー > Run > Run... をクリックします。

Runダイアログから、先ほど作ったコンフィギュレーションをクリックします。

PHPUnitのユニットテストが実施されます。

実施結果は画面下に表示されます。
以上、これで全ての手順は完了です。
",True,https://qiita.com/J_Sugar__/items/624c2c7826e18f6960c4
"Djangoでテストを書いて、テストレポート・カバレッジレポートを出力し、Jenkinsにテスト結果を出力するまでのメモ
以下の記事を参考にさせてもらいました。
Django でのテスト

前提
BitbacketとJenkinsを連携済み。
ローカル環境からBitbacketへpushすると、自動でJenkinsのビルドが走り、テストが実行されるようになっている状態。
これらの環境構築については以下のブログを参照のこと。
Bitbucketのprivate repositoryとJenkinsの連携について

Djangoでのテスト環境の構築

モジュールのインストール
pipでdjango-noseとcoverageをインストール。
以前、unittest-xml-reportingを使って同じような環境を作ったことがあったけど、django-noseの方が色々すんなりいった気がする。
$ pip install django-nose coverage


テスト用settingsファイルの設定
テスト用のsettingsファイルにテスト用の設定を記述。
# settings_test.py

# -*- coding:utf-8 -*-
from mysite.settings import *
import os

TEST_RUNNER = 'django_nose.NoseTestSuiteRunner'
#カバレッジレポート(html)を出力するフォルダを指定
COVERAGE_REPORT_HTML_OUTPUT_DIR = '.cover'

# テスト実行時の引数の設定(noseの引数)
NOSE_ARGS = [
    '--with-xunit',
    '--with-coverage',
    '--cover-xml',
    '--cover-html',
    '--cover-package=app', #テストを行うapp名を指定
]

#テスト用のB環境の設定(settings.pyのDB環境と分けている)
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'test.db'),
    }
}



テスト実行
テストコードをapp配下に作成(app/tests.py)して、以下のようにsettings_testを設定ファイルに指定してテストを実行する。
$ python manage.py test app --settings=mysite.settings_test

mysite/coverにカバレッジレポート(html)が出力され、mysite直下にnosetests.xml、coverage.xmlが出力されていれば成功。
これでJenkinsでCIする準備完了。

Jenkinsでテストレポートを表示
対象プロジェクトの設定画面にて、テストレポートの出力設定を行う。

テストを実行するコマンドを記述
ビルドのシェルの実行を選択し、シェルスクリプトテキストエリアに以下のようなテストを実行するコマンドを記述する。
Jenkinsのビルドが実行された際に、記述したコマンドが実行される。
# virtualenvを実行(あらかじめ任意の場所に環境を構築しておく)
source /var/www/django/mysite/venv/bin/activate
# テストを実行
python manage.py test app --settings=mysite.settings_test


テストレポート(xmlファイル)の指定
ビルド後の処理のCobertura XMLレポートパターン、JUnit結果の集計にそれぞれ出力されたxmlファイル名を入力する。
今回はmysite直下にxmlを出力しているので、そのままファイル名を入力すればよい。

以上の設定を行うことにより、ビルド実行時にJenkinsにテストレポートが表示されるようになった。

",True,https://qiita.com/yuizho/items/0bb72ea4dddacf34c456
"

はじめに
Railsの キャッシュ機能について調べてみました。 基本、Rails5.2の情報を元にしています。
フラグメントキャッシュ中心です。

キャッシュの種類
Rails ガイドによれば、の 以下の6つのキャッシュの方法がある模様。

ページキャッシュ
アクションキャッシュ
フラグメントキャッシュ
ロシアンドール・キャッシュ
低レベル・キャッシュ
SQLキャッシュ


キャッシュ情報の格納先
Rails ガイドによれば、自分で格納先を作成することもできるみたいですが、Railsに既に組み込まれているものがいくつかあります。

メモリにストアする。 development環境でキャッシュ機能を有効にした場合、デフォルトで使われるようになってます。
ファイルにストアする。キャッシュ情報の格納先を指定しなかった場合に使われます。
memcached にストアする。 production 環境で最も良く使われる方法らしい。memcached を使う場合は、 dalli を使うことをRailsは想定しているみたいです。
キャッシュしない。 キャッシュ情報の格納先として :null_store ( ActiveSupport::Cache::NullStore )  を指定することにより、development環境や test環境で動作を確認する時に、 キャッシュさせないようにすることができます。


development 環境でキャッシュを使ってみる
今回は、memcached を使って、フラグメントキャッシュをやってみます。
memcached はあらかじめインストールして、使えるようにしておきます。（今回は、ローカル環境にインストールしている想定で話を進めます。）

dalli gem を追加
Gemfile に以下の１行を追加。

Gemfile
gem 'dalli'


bundle install を実行します。
$ bundle install


development 環境でキャッシュを有効にする
development 環境で、キャッシュを有効にするには、 rails コマンドを使います。
$ bin/rails dev:cache
Development mode is now being cached.

ちなみにキャッシュを無効にするには、同じコマンドを再度実行します。
$ bin/rails dev:cache
Development mode is no longer being cached.

なお、 bin/rails s でサーバーを起動した状態で、 bin/rails dev:cache を実行すると、サーバーが勝手に再起動してくれます。便利。

memcached を使うように設定を変更する
development 環境では、デフォルトでメモリにストアするようになってます。
config/environments/development.rb を修正して、memcached を使うようにします。

config/environments/development.rb
config.cache_store = :mem_cache_store



キャッシュの情報をログに出力するようにする
config/environments/development.rb に１行追加します

config/environments/development.rb
config.action_controller.perform_caching = true
config.action_controller.enable_fragment_cache_logging = true # この行を追加



今回の model
今回の model です。
Book モデルと Author モデルがあって、 Author モデルは複数の Bookモデルを持っています。

app/models/book.rb
class Book < ApplicationRecord
  belongs_to :author

  scope :recently, -> { order(updated_at: :desc) }
end



app/models/author.rb
class Author < ApplicationRecord
  has_many :books

  scope :recently, -> { order(updated_at: :desc) }
end



今回の controller
N+1問題がというツッコミは却下します。キャッシュを有効にするとどれだけ速くなるのか、その差が見たいため、N+1問題があるのは意図的です。

app/controllers/books_controller.rb
class BooksController < ApplicationController

  def index
    @books = Book.all.recently
  end



book の一覧ページでキャッシュする
bookの一覧ページは、 Book#title とそれに紐づく Author#name を表示してます。
Rails Guide では、フラグメントキャッシュのキーとして Model のインスタンスを使う例が説明されていますが、Modelのインスタンスではないものも指定できます。（ただし、キーに何を指定するかはちゃんと考える必要があります。）
良くありがちな以下のような一覧ページ（の一部）を cache するには <% cache(cache_key) %>
と <% end %> を追加します。

app/views/books/index.html.erb
<table>
  <thead>
    <tr>
      <th>Title</th>
      <th>Author</th>
      <th colspan=""3""></th>
    </tr>
  </thead>

  <tbody>
    <% @books.each do |book| %>
      <tr>
        <td><%= book.title %></td>
        <td><%= book.author.name %></td>
        <td><%= link_to 'Show', book %></td>
        <td><%= link_to 'Edit', edit_book_path(book) %></td>
        <td><%= link_to 'Destroy', book, method: :delete, data: { confirm: 'Are you sure?' } %></td>
      </tr>
    <% end %>
  </tbody>
</table>


cache のキーとして book_index を指定してみます。(これはダメダメなキーです。理由は後述。）

app/views/books/index.html.erb
<% cache('book_index') %> ← これを追加する
<table>
  <thead>
    <tr>
      <th>Title</th>
      <th>Author</th>
      <th colspan=""3""></th>
    </tr>
  </thead>

  <tbody>
    <% @books.each do |book| %>
      <tr>
        <td><%= book.title %></td>
        <td><%= book.author.name %></td>
        <td><%= link_to 'Show', book %></td>
        <td><%= link_to 'Edit', edit_book_path(book) %></td>
        <td><%= link_to 'Destroy', book, method: :delete, data: { confirm: 'Are you sure?' } %></td>
      </tr>
    <% end %>
  </tbody>
</table>
<% end %> ← これを追加する



railsを実行して、ブラウザからアクセス
railsを起動して、ブラウザから /books ページにアクセスします。
１回目にアクセスした時は、キャッシュには何もないので、memcached にViewの一部（cach(...) ... endで囲まれた中身）を書き込みます。
$ bin/rails s
Started GET ""/books"" for ...
....
Write fragment views/books/index:403214db429cc3cf8941bbae328c8e95/book_index (2.9ms)
  Rendered books/index.html.erb within layouts/application (791.9ms)
Completed 200 OK in 814ms (Views: 789.4ms | ActiveRecord: 17.9ms)

ブラウザでリロードして、２回目にアクセスした時は、１回目の表示でmemcachedに書き込んだキャッシュを読んでいることがわかります。
Started GET ""/books"" for...
Read fragment views/books/index:403214db429cc3cf8941bbae328c8e95/book_index (2.5ms)
  Rendered books/index.html.erb within layouts/application (6.3ms)
Completed 200 OK in 45ms (Views: 35.4ms | ActiveRecord: 2.9ms)

１回目が、814ms なのに対し、２回目は、45ms と明らかに速くなってます。
ここで、403214db429cc3cf8941bbae328c8e95 はキャッシュ対象のフラグメントから計算された hash digest です。
views/books/index.html.erb のキャッシュ対象の部分を修正すると、hash_digest も変わるので、キャッシュのキーが変わります。したがって、キャッシュはヒットせず、Viewの修正内容が反映されます。

キャッシュされている内容を確認する
rails コンソールを開いてキャッシュの内容を確認することができます。
$ bin/rails c
irb(:main):001:0> Rails.cache.fetch('views/books/index:403214db429cc3cf8941bbae328c8e95/book_index')
 .... # (HTMLの一部がだーっと表示される）


キャッシュの有効期限を指定する
キャッシュの有効期限を設定するには、 :expires_in を指定します。
<% cache('book_index', expires_in: 10.seconds) %>

キャッシュが書き込まれた直後に、ブラウザでリロードした場合はキャッシュがきいて素早く表示されます。10秒以上経過してからリロードするとキャッシュのフラグメントが消えているため、表示するのに初回と同じくらい時間がかかります。

book_index がダメダメな理由
アプリの仕様にもよりますが、book_index はキャッシュのキーとしてはダメダメです。
キャッシュが有効だと、Bookのデータを追加、更新、削除された時に、その内容が即座に一覧画面に反映されません。
普通はデータが変わったらその内容が即座に一覧にも反映されるのが望ましい動作と考えられます。（スピード優先でタイムラグがあっても良い場合もあるでしょうが...）

ダメダメでないキャッシュのキーを考える
まずは、@books をそのままキャッシュのキーに指定します。
キャッシュが効いてないとき
Write fragment views/books/index:75ff4d7911aeebfbe80e0f816333f57a/books/query-6167f02f4ae8188b5665cebde53451bd-1001-20180519063757574979 (8.8ms)
  Rendered books/index.html.erb within layouts/application (984.1ms)
Completed 200 OK in 1009ms (Views: 980.5ms | ActiveRecord: 21.9ms)

キャッシュが効いているとき
  Rendering books/index.html.erb within layouts/application
   (0.5ms)  SELECT COUNT(*) AS ""size"", MAX(""books"".""updated_at"") AS timestamp FROM ""books""
  ↳ app/views/books/index.html.erb:6
  Book Load (4.0ms)  SELECT ""books"".* FROM ""books"" ORDER BY ""books"".""updated_at"" DESC # これが気になる
  ↳ app/views/books/index.html.erb:6
Read fragment views/books/index:75ff4d7911aeebfbe80e0f816333f57a/books/query-6167f02f4ae8188b5665cebde53451bd-1001-20180519063757574979 (74.1ms)
  Rendered books/index.html.erb within layouts/application (76.5ms)
Completed 200 OK in 106ms (Views: 91.5ms | ActiveRecord: 7.1ms)

キャッシュのキーが変わっていることに注意
これなら、Bookのデータに何らかの変更があった場合、Viewの情報が最新の状態に保たれます。
とはいえ、SELECT ""books"".* FROM ""books"" ORDER BY ""books"".""updated_at"" がSQLを発行しているのかどうか微妙に気になります。
また、Bookのデータに紐づく、Authorのデータのnameが変更された場合に、Viewの Author#name が最新になりません。

キャッシュキーを改善する
という訳で、キャッシュキーを controller で生成します。
最後に更新されたBookのデータと最後に更新されたAuthorのデータをキャッシュキーにしてみます。
（キャッシュキーに配列を指定することもできます）

app/controllers/books_controller.rb
  def index
    @flagment_cache_key = [Book.recently.limit(1).first, Author.recently.limit(1).first]

    @books = Book.all.recently
  end



app/views/books/index.html
<% cache(@flagment_cache_key) do %>


キャッシュが効いていないとき
Write fragment views/books/index:a5e83f8f6f07fcdcbce16d7161ae4dd4/books/1001-20180519063757574979/authors/2-20180519065250666509 (2.6ms)
  Rendered books/index.html.erb within layouts/application (798.0ms)
Completed 200 OK in 823ms (Views: 795.4ms | ActiveRecord: 19.7ms)

キャッシュが効いているとき
Processing by BooksController#index as HTML
  Book Load (1.7ms)  SELECT  ""books"".* FROM ""books"" ORDER BY ""books"".""updated_at"" DESC LIMIT ?  [[""LIMIT"", 1]]
  ↳ app/controllers/books_controller.rb:7
  Author Load (0.5ms)  SELECT  ""authors"".* FROM ""authors"" ORDER BY ""authors"".""updated_at"" DESC LIMIT ?  [[""LIMIT"", 1]]
  ↳ app/controllers/books_controller.rb:7
  Rendering books/index.html.erb within layouts/application
Read fragment views/books/index:a5e83f8f6f07fcdcbce16d7161ae4dd4/books/1001-20180519063757574979/authors/2-20180519065250666509 (2.6ms)
  Rendered books/index.html.erb within layouts/application (5.7ms)
Completed 200 OK in 40ms (Views: 30.6ms | ActiveRecord: 2.2ms)

これで、うまくいくようになりました。

ActiveRecordオブジェクトをキャッシュキーに指定したとき
ちなみに、ActiveRecordオブジェクトをキャッシュのキー（の一部）に指定した場合は cache_key_with_version メソッドが使われるのではないかと思われます。
bin/rails c
irb(main):001:0> book = Book.recently.limit(1).first # controller でキャッシュキーに指定したBookオブジェクト
  Book Load (1.0ms)  SELECT  ""books"".* FROM ""books"" ORDER BY ""books"".""updated_at"" DESC LIMIT ?  [[""LIMIT"", 1]]
=> #<Book id: 1001, title: ""test"", author_id: 9, created_at: ""2018-05-12 03:15:20"", updated_at: ""2018-05-19 06:37:57"">
irb(main):002:0> book.cache_key_with_version  # Bookオブジェクトからキャッシュキー生成
=> ""books/1001-20180519063757574979""

実戦では、改ページ処理があったり、検索条件があったり、一般ユーザーと管理ユーザーで表示できるデータに違いがあったりとか色々複雑なこともあるので、それらを考慮したキャッシュキーを決定する必要がありそうです。

まとめ

フラグメントキャッシュを使えば表示が速くなります。
逆にフラグメントキャッシュを使うと、最新のDBの内容が反映されないなど、ちゃんと考えずに使うとバグでないのにバグと勘違いしそうです。
キャッシュの対象範囲を考慮した方が良いです。
キャッシュのキーと有効期限に何を設定するかしっかり考えた方が良いです。
安易にキャッシュに頼るよりは、DBのインデックスやQueryの書き方など、他に改善の余地がないか検討するのが先。
SPAでは使えないので、やるなら低レベルキャッシュ( Rails.cache.fetch を使う）になりそう。（これは機会があれば別途）


調査できていないところ
今回は、色々試してみるだけで、Railsのソースを追いかけていないため、以下のあたりが良くわかってません。

rspec の feature を実行する時には、ファイルキャッシュが有効になるのか？
キャッシュキーにオブジェクトを指定したときのキャッシュキーへの変換方法。


参考資料
Google で検索するといろんなサイトがヒットしますが、情報が古かったりします。結局のところ、最も役に立ったのは、本家本元のRails Guideの以下のサイトです。

http://guides.rubyonrails.org/caching_with_rails.html

APIの説明も役に立ちました。

http://api.rubyonrails.org/classes/ActionView/Helpers/CacheHelper.html#method-i-cache
http://api.rubyonrails.org/classes/ActiveSupport/Cache/Store.html#method-i-fetch

最近、公開された以下のサイトもかなり参考になりました。

https://medium.com/rubyinside/https-medium-com-wintermeyer-caching-in-ruby-on-rails-5-2-d72e1ddf848c

",True,https://qiita.com/suketa/items/eeae7e2196520323f694
"

はじめに
こちらがEvan Youさんが9月30日に書いた原文です。
https://medium.com/the-vue-point/plans-for-the-next-iteration-of-vue-js-777ffea6fabf
この中から一般的な利用ユーザーにとっては把握しておかなければならない重要な変更点、破壊的かもしれない公開APIの変更点のみを抜粋します。
編集リクエストどんどん受け付けています！

TL;DR
render関数とスコープ付きslotの構文以外は、互換性ビルドを介して同じままにするか、2.x互換にすることができます。

Everything except render function API and scoped-slots syntax will either remain the same or can be made 2.x compatible via a compatibility build.

なるほど、全て移行できれば移行したいですが、もし難しい部分があればそれだけ2.xの時の互換性を持たせたまま3系を使うことができるんですね！なんて優しいんでしょう😊

前置き
新しいメジャーバージョンなので、いくつかの大きな変更があります。
しかし我々は下位互換性を真剣に受け止めているので、できるだけ早くこれらの変更を伝えます。
現在計画されている公開APIの変更点は次のとおりです。

Since it’s a new major, there is going to be some breaking changes. However, we take backwards compatibility seriously, so we want to start communicating these changes as soon as possible. Here’s the currently planned public API changes:


1. テンプレートの構文
テンプレートの構文は同じ99％のままです。
スコープ付きスロットのシンタックスには微妙な微調整があるかもしれませんが、それ以外にテンプレートの変更は計画されていません。

Template syntax will remain 99% the same. There may be small tweaks in scoped slots syntax, but other than that we have no plans to change anything else for templates.


2. #TypeScript対応 #babel変換を必要としない、ネイティブでclass-based componentsを対応
3.0は、babelのtranspilationやstage-x機能を必要とせずにネイティブES2015で快適に使用できるAPIを提供することを目的として、class-based componentsをネイティブにサポートします。
現在のほとんどのoptionsは、class-basedのAPIで妥当なマッピングを持ちます。
class fieldsやdecoratorsなどのstage-xの機能は、DXを向上させるためにオプションとして使用できます。
さらに、APIはTypeScript型推論を考慮して設計されています。
3.xのコードはTypeScriptで記述され、改良されたTypeScriptサポートを提供します。（つまり、アプリケーションでのTypeScriptの使用は完全にオプションです。）

3.0 will support class-based components natively, with the aim to provide an API that is pleasant to use in native ES2015 without requiring any transpilation or stage-x features. Most current options will have a reasonable mapping in the class-based API. Stage-x features such as class fields and decorators can still be used optionally to enhance the authoring experience. In addition, the API is designed with TypeScript type inference in mind. The 3.x codebase will itself be written in TypeScript, and providing improved TypeScript support. (That said, usage of TypeScript in an application is still entirely optional.)


3. 2.xオブジェクトベースのコンポーネントとの互換性
2.xオブジェクトベースのコンポーネントは、内部的にオブジェクトに対応するclassへ変換することによって引き続きサポートされます。z

The 2.x object-based component format will still be supported by internally transforming the object to a corresponding class.


4. Mixinは引き続きサポートされます
Mixinは引き続きサポートされます

Mixins will still be supported.*


5. トップレベルのAPIについて
これの翻訳難しかった。。。
Vuexとかを新たなVueインスタンスへinstallした場合、Vueの子コンポーネントとかが生成される際、その子インスタンスにだけ$storeを生やしてstoreのグローバル汚染とかを避けてます。
そのコードがVuexのinstall関数内に見受けられますが、その辺の事を言ってる気がします。

Top level APIs will likely receive an overhaul to avoid globally mutating the Vue runtime when installing plugins. Instead, plugins will be applied and scoped to a component tree. This will make it easier to test components that rely on specific plugins, and also make it possible to mount multiple Vue applications on the same page with different plugins, but using the same Vue runtime.*


6. 非同期コンポーネント
Functional componentsは最終的にはplain functionsになりますが、非同期コンポーネントはヘルパー機能を使って明示的に作成する必要があります。

Functional components can finally be plain functions —however, async components will now need to be explicitly created via a helper function.


7. jsx使わないでrender関数を書いてる人は要注意
ほとんどの変更を受け取る部分は、render関数で使用される仮想DOM形式です。 
現在、主要なライブラリの製作者からのフィードバックを集めており、変更にもっと自信が持てるように詳細を共有しますが、アプリで手書きの（非JSX）render機能に大きく依存しない限り、 合理的に簡単なプロセスでなければなりません。

The part that will receive the most changes is the Virtual DOM format used in render functions. We are currently collecting feedback from major library authors and will be sharing more details as we are more confident of the changes, but as long as you don’t heavily rely on hand-written (non-JSX) render functions in your app, upgrading should be a reasonably straightforward process.


最後に
Vue.js3.0めっちゃ楽しみですね！
個人的にはclass-based componentが大好きで普段から使ってるので、ネイティブ対応はめっちゃ嬉しいです！
また、h関数を自分で書いてたりするのでその部分の対応は大変そうですね。
elementuiの内部コードもh関数を割と使ってたので結構大変そうな匂いがしてますが、リリースされたら頑張ってバージョンアップして新たなパワフルな機能達を使っていきたいですね！
",True,https://qiita.com/k-okina/items/66b0ea212e7afc57631c
"このエントリは Kubernetes Advent Calendar の 2 日目の記事です。公開が遅れてすみません。

Kubernetes
最近の Kubernetes の動向は 1 日目の記事 で語られた通りです。
ついに v1.0.0 General Availability を迎え、
様々な場所で Kubernetes の話を聞くようになりました。
さて、本稿では一段階進んで、
Kubernetes が大体どんなものかは分かった、
でも実際どんな風に使われてるの？という疑念に対して、
Google の傘下である Youtube によって公開されている、
スケールする MySQL クラスタ Vitess を Kubernetes で試してみる事とします。

Vitess
Vitess は Youtube が MySQL をスケールさせるために開発した Go 製の OSS です。
公式サイトの画像ですが、次のようなアーキテクチャとなっています。

それぞれのコンポーネントは以下の様な役割を担っています。

vtctl


CLI から Vitess クラスタを操作するためのツール


vtctld


GUI から Vitess クラスタを操作するための HTTP サーバ


vtgate


アプリケーションからのクエリをルーティングする Proxy サーバ


vttablet からの結果をまとめてアプリケーションに返す




vttablet


MySQL のスループットを最適化する Proxy サーバ


コネクションプーリング
クエリ書き換え・重複除去 




Topology


稼働しているサーバに関する情報を保存する


シャーディング・スキーマ
レプリケーショングラフ


Kubernetes では etcd を使用
etcd 以外には ZooKeeper をサポート




Vitess を GKE 上で動かしてみる
さて、Vitess を Kubernetes を使って実際に動かしてみます。
ここでは、Kubernetes クラスタの構築に関しては本質では無いので、
現在最も簡単に Kubernetes クラスタを準備できる Google Container Engine (GKE) を使用します。
公式ドキュメントの Running Vitess on Kubernetes に従って作業を進めます。
まず vtctl をインストールします。
$ go get github.com/youtube/vitess/go/cmd/vtctlclient

公式だと上記の手順になっていますが、現在はもしかしたら protobuf が必要かもしれません。
go get 時にエラーが出た場合は次のコマンドを実行してから再度実行してみます。
go get -u github.com/golang/protobuf/{proto,protoc-gen-go}


GKE 環境の準備
GKE の環境を用意します。
gcloud コマンドは既に使用できる前提としておきます。
使用する方法に関しては gcloud Tool Guide 等を参考にすると良いでしょう。
今なら日本語の情報も多く見かけます。
Vitess クラスタを稼働させる対象の project をセットします。
$ gcloud config set project PROJECT

GKE クラスタを稼働させるための kubectl をアップデートしておきます。
$ gcloud components update kubectl

GKE クラスタを稼働させるリージョンを選択します。
日本から最も近いのは asia-east1 なのでそれにしておきます。
$ gcloud config set compute/zone asia-east1

実際に Kubernetes クラスタを起動します。
また、後述のバックアップ用 bucket に書き込むために、 storage-rw のスコープを付与します。
$ gcloud container clusters create example --machine-type n1-standard-4 --num-nodes 5 --scopes storage-rw

また、Vitess のバックアップを保存するための、
Google Cloud Storage (GCS) の bucket を作成しておきます。
Amazon の S3 等と同様に、bucket 名はユニークである必要があるので、
必要に応じて書き換えてください。
$ gsutil mb gs://my-backup-bucket


Vitess クラスタの初期設定
起動した Kubernetes クラスタを vitess に認識させるために初期設定を行います。
$ cd $GOPATH/src/github.com/youtube/vitess/examples/kubernetes

$ ./configure.sh

ここで BSD の sed だと -r オプションが無いとエラーが出るかもしれませんが、
スクリプトの中身を確認したところ、
gcloud コマンドからプロジェクトを自動決定するのに失敗するだけなので、
大きな問題は無さそうです。
次のように GCP の Project と先ほど作成した bucket の名前を入力します。
Backup Storage (file, gcs) [gcs]: gcs
Google Developers Console Project []: my-project-id
Google Cloud Storage bucket for Vitess backups: my-backup-bucket

なお、backup storage に file を指定した場合は、
読み書き可能なネットワークディスクを、
vttablet 並びに vtctld の pods に mount しておく必要があります。
また、GCS 以外の blob storage store へのバックアップは、
backupstorage plugin を使用して実装することが出来るみたいです。

Topology の起動
次に、Topology を担う etcd のクラスタを起動します。
$ ./etcd-up.sh

実際に起動したかどうかは次のコマンドで確認します。
$ kubectl get pods
NAME                READY     STATUS    RESTARTS   AGE
etcd-global-ab123   1/1       Running   0          7m
etcd-global-cd456   1/1       Running   0          7m
etcd-global-ef789   1/1       Running   0          7m
etcd-test-a1111     1/1       Running   0          7m
etcd-test-b2222     1/1       Running   0          7m
etcd-test-c3333     1/1       Running   0          7m


Vtctld の起動
次に vtctld を起動します。
$ ./vtctld-up.sh

同様に確認します。
$ kubectl get pods
NAME                READY     STATUS    RESTARTS   AGE
etcd-global-ab123   1/1       Running   0          7m
etcd-global-cd456   1/1       Running   0          7m
etcd-global-ef789   1/1       Running   0          7m
etcd-test-a1111     1/1       Running   0          7m
etcd-test-b2222     1/1       Running   0          7m
etcd-test-c3333     1/1       Running   0          7m
vtctld-abc12        1/1       Running   0          6m

vtctld が正しく起動しているか確認するために kube-proxy を起動します。
$ kubectl proxy --port=8001

その後、次の URL にアクセスすると次のような画面が表示されます。


http://localhost:8001/api/v1/proxy/namespaces/default/services/vtctld:web/ 



vttablet 並びに MySQL クラスタの起動
いよいよ vttablet 並びに MySQL のクラスタを起動します。
$ ./vttablet-up.sh

このコマンドで 5 つの vttablet の pods が起動します。
先ほどと同様 kubectl get pods で確認できます。
また、vtctl のラッパーである CLI ツールの kvtctl.sh でも確認できます。
$ ./kvtctl.sh ListAllTablets test

古い Mac だと kvtctl.sh が呼んでいる、
env.sh の中にある mktemp でエラーが発生するかもしれません。
その場合 27 行目付近を次のように書き換える必要があります。
     27 -  tmpfile=`mktemp`
     28 +  tmpfile=`mktemp -t tmp`

また、新しい kubectl を使っている場合は、
deprecated warning が出るかもしれませんが、とりあえず気にする必要はありません。
どうしても気になる場合は kubectl の引数 -t を、
全て --template に置き換えると上手く行きそうです。

MySQL クラスタのセットアップ
まず、新しいシャードを作成するために keyspace を作成します。
$ ./kvtctl.sh RebuildKeyspaceGraph test_keyspace

次にシャードを作成します。
最初のマスターになるノードを指定します。
他のノードは指定したマスターノードに接続しレプリケーションを開始します。
$ ./kvtctl.sh InitShardMaster -force test_keyspace/0 test-0000000100

また、この時に併せてデフォルトのデータベースも作成します。
最初に作成した keyspace が test_keyspace なので、
MySQL には vt_test_keyspace というデータベースが作成されます。
再度 ListAllTablets を実行すると、
指定したノードが Master になり、
幾つかのノードが replica もしくは rdonly になっている事が確認できます。
$ ./kvtctl.sh ListAllTablets test

rdonly のノードはバッチ処理やバックアップ処理に使用する事を想定しているようです。
また、replica に関しては、
Web トラフィックの(恐らく参照クエリ)を処理する事を想定しているようです。
中身に関して全く見てないのですがレプリケーションの種類が違ったりするんでしょうか。
気になります。

テーブルの作成
作成した MySQL クラスタにテーブルを作成してみます。
ApplySchema コマンドに -sql フラグと SQL 自体を渡し、keyspace を指定します。
$ ./kvtctl.sh ApplySchema -sql ""$(cat create_test_table.sql)"" test_keyspace

create_test_table.sql の中身は普通の CREATE TABLE 句になっています。
CREATE TABLE messages (
  page BIGINT(20) UNSIGNED,
  time_created_ns BIGINT(20) UNSIGNED,
  keyspace_id BIGINT(20) UNSIGNED,
  message VARCHAR(10000),
  PRIMARY KEY (page, time_created_ns)
) ENGINE=InnoDB

作成したスキーマは GetSchema で確認することが出来ます。
$ ./kvtctl.sh GetSchema test-0000000100

最後に、アプリケーションから利用するために vtgate を起動します。
$ ./vtgate-up.sh

また、次のようにしてバックアップを取得することも出来ます。
この時、恐らく rdonly の pod を指定するのが良いみたいです。
$ ./kvtctl.sh Backup test-0000000104

取得したバックアップは次のようにして確認することが出来ます。
$ ./kvtctl.sh ListBackups test_keyspace/0


アプリケーションからの利用 (guestbook)
Vitess を利用したアプリケーションの例として、
Python で書かれた guestbook が用意されています。
次のようにして起動します。
$ ./guestbook-up.sh

GCP の Firewall やロードバランサ等を設定する手順もありますが割愛して、
ここでは実際のアプリケーションがどのようにして Vitess に接続するのか見てみます。
アプリケーションは guestbook/ 配下に設置されています。
中身は普通の Flask アプリケーションのようです。
次のようなモジュールをインポートしています。
from vtdb import keyrange
from vtdb import keyrange_constants
from vtdb import vtgatev2
from vtdb import vtgate_cursor

アプリケーションはまず vtgate に接続します。
  # Connect to vtgate.
  conn = vtgatev2.connect({'vt': [addr]}, timeout)

次に、keyspace と接続する vttablet の種類を指定します。
その後、普通の PEP249 のような形式で SQL を実行出来ます。
  # Insert a row on the master.
  keyspace_id = get_keyspace_id(page)
  keyspace_id_int = unpack_keyspace_id(keyspace_id)
  cursor = conn.cursor('test_keyspace', 'master', keyspace_ids=[keyspace_id], writable=True)

  cursor.begin()
  cursor.execute(
      'INSERT INTO messages (page, time_created_ns, keyspace_id, message)'
      ' VALUES (%(page)s, %(time_created_ns)s, %(keyspace_id)s, %(message)s)',
      {
        'page': page,
        'time_created_ns': int(time.time() * 1e9),
        'keyspace_id': keyspace_id_int,
        'message': value,
      })
  cursor.commit()

参照の際は、 replica を指定して参照クエリを発行しています。
  cursor = conn.cursor('test_keyspace', 'replica', keyspace_ids=[keyspace_id])

  cursor.execute(
      'SELECT message FROM messages WHERE page=%(page)s ORDER BY time_created_ns',
      {'page': page})
  entries = [row[0] for row in cursor.fetchall()]

肝心の keyspace の指定ですが、次のようになっていました。
def get_keyspace_id(page):
  """"""Compute the keyspace_id for a given page number.

  In this example, the keyspace_id is the first 64 bits of the MD5 hash of
  the sharding key (page number). As a result, pages are randomly distributed
  among the range-based shards.

  The keyspace_id is returned as a packed string. Use unpack_keyspace_id() to
  get the integer value if needed.

  For more about keyspace_id, see these references:
  - http://vitess.io/overview/concepts.html#keyspace-id
  - http://vitess.io/user-guide/sharding.html
  """"""
  m = hashlib.md5()
  m.update(uint64.pack(page))
  return m.digest()[:8]

今回の場合は page の値をベースに keyspace を決定している模様です。
MD5 hash の最初の 64bit を使用しています。
Vitess を使ってパフォーマンスやスケールを意識する場合は、
この keyspace_id の設計が重要になると考えられます。
この Guestbook の例は Python ですが、
Vitess には次の言語のクライアントライブラリが存在しているようです。

Go
Python
PHP
Java

Vitess Client Libraries

まとめ
Kubernetes の使われている例として、スケールする MySQL クラスタの Vitess を紹介しました。
このようにトポロジを持ち、クラスタに対してプロキシを行ったり、
特定のコンテナをセットで管理するアプリケーションの配置先として、
Kubernetes は非常に適しているのだろうと考えられます。
また、Vitess には本稿で紹介した以外にも多数の機能があり、
大規模の MySQL クラスタを管理するにあたって必要な機能を多く持っているように見えます。
特に GKE を使えば Kubernetes を管理するのも非常に簡単なので、
GCP 上で大規模な MySQL を使用したい場合には、
CloudSQL を使うより魅力的な選択肢かもしれません。
Google 社の事だから Vitess もそのうち CloudSQL の代替もしくは改善案として、
GCP のマネージドサービスとして出てくるかもしれませんね。

参考文献

Vitess
Running Vitess on Kubernetes
Vitess Client Libraries
Scaling MySQL in the cloud with Vitess and Kubernetes
Cloud Native MySQL Sharding with Vitess and Kubernetes

",True,https://qiita.com/rrreeeyyy/items/90b3b1c73d22edc913ba
"この記事はRuby on Rails Advent Calendar 2016の19日目の記事です。

概要
いまゲームのAPIサーバとしてRailsを使っているのですが、dbを水平分割する必要があってgemを探していました。
最終的に自前でgemを作ったのですが、その調査内容と作成した経緯についてお話します。

要件

（なるべく）ノーメンテでスケールアウトしたい
ゲームでシャードを追加するような状況で一番想定されるのが、初期導入によるユーザ数の爆発的な増加が上げられます。
このときにメンテをするのはビジネス的な損失が非常に大きいので、なるべくスケールアウトはノーメンテでやりたいですね。
（なので、スケールインはメンテありでも問題ありません）

自動で振り分けるのでなくある程度自前でハンドリングしたい
ゲームではアイテムやスキル、フレンドなど「キャラクター*N個」のデータになるものがほとんどです。
そのため、「キャラクター*N個」の方のテーブルのレコード数が先に性能限界を迎えやすいのですが、なるべくキャラクターデータの保存されているシャード番号とキャラクターの関連しているデータのシャード番号は揃えておく方が都合が良いことが多いです（調査のときや、スケールインのときなど）。
そのため、自前のルールで振り分けるシャードを決定出来るというのが要件に上げられました。

調査
いろいろ調べて見つけたのが、クックパッドさんのmixed_gaugeというgemです。
詳しくはクックパッド開発者ブログのシンプルで移行しやすいデータベースシャーディングの記事を参照していただいたほうが早いでしょう。

mixed_gauge
mixed_gaugeは、

水平分割対応
レプリケーション対応
rails5対応
モンキーパッチがほとんどない
コードベースが小さい

と機能も多く、バージョンアップなども容易であるように思えたのでこれを使いたかったのですが、「シャードへの振り分けをこちらでコントロールしたい」という要件だったのでそのまま採用といきませんでした。

activerecord-sharding
もう一つ候補に上がったのが、activerecord-shardingです。
こちらは採番テーブルによるシャーディングが採用されています。
CEDEC2016のモンスターストライクを支える負荷分散手法でも紹介されており、モンストのバックエンドでも使われているため信頼性は高いでしょう。
mixed_gaugeと比べた時のメリット／デメリットとしては、

レプリケーション機能がない
振り分け方式はプラグイン形式になっており採番テーブル以外でも使えるが、対応が中途半端（モンキーパッチが必要）

といった感じですね。
ただこちらも mixed_gauge と同じくシャードへの振り分けをこちらでコントロールしたいという要件を満たせなかったため、採用にはならずでした。

activerecord-shard_for
いろいろ調べはしましたが、結局要件を満たせるgemが見つからなかったので自前で作成することにしました。
今回作成した activerecord-shard_for は（「作成した」と言うのがおこがましいぐらいに）、前者2つのgemのコードベースを 丸パクリ 流用して作成しました。
2つのgemの良いところをピックアップして、

水平分割対応
レプリケーション対応
振り分け方式をプラグイン形式に
分割方法をキー指定と範囲指定と両方を可能に

というgemになっております。
また、octopusのような using シンタックスでシャードを明示的に指定して呼ぶことも可能です。
最も特徴的なところは「振り分けをプラグイン方式にした」ところで、要件に合わせてシャードへの振り分けルールだけを実装することが出来るようになっています。
詳しくはwikiを見てもらうのが良いですが、簡単に例を挙げるとこんな感じです。
# database.yml
production_user_001:
  adapter: mysql2
  username: user_writable
  host: db-user-001
production_user_002:
  adapter: mysql2
  username: user_writable
  host: db-user-002
production_user_003:
  adapter: mysql2
  username: user_writable
  host: db-user-003
production_user_004:
  adapter: mysql2
  username: user_writable
  host: db-user-004

# router plugin
class SimpleModuloRouter < ActiveRecord::ShardFor::ConnectionRouter
  # keyには、def_distkeyに設定したカラムの値が渡される
  def route(key)
    key.to_i % connection_count
  end
end

# initializer
ActiveRecord::ShardFor.configure do |config|
  # SimpleModuloRouterを登録する
  config.register_connection_router(:modulo, SimpleModuloRouter)

  # シャーディングクラスタの設定
  config.define_cluster(:user) do |cluster|
    # unique identifier, connection name
    cluster.register(0, :production_user_001)
    cluster.register(1, :production_user_002)
    cluster.register(2, :production_user_003)
    cluster.register(3, :production_user_004)
  end
end

# AR model
class User < ActiveRecord::Base
  include ActiveRecord::ShardFor::Model
  use_cluster :user, :modulo # 登録したシャーディングクラスタとSimpleModuloRouterを使う宣言

  # idをルーティングキーとして設定
  # Routerのrouteメソッドにレコードのidが渡される
  def_distkey :id

  def self.generate_unique_id
    # Implement to generate unique id
  end

  before_put do |attributes|
    attributes[:id] = generate_unique_id unless attributes[:id]
  end
end

この例だと、 SimpleModuloRouter#route の結果が0なら :production_user_001 に、 1なら :production_user_002 に振り分けられるようになります。
Routerクラスの route メソッドをclusterに登録したshardのkeyに振り分けるようにするだけなので、要件にあわせて柔軟にシャードへの振り分けルールが実装可能になっているはずです。
DBを水平分割することがないのが一番ですが、水平分割する必要が出てきた際は選択肢の一つにどうでしょうか。
",True,https://qiita.com/wakaba260/items/5c332f5085652af8a9a9
"docker run実行時にコンテナのメモリ上限を設定できます。久しぶりに触ってみたらコンテナのメモリ設定のオプションがいくつか追加されていたのでメモ。Dockerのバージョンは1.12.1です。



オプション
説明




-m , --memory=""""
メモリの上限（書式： <数値> [<単位>] 、単位は b、k、m、g のいずれか）


--memory-swap=""""
合計メモリの上限（メモリ＋スワップ、書式： <数値> [<単位>] 、単位は b、k、m、g のいずれか）


--memory-reservation=""""
メモリのソフト・リミット（書式： <数値> [<単位>] 、単位は b、k、m、g のいずれか）


--kernel-memory=""""
カーネル・メモリの上限（書式： <数値> [<単位>] 、単位は b、k、m、g のいずれか）


--oom-kill-disable=false
コンテナを OOM killer による停止を無効化するかどうか指定


--memory-swappiness=""""
コンテナがメモリのスワップ度合いを調整。整数値の 0 ～ 100 で指定



引用:Docker run リファレンス
ここでは--memory-reservationと--oom-kill-disable=false、--memory-swappinessについて調べました。

環境

OSX 10.12
Docker for MAC(docker 1.12.1)


--memory-reservation

メモリのソフトリミットということで、ホストのメモリに余裕がある時はは制限はかからず、他に使用している時は制限がかかるということらしい。  
試してみました。
コンテナ2つ起動し、1つは--memory-reservationを512MB、もうひとつは制限なしで設定します。Linuxの負荷ツールstressで以下のシナリオでメモリに負荷をかけます。
前提：ホストのメモリは2GB
1. 両方のコンテナに1.25GBの負荷をかける
2. メモリ制限のないコンテナへの負荷を停止し、512MBのメモリ制限をしているコンテナのみに負荷をかける
1の状態ではメモリ制限したコンテナでは512MBしかメモリが使用されず、制限していないコンテナでは1.25GBのメモリを使用している状態になるはず。
また2の状態では、ホストのメモリに空きができたので、メモリ制限したコンテナでも1.25GBのメモリが使用できるはず。  
それでは試していきます。
コンテナ起動&コンテナにstressインストール
$ docker run --name mem-no-limit -dit centos:7 /bin/bash
$ docker run --name mem-limit -dit --memory-reservation 512m centos:7 /bin/bash

$ docker exec mem-no-limit curl http://ftp.tu-chemnitz.de/pub/linux/dag/redhat/el7/en/x86_64/rpmforge/RPMS/stress-1.0.2-1.el7.rf.x86_64.rpm -o stress-1.0.2-1.el7.rf.x86_64.rpm
$ docker exec mem-no-limit yum install stress-1.0.2-1.el7.rf.x86_64.rpm -y 
$ docker exec mem-limit curl http://ftp.tu-chemnitz.de/pub/linux/dag/redhat/el7/en/x86_64/rpmforge/RPMS/stress-1.0.2-1.el7.rf.x86_64.rpm -o stress-1.0.2-1.el7.rf.x86_64.rpm
$ docker exec mem-limit yum install stress-1.0.2-1.el7.rf.x86_64.rpm -y 


両方のコンテナに1.25GBの負荷をかける

$ docker exec -d mem-no-limit stress --vm 5 --vm-bytes 256M --vm-keep  
$ docker exec -d mem-limit stress --vm 5 --vm-bytes 256M --vm-keep  
$ docker stats -a

CONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O               BLOCK I/O             PIDS
7bc27e459b9d        67.66%              512.9 MiB / 1.953 GiB   25.65%              39.68 kB / 2.745 kB   4.17 GB / 839.2 MB    9
2544567cad9d        101.66%             1.312 GiB / 1.953 GiB   67.18%              41.02 kB / 2.745 kB   1.026 GB / 1.528 GB   9

おお、ちゃんと512MBで制限されてますね。  
続いてメモリ制限されてないコンテナの負荷を止めます。(コンテナの再起動してstreeプロセスをなくしてます。)
2. メモリ制限のないコンテナへの負荷を停止し、512MBのメモリ制限をしているコンテナのみに負荷をかける
$ docker restart memo-no-limit
$ docker stats -a

CONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O               BLOCK I/O             PIDS
7bc27e459b9d        196.75%             1.504 GiB / 1.953 GiB   77.02%              39.68 kB / 2.745 kB   3.934 GB / 818.1 MB   9
2544567cad9d        0.00%               420 KiB / 1.953 GiB     0.02%               41.02 kB / 2.745 kB   845.2 MB / 724.5 MB   2

メモリ制限してるコンテナでも--memory-reservationで設定した512MBを超えてメモリを使用してますね。

--oom-kill-disable

Apacheのように子プロセスが生成されるようなミドルウェアではそんなに気になりませんが、OpenLDAPのようなslapdプロセス1つで動くようなものは、--oom-kill-disable=trueを設定しないとアウト・オブ・メモリエラーが発生するとカーネルよって停止されてしまいます。--oom-kill-disable=trueを設定しておけば強制的に停止されることはなくなります。ただし、これは--memoryと一緒に設定しないと、ホストメモリを使い切ったときにシステム・プロセスを停止しなければならなくなります。  
メモリを128MBで制限し、--oom-kill-disableのあり/なしで2つのコンテナを作ります。（負荷ツールのインストールは省略してます。）
$ docker run --name oom-kill -dit --memory 128m centos:7 /bin/bash
$ docker run --name no-oom-kill -dit --memory 128m --oom-kill-disable=true centos:7 /bin/bash

まずは--oom-kill-disableなしのコンテナにメモリ上限以上のメモリ負荷をかけてみます。
$ docker exec oom-kill stress --vm 3 --vm-bytes 128M --vm-keep
stress: info: [27] dispatching hogs: 0 cpu, 0 io, 3 vm, 0 hdd
stress: FAIL: [27] (420) <-- worker 32 got signal 9
stress: WARN: [27] (422) now reaping child worker processes
stress: FAIL: [27] (456) failed run completed in 2s

プロセスがKillされました。
つぎは--oom-kill-disableのコンテナにメモリ上限以上のメモリ負荷をかけてみます。
$ docker exec no-oom-kill stress --vm 3 --vm-bytes 128M --vm-keep
stress: info: [27] dispatching hogs: 0 cpu, 0 io, 3 vm, 0 hdd

とりあえずプロセスは動き続けているようです。
$ docker stats -a
CONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O               BLOCK I/O             PIDS
fac173bff8f9        0.00%               127.7 MiB / 128 MiB   99.75%              40.2 kB / 2.745 kB    57.34 kB / 155.1 MB   5
556fae8a9b3e        0.00%               44 KiB / 128 MiB      0.03%               40.37 kB / 2.745 kB   360.4 kB / 155.2 MB   1

ただコンテナはメモリを使い切っているので、ビジー状態でなにも反応しません。
$ docker exec no-oom-kill ps aux
rpc error: code = 2 desc = containerd: container did not start before the specified timeout


--memory-swappiness

コンテナのカーネルは、アノニマス・ページ・メモリ上の何パーセントかをスワップ・アウトします。そこで--memory-swappinessを設定して、そのパーセンテージを制御できるようです。  0であれはアノニマス・ページ・メモリは無効、100であれば全ページがアノニマス・ページ・メモリ可能という設定になります。
使い所はそんなに多くなさそうですが、パフォーマンスを優先して-memory-swappiness=0することはありそうです。
$ docker run -itd -memory-swappiness=0 centos:7 /bin/bash


おわり
より柔軟にリソースを制限できるようになってますね。これらを駆使し、リソースをよりケチって高使用率でDockerを運用したいものです。
",True,https://qiita.com/irotoris/items/944aba5e448a8e723ff6
"

はじめに
この記事では、Docker環境でしかおこらない厄介な問題の原因を突き止めるためにGDBを利用する際のヒントを解説します。これにより、怪しい部分を絞り込んでいくための取っ掛かりを作ることができます。

前提となる知識・スキル
知識・スキルとして以下をおおよその前提としています。

C言語およびGo言語によるプログラミングができる。
Dockerの基本操作ができる。
Linux(UNIX)上でのGDBによる実行の追跡ができる。


なぜDockerでGDBか
Docker環境であるプログラム(Aとする)を実行してトラブルが起こった場合、

Aにもともと問題があった
Dockerに問題がある
DockerとAの組み合わせに問題がある

のどれなのかを見極めて解決する必要がありますが、この切り分けはかならずしも簡単ではありません。そこできわめて強力な道具として登場するのがGDB(The GNU Project Debugger)です。GDBを使えばプログラムAだけでなくdockerの実行も同時に追跡するなどして、問題の源をすみやかに突き止めることが可能です。

追跡のステップ
ここでは、Docker環境でのGDBによる追跡の方法を以下のようなステップにわけて解説します。

Docker環境を用意する
Docker環境で動作するプログラムを途中から追跡する
デバッグ用のDockerを用意する
Dockerの動作モードについて
Docker clientを追跡する
Docker daemonを追跡する
Docker initを追跡する
Docker環境で動作するプログラムを最初から追跡する


環境
使用した環境は以下のとおりです。この記事の内容は他の環境の場合でもわずかな修正で適用可能です。

Ubuntu 14.04.3 amd64(x64, x86-64)版
Docker 1.9.1

GDBはUbuntuに標準インストールされている7.7.1を使用しています。
DockerはGo言語で記述されていますが、GDBによる実行の追跡が可能です。詳細についてはGo言語公式ページの解説をごらんください。

Docker環境を用意する
まず、デスクトップ用のamd64(x64, x86-64)版Ubuntu 14.04.3の環境を用意します。
この環境にDockerをインストールします。手順はDockerの公式ドキュメントに従いますが、冗長な部分(aptパッケージ索引の更新を連続して実行、など)は省略しています。内容はたびたび変更されるようですので最新のものを参照してください。

準備

gpgキーの追加
root@ubuntu:~# apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
Executing: gpg --ignore-time-conflict --no-options --no-default-keyring --homedir /tmp/tmp.lS9KM2wwRu --no-auto-check-trustdb --trust-model always --keyring /etc/apt/trusted.gpg --primary-keyring /etc/apt/trusted.gpg --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
gpg: requesting key 2C52609D from hkp server p80.pool.sks-keyservers.net
gpg: key 2C52609D: public key ""Docker Release Tool (releasedocker) <docker@docker.com>"" imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)
root@ubuntu:~#


aptソースの更新
ログイン後、Terminalを開いてスーパーユーザーになります。
user@ubuntu:~$ sudo bash
[sudo] password for user: xxxxxxx
root@ubuntu:~# 

/etc/apt/sources.list.d/docker.listを作成します。
root@ubuntu:~# echo ""deb https://apt.dockerproject.org/repo ubuntu-trusty main"" > /etc/apt/sources.list.d/docker.list

aptパッケージの索引を更新します。
root@ubuntu:~# apt-get update
Ign http://us.archive.ubuntu.com trusty InRelease
Get:1 http://security.ubuntu.com trusty-security InRelease [64.4 kB]
(中略)
Ign http://us.archive.ubuntu.com trusty/universe Translation-en_US             
Fetched 3,800 kB in 17s (215 kB/s)                                             
Reading package lists... Done
root@ubuntu:~# 

旧repoを削除します(新しい環境なので入っていません)。
root@ubuntu:~# apt-get purge lxc-docker
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Package 'lxc-docker' is not installed, so not removed
0 upgraded, 0 newly installed, 0 to remove and 240 not upgraded.
root@ubuntu:~# 

atpが意図したレポジトリを使用することを確認します。
root@ubuntu:~# apt-cache policy docker-engine
docker-engine:
  Installed: (none)
  Candidate: 1.9.1-0~trusty
  Version table:
     1.9.1-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.9.0-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.8.3-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.8.2-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.8.1-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.8.0-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.7.1-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.7.0-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.6.2-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.6.1-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.6.0-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
     1.5.0-0~trusty 0
        500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 >Packages
root@ubuntu:~# 

aufsストレージドライバーを利用できるよう、linux-image-extraカーネルパッケージをインストールします。
root@ubuntu:~# sudo apt-get install linux-image-extra-$(uname -r)
Reading package lists... Done
Building dependency tree       
Reading state information... Done
linux-image-extra-3.19.0-25-generic is already the newest version.
linux-image-extra-3.19.0-25-generic set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 240 not upgraded.
root@ubuntu:~# 

この時点では最新版がすでにインストールされていました。

Dockerパッケージのインストール
Dockerのパッケージはdocker-engineという名前になっていることに注意してインストールします。
root@ubuntu:~# sudo apt-get install docker-engine
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following extra packages will be installed:
  aufs-tools cgroup-lite git git-man liberror-perl
Suggested packages:
  git-daemon-run git-daemon-sysvinit git-doc git-el git-email git-gui gitk
  gitweb git-arch git-bzr git-cvs git-mediawiki git-svn
The following NEW packages will be installed:
  aufs-tools cgroup-lite docker-engine git git-man liberror-perl
0 upgraded, 6 newly installed, 0 to remove and 240 not upgraded.
Need to get 11.0 MB of archives.
After this operation, 60.3 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 http://us.archive.ubuntu.com/ubuntu/ trusty/universe aufs-tools amd64 1:3.2+20130722-1.1 [92.3 kB]
(中略)
Setting up cgroup-lite (1.9) ...
cgroup-lite start/running
Setting up docker-engine (1.9.1-0~trusty) ...
docker start/running, process 7922
Processing triggers for libc-bin (2.19-0ubuntu6.6) ...
Processing triggers for ureadahead (0.100.0-16) ...
root@ubuntu:~# 


Dockerの動作確認
Dockerデーモン起動
root@ubuntu:~# sudo service docker start
start: Job is already running: docker

すでに起動されていました。
Dockerが正しくインストールされているか確認します。
root@ubuntu:~# sudo docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
b901d36b6f2f: Pull complete 
0a6ba66e537a: Pull complete 
Digest: sha256:8be990ef2aeb16dbcb9271ddfe2610fa6658d13f6dfb8bc72074cc1ca36966a7
Status: Downloaded newer image for hello-world:latest

Hello from Docker.
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the ""hello-world"" image from the Docker Hub.
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker Hub account:
 https://hub.docker.com

For more examples and ideas, visit:
 https://docs.docker.com/userguide/

root@ubuntu:~# 

ここではhello-worldというコンテナを起動してメッセージが出力されることを確かめています。以上でDockerのインストールは終わりです。

Dockerの環境を確認
dockerの環境を確認するにはdocker infoコマンドを使用します。
root@ubuntu:~# uname -r
3.19.0-25-generic
root@ubuntu:~# docker info
Containers: 1
Images: 2
Server Version: 1.9.1
Storage Driver: aufs
 Root Dir: /var/lib/docker/aufs
 Backing Filesystem: extfs
 Dirs: 4
 Dirperm1 Supported: true
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 3.19.0-25-generic
Operating System: Ubuntu 14.04.3 LTS
CPUs: 2
Total Memory: 1.938 GiB
Name: ubuntu
ID: Y3NE:3SYW:H2CE:Y36R:DJSK:3CB7:UXBZ:ZI4B:BE4V:FWJZ:4W4T:Y4M5
WARNING: No swap limit support
root@ubuntu:~# 

Dockerのバージョンは1.9.1であることがわかります。

Docker環境で動作するプログラムを途中から追跡する

glibcのソースコードパッケージをインストール
準備として、glibcのソースコードパッケージを入手してインストールしておきます。
root@ubuntu:~/tmp# apt-get install eglibc-source
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  eglibc-source
0 upgraded, 1 newly installed, 0 to remove and 198 not upgraded.
Need to get 14.0 MB of archives.
After this operation, 25.3 MB of additional disk space will be used.
Get:1 http://us.archive.ubuntu.com/ubuntu/ trusty-updates/main eglibc->source all 2.19-0ubuntu6.6 [14.0 MB]
Fetched 14.0 MB in 7s (1,869 kB/s)                                             
Selecting previously unselected package eglibc-source.
(Reading database ... 147228 files and directories currently installed.)
Preparing to unpack .../eglibc-source_2.19-0ubuntu6.6_all.deb ...
Unpacking eglibc-source (2.19-0ubuntu6.6) ...
Setting up eglibc-source (2.19-0ubuntu6.6) ...
root@ubuntu:~/tmp# mkdir -p /build/buildd
root@ubuntu:~/tmp# tar xJCf /build/buildd /usr/src/glibc/eglibc-2.19.tar.xz 
root@ubuntu:~/tmp# 


追跡するソースコードの用意
ここでは対象となるソースコードの例として以下のファイル~/tmp/helloloop.cを用意します。

helloloop.c
#include <stdio.h>
#include <unistd.h>

int main(int argc, char **argv) {
  int i;
  for (i = 0;;) {
    printf(""Hello, world! (%d)\n"", i++);
    sleep(1);
  }
}


これをデバッグ用にコンパイルします。
root@ubuntu:~/tmp# gcc -g -o helloloop helloloop.c
root@ubuntu:~/tmp# 


通常の追跡手順
まず、Dockerを使わずにGDBにより実行を追跡してみましょう。
root@ubuntu:~/tmp# gdb ./helloloop 
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
Copyright (C) 2014 Free Software Foundation, Inc.
(中略)
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from ./helloloop...done.
(gdb) list
1   #include <stdio.h>
2   #include <unistd.h>
3   
4   int main(int argc, char **argv) {
5     int i;
6     for (i = 0;;) {
7       printf(""Hello, world! (%d)\n"", i++);
8       sleep(1);
9     }
10  }
(gdb) break 8
Breakpoint 1 at 0x4005ad: file helloloop.c, line 8.
(gdb) run
Starting program: /home/nao/tmp/helloloop 
Hello, world! (0)

Breakpoint 1, main (argc=1, argv=0x7fffffffe6a8) at helloloop.c:8
8       sleep(1);
(gdb) c
Continuing.
Hello, world! (1)

Breakpoint 1, main (argc=1, argv=0x7fffffffe6a8) at helloloop.c:8
8       sleep(1);
(gdb) quit
A debugging session is active.

    Inferior 1 [process 5514] will be killed.

Quit anyway? (y or n) y
root@ubuntu:~/tmp# 


Docker上での追跡手順
次に、Docker上で実行中のhelloloopの実行を追跡してみます。まず、適当なコンテナ(ここではubuntuを使っています)でhelloloopを実行します。volumeの指定によりホストとコンテナでディレクトリを同じ名前で共有しています。
root@ubuntu:~/tmp# docker run -it --rm -v ${HOME}/tmp:${HOME}/tmp ubuntu ${HOME}/tmp/helloloop
Hello, world! (0)
Hello, world! (1)

helloloopが実行を開始したところで(ホスト側で)別のターミナルを開き、プロセスを調べます。GDBでこのプロセスを指定して追跡します。
root@ubuntu:~# cd tmp
root@ubuntu:~/tmp# ps a|grep helloloop
  6021 pts/7    Sl+    0:00 docker run -it --rm -v /home/nao/tmp:/home/nao/tmp ubuntu /home/user/tmp/helloloop
  6041 pts/13   Ss+    0:00 /home/user/tmp/helloloop
  6059 pts/4    S+     0:00 grep --color=auto helloloop
root@ubuntu:~/tmp# gdb
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
Copyright (C) 2014 Free Software Foundation, Inc.
(中略)
Type ""apropos word"" to search for commands related to ""word"".
(gdb) attach 6041
Attaching to process 6041
Reading symbols from /home/nao/tmp/helloloop...done.
Reading symbols from /lib/x86_64-linux-gnu/libc.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libc-2.19.so...done.
done.
Loaded symbols for /lib/x86_64-linux-gnu/libc.so.6
Reading symbols from /lib64/ld-linux-x86-64.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/ld-2.19.so...done.
done.
Loaded symbols for /lib64/ld-linux-x86-64.so.2
0x00007f652d5a8f20 in __nanosleep_nocancel ()
    at ../sysdeps/unix/syscall-template.S:81
81  T_PSEUDO (SYSCALL_SYMBOL, SYSCALL_NAME, SYSCALL_NARGS)
(gdb) where
#0  0x00007f652d5a8f20 in __nanosleep_nocancel ()
    at ../sysdeps/unix/syscall-template.S:81
#1  0x00007f652d5a8dd4 in __sleep (seconds=0)
    at ../sysdeps/unix/sysv/linux/sleep.c:137
#2  0x00000000004005b7 in main (argc=1, argv=0x7ffd9674b508) at helloloop.c:8
(gdb) frame 2
#2  0x00000000004005b7 in main (argc=1, argv=0x7ffed5cbbff8) at helloloop.c:8
8       sleep(1);
(gdb) list
3   
4   int main(int argc, char **argv) {
5     int i;
6     for (i = 0;;) {
7       printf(""Hello, world! (%d)\n"", i++);
8       sleep(1);
9     }
10  }
(gdb) break 8
Breakpoint 1 at 0x4005ad: file helloloop.c, line 8.
(gdb) cont
Continuing.

Breakpoint 1, main (argc=1, argv=0x7ffed5cbbff8) at helloloop.c:8
8       sleep(1);
(gdb) cont
Continuing.

Breakpoint 1, main (argc=1, argv=0x7ffed5cbbff8) at helloloop.c:8
8       sleep(1);
(gdb) quit
A debugging session is active.

    Inferior 1 [process 6041] will be detached.

Quit anyway? (y or n) y
Detaching from program: /home/nao/tmp/helloloop, process 6041
root@ubuntu:~/tmp# 

このようにDocker上で動作しているプログラムは、ホストからもプロセスとして見えていて(プロセス番号はDocker内でのものと異なることに注意)ホスト上のGDBからデバッグすることが可能です。ただし、ここで見たようにプログラムの起動後にプロセス番号を調べてGDBに知らせる手順が必要です。プログラムが起動直後に終了する場合はソースコードを書き換えて待ち時間を入れるか、後述するようにDocker serverと同時にデバッグをおこなうといった方法をとる必要があります。

デバッグ用のDockerを用意する
Dockerのレポジトリからソースコードを取得して1.9.1に切り替えます。コンパイル等はDocker構築用コンテナdocker-devの内部で行われますが、その環境に合わせてソースコードのディレクトリを用意しておきます。
root@ubuntu:~# git clone https://github.com/docker/docker.git
root@ubuntu:~# cd docker
root@ubuntu:~/docker# git checkout v1.9.1
root@ubuntu:~/docker# mkdir -p /go/src/github.com/docker/
root@ubuntu:~/docker# ln -s `pwd` /go/src/github.com/docker/

次に、docker-dev内部のソースコードを取り出せるよう、Makefile中で指定しているDocker実行時のオプション--rmを--name docker-dev-tmpで置き換えます。すなわち、

Makefile
DOCKER_RUN_DOCKER := docker run --rm -it --privileged $(DOCKER_ENVS) $(DOCKER_MOUNT) ""$(DOCKER_IMAGE)""


この行を以下のように書き換えます。

Makefile
DOCKER_RUN_DOCKER := docker run --name docker-dev-tmp -it --privileged $(DOCKER_ENVS) $(DOCKER_MOUNT) ""$(DOCKER_IMAGE)""


さらに、環境変数DOCKER_DEBUGでデバッグ用Dockerのビルドを指定します。
root@ubuntu:~/docker# export DOCKER_DEBUG=1

Dockerをビルドします。
root@ubuntu:~/docker# make
docker build -t ""docker-dev:HEAD"" .
Sending build context to Docker daemon 262.1 MB
Step 1 : FROM ubuntu:14.04
 ---> 89d5d8e8bafb
(中略)
---> Making bundle: binary (in bundles/1.9.1/binary)
Building: bundles/1.9.1/binary/docker-1.9.1
Created binary: bundles/1.9.1/binary/docker-1.9.1

docker-dev-tmpからgolangのソースコードを取り出します。
root@ubuntu:~/docker# mkdir -p /usr/local/go
root@ubuntu:~/docker# docker cp docker-dev-tmp:/usr/local/go/src /usr/local/go/
root@ubuntu:~/docker# docker cp docker-dev-tmp:/go/src/github.com/golang /go/src/github.com/

これでdocker-dev-tmpは不要になったので消去しておきます。
root@ubuntu:~/docker# docker rm docker-dev-tmp

Dockerを停止して、バイナリーを新しく構築したものと入れ替えて再開します。
root@ubuntu:~/docker# service docker stop
docker stop/waiting
root@ubuntu:~/docker# mv /usr/bin/docker /usr/bin/docker-distributed
root@ubuntu:~/docker# cp bundles/1.9.1/binary/docker /usr/bin/
root@ubuntu:~/docker# service docker start 
docker start/running, process 10774

新しく構築したDockerが動作していることを確認できます。
root@ubuntu:~/docker# docker info
Containers: 0
(中略)
WARNING: No swap limit support
root@ubuntu:~/docker# 

以上でデバッグ用のDocker(とデバッグに必要なファイル類)が用意できました。

Dockerの動作モードについて
Dockerにはclient, daemon, initの3つのモードがあります。この3つは実質的には別のプログラムなのですがひとつのDockerバイナリー/usr/bin/dockerにまとめられており、起動時のオプションでモードを切り替えます。Dockerによりプログラムをコンテナ内で実行する場合はこの3つのモードが協調して動作します。
Docker clientとはdocker runなど、ユーザーがコマンドラインからdockerを利用する際のインターフェースとなるモードです。Docker daemonと通信してユーザーによる入力情報を送信し、処理結果を受信してユーザーに通知します。
Docker daemonはコンテナの生成・実行・削除などのサービスを提供するモードです。Linux上のサーバープロセスとして動作します。
root@ubuntu:~/docker# service docker status
docker start/running, process 10774

Docker initはコンテナ内部の実行環境を整備するためのモードです。Docker daemonによるコンテナの実行環境作成後、コンテナ内で最初のプログラムとして起動します。そして環境を整備したのちにユーザーの指定したコマンドを実行します。
Docker内部の実行を追跡するには、この3つのモードを意識して使い分ける必要があります。

Docker clientを追跡する
Docker clientは通常の手順で追跡できます。main.mainにブレークポイントを設定して実行し、nextコマンドによるステップ実行を繰り返してみましょう。
root@ubuntu:~/docker# gdb /usr/bin/docker
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
(中略)
Reading symbols from /usr/bin/docker...done.
(gdb) break main.main
Breakpoint 1 at 0x495570: file /go/src/github.com/docker/docker/docker/docker.go, line 17.
(gdb) run run -it --rm ubuntu ls
Starting program: /usr/bin/docker run -it --rm ubuntu ls
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
[New Thread 0x7ffff7e4a700 (LWP 11061)]
[New Thread 0x7ffff7649700 (LWP 11062)]
[New Thread 0x7ffff6e48700 (LWP 11063)]
[New Thread 0x7ffff6647700 (LWP 11064)]

Breakpoint 1, main.main ()
    at /go/src/github.com/docker/docker/docker/docker.go:17
17  func main() {
(gdb) list
12      ""github.com/docker/docker/pkg/reexec""
13      ""github.com/docker/docker/pkg/term""
14      ""github.com/docker/docker/utils""
15  )
16  
17  func main() {
18      if reexec.Init() {
19          return
20      }
21  
(gdb) n

Breakpoint 1, main.main ()
    at /go/src/github.com/docker/docker/docker/docker.go:17
17  func main() {
(gdb) 
(中略)
(gdb) 
65      if err := c.Run(flag.Args()...); err != nil {
(gdb) 
bin   dev  home  lib64  mnt  proc  run   srv  tmp  var
boot  etc  lib   media  opt  root  sbin  sys  usr
[New Thread 0x7ffff5dc6700 (LWP 11117)]
[Thread 0x7ffff6647700 (LWP 11064) exited]
[Thread 0x7ffff6e48700 (LWP 11063) exited]
[Thread 0x7ffff7649700 (LWP 11062) exited]
[Thread 0x7ffff7e4a700 (LWP 11061) exited]
[Thread 0x1dd3880 (LWP 11057) exited]
[Inferior 1 (process 11057) exited normally]
(gdb) 
The program is not being run.
(gdb) quit
root@ubuntu:~/docker# 


Docker daemonを追跡する
Docker daemonを追跡するには、まずDockerのサービスを停止します。
root@ubuntu:~/docker# service docker stop
docker stop/waiting

そして、Docker daemonをGDB内から起動します。先ほどと同様、main.mainにブレークポイントを設定しておきます。そのまま実行を進めていくと、Docker clientからの通信待ちになります。
root@ubuntu:~/docker# gdb docker
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
Copyright (C) 2014 Free Software Foundation, Inc.
(中略)
Reading symbols from docker...done.
(gdb) break main.main
Breakpoint 1 at 0x495570: file /go/src/github.com/docker/docker/docker/docker.go, line 17.
(gdb) run daemon
Starting program: /usr/bin/docker daemon
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
[New Thread 0x7ffff7e4a700 (LWP 11138)]
[New Thread 0x7ffff7649700 (LWP 11139)]
[New Thread 0x7ffff6e48700 (LWP 11140)]
[New Thread 0x7ffff6647700 (LWP 11141)]

Breakpoint 1, main.main ()
    at /go/src/github.com/docker/docker/docker/docker.go:17
17  func main() {
(gdb) n
18      if reexec.Init() {
(gdb) 
(中略)
(gdb) 
65      if err := c.Run(flag.Args()...); err != nil {

別のターミナルを開いて、Docker clientから適当なDocker操作をしてみましょう。ここでは先ほどと同様、docker run --rm ubuntu lsを実行します。すると、Docker clientからの通信を受け取ってさまざまな情報を出力しながら処理を行います。
(gdb) 
INFO[0011] [graphdriver] using prior storage driver ""aufs"" 
INFO[0011] API listen on /var/run/docker.sock           
INFO[0011] Firewalld running: false                     
INFO[0011] Default bridge (docker0) is assigned with an IP address 172.17.0.1/16. Daemon option --bip can be used to set a preferred IP address 
WARN[0012] Your kernel does not support swap memory limit. 
INFO[0012] Loading containers: start.                   

INFO[0012] Loading containers: done.                    
INFO[0012] Daemon has completed initialization          
INFO[0012] Docker daemon                                 commit=a34a1d5-dirty execdriver=native-0.2 graphdriver=aufs version=1.9.1
INFO[0044] POST /v1.21/containers/create                
[New Thread 0x7ffff5dc6700 (LWP 11187)]
INFO[0044] POST /v1.21/containers/2747dd5228b12047397adf5793733c7740f1e5f7b924923e7467e784dc176655/attach?stderr=1&stdout=1&stream=1 
INFO[0044] POST /v1.21/containers/2747dd5228b12047397adf5793733c7740f1e5f7b924923e7467e784dc176655/start 
INFO[0044] No non-localhost DNS nameservers are left in resolv.conf. Using default external servers : [nameserver 8.8.8.8 nameserver 8.8.4.4] 
INFO[0044] IPv6 enabled; Adding default IPv6 external servers : [nameserver 2001:4860:4860::8888 nameserver 2001:4860:4860::8844] 
[New Thread 0x7ffff55c5700 (LWP 11210)]
[New Thread 0x7ffff4dc4700 (LWP 11212)]

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ffff5dc6700 (LWP 11187)]
net.networkNumberAndMask (n=0x0, ip=..., m=...)
    at /usr/local/go/src/net/ip.go:436
436     if ip = n.IP.To4(); ip == nil {

途中、SIGSEGVで実行が停止してエラーが起きたように見えます。これはGo言語の実行環境のトリックなので気にせず継続してかまいません。
(gdb) c
Continuing.
INFO[0049] POST /v1.21/containers/2747dd5228b12047397adf5793733c7740f1e5f7b924923e7467e784dc176655/wait 
INFO[0049] GET /v1.21/containers/2747dd5228b12047397adf5793733c7740f1e5f7b924923e7467e784dc176655/json 
INFO[0049] DELETE /v1.21/containers/2747dd5228b12047397adf5793733c7740f1e5f7b924923e7467e784dc176655?v=1 

Dockerコンテナ上でのコマンドを実行後、再び通信待ちになりますのでCtrl-Cを入力するなどして実行を中断します。
^C
Program received signal SIGINT, Interrupt.
[Switching to Thread 0x1dd3880 (LWP 11134)]
runtime.futex () at /usr/local/go/src/runtime/sys_linux_amd64.s:278
278     MOVL    AX, ret+40(FP)
(gdb) quit
A debugging session is active.

    Inferior 1 [process 11134] will be killed.

Quit anyway? (y or n) y
root@ubuntu:~/docker# 


Docker initを追跡する
Docker initを追跡するにはDocker serverを経由します。まず、Docker initを起動する関数github.com/opencontainers/runc/libcontainer.(*initProcess).startにブレークポイントを設定してDocker Serverを起動します。
root@ubuntu:~/docker# gdb /usr/bin/docker
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
(中略)
Make breakpoint pending on future shared library load? (y or  [n]) n
(gdb) break github.com/opencontainers/runc/libcontainer.(*initProcess).start

Breakpoint 1 at 0x98b7b0: file /go/src/github.com/docker/docker/vendor/src/github.com/opencontainers/runc/libcontainer/process_linux.go, line 178.
(gdb) run daemon
Starting program: /usr/bin/docker daemon
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
[New Thread 0x7ffff7e4a700 (LWP 11488)]
(中略)
INFO[0000] Docker daemon                                 commit=a34a1d5-dirty execdriver=native-0.2 graphdriver=aufs version=1.9.1

通信待ちに入るので、別のターミナルを開いて、Docker clientから適当なDocker操作をしてみましょう。ここでは先ほどと同様、docker run --rm ubuntu lsを実行します。
INFO[0095] POST /v1.21/containers/create                
[New Thread 0x7ffff5dc6700 (LWP 11744)]
(中略)
[Switching to Thread 0x7ffff5e46700 (LWP 11492)]

Breakpoint 1, github.com/opencontainers/runc/libcontainer.>(*initProcess).start
    (p=0xc208685220, err=...)
    at /go/src/github.com/docker/docker/vendor/src/github.com/opencontainers/runc>/libcontainer/process_linux.go:178
178 func (p *initProcess) start() (err error) {

ブレークポイントで停止したらこれを無効化し、GDBが子プロセスであるDaemon initを追跡するよう設定します。Daemon initの入り口の関数github.com/docker/docker/pkg/reexec.Initにブレークポイントを設定して実行を再開します。
(gdb) disable 1
(gdb) set follow-fork-mode child
(gdb) break github.com/docker/docker/pkg/reexec.Init
Breakpoint 2 at 0x684a00: file /go/src/github.com/docker/docker/pkg/reexec/reexec.go, line 23.
(gdb) c
Continuing.
[New process 11659]
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
process 11659 is executing new program: /usr/bin/docker
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
warning: Can't attach LWP 2: No child processes
warning: Can't attach LWP 3: No child processes
warning: Can't attach LWP 5: No child processes
[Switching to LWP 11659]

Breakpoint 2, github.com/docker/docker/pkg/reexec.Init (~r0=152)
    at /go/src/github.com/docker/docker/pkg/reexec/reexec.go:23
23  func Init() bool {

Docker initの入り口にたどり着きました。nextコマンドでステップ実行していくとコンテナ内でlsが実行されるのがわかります。
(gdb) list
18      registeredInitializers[name] = initializer
19  }
20  
21  // Init is called as the first part of the exec process and returns true if an
22  // initialization function was called.
23  func Init() bool {
24      initializer, exists := registeredInitializers[os.Args[0]]
25      if exists {
26          initializer()
27  
(gdb) n
24      initializer, exists := registeredInitializers[os.Args[0]]
(gdb) 
25      if exists {
(gdb) 
26          initializer()
(gdb) 
process 11659 is executing new program: /bin/ls
[Inferior 2 (process 11659) exited normally]
(gdb) INFO[0061] POST /v1.21/containers/fe209b10dc7cbc31084a76c0920862274c7372d456a59c7baf758fd1b1fecf4f/wait 

The program is not being run.
(gdb) INFO[0062] GET /v1.21/containers/fe209b10dc7cbc31084a76c0920862274c7372d456a59c7baf758fd1b1fecf4f/json 
INFO[0062] DELETE /v1.21/containers/fe209b10dc7cbc31084a76c0920862274c7372d456a59c7baf758fd1b1fecf4f?v=1 
Quit
(gdb) quit
root@ubuntu:~/docker# 


Docker環境で動作するプログラムを最初から追跡する
先ほど作成したプログラムhelloloopのDocker環境での実行を追跡することにしましょう。今度はプログラムの先頭から追跡することが可能です。
今回はDocker initのときの手順と同様です。まず、Docker initを起動する関数github.com/opencontainers/runc/libcontainer.(*initProcess).startにブレークポイントを設定してDocker daemonを起動します。
root@ubuntu:~/docker# gdb /usr/bin/docker
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1
(中略)
Reading symbols from /usr/bin/docker...done.
(gdb) break github.com/opencontainers/runc/libcontainer.(*initProcess).start
Breakpoint 1 at 0x98b7b0: file /go/src/github.com/docker/docker/vendor/src/github.com/opencontainers/runc/libcontainer/process_linux.go, line 178.
(gdb) run daemon
Starting program: /usr/bin/docker daemon
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
[New Thread 0x7ffff7e4a700 (LWP 12954)]
[New Thread 0x7ffff7649700 (LWP 12955)]
[New Thread 0x7ffff6e48700 (LWP 12956)]
[New Thread 0x7ffff6647700 (LWP 12957)]
INFO[0000] [graphdriver] using prior storage driver ""aufs"" 
INFO[0000] API listen on /var/run/docker.sock           
INFO[0000] Firewalld running: false                     
INFO[0000] Default bridge (docker0) is assigned with an IP address 172.17.0.1/16. Daemon option --bip can be used to set a preferred IP >address 
WARN[0000] Your kernel does not support swap memory limit. 
INFO[0000] Loading containers: start.                   
....
INFO[0000] Loading containers: done.                    
INFO[0000] Daemon has completed initialization          
INFO[0000] Docker daemon                                 commit=a34a1d5-dirty execdriver=native-0.2 graphdriver=aufs version=1.9.1

別のターミナルを開いて、Docker clientからhelloloopを実行します。
root@ubuntu:~docker run -v \${HOME}/tmp:\${HOME}/tmp -it ubuntu \${HOME}/tmp/helloloop

INFO[0003] POST /v1.21/containers/create                
[New Thread 0x7ffff5dc6700 (LWP 12990)]
(中略)
178 func (p *initProcess) start() (err error) {

ブレークポイントで停止したらこれを無効化し、GDBが子プロセスであるDaemon initを追跡するよう設定します。Daemon initの入り口の関数github.com/docker/docker/pkg/reexec.Initにブレークポイントを設定して実行を再開します。
(gdb) disable 1
(gdb) set follow-fork-mode child
(gdb) break github.com/docker/docker/pkg/reexec.Init
Breakpoint 2 at 0x684a00: file /go/src/github.com/docker/docker/pkg/reexec/reexec.go, line 23.
(gdb) c
Continuing.
[New process 13012]
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
process 13012 is executing new program: /usr/bin/docker
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
warning: Can't attach LWP 2: No child processes
warning: Can't attach LWP 3: No child processes
warning: Can't attach LWP 5: No child processes
[Switching to LWP 13012]

Breakpoint 2, github.com/docker/docker/pkg/reexec.Init (~r0=96)
    at /go/src/github.com/docker/docker/pkg/reexec/reexec.go:23
23  func Init() bool {

ここで対象を helloloop に切り替えます。ブレークポイントをmainに設定し、実行を再開します。
(gdb) file ~/tmp/helloloop
A program is being debugged already.
Are you sure you want to change the file? (y or n) y

Load new symbol table from ""~/tmp/helloloop""? (y or n) y
Reading symbols from ~/tmp/helloloop...done.
(gdb) break main
Breakpoint 3 at 0x40057d: file helloloop.c, line 4.
(gdb) c
Continuing.
process 13012 is executing new program: /home/nao/tmp/helloloop

Breakpoint 3, main (argc=1, argv=0x7ffcb92cbae8) at helloloop.c:6
6     for (i = 0;;) {

mainの先頭で停止したことがわかります。
(gdb) list
1   #include <stdio.h>
2   #include <unistd.h>
3   
4   int main(int argc, char **argv) {
5     int i;
6     for (i = 0;;) {
7       printf(""Hello, world! (%d)\n"", i++);
8       sleep(1);
9     }
10  }
(gdb) c
Continuing.

このようにDocker serverとDocker initを経由することにより、Dockerコンテナ内で動作するプログラムの実行をホストのGDBで追跡することができます。

まとめ
DockerもDocker内のプログラムも実行をホスト側のGDBで追跡することができます。この方法を使うとDocker特有のトラブルの原因をすみやかに特定できます。

連絡先
著者に直接コンタクトを取りたい場合はnaoアットマークatbsd.comあてに電子メールを送ってください。
Dockerやその他の仮想環境に関するご相談等は株式会社あっとBSDまで。
以上。
",True,https://qiita.com/xylnao11/items/461d1d7908300041b26f
"ふだんはあまり使わないのだけど、忘れた頃に必要になるのでここに書いておく。

前提

デバッグ対象
ARM Linuxのユーザープロセスが今回のデバッグ対象。
gcc で書かれている。(今回は gcc 4.9)
ARM LinuxのIPアドレスは 192.168.129.32
gdbで使用するポート番号は 1234

デバッグホスト
x86_64 のPC Linux
デバッグ対象のARM Linuxとは同一ネットワーク内にある。
デバッグ対象のビルドはここで行っている。ここにソースコードがある。
ソースコードをgistに貼った。loop.c
ビルドに使用するツールチェイン、gdbはbuildrootで作った。

デバッグ用のgccのコンパイルオプション
デバッグ対象のプログラムをコンパイルするときに、gccに -g オプションをつける。(必須)
最適化オプションは -Og に変更する。(推奨)
-Og は最適化をデバッグ情報の妨げにならない範囲で行う。
gccでは最適化オプションが複数ついていた場合には最後のものが有効になる。よってMakefileの中に以下を追加するとよい。
CFLAGS += -g -Og

$ make
arm-buildroot-linux-gnueabi-gcc  -g -Og -c -o loop.o loop.c
arm-buildroot-linux-gnueabi-gcc  -o loop loop.o


gdbの設定ファイル
デバッグホストの中でデバッグ対象のソースコードがあるディレクトリでgdbを起動することになるが、そのディレクトリに以下の内容を .gdbinit のファイル名で書いておく。

.gdbinit
set sysroot ../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/

target remote 192.168.129.32:1234


sysroot の設定が重要。デバッグ情報付きのlibc.soなどの共有ライブラリのある場所を指定する。これが無いとバックトレースの表示が正しくできない。
$sysrot/lib/の下にlibc.soがあることを確認する。
$ ls -l ../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/lib/libc.so*
lrwxrwxrwx 1 koba koba 12 Dec 12 19:09 ../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/lib/libc.so.6 -> libc-2.23.so
$ file ../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/lib/libc-2.23.so 
../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/lib/libc-2.23.so: ELF 32-bit LSB  shared object, ARM, EABI5 version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 3.4.0, not stripped


デバッグ対象のARM Linuxでgdbserver を起動する
今回はすでに実行中のプロセスにアタッチする。プロセスIDは361
gdbとの通信に使用するポートは1234
必ずroot権限で実行すること。
ARM# gdbserver --attach :1234 361
Attached; pid = 361
Listening on port 1234


デバッグホストでgdbを起動する
デバッグ対象のオブジェクトファイルをloop とする。
$ file loop
loop: ELF 32-bit LSB  executable, ARM, EABI5 version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 3.4.0, not stripped

ARM用にビルドされたgdbを起動する。
t$ arm-buildroot-linux-gnueabi-gdb loop
GNU gdb (GDB) 7.10.1
Copyright (C) 2015 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""--host=x86_64-unknown-linux-gnu --target=arm-buildroot-linux-gnueabi"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from loop...done.
0xb6e3abac in nanosleep ()
   from ../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/lib/libc.so.6
(gdb) 


バックトレースを表示してみる
(gdb) bt
#0  0xb6e3abac in nanosleep ()
   from ../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/lib/libc.so.6
#1  0xb6e3aaa4 in sleep ()
   from ../buildroot/buildroot-2016.05/output/host/usr/arm-buildroot-linux-gnueabi/sysroot/lib/libc.so.6
#2  0x0001042c in func2 () at loop.c:16
#3  0x0001045c in func1 () at loop.c:7
#4  0x00010468 in main () at loop.c:23
(gdb) 


ブレークポイントをかけたり、変数を見たりしてみる
(gdb) b func1
Breakpoint 1 at 0x10458: file loop.c, line 7.
(gdb) c
Continuing.

Breakpoint 1, func1 () at loop.c:7
7           func2();
(gdb) p cnt
$1 = 82
(gdb) 


参考
gdbのドキュメント
http://sourceware.org/gdb/download/onlinedocs/gdb/index.html
",True,https://qiita.com/tetsu_koba/items/72361225f759122dcd57
"

だいたいPLCってなによ
Qiitaはプログラム、主にサーバ・PC・スマホに対するソフトウェアの技術サイトなので
馴染みがない人が多いと思いますが、電気回路に近く、
ノイズ耐性や安定性・メーカー部品保守部品の入手性に特化している演算器で
す。
すごくざっくり言えば、Arduinoの容量とスピード、I/O関連を強化させたものと考えれば十分です。
シーケンサなどとも呼ばれ(三菱FAの商標) 工場の装置制御に使用されています。
ラダー図とか聞いたことがあればそれです。
※ここでも三菱のPLCをベースに記事を書いております。

なぜラズパイを使う？
PLCは元々工作機械であったり、工場のラインをベースにスタンドアロンを前提に使用していたため、
昨今のネットワーク関連にすこぶる弱いです。
最近はLANの端子で通信ができるようになりましたが、
専用のソフトを基準に通信しています。
（しかもそこそこのお値段。）
でもこれからの時代、IoTも含めてネットに繋がらないと不便でしかないので、
ラズパイに橋渡ししてもらおうって算段です。

相性のいい理由

電源が近くにある
通信もLANケーブル１本でOK
装置の中に設置する為のケースもある
簡単なI/Oも付いているから、簡単なON/OFFも検知できる
PLCが不得手な計算処理が得意
PCと比較して消費電力は格段に少ない。

なにより、安いじゃないですか、コミコミで１万あれば、マシンのIoT化出来るんですよ？
ウン十万のソフト買って、サーバ役のPCを設置するより
ラズパイ上でお得意の言語を使ったほうが保守しやすいです。

何が出来るの？
端的にいうと、PLC内部のデータを読み書きできます。
つまりやろうと思えば、乗っ取り出来ます。

外部からの緊急停止
現状のセンサー読み取り
メンテナンスフラグを立てたり消したり


どうやるの？
この記事、お待ちかねの方法ですね。
三菱のPLCに備わっているMCプロトコルを使用します。
三菱が無料で技術書を配布しており、
具体的にはソケット通信により、リクエストを送信、応答分によりデータを取得できます。
つまり、ソケット通信ができればどんな言語でもOKです。
命令文の例

Command_Ex.txt
送信文：：500000FF03FF000018002004010000D*0002110001
受信文：：D00000FF03FF00000800000001



例・Pythonでの、最も簡素な通信方法

PLC.py

import socket

host = ""192.168.1.1"" # <- PLCに割り振り or 設定したIP
port = 5015          # <- 同じくPLCに設定したポート(後述)

client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
client.connect((host, port))

client.send(""500000FF03FF000018002004010000D*0002110001"") # 送信文
response = client.recv(1024)   # バッファはこんなにいらないけど、一応。
# ↑これが""D00000FF03FF00000800000001""となる。



これだけ。
PLCへの書き込みの場合は、返答文が正常終了かエラーメッセのどちらかになる。

命令文の解説
送信している内容は、PLCの接続構成によるが、
ここでは装置１台に付き、1台のPLCとして話をします。
※複数台の場合はここをヒントに三菱のリファレンスを見て下さい

送信文
5000 00 FF 03FF 00 0018 0020 0401 0000 D*000211 0001

先頭から順に。

5000 ：サブヘッダ
PLCの機種によってフレームが決まっている？
5000とすることによって、""3Eフレーム""での通信を指定する。
5400は""4Eフレーム""
ちなみに4Eフレームは、データ文そのものにシリアルを追加することが出来て、
応答分が誰のモノかわかる。
00 ：ネットワーク番号
ネットワーク間にPLCを中継機として使用している場合は、01h-EFhまでを指定できる。
同じネットワーク上なら、""00""
FF ：PC番号
同じネットワーク内にPLCが複数ある場合、01h-EFhを指定。
１つだけならFFh
03FF ：要求先ユニットI/O番号
マルチドロップ・マルチCPU・二重化システムの場合は指定、
１台なら""03FF""
00　：要求先ユニット局番号
上記と同じ、マルチドロップ・マルチCPU・二重化システムの場合は指定、
１台なら""03FF""


ここからコマンド

0018 ：送信データ長
ここから続くバイト数を、16進で記入
次の0020から最後の0001まで24文字 => 0018hとなる。
0020 ：レスポンス待ち時間
処理時間のタイムアウト指定、16進指定で 1単位 250ミリ秒
ここでは32単位、0.8秒まで待つ。
0401　：ワード単位(一括)読み出し
おまたせしました。やっと読み書き命令の登場です。
ここの場合、ワード単位で、アドレスを範囲、一括で読み出します。
1点読み出しも出来るので、使い勝手良し。
1点なら0403でも可能、というか正道。
0000　：上記読み出しコマンドの、サブコマンド
ほぼ使用しないが、「ビットをセットしたい」or「ビットをリセットしたい」などの場合、
ここが0001になったりする。
D*000211　：データアドレス指定
PLCはワード単位でD211など、番号ごとのデバイス(データレジスタ)があり、
これを指定、その他にも ""M,X,Y,B,W""など、PLC開発者にはおなじみのデバイスが指定できる
ここではD211を指定、この中身を見たい。
注：ZRデバイスだけは*を省略、ZR000001のようにする。
0001　：連続データ数指定
このコマンドは一括読み出しなので、データ数(デバイス数=ワード数)を
指定している、ここでは1個
ここを0003にすると、D211,D212,D213のデータが返ってくる。
000Aなら、D211～D220まで。

つまり、送信データ長の数字と、0401以降のコマンドをいじると、
欲しいデータが降ってくることになる。

受信文
D000 00 FF 03FF 00 0008 0000 0001

正常な返答だとこの様になる。

D000 00 FF 03FF 00 0008：各種情報

　D000:返答固定文　
　00:ネットワーク番号
　FF:PC番号
　03FF:要求先ユニットI/O
　00:要求先ユニット局番号
　0008:以降の文字バイト数
あまり使わないデータですね。
バイト数は使うかもしれませんが、
返ってくる量がわかっていればベリファイ程度に使用します。

0000 ：終了コード
0000が正常な終了コードなので、ここだけはチェック
0001　：データ(16進)
これが本題のデータ、煮るなり焼くなり、でもまずは10進数変換かな？


PLCの設定
IPやポートなどをこうして設定します。
GX-Woks2で説明。
PCパラメータを選択

いろいろ設定！

空いている欄、ここでは３段目に[TCP]
プルダウンから[MCプロトコル]
自局ポート番号には好きなポート番号を。

これで、PLCに書き込むとソケット通信(MCプロトコル)にて通信ができます。
改めて、プログラム側でIP,ポートを確認しましょう。

で、これからなにをする？
こっからはネット＋PCソフトの本領発揮ですよ。
0.5秒単位でデータ収集して、社内DBに記録や、
生産実績の記録、振動センサや温度・圧力を監視して
IoTの事例お得意の ""壊れる前に、治しに来ました"" が出来るかもしれません。
また、許されるならAWS,GCPに投げて遠隔監視や操作まで可能です。
（私の現場では、Firebaseでデータの監視装置で使用してます。）
（メンテ時には、ログも取得したり。）
まだまだ、一部分の紹介ですが、
三菱FAのサイトにて、SH-080003 を検索すると
「Q対応シリアルコミュニケーションユニットユーザーズマニュアル」があるので
無料で落として、PDFの63ページ(書物内では61ページ)にコマンド一覧が載っています。
その他もここより詳しく書いてあるので、一回は見てみて下さい。
ここまでわかると、その他のPLCはどうなっているのか、気になりますね。
では。
三菱電機FA
http://www.mitsubishielectric.co.jp/
",True,https://qiita.com/Tatsuya-8888/items/28cc12dd637c0866fd1c
"

ファイルシステムをROM化する
組込み用途でRaspberryPiを使うときに考慮すべきなのが、SDカードの摩耗と、正規のシャットダウン手順を踏まない強制的な電源断です。SDカードへの書き込みは、SDカード内のフラッシュメモリの寿命を縮め、強制的な電源断は、最悪、ファイルシステムの破損を起こします。
LinuxのファイルシステムをROM化する方法は色々あるようですが、とりあえず簡単に導入できそうな、overlayfsを使用する方法です。以下のコマンドを淡々と打ち込んでください。カーネルの再構築など不要で、あっという間にできます。

INSTALL
cd /home/pi
sudo bash
apt-get install git rsync gawk busybox bindfs
dphys-swapfile swapoff
dphys-swapfile uninstall
update-rc.d dphys-swapfile disable
systemctl disable dphys-swapfile
git clone https://github.com/josepsanzcamp/root-ro.git
rsync -va root-ro/etc/initramfs-tools/* /etc/initramfs-tools/
mkinitramfs -o /boot/initrd.gz
echo initramfs initrd.gz >> /boot/config.txt
reboot


再起動が済んだら既にROM化されています。この状態でファイルに書き込むと、RAMを消費してtmpfs（所謂RAMディスク）上に書き込まれるので要注意です。ログファイルなどを無駄に出力しないように設定しておきましょう。
書き込み可能に切り替えるには以下のスクリプトを使います。

WR-ENABLE
#!/bin/sh

if [ -e /mnt/boot-ro/config.txt ]; then
    sudo mount -o remount,rw /dev/mmcblk0p1
    sudo grep -v initramfs /mnt/boot-ro/config.txt >/tmp/config.txt
    sudo cp /tmp/config.txt /mnt/boot-ro/config.txt
    sudo reboot
else
    echo Already write enabled
fi


書込み禁止に切り替えるには以下のスクリプトを使います。

WR-PROTECT
#!/bin/sh

if [ -e /mnt/boot-ro/config.txt ]; then
    echo Already write protected
else
    sudo grep -v initramfs /boot/config.txt >/tmp/config.txt
    sudo echo initramfs initrd.gz >> /tmp/config.txt
    sudo cp /tmp/config.txt /boot/config.txt
    sudo reboot
fi


",True,https://qiita.com/felis_silv/items/e69f3490091bee9fe619
"友人とScalaのBooleanについてチャットで話していたときに、

「コップ本5.6節に短絡は名前渡しパラメーターで作られてるよ(意訳)と書いてあるけど、
scaladoc見るとわかるように実は && と || は名前渡しパラメータで定義されてないけ
ど言語上特別扱いで名前渡し扱いされるやつだよね」（大意）

という話を聞いて、自分の記憶では昔のScalaでは本当に名前渡しだった記憶があるのだけどなあ…と思ったので、昔のScalaのAPIとソースを掘り起こして、この記述の真偽について確かめてみました。
結論としては、

昔（2.7くらい）は本当に名前渡し（と想定されていた）
現在は、仕様上名前渡しだけど、挙動は、シグニチャとしては値渡しだけどコンパイラが特別扱いしているとした方が妥当な説明。つまり、仕様と挙動の間に差異がある（より正確には言語仕様書と、APIリファレンスの記述の間にずれがある）

ということになりました。
まずは、Scala 2.7.0時代のAPIについて。
https://docs.scala-lang.org/api/all.html
から各バージョンのAPIリファレンスが見られますが、2.7.0時代 には、そもそも、scala.Boolean という型が APIリファレンスに存在しませんし、ソースコードにも今は存在する Boolean.scala ファイルが存在しません。じゃあ、この型に関する操作はどこに記載されていたかというと、Scala Language Specification（以降SLS）に該当する記述が残っていました。
2.7.x時代のSLSが見あたらなかったので、2.9.x時代のSLSを読んだところ、

Class Boolean has only two values: true and false. It implements operations
as given in the following class definition.

package scala
abstract sealed class Boolean extends AnyVal {
  def && (p: => Boolean): Boolean = // boolean and
    if (this) p else false
  def || (p: => Boolean): Boolean = // boolean or
    if (this) true else p
  def & (x: Boolean): Boolean = // boolean strict and
    if (this) x else false
  def | (x: Boolean): Boolean = // boolean strict or
    if (this) true else x
  def == (x: Boolean): Boolean = // boolean equality
    if (this) x else x.unary_!
  def != (x: Boolean): Boolean // boolean inequality
    if (this) x.unary_! else x
  def unary_!: Boolean // boolean negation
    if (this) false else true
}

という記述があり、Scala 2.9.x以前の時代には、少なくとも「仕様としては」名前渡しパラメータである、ということは正しいと言えそうです。
この記述は、SLS (2.12）でもそのまま残っており、「仕様書の通りの挙動ならば」問題ありません。しかし、eta-expansionを試してみたところ、
scala> val x = true
x: Boolean = true

scala> x.&& _
res0: Boolean => Boolean = $$Lambda$1047/1553616699@2bb0e277

scala> x.|| _
res1: Boolean => Boolean = $$Lambda$1048/2129021779@1131aead

scala> x.| _
res2: Boolean => Boolean = $$Lambda$1049/1592671657@ace2408

scala> def foo(x: => Int): Int = x
foo: (x: => Int)Int

scala> foo _
res3: (=> Int) => Int = $$Lambda$1057/1348844972@2e9dcdd3

となり、少なくとも名前渡しパラメータである場合と型が違ってしまい、整合性が取れていません。現在のScala API Referenceに書かれているように、
Note
This method uses 'short-circuit' evaluation and behaves as if it was declared as def &&(x: => Boolean): Boolean. If a evaluates to false, false is returned without evaluating b.

というのが、挙動をもっともうまく説明していると言えそうです。これは言語仕様か実装のどっちのバグかというのはきわどいところですが、いずれにせよどちらかを修正する必要がありそうです（とりあえず、言語仕様書を修正するのが手っ取り早そうです）。
なんでこんな面倒なことになったかについてはよくわかていないのですが、
https://github.com/scala/scala/blob/v2.12.6/src/library/scala/Boolean.scala#L73-L75
というコメントが残されており、仕様通りのシグニチャとしてビルドしようとしたら何らかの問題が発生したということのようです。
ところで、こういう調査をする場合に、古いバージョンの言語仕様書やAPIリファレンスが速やかにとって来られるというのは便利ですね。
",True,https://qiita.com/kmizu/items/0925ab2b642992bb4cb0
"$\require{AMScd}$

はじめに
本記事では、関数型言語に触れているよくでてくる モナド について書きます。モナドとは何かを説明したり理解することは割と難しいと思うのですが、その理由の一つとしては、モナドという概念が 数学 の圏論に由来しているということにあると思います。何年も前にモナドを理解するために圏論を勉強したりしてみましたが、その中に出てくるモナドと、プログラミングにおけるモナドが本当に同じものなのか、というのはしばらく疑問でした。あるとき、Stateモナド(状態モナド) を具体例として、それを数学的な定義に基づいて実装してみたことで、プログラミングと数学の関係がより明確になってモナドの理解が深まったように思うので、この機会にまとめてみます。
プログラミングと数学が密接に関係する面白い例の一つだと思うのですが、記事が長くなってしまいモナドについて興味がないと読むのは大変だと思います。適当に斜め読みしてもらってその雰囲気だけでも伝えられれば幸いです。
Stateモナド(状態モナド)の数学の定義として参考にしたのは、Wikipediaに記載される、以下の一文です。

圏論的な見方では、状態モナドはデカルト閉圏の定義に現れる積函手と指数函手の随伴から作られる。

この一文を手掛かりに数学的にモナドを実装していきます。Haskell の基礎知識を前提としていますのでご了承ください。
まず State モナドとは何か、についてですが、Haskell では以下のように定義できるものです。
data State s x = State { runState :: s -> (x, s) }

instance Monad (State s) where
  m >>= k = State $ \s ->
              let (x, s0) = runState m s
              in runState (k x) s0
  return x = State $ \s -> (x, s)

1行目は、State という名前の型を新たに定義しているところで、型 s から 型 (x, s) への関数、という意味の、s -> (x, s) の部分が State モナドの実体です。s や x には任意の型(例えば Int型や String型など) を指定できます。(x, s)は数学でいうところの直積、プログラム(例えばpython)の用語でいうところのタプルに相当するものです。3行目以降の実装で、State を Haskell におけるモナドとして扱えるようにしています。
State モナドを使うと、値の変更を許さない純粋関数型言語においてあたかも ""可変な"" 状態が扱えるかのうように、プログラムを書くことができます。といっても、State モナドを知らない人にとってこの説明では意味が分からないと思うので、以下に使用例を示します。
1度の通過では入力値をそのまま返し、2度通過することで入力値に1を足した値を返すゲートを考えます。(通過回数は2回でリセット)。これは通過回数を記録できる""状態""を持たせることで実現できます。

このような性質を持つゲートを5つつなげたときに、入力値に対してどのような値を返すかを状態モナドを使ってシミュレートしてみます。(答えは簡単で、入力値+2が出力値となります)

Haskellでの実装例が以下です。
import System.Environment

data State s x = State { runState :: s -> (x, s) }

instance Monad (State s) where
  m >>= k = State $ \s ->
              let (x, s0) = runState m s
              in runState (k x) s0
  return x = State $ \s -> (x, s)

get = State $ \s -> (s, s)

put s = State $ \_ -> ((), s)

-- 2回通らないと1増えないゲートを再現
gate x = do
  state <- get     -- 現在の状態(Bool値)を取得
  put (not state)  -- 状態を(Bool値を反転させて)更新
  return $ if state then (x + 1) else x  -- 2回目(True)なら +1, 1回目(False)ならそのまま

-- 5つのゲートの連結を表現
five_gates x = return x >>= gate >>= gate >>= gate >>= gate >>= gate

main = do
  input <- fmap (read . head) getArgs
  putStrLn $ show . fst $ runState (five_gates (input :: Int)) False
--                                              ~~~~~          ~~~~~
--                                              入力値(Int型)  初期状態

get は現在の状態を取得する関数で、put は状態を新しい値で更新する関数です。gate は図のゲートを再現したものです。内部状態である通過回数をBool値で表現していて、1回目の通過をFalse、2回目の通過を True と扱っています。gate 関数では、現在の状態(Bool値)を get で取得し、それを反転(True -> False, False -> True)させたものを新たな状態として put で保存しておきます。get で取得した状態が True であれば 2回目の通過とみなして入力値+1を出力として返し、そうでなければ1回目の通過とみなして入力値をそのまま返します。five_gates 関数で、gate を図のように5つ連結して、runState で状態モナドを実行しています。
最終結果は入力値(input)に対して +2 した値になるはずで、実際に上記を実行すると、入力+2の値が出力されます。
説明を簡単にするためにあまり使い道のない例を示しましたが、関数型言語の世界においては Stateモナドは入門書などでも紹介される代表的なモナドの一つであり、数学と密接に関わっており、(例えばリストモナドのように)単純すぎる例でもないため、数学との関係の理解を深める例として最適だと思い選びました。
本記事では、まず、数学的なベースとなる圏論について触れ、数学におけるモナドとは何かを説明して、その数学的な定義から Haskell での状態モナドを構築します。それが上記の State モナドと同じようにふるまうことを確認します。圏論に関しては以下の 圏論の基礎 を参考にしました。


圏論(数学)におけるモナドの定義

圏とは
まず、圏論における 圏 を定義します。

圏の定義:  圏は対象(各対象を $a, b, c, \cdots$などで表す) の集まりと、対象の間の射(対象 $a$ から対象 $b$ への射を $f:a \to b$のように書く)の集まりから成るもので、以下を満たすものである。

任意の射 $f : a \to b$, $g : b \to c$に対して、射の合成 $g \circ f : a \to c$が定義される。
任意の射 $f: a \to b, g :b \to c, h : c \to d$ に対して、$ (h \circ g) \circ f = h \circ (g \circ f) $が成り立つ
任意の対象 $a$ に対して、(恒等射と呼ばれる) 射 $\rm id_{\it a} : \it a \to a$ が存在して、任意の射 $f : a \to b$, $g : c \to a$ に対して、$f \circ \rm id_{\it a} = \it f$, $ { \rm id }_a \circ g = g$が成り立つ


一般的な圏のイメージを以下に示します。$C$は圏の名前です。 対象 を黒い点、 射 を矢印で表しています。圏の定義を満たすために必要な射(例えば恒等射など)をすべて図に描いているわけではないのでご注意ください。

上記の圏の定義を満たすものは無数にありますが、代表的なものとして、全ての(小さな1) 集合から成る圏 $\bf Set $があります。$ \bf Set $の対象は集合、射は集合から集合への全ての写像です。 $\bf Set$は全ての集合を含む圏なので、例えば、プログラミングで扱うInt型、Bool型、Char型、String型(=[Char]型)、そのリストやタプル、構造体、共用体、あるいは関数の型など、あらゆる型(の値の集合)を圏$\bf Set $の対象としてみなすことができます。また、プログラミングにおける関数は、圏論における射に相当します。ただし、ここで言う関数とは、一般の(c言語などの)プログラミング言語の関数の概念を全て含むものではなく、同じ入力(引数)を与えれば必ず同じ出力(返り値)を返す関数に限られます。
圏${\bf Set}$とプログラミングとの対応の大雑把なイメージ:


函手と自然変換
複数の圏があるときにそれらを関連付ける概念として、函手 というものがあります。

函手の定義: 圏$C$, $D$が与えられたとき、函手$T$は、$C$の各対象$a$を$D$の対象$T_a$に割り当て、$C$の各射 $f: a \to b$ を $D$の射 $Tf: Ta \to Tb$に割り当てるもので、以下を満たすものである。
$$ T( { \rm id_{\it a} }) = { \rm id_{\it T a} }, \hspace{5pt} T ( g \circ f )  = Tg \circ Tf \tag{1} $$


函手に相当するものとして、 Haskell には Functor があります。
class  Functor t where
    fmap  :: (a -> b) -> t a -> t b

Functor は (a -> b) -> t a -> t b 型のfmap という名前のメソッドを一つ持ちます。このtを函手$T$だと解釈すれば、fmapは上図のように、対象$a$から対象$b$への射$f$ (f :: a -> b)を与えると、$T a$ から $T b$ への射 $T f$ (fmap f :: t a -> t b) を返すもの、とみなすことができます。つまり、Functor は上の図のような働きをするので、数学での函手に相当すると言えます。
なお、Haskell で Functorを定義する場合には式 (1) の関係を満たしていることまでは要請していないので、Functorクラスのインスタンスは必ずしも数学的な意味の函手ではありません。
函手が複数与えられたときに、それらを関係づける概念として 自然変換 とよばれるものがあります。

自然変換の定義:
圏$C,D$と、二つの函手$S, T:C \to D$が与えられたとき、自然変換 $\tau : S \xrightarrow{.} T$ とは、$C$の各対象$c$に$D$の射$\tau_c = \tau c: S c \to T c$を割り当てる関数で、$C$のすべての射$f: c \to c^\prime$について図式
\begin{CD}
c            @. @. Sc @>{\tau c}>> Tc \\
@V{f}VV        @.   @V{Sf}VV @V{Tf}VV                     \\
c^\prime     @. @.Sc^\prime  @>{\tau c^\prime}>> T c^\prime
\end{CD}

が可換になるようなものである。

函手を別の函手に対応させる何か、という意味で、自然変換は以下の図のようなイメージです。

自然変換は「垂直」合成と「水平」合成を考えることができます。

図の$A,B,C$は圏、$\sigma, \tau, \sigma^{\prime}, \tau^{\prime}$は自然変換です。垂直合成とは、上図で示すところの、$\sigma$ と $\tau$、あるいは、$\sigma^{\prime}$ と $\tau^{\prime}$ を合成して得られる自然変換であり、それぞれ、$\tau \cdot \sigma$、$\tau^{\prime} \cdot \sigma^{\prime}$ と書きます。また、水平合成とは、上図で示すところの、$\sigma$ と $\sigma^{\prime}$ (あるいは、$\tau$と$\tau^{\prime}$など)を合成して得られる自然変換であり、その合成を、$\sigma^{\prime} \circ \sigma$、あるいは $\circ$ を省略して単に $\sigma^{\prime} \sigma$ と書きます。水平合成と垂直合成に関しては、以下の相互交換法則が成り立ちます(証明は省略します)。

(\tau^\prime \cdot \sigma^\prime) \circ (\tau \cdot \sigma) = (\tau^\prime \circ \tau) \cdot (\sigma^\prime \circ \sigma) \tag{2}


以降では、函手 $S:C \to B$に対して、(圏$B$の対象の恒等射を割り当てる)恒等自然変換 $ \varsigma : S \xrightarrow{.} S $ を、単に$S$と書くことにします。恒等自然変換は垂直合成 $\cdot$ について恒等射です($S \cdot \sigma = \sigma \cdot S = \sigma$)。また、恒等函手 $I_B: B \to B$ の恒等自然変換 $1_B : I_B \xrightarrow{.} I_B $は、水平合成 $\circ$ について恒等射です($1_B \circ \sigma = \sigma$,  $\sigma^{\prime} \circ 1_B = \sigma^{\prime}$)。

Hom 集合
圏 $C$ の対象 $a, b$について、hom集合と呼ばれる、$a$ から $b$ への射全体を表す集合を ${\rm hom}_{C} (a, b)$ あるいは $C(a, b)$ と表現します。圏$C$を省略して${\rm hom} (a, b)$ と表記することもあります。参考にした圏論の書籍での表記とは異なりますが、Haskell における a -> b のような型の書き方に似せて、上記の表記の他に、本記事では hom集合を $a \to b$ のように表記することもあります。 $\bf Set$ における任意の ${\rm hom} (a, b)$ は (射の""集まり""という意味で1つの集合であるため) $\bf Set$の対象として扱えます。
プログラミングの言葉を使うと、例えば ${\rm hom} (\text{Int}, \text{String})$ あるいは $\text{Int} \to \text{String}$ と書かれた hom 集合は、$\text{Int}$型の値を引数として受け取って$\text{String}$型の値を戻り値として返す関数を、もれなくすべて集めた集合を意味します。

随伴
少し難しい概念ですが、 随伴 を定義します。

随伴の定義: $ A $ と $ X $ を圏とする。$ X $ から $ A $ への随伴とは、次の条件を満たすような三つ組み $ \langle F, G, \phi \rangle : X \rightharpoonup A $である。ここで、$ F $ と $ G $ は函手
X \overset{F}{\underset{G}{\rightleftarrows}} A

であり、$ \phi $ は対象 $ x \in X $ と $ a \in A $ による各組に集合間の 全単射
\phi = \phi_{x,a} : A(Fx, a) \cong X(x, Ga)  \tag{3}

で $ x $ と $ a $ において 自然 なものを割り当てる関数である。

Hom集合のおさらいですが、$A(Fx, a)$ は、圏$A$における 対象 $Fx$ から 対象$a$への射全体の集合を意味し、${\rm hom}_A(Fx, a)$あるいは$Fx \to a$とも書かれるものです。また、$x$ と $a$ において 自然 とは、$k:a \to a^{\prime}, h:x^{\prime} \to x$ に対して以下の可換図式が成り立つことです。
\begin{CD}
A(F x, a) @>{\phi}>> X(x, G a) @. A(F x, a) @>{\phi}>> X(x, G a) \\
@V{k_*}VV     @V{(Gk)_*}VV @V{(Fh)^*}VV @V{h^*}VV \\
A(F x, a^{\prime}) @>{\phi}>> X(x, G a^{\prime}) @. A(F x^{\prime}, a) @>{\phi}>> X(x^{\prime}, G a)
\end{CD}

上記の可換図式を、$f : Fx \to a$に対して、数式で表すと以下のようになります。
\phi (k \circ f) = Gk \circ \phi f,\hspace{5pt} \phi (f \circ F h) = \phi f \circ h    \tag{4}

随伴の定義から、以下の定理を導くことができます。

定理: 随伴 $\langle F, G, \phi \rangle $は以下のものを決定する。
(i)各 $f : Fx \to a$の右随伴射が
\phi f = Gf \circ \eta_x : x \to Ga

であるような自然変換 $\eta: I_X \xrightarrow{.} GF$。
(ii)各$g:x\to Ga$が左随伴射
\phi^{-1}g = \varepsilon_a \circ Fg : Fx \to a

を持つような自然変換 $\varepsilon : FG \xrightarrow{.} I_A$。
さらに、次の両方の合成は(それぞれ$G$ および$F$の)恒等変換である。
G \xrightarrow{\eta G} GFG \xrightarrow {G \varepsilon } G,
F \xrightarrow{F \eta} FGF \xrightarrow {\varepsilon F } F  \tag{5}

$\eta$を随伴の単位元と呼び、$\varepsilon$を余単位元と呼ぶ。

証明は以下の通りです。まず定理の(i)についてですが、$\eta_x = \phi( \rm id_{\it Fx}) $で定義すると、($k:=f, f:= \rm id_{\it Fx} $と置いて)随伴の定義である式 (4) より 、 $ \phi f = G f \circ \eta_{x} $ は明らかです。また、この結果を用いて、$h:x \to x^{\prime}$に対して、
\begin{align}
GFh \circ \eta_{x} &= \phi(Fh) \\
 &= \phi({\rm id}_{Fx^{\prime}} \circ Fh) \\
 &= \phi({\rm id}_{Fx^{\prime}}) \circ h \\
 &= \eta_{x^{\prime}} \circ h
\end{align}

が成り立つ、すなわち以下の可換図式が成り立つので、$\eta: I_X \xrightarrow{.} GF$ は自然変換です。
\begin{CD}
x  @. @. I_X x @>{\eta_{x}}>> GF x \\
@V{h}VV       @.   @V{I_X h}VV   @V{GF h}VV                     \\
x^{\prime}           @. @. I_X x^{\prime}       @>{\eta_{x^{\prime}}}>> GF x^{\prime}
\end{CD}

定理の(ii)についても、$\varepsilon_a = \phi^{-1}({\rm id}_{Ga})$ とおいて、上と同様に証明できます。また、定理の (i) の結果を用いて、
\begin{align}
(G \varepsilon \circ \eta G)_a &= G \varepsilon_a \circ \eta_{Ga} \\
 &= \phi(\varepsilon_a) \\
 &= {\rm id}_{Ga}
\end{align}

より、$G \varepsilon \circ \eta G$は恒等変換です。$\varepsilon F \circ F \eta$が恒等変換であることも同様に示すことができます。以上で定理の証明は終わりです。



単位元 $\eta$ の定義のイメージ
余単位元 $\varepsilon$ の定義のイメージ









これで、冒頭のWikipediaの一文

状態モナドはデカルト閉圏の定義に現れる積函手と指数函手の随伴から作られる。

の 随伴 の部分の意味が明らかになりました。この一文を読み解くためには、""随伴""以外の部分の意味を明らかにする必要があります。
この一文のうち、状態モナドの数学的な定義として重要なのは、後半の「積函手と指数函手の随伴から作られる」の部分です(""デカルト閉圏の定義に現れる""の部分は参考程度の情報です)。そのため、次に、まだ説明していない「積函手」と「指数函手」について定義して、それらが随伴であることを確認します。

積函手と指数函手
これ以降扱う圏は、はじめの方で紹介した圏$ { \bf Set }$ (すべての集合から成る圏)とします。
積函手 は、対象を適当な対象$s$との直積(デカルト積、カルテシアン積とも呼ばれ、プログラミングではタプルに相当)に移す函手です。具体的には、積函手を、以下を満たすような函手  $\hspace{5pt} \cdot \times s : { \bf Set } \to { \bf Set }$ であると定義します。

対象(集合) $a$ を $a \times s$ に移す
射 $f: a \to b$ を $\hspace{3pt} ( f, { \rm id }_s ) : a \times s \to b \times s : ( \tilde{a}, \tilde{s} ) \mapsto ( f(\tilde{a}), \tilde{s} ) $ に移す 
($f$を、積函手によって、タプルの1番目の要素にのみ$f$を適用するような関数に移す、というイメージです)

このように定義した""積函手""が本当に函手であることを確認するため、上記の対応を $F$とおいて($F= \hspace{2pt} \cdot \times S$です)、式(1)の関係を満たしていることを確認します。$\tilde{a}\in a, \tilde{s} \in s$に対して、
\begin{align}
F ({\rm id}_a) (\tilde{a}, \tilde{s}) &= ({\rm id}_a(\tilde{a}), \tilde{s}) \\
  &= (\tilde{a}, \tilde{s})
\end{align}

より、$F (\rm id_{\it a} ) = id_{\it F a}$ です。また、
\begin{align}
F(g \circ f) (\tilde{a}, \tilde{s}) &= (g \circ f(\tilde{a}), \tilde{s}) \\
  &= Fg (f(\tilde{a}), \tilde{s}) \\
  &= Fg (Ff (\tilde{a}, \tilde{s})) \\
  &= (Fg \circ Ff)(\tilde{a}, \tilde{s})
\end{align}

より、$F(g \circ f) = Fg \circ Ff$が成り立ちます。式(1)を満たすことが確認できるため、上記の対応$F$は函手です。
指数函手 は、対象を適当な対象$s$からの hom 集合に移す函手です。具体的には、指数函手を、以下を満たすような函手 $\hspace{5pt} s \to \cdot : { \bf Set } \to { \bf Set }$であると定義します。

対象(集合) $a$ を hom 集合 $s \to a \hspace{5pt} (= {\rm hom} (s, a))$ に移す
射 $f: a \to b$ を $\hspace{3pt} {\rm hom} (s, f) : {\rm hom} (s, a) \to {\rm hom} (s, b) : (k: s \to a) \mapsto (f \circ k)$ に移す

積函手の場合と同様に、このような対応を $G$ とおいて($G=s \to \cdot \hspace{2pt}$です)、$G$ が函手の定義(1)を満たしていることを確認します。 $k: s \to a$ に対して、
\begin{align}
G ({\rm id}_a) (k) &= {\rm id}_a \circ k \\
  &= k
\end{align}

より、$G (\rm id_{\it a} ) = id_{\it G a}$ です。また、
\begin{align}
G (g \circ f) (k) &= (g \circ f) \circ k \\
  &= g \circ (f \circ k) \\
  &= Gg (f \circ k) \\
  &= Gg (Gf(k)) \\
  &= (Gg \circ Gf) (k)
\end{align}

より、$G(g \circ f) = Gg \circ Gf$が成り立ちます。やはり、式(1)を満たすため、上記の対応$G$も函手です。
本記事では見た目の分かりやすさから積函手、指数函手をそれぞれ $\cdot \times s,  s \to \cdot$ と表記していますが、一般的な書き方ではないと思いますのでご注意ください。(指数函手という呼び方もGoogle検索であまりヒットしないため、一般的な呼び方ではない可能性があります。参考にした書籍では共変hom函手と呼ばれるものに相当します。)
式だけを見ると難しく見えますが、以下の図のようなイメージです。



積函手のイメージ
指数函手のイメージ









さて、次に、 積函手と指数函手が随伴になっていること(随伴の定義を満たしていること)を確認します。
随伴の定義に現れる圏$A$,$X$をいずれも$\bf Set$ に、函手$F, G$ をそれぞれ積函手、指数函手に置き換えて、これらが随伴の定義を満たしていることを確認します。具体的には、 各対象$x$, $a$ に対して式(3)の 自然な全単射
\phi=\phi_{x,a}: \hspace{4pt} (x \times s) \to a  \hspace{3pt} \cong \hspace{3pt} x \to (s \to a)  \tag{6}

が存在することを示すことで、積函手と指数函手が随伴であることを確認します。この全単射はカリー化(あるいはその逆)の考え方で構築できます。
より具体的には、$ f \in x \times s \to a$ に対して、$g=\phi(f) \in x \to (s \to a)$ を
\forall \tilde{x} \in x, \tilde{s} \in s,  \hspace{10pt}　g(\tilde{x})(\tilde{s}) = f(\tilde{x}, \tilde{s})  \tag{7}

によって定義することで、全単射$\phi$を得ることができます。$f=\phi^{-1}(g)$も上式の関係を使って得ることができます。この全単射が自然であること、すなわち式(4)を満たすことは、$k:a \to a^{\prime}$, $h:x^{\prime} \to x$,  $f \in x \times s \to a$, $\tilde{x} \in x$, $\tilde{x}^\prime \in x^\prime$, $\tilde{s} \in s$ について以下が成り立つことから確認できます。
\begin{align*}
\phi(k \circ f) (\tilde{x})(\tilde{s}) &= k \circ f(\tilde{x}, \tilde{s}) \\
  &= k (f(\tilde{x}, \tilde{s})) \\
  &= k (\phi f(\tilde{x})(\tilde{s})) \\
  &= k \circ (\phi f(\tilde{x})) (\tilde{s}) \\
  &= Gk (\phi f(\tilde{x})) (\tilde{s}) \\
  &= (Gk \circ \phi f)(\tilde{x})(\tilde{s}),\\
\end{align*}

\begin{align*}
\phi(f \circ Fh)(\tilde{x}^\prime)(\tilde{s}) &= f \circ Fh(\tilde{x}^\prime, \tilde{s}) \\
  &= f(Fh (\tilde{x}^\prime, \tilde{s})) \\
  &= f(h(\tilde{x}^\prime), \tilde{s}) \\
  &= \phi f (h(\tilde{x}^\prime)) (\tilde{s}) \\
  &= (\phi f \circ h) (\tilde{x}^\prime) (\tilde{s}).
\end{align*}

以上より、積函手と指数函手は随伴であることが確認できました。積函手と指数函手の随伴における、単位元$\eta$ と余単位元 $\varepsilon$ の定義については、以下のように可視化できます。$A, X$はそれぞれ圏$ { \bf Set }$です。

具体的に数式で書くと、$\eta_x : x \to (s \to (x \times s))$は、$\tilde{x} \in x, \tilde{s} \in s$ に対して、式(7)より
\begin{align}
\eta_x (\tilde{x})(\tilde{s}) &= \phi^{-1} (\eta_x) (\tilde{x}, \tilde{s}) \\ 
                              &= { \rm id } (\tilde{x}, \tilde{s}) \\
                              &= (\tilde{x}, \tilde{s})   \tag{8}
\end{align}

となり、また、$\varepsilon_a : ((s \to a) \times s) \to a$ は、$\tilde{h} \in (s \to a), \tilde{s} \in s$に対して、
\begin{align}
\varepsilon_a (\tilde{h}, \tilde{s}) &= \phi (\varepsilon_a) (\tilde{h}) (\tilde{s}) \\
                                     &= { \rm id } (\tilde {h})(\tilde {s} ) \\
                                     &= \tilde{h} (\tilde{s})   \tag{9}
\end{align}

となります。
以上で、

デカルト閉圏の定義に現れる積函手と指数函手の随伴

の部分は一通り説明しました。さて、ここでようやく、(数学的な)モナドの定義が現れます。

モナド
圏論におけるモナドの定義は以下の通りです。

モナドの定義: 圏$X$におけるモナド $ T=\langle T, \eta, \mu \rangle$は、函手 $T : X \to X $ と二つの自然変換
\eta: I_X \xrightarrow{.} T, \hspace {5pt} \mu : T^2 \xrightarrow{.} T \tag{10}

からなり、次の図式を可換にするものである。
\begin{CD}
T^3 @>{T_\mu}>> T^2     @. IT @>{\eta T}>> T^2 @<{T \eta}<< TI \\
@V{\mu T}VV @V{\mu}VV  @| @V{\mu}VV @|                    \\
T^2 @>{\mu}>> T        @. T  @= T @= T
\end{CD}  \tag{11}


Wikipediaの一文に戻ると、「積函手と指数函手の随伴」によってモナドを定義できるとありましたが、一般に、随伴によってモナド(モナドの定義を満たすもの)を構成できることが知られています。
随伴$\langle F, G, \phi \rangle $の単位元を$\eta$、余単位元を$\varepsilon$とすると、恒等自然変換($F:F \xrightarrow{.} F $ や $ G:G \xrightarrow{.} G$ など)が垂直合成に対して恒等的であることに注意して、(また水平合成の演算$\circ$を省略して書いていることにも注意して) 水平合成と垂直合成の相互交換法則 (2) から、
\begin{align}
\varepsilon \cdot FG \varepsilon
  &= \varepsilon {\rm 1}_A \cdot FG \varepsilon \\
  &= (\varepsilon \cdot FG)({\rm 1}_A \cdot \varepsilon) \\
  &= \varepsilon \varepsilon \\
  &= ({\rm 1}_A \cdot \varepsilon) (\varepsilon \cdot FG) \\
  &= {\rm 1}_A \varepsilon \cdot \varepsilon FG \\
  &= \varepsilon \cdot \varepsilon FG
\end{align}

が成り立ちます。すなわち、以下の可換図式が成り立ちます。
\begin{CD}
FGFG @>{FG \varepsilon}>> FG  \\
@V{\varepsilon FG}VV @V{\varepsilon}VV         \\
FG  @>{\varepsilon}>> I_A
\end{CD}

この関係に左から函手$G$、右から函手$F$を合成した結果として(厳密に言えば式(2)の相互交換法則も用いて)以下が成り立ちます。
\begin{CD}
GFGFGF @>{GFG \varepsilon F}>> GFGF \\
@V{G \varepsilon FGF}VV @V{ G \varepsilon F }VV  \\
GFGF @>{G \varepsilon F}>> GF
\end{CD}

また、随伴の定理で示した式(5)の恒等変換
G = G \varepsilon \cdot \eta G : G \xrightarrow{.} G, \hspace{10pt} F = \varepsilon F \cdot F \eta : F \xrightarrow{.} F

の性質より、以下の可換図式も成り立ちます。
\begin{CD}
I_X GF @>{\eta GF}>> GFGF @<{GF \eta}<< GF I_X \\
@| @V{G \varepsilon F}VV @|                    \\
GF  @= GF @= GF
\end{CD}

$T :=GF$、$\mu := G\varepsilon F : GFGF (=T^2) \xrightarrow{.} GF (= T)$ とおくと、これはまさに、モナドの定義で示した式(11)の2つの可換図式を満たすことを意味します。すなわち、随伴$\langle F, G, \phi \rangle $があれば、モナド$ T=\langle T, \eta, \mu \rangle$を構築することができます。
これを「積函手と指数函手の随伴」にあてはめて、その結果得られるモナドを(数学的な意味での)状態モナドとして定義して具体的に書き下すと、以下のようになります。
状態モナドの自己函手$T: X \to X$ は、$T=GF$($T$は積函手と指数函手の合成)より、
Tx : s \to (x \times s)

です。これはちょうど、Haskell におけるStateモナドの型の定義部分である、
data State s x = State { runState :: s -> (x, s) }

に相当します。 また、自然変換 $\eta: I \xrightarrow{.} T, \hspace{5pt} \eta_x : x \to (s \to (x \times s))$ は、式(8) をそのまま使って、$\tilde{x} \in x, \tilde{s} \in s$に対して、
\eta_x(\tilde{x})(\tilde{s}) = (\tilde{x}, \tilde{s})

です。これは Haskell におけるStateモナドの定義の return x = State $ \s -> (x, s) に相当します。
自然変換 $\mu : T^2 \xrightarrow{.} T, \hspace{5pt} \mu_x: T^2 x \to T x$ については、式で表現しにくいですが、式(9)の関係を考慮すると、以下の図の青い枠と線で示すような$s$の適用によって得られる、$T^2 x = s \to ((s \to (x \times s)) \times s)$ から $Tx = s \to (x \times s)$ への射です。

$\mu_x$ は Haskell では join :: Monad m => m (m a) -> m a という関数に相当します。join は join x = x >>= id と定義できるもので、ここでは詳しく示しませんが、Stateモナドをあてはめて join の実装を評価すると、上図の$\mu_x$のふるまいと一致することが確認できます。 逆にjoin が定義されていれば、そこから >>= :: m a -> (a -> m b) -> m b を定義することができるので、このことを使って、以下で数学的な定義に基づいたState モナドを実装します。

数学的な定義に基づくStateモナドの実装
上記の(数学的な)状態モナドを Haskell で実装します。
数学的に構築する手順(ポイント)はざっくり以下になります。
1. 積函手 $F$ と 指数函手 $G$ を実装する。
2. $F, G$を合成して、自己函手 $T$ を実装する
3. (自然な)全単射$\phi = \phi_{x,a} : (x \times s) \to a  \hspace{2pt} \cong \hspace{2pt} x \to (s \to a)$ とその逆射 $\phi^{-1}$を実装する
4. 恒等射 $\rm id $ を$\phi$や$\phi^{-1}$で移すことにより、自然変換 $\eta$と$\varepsilon$ を実装する
5. 自然変換$\varepsilon$を使って、自然変換$\mu = G \varepsilon F$を実装する
6. 自然変換$\eta$と自然変換$\mu$を使って、Monad クラスのメソッドである、return と >>= を実装する
まず、積函手$F$($=\cdot \times s$)の実装です。
data F s x = F { run_f :: (x, s) }

instance Functor (F s) where
  fmap f (F (x, s)) = F (f x, s)

数学の定義の通り、対象 x が函手(Functor) F によって対象 (x, s) に、射 f :: x -> x'  が函手 F によって射 (fmap f) :: (x, s) -> (x', s) に写されることを表現しています。
次に、指数函手$G$($= s \to \cdot$)の実装です。
data G s a = G { run_g :: (s -> a) }

instance Functor (G s) where
  fmap f (G k) = G (f . k)

対象 a が G によって対象 s -> a に、射 f :: a -> b が函手 G によって射 (fmap f) :: (s -> a) -> (s -> b) に写されることを表現しています。
次に、自己函手$T$です。$T$は$GF$で定義できるので、
data T s x = T { run_t :: G s (F s x) }

instance Functor (T s) where
  fmap f (T k) = T $ fmap (fmap f) k

となります。少々扱いが面倒になりますが、$GF$のセットで$T$であるということを強調するために、G s (F s x) を新しい型 T として定義しています。
次に、式(6) の $\phi, \phi^{-1}$ を実装します。$\phi$ を phi, $\phi^{-1}$を psi として定義します。
phi :: ((F s x) -> a) -> (x -> (G s a))
phi f = \x -> G $ \s -> f (F (x, s))

psi :: (x -> (G s a)) -> ((F s x) -> a)
psi g = \(F (x, s)) -> run_g (g x) s

今定義した phi, psi を用いて、$\eta_x =\phi(\rm id_{\it Fx})$, $\varepsilon_a =\phi^{-1}(\rm id_{\it Ga})$ を実装します。
eta :: x -> (G s (F s x))
eta = phi id

epsilon :: (F s (G s a)) -> a
epsilon = psi id

次に$\mu_x$ の実装です。
mu :: (T s (T s x)) -> T s x
mu ttx = T $ fmap epsilon $ fmap (fmap run_t) (run_t ttx)

この実装を右から読んでいくと、fmap (fmap run_t) (run_t ttx) は T s (T s x) 型の値である ttx から T を一旦はがして G s (F s (G s (F s x))) 型の値を得る操作です。その結果に fmap epsilon (+ 型推論) を適用することで、数学における $ G\varepsilon F$ を再現しています。この結果として得られる G s (F s x) 型の値に T をつけて、最終的に T s x 型の値を返します。
あとは、今までに定義した eta と mu を使って、Haskell における Monad として状態モナド T を実装します。
instance Monad (T s) where
  return = fmap T $ eta
  m >>= f = mu $ (fmap f) m

これで、積函手と指数函手の随伴として実装されるモナド (数学的に構築した状態モナド)が 完成 しました。
使いやすいように、get と put も定義します。
construct_t :: (s -> (x, s)) -> T s x
construct_t f = T $ fmap F (G f)

get_t :: T s s
get_t = construct_t $ \s -> (s, s)

put_t :: s -> T s ()
put_t s = construct_t $ \_ -> ((), s)

実際に使ってみます。
gate x = do
  state <- get_t
  put_t (not state)
  return $ if state then (x + 1) else x

five_gates x = return x >>= gate >>= gate >>= gate >>= gate >>= gate

run_state t_monad s = run_f $ run_g (run_t t_monad) s

main = do
  input <- fmap (read . head) getArgs
  putStrLn $ show . fst $ run_state (five_gates (input :: Int)) False

これを ghc でコンパイルして実行すると、以下のようになります。
$ ghc math_state_monad.hs
[1 of 1] Compiling Main             ( math_state_monad.hs, math_state_monad.o )
Linking math_state_monad ...
$ ./math_state_monad 10
12
$ ./math_state_monad 30
32

入力した値に対して、+2 した値が実行結果として返ってきます。積函手と指数函手の随伴で作ったモナドが、冒頭に示した State モナドの使用例と同じようにふるまっており、期待した結果となりました。
よくよく考えると当たり前なのかもしれませんが、随伴を使ってプログラムとして実際に動作する Stateモナドを実装できるのは面白いと思いました。これによって、プログラミングにおけるモナドと数学におけるモナドの関係をより明確にできたような気がします。

付録1 (ソースコード)
本文中で示した、数学的に構築した State モナドの実装を以下に載せておきます。
import Control.Applicative
import System.Environment

data F s x = F { run_f :: (x, s) }

instance Functor (F s) where
  fmap f (F (x, s)) = F (f x, s)

data G s a = G { run_g :: (s -> a) }

instance Functor (G s) where
  fmap f (G k) = G (f . k)

data T s x = T { run_t :: G s (F s x) }

instance Functor (T s) where
  fmap f (T k) = T $ fmap (fmap f) k

phi :: ((F s x) -> a) -> (x -> (G s a))
phi f = \x -> G $ \s -> f (F (x, s))

psi :: (x -> (G s a)) -> ((F s x) -> a)
psi g = \(F (x, s)) -> run_g (g x) s

eta :: x -> (G s (F s x))
eta = phi id

epsilon :: (F s (G s a)) -> a
epsilon = psi id

mu :: (T s (T s x)) -> T s x
mu ttx = T $ fmap epsilon $ fmap (fmap run_t) (run_t ttx)

instance Monad (T s) where
  return = fmap T $ eta
  m >>= f = mu $ (fmap f) m

instance Applicative (T s) where
  pure = return
  g <*> m = g >>= flip fmap m

construct_t :: (s -> (x, s)) -> T s x
construct_t f = T $ fmap F (G f)

get_t :: T s s
get_t = construct_t $ \s -> (s, s)

put_t :: s -> T s ()
put_t s = construct_t $ \_ -> ((), s)

gate x = do
  state <- get_t
  put_t (not state)
  return $ if state then (x + 1) else x

five_gates x = return x >>= gate >>= gate >>= gate >>= gate >>= gate

run_state t_monad s = run_f $ run_g (run_t t_monad) s

main = do
  input <- fmap (read . head) getArgs
  putStrLn $ show . fst $ run_state (five_gates (input :: Int)) False


付録2(モナドの解釈について)
モナドとは何かと聞かれれば、モナドの定義に従うもの、としか答えようがないと思いますが、それだけだとイメージが湧きにくいと思います。こちらの動画 で1:31:30 あたりから解説されている、計算作用(Computational effect)にモナドを利用する、という考え方が分かりやすかったので、特に「状態モナド」を「副作用」とみなす、という観点からモナドの解釈を書いてみます。自分なりの解釈なので、参考にさせていただいた動画やその動画で紹介される元の論文での説明と意味合いが異なる部分があるかもしれませんがご容赦ください。
数学で言うところの関数(圏論での射)は、
f : a \to b

のように書きます。関数 $f$は、入力として集合(圏論で言うところの対象) $a$の元を1つ与えると、出力として集合 $b$ の元を1つ返すものです。数学における関数は、同じ入力($a$の元)に対しては、必ず同じ出力($b$の元)を返します。一方で、一般的なプログラミング言語における""関数""は、(例えばグローバル変数などの影響よって)同じ入力を与えても同じ出力を返すとは限りません。同じ入力を与えても必ずしも同じ結果を返すとは限らない性質のことを副作用と呼ぶことにします。
数学で扱う関数には副作用はありませんが、あえて圏論の世界でプログラミングにおける副作用を表現するために、状態モナドの自己函手 $T$ を、ここでは「副作用」と解釈してみます。そして、型 $a$ の入力に対して、副作用 $T$ の影響を受けた型$b$ の出力を返す関数$f$ を
f: a \to T b

のように表現します。$f$に$a$の元を与えると$b$の元を返すが、その結果には副作用$T$が伴っている、という解釈です。冒頭で示した""ゲート""で例えると分かりやすいのではないかと思いますが、ゲート1つ1つはいずれも、Int 型の値を入力として、内部状態に依存する(副作用を伴った) Int 型の値を出力するので、上の式の $a$ と $b$ にいずれも Int があてはまるというイメージです。このような副作用$T$を持つ複数の関数
\begin{align}
f_0 &: a_0 \to T a_1, \\
f_1 &: a_1 \to T a_2, \\
f_2 &: a_2 \to T a_3, \\
\cdots\\
f_{n-1} &: a_{n-1} \to T a_n \\
\end{align}

が与えられたときに、$f_0, f_1, \cdots$ を直接合成することはできませんが、$f_k$ に函手$T^k$ ($k=0,1,\cdots,n-1$)を作用させた、
\begin{align}
f_0 &: a_0 \to T a_1, \\
T f_1 &: Ta_1 \to T^2 a_2, \\
T^2 f_2 &: T^2 a_2 \to T^3 a_3, \\
\cdots\\
T^{n-1} f_{n-1} &: T^{n-1} a_{n-1} \to T^n a_n \\
\end{align}

であれば以下のように合成できて、
T^{n-1} f_{n-1} \circ \cdots \circ T^2 f_2 \circ T f_1 \circ f_0 : \hspace{10pt} a_0 \to T^n a_n

結果として型 $a_0$ を受け取って型 $T^n a_n$ を返す関数が得られます。
出力の型 $a_n$ に作用している函手 $T^n$ は、一見すると、最初に副作用としてみなした $T$ とは別物の副作用として扱うべきもののように見えます。
ここで一旦モナドの定義を見直してみます。モナドの定義に現れる、式(10)、式(11)は、2つの同じ函手$T$の合成 $T^2 = T \circ T$を ""結合"" して函手 $T$ を得る手段 $\mu$ があり、その結果は""結合""の順序に依存しない、というように解釈できます。 $T^n = T \circ T \circ \cdots \circ T$ にあてはめると、左の式の任意の箇所の合成 $\circ$ から順次 $\mu$ により""結合"" して、その結合の順序に依存しない形で、$T^n$ から $T$ への一意的な自然変換を得ることができます。
つまり $T$がモナドの函手であったことを思い出すと、$f_0, \cdots, f_{n-1}$ の合成によって最終的に得られる関数は、$\mu$ による""結合"" も込みで、合成前の関数の形 $f : a \to T b$ になるということです。
以上をまとめると、状態モナドの自己函手 $T$ を副作用と解釈すれば、副作用を伴う計算は $f:a \to T b$ と表現でき、副作用のある計算が与えられたときにそこから副作用のない計算を得ること($a \to T b$ から $a \to b$ を得ること)はできませんが、そのような副作用を持つ計算同士であればそれらを好きなようにつなぎ合わせたより大きな計算を、同じ副作用の枠組みの中で新たに構築することができるので、圏論の世界で副作用を伴う計算をモデル化できていると言えるのだと思います。
上記は、$a$ から $b$ への関数の副作用を$T$を使って表現しているのであって、$f: a \to T b$ 自体は副作用のない数学的な意味での関数(圏論での射)であることに注意する必要があります。$f: a \to T b$ が圏論での射として扱えるために、純粋関数型言語である Haskell においても、(本物の)副作用を IO モナドという形で扱えているのだと思います。
■終わり■




「小さな集合」について。あまり詳しく説明はできませんが、「小さな」集合というときには、(有限集合のように)数の少ない集合のことを指すのではなく、自然数全体の集合や実数全体の集合などのような無限集合も含みます。集合論における標準的な演算で閉じている十分大きな集合である「ユニバース」が存在すると仮定して、集合がそのユニバースのメンバであるときに「小さな集合」と言うらしいです。プログラミングで扱う範囲では特に気にする必要はない(プログラミングで扱う型・関数はそのユニバースにすべて含まれていると考えてよい)と思うので、本文中では、本来「小さな」をつけるべきところでも、それを省略して書いています。 ↩



",True,https://qiita.com/sg-matsumoto/items/de6874149ccbeaedac3e
"コーディングスタイルメモと合わせて C++ を記述する際の指針も書いていく．

C++ の利用理由
まず， _C++ _を使う以前に，C++ 以外の適切な言語がないか検討する．C++ はややこしいことで有名であるため．後に出た言語の方が良く設計されている場合が多い．本当に C++ を使う必要があるのか? Java，Python，ruby，VB.Net，C# 他の選択肢は? 速度重視のときは C++か?

方針

メンテナンスができること


自分1人だけなら好き勝手書いていいけど，他の人が居る場合はみんなに分かるコードで．でも，プロジェクトを通してスキルアップができるといいね！


エラーはできるだけコンパイル時に出るようにすること


エラー系に関しては，実行時エラーではなくコンパイル時にエラーが発生するようにするべきである．その方が発見しやすく，原因も明確でかつ，コード修正も容易である．


速度より可読性を


基本的に可読性の良いコードは早いらしいよ




C++ 使う上でメンバに要求したいこと

絶対理解して欲しいこと：理解してもらなわないと困る，C++ 使う意味が無い


クラス：理解しなかったら C++ を使う意味が無い


委譲：クラスの中にクラス変数を宣言して宣言したクラスに処理を任せること，継承より委譲を検討する
継承：クラスの大きな機能，上位クラスの機能をそのままに下位クラスで機能を拡張する．別に委譲でいいのではと思えば委譲で実現する．


名前空間：グローバル定数をなくす，関数，クラスの衝突をなくす
参照：ポインタのように使えてポインタ以上に使いやすい
C++ のキャスト(static_cast)：C のキャストはダメ，C++ ではキャストの種類が明確になっており，特に上位クラスから下位クラスに下る dynamic_cast があるため，C風キャストではそれらどのキャストが使われたか非常に不明確なため．


そうでもないもの，100% 理解しなくていいけどある程度知っておいてよね．


テンプレート
演算子のオーバーロード
dynamic_cast


無理して理解しなくていいこと


多重継承
テンプレートメタ




コンパイラの設定
*全ての警告を ON にして，エラーとする *．警告は，潜在的なエラーとなり得る．そのため，全ての警告について正しく対処しなければならない．Visual Studio なら /W4 /WX

C++11
C++11 を使えるなら当然使うべきである．スマートポインタ，nullptr，constexpr，static_assert が利用できるのは大きいと思う．

積極的に使いたい新機能 (ある程度機能が単純そう)


nullptr：どこも指し示さない null が明確になる
constexpr：const よりコンパイル時定数であることを強調できる
スマートポインタ(shared_ptr, unique_ptr)：ポインタの管理が楽になる
static_assert：うまく使えばコンパイル時に定義エラーなどの誤りを検出できる
enum class：暗黙変換が無くなるため．


考えて使う必要があると思われる機能 (理解してないから)


ラムダ式：果たしてチーム全員が正しく理解できるか．逆にややこしくならないか．使用するにしても限定的にする必要があるのではないか?
move：右辺値参照は結構ややこしいと思う．すんなり理解できなかった．


よくわかんない(理解している気になってるだけ)


auto：イテレーターの時は使いたい，でも，使いすぎると意味不明になりそう
decltype：変数の型を指定できる．テンプレートの返り値の型推論?




ライブラリ
積極的に利用するべきだが，学習コストが高いライブラリは控える必要がある．特に boost はテンプレートメタプログラミングを多用しており学習コストが高くなりがちになる．取捨選択が大事になってくるチームのスキルに合わせて考える(google が使用に制限を与えているというのは大きいのではないか?)．C++11 では，標準ライブラリが充実するため boost 依存がなくなるのでどんどん減っていくと予想．

メンテナンスが継続的に行われているか
ドキュメントが充実しているか
世間的にメジャーであるか(例えば，分野で検索して選択して多数上がるのか)
分かりやすいか

そもそも，難しくて高度なライブラリを利用するぐらいなら，他の言語を利用した方がいいのでは?
少なくともスマートポインタは，積極的に利用しても良い．スマートポインタは，new，delete などのメモリ管理の煩わしさから解放してくれる．C++11 以降では，標準ライブラリに組み込まれており，#include <memory> から利用できる．

参照
C++ から参照型が導入された．変数の扱うようにできるが実際はポインタというもの．ポインタをあまり意識することなくポインタを扱えるので積極的に利用したい．
int val = 10;
int &ref = val;    // 参照，または，エイリアス(別名) の宣言，val の 別名と捉えると分かりやすい．val の値を ref でも取り出したり代入できたりする．
int &ref2;         // NG．参照は，必ず元となる変数を指定する必要がある．
ref = 100;         // ref は val と等価なため val にも影響する．

std::cout << ""val(value = "" << val << "", "" << ""address = "" << &val << std::endl;
std::cout << ""ref(value = "" << ref << "", "" << ""address = "" << &ref << std::endl;

例えば，以下の結果となり2つの変数が全く同じポインタを指していることが分かる．
val(value = 100, address = 00F9FA84)
ref(value = 100, address = 00F9FA84)


const 参照
参照の利点としてあげられるのは，メソッドの引数に用いるconst 参照である．const 参照は，入力であることを明確にし，ポインタなみのコピーコストで，さらに NULL の心配が少ない(Effective C++)．
void print(const std::string &str)
{
    str = ""abc"";    // NG. const なのでコンパイル時エラーがでる．
    std::cout << str << std::endl;
}


スマートポインタの利用
ポインタは，基本的にスマートポインタでラップする．スマートポインタは，new, delete のメモリ管理の煩わしさを無くしてくれる便利なポインタのラッパクラスである．



項目
boost
C++




ヘッダ
#include <boost/smart_ptr.hpp>
#include <memory>


所有権が単一
boost::shared_ptr
std::unique_ptr


所有権を共有
boost::shared_pt
std::shared_ptr



shared_ptr, unique_ptr は，所有者が明確でできる限りこれらを使用すること．コピーされることなくスコープから抜けると確実にメモリを解放してくれる．
int main()
{
    {
        std::unique_ptr<int> p  = std::make_unique<int>(100);
        std::unique_ptr<int> p2 = p;    // NG. 所有権は共有できない
    } // ここで，p は解放される


    std::shared_ptr<int> p = std::make_shared<int>(100);
    {
        std::shared_ptr<int> p2 = p;   // OK. shared_ptr は共有を許す
    } // この時点で p2 は解放されるが，p が解放されないためポインタは残る
    std::cout << *p << std::endl;
}


shared_ptr を C++08 では，boost，C++11 では std に切り替える
shared_ptr を C++08 では，boost::shared_ptr，C++11 では，std::shared_ptr としたいときは多いと思う．そのときは，自身の namespace で using *::shared_ptr とすればよい．
 #if __cplusplus >= 201103L  
namespace my_project {
#include <memory>
using std::shared_ptr;
using std::make_shared;
using std::static_pointer_cast;
using std::dynamic_pointer_cast;
using std::const_pointer_cast;
 } // namespace my_project
 #else
namespace my_project {
#include <boost/shared_ptr.hpp>
using boost::shared_ptr;
using boost::make_shared;
using boost::static_pointer_cast;
using boost::dynamic_pointer_cast;
using boost::const_pointer_cast;
} // namespace my_project
 #endif


 int main()
 {
     using namespace my_project;
     shared_ptr<int> p = make_shared<int>(100);
     return 0;
 }


関数

1行関数を恐れない
return のみの関数を恐れない．意味が明確になるのであれば，return のみの関数でも実装する．ただし，冗長的なのは禁止．このさじ加減は結構難しい．
/*
 * おそらくこれは冗長的
 * 
 * 普通に str + suffix と書いた方が伝わるか?
 */
std::string appedSuffix(const std::string &str, const std::string &suffix)
{
    return str + suffix;
}


/*
 * 画像保存の際 photo_<name>.png とフォーマットが決まっているとき
 * フォーマットに従った名前が欲しいときには有効か?
 */
const std::string PREFIX   = ""photo_"";
const std::string EXT_NAME = "".png"";
std::string createImageFilename(const std::string &name)
{
    return PREFIX + name + EXT_NAME;
}


テンプレート
C++ において最も注意深く利用しなければならない機能と思う．はまればテンプレートメタプログラミングになり，誰も理解できないコードが増えることに･･･．一方，いわゆるジェネリックとして利用すればオーバーロードによる複数の型定義が必要なくなる．
// まだ，このような型で共通の操作を行うようなテンプレートの使い方なら良いと思う．
template <typename T>
const T & addTwo(const T &value)
{
    return value + static_cast<T>(2);
}

// また，ジェネリック型のようにいろいろな型を保存するコンテナ的な扱いでも良いと思う
template <typename T>
class Point2D
{
    T x;
    T y;
};



テンプレートメタプログラミング
正しく理解するのに時間を要するので禁止．メンテナンスできる？<, > の荒らしと格闘できるか?

クラス
基本的に コピーコンストラクタ，代入演算子は禁止する．
// C++08 のとき
class A 
{
public:
    A();
    ~A();
private:
    A(const A &rhs);
    A& operator=(const A &rhs);
};


// C++11 のとき
class A
{
public:
    A() = default;
    ~A() = default;
private:

    // 特に move は明示的に禁止にしないと痛い目見そう
    A(A const&) = delete;
    A(A&&) = delete;
    A& operator =(A const&) = delete;
    A& operator =(A&&) = delete;
}

",True,https://qiita.com/shirakawa4756/items/7430f447883a74831bf9
"

迷路を解く
アルゴリズムの基本問題に迷路を解くというものがあります。
迷路をグラフの一種とすることで、グラフの探索アルゴリズムが使えます。
基礎的なグラフの探索アルゴリズムに幅優先探索と、深さ優先探索があります。

幅優先探索と深さ優先探索
それぞれ、どのように動作するのか見てみましょう。
まずは、空の迷路を幅優先探索で解いてみます。
左上をスタート、右下をゴールとすると、探索済みのセルを水色に塗ってます。
幅優先探索だと水が広がるように、探索が進んでいきます。これは、スタート地点から近いセルの順に見ていくというアルゴリズムだからです。余談ですが、幅優先探索は絵の塗りつぶしにも使われてます。
幅優先探索が迷路に向いてるのは、それぞれのセルに最短の前のセルと、そこまでの移動回数を保存しておくことで、迷路を解くと同時に最短経路も割り出せるからです。

こちらは、猪突猛進　　深さ優先探索です。とにかく自分がいる場所から先に進んでいきます。
どちらのアルゴリズムでも全部のセルを探索してますが、深さ優先探索の場合の経路は無駄が大きいですね。

まぁ、でも迷路によっては特に変わりません
こちらは、幅優先

こっちは、深さ優先

ゴールがスタート地点より、もっとも遠い場所にある場合で、最短距離が不要な場合は
そんなに変わらなく見えますね。
ゴールが近いと、幅優先探索の方が少ないステップで見つかります。
こちらは、深さ優先探索に不利な例
幅優先

深さ優先


実装
深さ優先探索は、再帰関数で書くことが多いのですが、個人的にはWhileループを使った方法が
オススメです。一旦、やり方を覚えてしまえば、データ構造を変えることで、深さ優先、幅優先どちらもシンプルに書けるからです。
まず、幅優先探索から
基本的に、行けるセルをチェックして、Queueに積んでいきます。
Queueの場合、積まれたセルが一番後回しになるため、近い順に迷路をチェックしていけます。
func BFS(movie []image.Image, m CellMap) []image.Image {
    var n Point
    // Queue に初期値を積む
    queue := []Point{Point{1, 1}}
    for len(queue) > 0 {
        // queueから取り出す
        n, queue = queue[0], queue[1:]
        // 行けるセルとをチェック
        for _, d := range directions {
            if m[n.X+d.X][n.Y+d.Y] == GOAL {
                return movie
            }
            if m[n.X+d.X][n.Y+d.Y] == 0 {
                m[n.X+d.X][n.Y+d.Y] = VISITED
                // Queueに積む
                queue = append(queue, Point{n.X + d.X, n.Y + d.Y})
            }
        }
        movie = shot(movie, m) // GIFアニメを一コマ撮影
    }
    movie = shot(movie, m) // GIFアニメを一コマ撮影
    return movie
}

こちらは、深さ優先。
Queueの代わりに、Stackを使います。Stackでは、最後に追加した要素が、次で使われます。
ですので、見つかった順に深堀していくことができます。
func DFS(movie []image.Image, m CellMap) []image.Image {
    stack := []Point{Point{1, 1}}
    var n Point
    for len(stack) > 0 {
        n, stack = stack[0], stack[1:]
        m[n.X][n.Y] = VISITED
        for _, d := range directions {
            if m[n.X+d.X][n.Y+d.Y] == GOAL {
                return movie
            }
            if m[n.X+d.X][n.Y+d.Y] == EMPTY {
                stack = append([]Point{Point{n.X + d.X, n.Y + d.Y}}, stack...)
            }
        }
        movie = shot(movie, m)
    }
    movie = shot(movie, m)
    return movie
}

幅優先はQueue 深さ優先は、Stackとすることで基本的なコードの構造は全く同じですし、
デバッグも容易です。

余談
余談ですが、このデータ構造を優先度付きQueue(Priority Queue)にすると、ダイクストラ法を実装できます。ダイクストラ法は、グラフの辺に重み付けがある場合に使う最短経路探索アルゴリズムです。
GoでのStackとQueueの実装にはもっとうまい方法があります。上記のやり方だと、メモリがうまくGCされないのという欠点があります。 
リンクリストを使うのが良いでしょう。　container/listを使った例
スライスを使った例でも、stackも最後の要素を使うようにすれば、append部分を早くできますね。
",True,https://qiita.com/nati-ueno/items/a789095aff0aec10d5d0
"経路探索の代表的なアルゴリズムであるダイクストラ法とA*探索の、探索過程を可視化してみました。

アルゴリズムの解説
ダイクストラ法はスタートから全方位に探索してスタートから近い順に経路を確定させていき、ゴールに到達するまでこれを繰り返します。A*探索はゴールまでの距離を推定するヒューリスティック関数を用いて探索を効率化します。
A*探索では、探索ノードnに対して評価値f(n) = g(n) + h(n)が小さい順に処理します。ここで


g(n) = スタートから探索ノードnまでの実際の経路コスト

h(n) = 探索ノードnからゴールまでの推定経路コスト (ヒューリスティック)

です。探索が終了するまでゴールまでの経路コストはわからないので、h(n)をどう与えるかというところで探索の効率が変わってくるのですが、ユークリッド距離を使用した幾何学的な最短経路探索ではゴールまでの直線距離を使うことができます。また、h(n) = 0の場合がダイクストラ法になります。
評価値f(n)が小さい順に探索ノードを取り出すことが必要ですが、単純に配列に入れると、最小値を検索するとき・要素を削除するときに配列の長さに比例した時間がかかり、あまり効率が良くありません。要素の追加・最小値の検索と削除を高速に(対数時間で)処理するデータ構造は “優先順位付きキュー (priority queue)” と呼ばれ、2分ヒープによる実装がもっとも有名です。

探索コード
A*探索の Javascript コードを以下に示します1。元のコードをここに記述しやすいように変更しています。queueはfを評価値とする優先順位付きキューです。
// すべての頂点に Closed でないとマークする
for (let vertex of vertices)
    vertex.isClosed = false;

let start = {};

start_node.parent = null;
start_node.target = start;

start_node.g = 0;
start_node.f = 0; // 計算不要(すぐにキューから取り除かれるため)

let queue = new PriorityQueue();
queue.push(start_node);

while (!queue.isEmpty()) {

    // 最小の評価値 f をもつ探索ノードを取り出して、queue から削除する
    let node = queue.pop();

    // node がすでに Closed な頂点を指している場合は無視する
    if (node.target.isClosed) continue;

    // node が指す頂点に Closed とマークする
    node.target.isClosed = true;

    // スタートから node.target への最短経路は node.parent から来た
    node.target.previous = node.parent;

    // ゴールに到達したら、来た道をたどって経路を完成させる
    if (node.target === goal) {
        let total_path = [];
        let vertex = goal;
        while (vertex !== null) {
            total_path.push(vertex);
            vertex = vertex.previous;
        }
        return total_path;
    }

    // node.target に隣接する各ノードについて...
    for (neighbor of node.target.neighbors) {

        // Closed な頂点は展開しなくてよい
        if (neighbor.is_closed) continue;

        // 新たな探索ノードを生成して、キューに追加する
        let new_node = {};

        new_node.parent = node.target;
        new_node.target = neighbor;

        new_node.g = node.g + real_cost(node.target, neighbor);
        new_node.f = new_node.g + heuristic_cost(neighbor, goal);

        queue.push(new_node);
    }
}

出発地点からの最短経路が確定している頂点にはClosedとマークします。Closedな頂点には最短経路が直前にどの頂点を通ったかを記録します。これを出発地点までたどることで最短経路が(逆向きで)得られます。
この情報はグラフデータの頂点に記録することもできますが、並列実行するときなどグラフデータに記録したくないときはHashMap的なものを使う形になると思います。後者の場合、HashMapにキーが存在する頂点が Closed となりますので、Closed であることを単独で管理する必要はありません。ここでは頂点に直接記録しています。なお、キューに挿入済みだが Closed でない頂点を Open であるといいます。
ここで探索ノードsearch_nodeはグラフの頂点とは異なることに注意してください。search_nodeは対応する頂点targetのほかに、どの頂点から来たかを示すparentを保持しています。targetへの経路を複数キューに入れておき、その中でキューから最初に出てきたsearch_nodeに対応するparentが、最短経路がどこから来たのかを表します。なぜなら、targetが同じならヒューリスティックhの値は等しいので、評価値f=g+hが最小というのは経路コストgが最小であるからです。
※インターネットで調べると探索ノードがparentを保持していない実装例も結構ありました。この場合、Openな頂点に対しては、新たに得られたfとそれまでのfを比較し、必要に応じてf, gと経路を更新する必要があります2。したがって、使用する priority queue は優先順位の変更、または要素の削除に対応している必要があります。(なお隣接頂点間の経路コストがすべて等しい場合、これは起こらないので無視できます。)上の擬似コードで示した手法ではこれは必要ありませんが、メモリ使用量は少し増えると思います。

許容的ヒューリスティック
すべての経路を調べ尽くしたわけではなく、経路コストgが最小といってもsearch_nodeが生成された中での最小なので、「本当にこれが最短経路か？」という疑問が出てきます。
実は、ヒューリスティックがゴールまでの実際の経路コストを上回ることがない、という許容的(admissible) と呼ばれる条件を満たせば、A*探索は常に真の最短経路を見つけることが保証されます。これを理解するため、次の図を考えます。

S から G への最短経路を赤の経路S→A→B→Gとします。この経路を進むときの探索ノードの評価値を考えると、
f(S→A)     = g(S→A)     + h(A) ≤ g(S→A→B→G)
f(S→A→B)   = g(S→A→B)   + h(B) ≤ g(S→A→B→G)
f(S→A→B→G) = g(S→A→B→G) + h(G) = g(S→A→B→G)

となります(探索ノードを経路で表しています)。ここで、最短ではない経路、例えばS→C→D→E→Gを考えます。
f(S→C→D→E→G) = g(S→C→D→E→G) > g(S→A→B→G)

なので、この値はf(S→A)やf(S→A→B)より大きくなり、したがってS→C→D→E→Gの経路が完成する前に

(A がClosedになる) → (B がClosedになる) → (S→A→B→G の経路が完成する)

が起こり、最短経路S→A→B→Gが先に見つかることになります。これはS→C→D→E→Gの代わりに任意の非最短経路について成り立つので、非最短経路が先に見つかることはありません。もしヒューリスティックが許容的でなければ、
f(S→A) > f(S→C→D→E→G)

のようなことが起こり得ます。この場合、頂点Aの評価値の大きさがボトルネックになって S→A→B→G の経路を見つけられないことになります。
直線距離によるヒューリスティックは許容的なので、この場合のA*探索が見つける経路は真の最短経路になります。

成果物
成果物はこちら。Google Chrome, Firefox, Safari, Microsoft Edge で動作確認済みですが、Microsoft Edge ではタッチパネルには非対応となります。ES6で書いているため Internet Explorer には対応しておりません。
https://raysphere24.github.io/AStarSearchDemo/
緑色で表示した辺は SearchNode が生成された辺を表します。また、2点間の経路探索のほかに、1点を指定して最短経路のツリーを表示する機能も用意しました。モデルは有名な Stanford Bunny です。
ソースコードはこちらにあります。
https://github.com/Raysphere24/AStarSearchDemo

使用した言語・ライブラリ
ブラウザ上で体験できるように、言語は Javascript を採用しました3。
JavaScript には標準の priority queue が無いので、mourner 氏の tinyqueue を使用しました。
可視化には three.js を使用しました。これは WebGL をラップして簡単に使えるようにしたもので、シェーダーなどを書かなくても使えるようになっています。グラフィックスオブジェクトをライブラリが保持して描画する、Retained Mode タイプのライブラリです。

実行例

ダイクストラ法とA*探索の比較


経路が直線的であれば、探索の効率は大きく違います。

ゴールを少し動かしただけで経路が大きく変わる例 (A*探索)


尻尾側がゴールです。ゴールを隣の頂点に動かすと経路が大きく変わることがわかります。
また、この例では直線距離と真の距離との誤差が大きいため、A*探索でもかなり広範囲に探索していることがわかります。

上の状況を最短経路木で観察する

尻尾に到達する経路が両脇を通っていることがわかります。

参考資料

A* search algorithm - Wikipedia
エージェントアプローチ 人工知能 第2版





当初擬似コードを投稿しましたが、シンタックスハイライトの要望がきたため Javascript の文法に変更しました。 ↩


WikipediaのA*探索の記事にある擬似コードもこの戦略をとっています。 ↩


もともとC++とOpenGLを用いて実装したものを移植しました。 ↩



",True,https://qiita.com/Raysphere24/items/5892cd8e623d20fcb308
"

概要
ゲームAIに関して勉強し始めたので，その備忘録．
本記事では，グラフ上の最良経路を算出するA*アルゴリズムと，
ノードに動的な重み付けを行う影響マップについて記述する．
本記事は【ゲームAI】基本的な経路探索のウェイポイントナビゲーションの続きとなる．
ウェイポイントナビゲーションについて知っている方は見なくてよいが，
知らない方は軽く見ておくと良いかもしれない．

参考
ありきたりではあるが以下の書籍を用いた．
ゲーム開発者のためのAI入門
また，以下のサイトがとても分かりやすい，且つ実践的だと思う．
ゲームAI -基礎編- 『知識表現と影響マップ』

事前準備
ベクトル計算用の構造体，便利関数の準備と，
角度計算用のマクロの準備を行う．

//角度計算用マクロ.
#define L_PI        (3.1415f)
#define L_2PI       (6.2830f)
#define L_H_DEG     (180.0000f)
#define L_DEG       (360.0000f)
#define DEG2RAD(e) ((e)*(L_PI)/(L_H_DEG))
#define RAD2DEG(e) ((e)*(L_H_DEG)/(L_PI))
#define ADJUST_RAD(e) (((e)<(0.0000f))?(e)+(L_2PI):((e)>(L_2PI))?(e)-(L_2PI):(e))
#define ADJUST_DEG(e) (((e)<(0.0000f))?(e)+(L_DEG):((e)>(L_DEG))?(e)-(L_DEG):(e))

#define CLIP(e,l,h) (min(max(e,l),h))

//ベクトル構造体.
#define VECTOR SVector2D<float>
template <class T>
struct SVector2D
{
    typedef T DataType;
    T x;
    T y;
    SVector2D(){ Init(); }
    void Init()
    {
        x = T();
        y = T();
    }
    SVector2D   operator +  ( const SVector2D& e ) const { SVector2D tmp; tmp.x = x + e.x; tmp.y = y + e.y; return tmp; }
    SVector2D&  operator += ( const SVector2D& e ){ x += e.x; y += e.y; return (*this); }
    SVector2D   operator -  ( const SVector2D& e ) const { SVector2D tmp; tmp.x = x - e.x; tmp.y = y - e.y; return tmp; }
    SVector2D&  operator -= ( const SVector2D& e ){ x -= e.x; y -= e.y; return (*this); }
    T           operator *  ( const SVector2D& e ) const { return ( x * e.x ) + ( y * e.y ); }
    SVector2D&  operator *= ( const int e ){ x *= e; y *= e; return (*this); }
    SVector2D&  operator *= ( const float e ){ x *= e; y *= e; return (*this); }
    SVector2D&  operator /= ( const int e ){ x /= e; y /= e; return (*this); }
    SVector2D&  operator /= ( const float e ){ x /= e; y /= e; return (*this); }
};

//数学関連の関数群.
namespace LMath
{
    VECTOR::DataType GetScalar( VECTOR vec )
    {
        return sqrtf( vec.x * vec.x + vec.y * vec.y );
    }

    VECTOR Normalize( VECTOR vec )
    {
        const VECTOR::DataType vecLen = GetScalar( vec );
        vec.x /= vecLen;
        vec.y /= vecLen;
        return vec;
    }

    VECTOR Normalize( VECTOR from, VECTOR to )
    {
        VECTOR tmp = to - from;
        return Normalize( tmp );
    }

    float GetRotateRad( const float from, const float to )
    {
        //角度候補1.
        const float dir1st      = ( to - from );
        const float dir1stVal   = fabsf( dir1st );

        //角度候補2.
        const float dir2ndVal   = ( L_2PI - dir1stVal );
        const float dir2nd      = ((dir1st>=0.0f)?-1.0f:1.0f) * dir2ndVal;

        //絶対値が小さい方を採用.
        return ( dir1stVal > dir2ndVal ) ? dir2nd : dir1st;
    }

    bool IsCollisionCircle( const VECTOR& pos1, const VECTOR& pos2, const float r )
    {
        VECTOR tmp = pos1 - pos2;
        return ( GetScalar( tmp ) < r );
    }
};


また，今回は優先度付きキューを使用する必要があるので，バイナリヒープも用意しておく．
namespace Util
{
    enum BH_COMP_TYPE
    {
        BH_COMP_TYPE_MIN = 0,
        BH_COMP_TYPE_MAX,
        BH_COMP_TYPE_NUM,
        BH_COMP_TYPE_INVALID = -1,
    };
    static bool IsValid( BH_COMP_TYPE e ){ return ( 0 <= e && e < BH_COMP_TYPE_NUM ); }

    template<class T>
    class BinaryHeap
    {
    public:
        typedef T DataType;

    public:
        BinaryHeap() : m_type( BH_COMP_TYPE_INVALID ), m_buf( NULL ), m_size( 0 ), m_count( 0 ){}
        ~BinaryHeap(){}

    public:
        void Init( BH_COMP_TYPE type, const int size )
        { 
            m_type  = type; 
            m_buf   = new T[size]; 
            m_size  = size;
            m_count = 0;
        }
        void Term(){ if( m_buf ){ delete [] m_buf; m_buf = NULL; } }

    public:
        void Push( T e )
        {
            if( IsFull() ){ return; }
            int self = m_count;
            while( self > 0 ){
                const int parent = ( self - 1 ) / 2;        //親.
                if( !bComp( e, m_buf[parent] ) ){ break; }  //親の方が上にくるべきなら終了.
                m_buf[self] = m_buf[parent];                //親を移動.
                self = parent;                              //自分を親の位置に移動.
            }
            m_buf[ self ] = e;
            m_count++;
        }
        bool bPop( T& e )
        {
            if( IsEmpty() ){ return false; }
            e           = m_buf[0];
            m_buf[0]    = m_buf[--m_count];
            int self    = 0;
            while( self < m_count ){
                const int child1 = ( self * 2 ) + 1;
                const int child2 = ( self * 2 ) + 2;
                int next = self;
                if( child1 < m_count ){ if( bComp( m_buf[child1], m_buf[next] ) ){ next = child1; } }
                if( child2 < m_count ){ if( bComp( m_buf[child2], m_buf[next] ) ){ next = child2; } }
                if( next <= self ){ break; }
                T tmp = m_buf[self];
                m_buf[self] = m_buf[next];
                m_buf[next] = tmp;
                self = next;
            }
            return true;
        }

    private:
        bool bComp( const T& child, const T& parent )
        {
            switch( m_type )
            {
            case BH_COMP_TYPE_MIN:  return ( child < parent );
            case BH_COMP_TYPE_MAX:  return ( child > parent );
            default:    return false;
            }
        }

    public:
        bool IsFull() const { return ( m_count >= m_size ); }
        bool IsEmpty() const { return ( m_count <= 0 ); }

    private:
        BH_COMP_TYPE    m_type;
        T*              m_buf;
        int             m_size;
        int             m_count;
    };
};


A*アルゴリズム

概要
グラフ探索の原理としては，ダイクストラアルゴリズムと同じであるが，
コストの扱い方が少し特殊になっている．
(ダイクストラ法：http://www.deqnotes.net/acmicpc/dijkstra/)
本来，ダイクストラアルゴリズムでは，ノード間のエッジコストを元に最短経路を算出する．
すなわち以下の式が成り立つ．
cost = edgeCost
しかしながら，A*アルゴリズムではエッジコストにヒューリスティックコストを加えたものをコストとして扱う．
すなわち以下の式のようになる．
cost = edgeCost + heuCost
A*アルゴリズムはヒューリスティックコストが入る関係上，
最短経路探索アルゴリズムではなく，最良経路探索アルゴリズムとなる．

ヒューリスティックコスト
ヒューリスティックとは，心理学においては「経験則」を指し，
計算機科学においては「精度保証のない近似解」を指す．
根本的な部分は同じことを表しており，「直感や経験に基いて精度の高い解を示すこと」である．
つまりヒューリスティックコストとは，
経路計算用コストとしてぱっとみ正しそうなコストのことである．
A*アルゴリズムでは，最短経路を計算する場合に，
目標地点と自分を繋いだ直線距離をヒューリスティックコストとして扱うことが多い．
目標地点から遠いノードは，ぱっとみ最短経路ではなさそうだし，
目標地点に近いノードは，ぱっとみ最短経路っぽいからだ．
しかしながら，間に障害物があって通れない，道が途絶えている，など
目標地点に近いからといって最短経路になるわけではない．
ヒューリスティックコストは，目的に応じた正確なコストを選択できれば，
最良経路探索の一助となってくれる．
ゲームAIにおいては，明らかに危険な道を突っ切るAIよりも，
多少遠回りでも安全な道を行くAIの方が賢く見える，といったことがあるので，
最良経路探索は相性が良い．

影響マップ
ゲームでは，敵との距離，敵の視野範囲など，ヒューリスティックコストとして使用できる情報は多い．
しかし，これらの情報はゲーム中に変動するものが多いため，事前にノードに設定しておいたものを用いることはできない．
(もちろん，情報の種類によっては，事前に設定したものを用いることはできる．)
(俗に，静的，焼き付けと呼ばれる手法は，事前に計算してデータを保持しておくことを指す．)
そこで，ゲーム状況に合わせて動的にヒューリスティックコストを設定しよう，というのが，影響マップである．
以下に例を挙げる．赤に近いノードはコストが高く，青に近いノードはコストが低い．
目標との距離
(目標に近い地点ほどコストが低く，遠い地点ほどコストが高い)

目標の視野範囲
(視野範囲に入っている地点ほどコストが高く，外れている地点ほどコストが低い)

上記二例を合成したもの
(視野に入っている所はコストが高めだが，死角はとても低い．距離が離れると一定のコストとなる．)
(合成比率は，距離が30%，視野範囲が70%)


実装
影響マップを使用したA*アルゴリズムの実装を以下に示す．
const int WAY_POINT_MAX_NUM = 20;
const float INF_COST = (1 << 29); 

enum INFLUENCE_MAP_TYPE
{
    INFLUENCE_MAP_TYPE_DIST = 0,
    INFLUENCE_MAP_TYPE_VIEW,
    INFLUENCE_MAP_TYPE_NUM,
    INFLUENCE_MAP_TYPE_INVALID = -1,
};
static bool IsValid( INFLUENCE_MAP_TYPE e ){ return ( 0 <= e && e < INFLUENCE_MAP_TYPE_INVALID ); }

//ヒューリスティックコスト取得.
//(ヒューリスティックコストはすでに計算済み想定).
float g_influenceMap[ INFLUENCE_MAP_TYPE_NUM ][ WAY_POINT_MAX_NUM ];
float GetHeuCost()
{
    float tmp = 0;
    tmp += g_influenceMap[INFLUENCE_MAP_TYPE_DIST][n] * 0.3f;
    tmp += g_influenceMap[INFLUENCE_MAP_TYPE_VIEW][n] * 0.7f;
    return tmp;
}

//最短経路計算.
//(g_edgeWeightはノードが繋がっていることを表している．設定済み想定．地形などに応じてコストを設定する場合は，ここを調整する).
float g_shortestPath[WAY_POINT_MAX_NUM][WAY_POINT_MAX_NUM];
float g_edgeWeight[WAY_POINT_MAX_NUM][WAY_POINT_MAX_NUM];
void UpdateShortestPath( const int startNode, const int endNode )
{
    //初期化.
    for( int i = 0; i < WAY_POINT_MAX_NUM; ++i ){
        for( int j = 0; j < WAY_POINT_MAX_NUM; ++j ){
            g_shortestPath[i][j] = INF_COST;
        }
    }

    //データ.
    struct SData
    {
        int nodeIndex;
        float cost;
        SData(){ Init(); }
        void Init(){ nodeIndex = -1; cost = 0.0f; }
        bool operator < ( const SData& e ) const { return (cost) < (e.cost); }
        bool operator > ( const SData& e ) const { return (cost) > (e.cost); }
    };

    //キューを用意.
    Util::BinaryHeap< SData > queue;
    queue.Init( Util::BH_COMP_TYPE_MIN, WAY_POINT_MAX_NUM );

    //A*.
    {
        g_shortestPath[startNode][startNode] = 0;

        //始点を入れてスタート.
        SData start; 
        start.nodeIndex = startNode;
        start.cost = 0;
        queue.Push( start );

        while( !queue.IsEmpty() ){
            SData s;
            if( !queue.bPop( s ) ){ continue; }

            for( int i = 0; i < WAY_POINT_MAX_NUM; ++i ){
                if( s.nodeIndex == i )                          { continue; }   //自分.
                if( g_edgeWeight[s.nodeIndex][i] >= INF_COST )  { continue; }   //辺がつながっていない.
                if( g_shortestPath[i][i] < INF_COST )           { continue; }   //すでに訪れている.

                g_shortestPath[i][i] = s.cost + g_edgeWeight[s.nodeIndex][i] + GetHeuCost( i );

                SData d;
                d.nodeIndex = i;
                d.cost      = g_shortestPath[i][i];
                queue.Push( d );
            }
        }
    }

    //経路復元.
    {
        int goal = endNode;
        while( goal != startNode ){
            bool bSuccess = false;

            for( int i = 0; i < WAY_POINT_MAX_NUM; ++i ){
                if( goal == i )                         { continue; }   //自分.
                if( g_edgeWeight[i][goal] >= INF_COST ) { continue; }   //辺がつながっていない.

                const float startCost = g_shortestPath[goal][goal] - ( g_edgeWeight[i][goal] + GetHeuCost( goal ) );
                if( abs( g_shortestPath[i][i] - startCost ) < 0.001f ){
                    //一つ前のノードを発見したので，最短パス用のバッファに記録しておく.
                    g_shortestPath[i][goal] = g_edgeWeight[i][goal];
                    goal        = i;
                    bSuccess    = true;
                    break;
                }
            }

            if( !bSuccess ){ break; }   //次のノードが発見できなかったので，無限ループ防止で抜ける.
        }
    }

    queue.Term();
}

//次のノードを取得する.
int GetNextNode( const int s, const int e )
{
    for( int i = 0; i < WAY_POINT_MAX_NUM; i++ ){
        if( i == s ){ continue; }
        if( g_shortestPath[s][i] < INF_COST ){
            return i;
        }
    }
    return -1;
}


注意
影響マップは，1マップに対して，1要素に関するコストのみ入れるようにした方が良い．
調整時やデバッグ時に，何がどれくらい影響しているかが一目瞭然だからだ．
経路復元に関しては，かなり適当に作ったので，正しく動かないケースもあると思われる．
正しい実装を知りたい場合は，google先生に質問してください．

動作状況

目標の視野に入らないルートで，最短経路を移動していることがわかる．
単純に最短経路を移動して，相手の正面から突っ込んでしまうよりも多少賢く見える．

総括
賢いAIを作るに当たって，様々な要因を鑑みた経路探索を行うことは重要であり，
その点において，影響マップは非常に有用な手法である．
A*アルゴリズム，影響マップ共に，良く用いられる手法であるので，知っていて損はないと思う．
",True,https://qiita.com/keny30827/items/52d589005cbc73ba0501
"

 モンテカルロ法を用いたダイクストラ法の紹介
確率的に移動時間が変わるグラフ上の最短路を求める方法を紹介します。
サンプルのグラフを作ります。

python3
%matplotlib inline
import numpy as np, networkx as nx

m = 4
g = nx.Graph()
for i in range(m):
    if i==0:
        g.add_edge(i, i+m, prob=[1], time=[1.9]) # 0-> 4
    else:
        g.add_edge(i, i+m, prob=[0.8, 0.2], time=[1, 6]) # 縦
    if i < m-1:
        g.add_edge(i, i+1, prob=[1], time=[2]) # 横
        g.add_edge(i+m, i+m+1, prob=[1], time=[2]) # 横

n = g.number_of_nodes()
pos = {i:[i%m, i//m] for i in range(n)}
nx.draw_networkx_nodes(g, pos, node_color='w')
nx.draw_networkx_edges(g, pos)
nx.draw_networkx_labels(g, pos, {i:str(i) for i in range(n)});




上記の点 0から点 7への最短路を求めます。
横の道は、確定的に2時間かかります。
縦の道は、確率80%で1時間ですが、確率20%で6時間かかります。平均すると2時間かかります。
点 0から点 4までは、確定的に1.9 時間で行けます。
ある点に到達したとき、その点に繋がる辺の移動時間だけは、確定するものとします。

平均時間で見れば、""0 -> 4 -> 5 -> 6 -> 7""のルートで、7.9時間が最短路です。
しかし、縦の道で、下から上へは、確率 80% で1時間で行けます。このことから、右に進みながら、縦に1時間で行ければ、上へ進む方針が、よさそうです。

考案したモンテカルロダイクストラ法

予め、確率で定まる辺に対し各々nn個の乱数を用意しておきます。
全ての点において終点への到達時間を∞にし、全ての点を未探索にします。
次の点を終点にし、次の点の到達時間を0にします。
始点が探索済みになるまで、以下を繰返します。


次の点を探索済みにします。
次の点に接続する点の到達時間を後述のように更新します。
探索済みでない点の中で、到達時間が最小のものを次の点にします。




到達時間の更新

以下のサンプル値のnn回の平均が、現在の到達時間より短ければ、更新します。


サンプル値を接続する点について「到達時間と接続辺の時間の和」の最小値とします。




計算してみる

python3
def monte_min(g, s, t, nn=1000):
    n = g.number_of_nodes()
    dd = [np.inf] * n
    bb = [False] * n
    for i, j in g.edges():
        d = g.edge[i][j]
        d['log'] = (np.random.multinomial(1, d['prob'], nn) * d['time']).sum(axis=1)
    nx = t
    dd[nx] = 0
    while not bb[s]:
        bb[nx] = True
        for nd in g.edge[nx].keys():
            dd[nd] = min(dd[nd], np.mean([calcmin(dd, g.edge[nd], i) for i in range(nn)]))
        nx = np.argmin([np.inf if bb[i] else dd[i] for i in range(n)])
        if dd[nx] == np.inf: break
    return dd
def calcmin(dd, dc, i):
    return min([dd[nd] + d['log'][i] for nd, d in dc.items()])

print(monte_min(g, 0, 7))
>>>
[7.0436741200000021,
 5.0366892306401603,
 3.1682992231199996,
 1.7938642600000001,
 6.0,
 4.0,
 2.0,
 0]


monte_min で各点ごとの到達時間を出力します。
平均値のダイクストラでは 7.9でしたが、モンテカルロで計算すると 7.04になりました。
また、点4 を通ると、7.9 (= 6.0+1.9) ですが、点1経由にすれば、7.04 (= 5.04+2)なので、点0からは点1に向かうのがよいことになります。
点1についたら、点2に向かうと、5.17 (= 3.17+2)です。このとき、辺(1-5)が1ならば 5 (=4.0+1)となり、辺(1-5)が6ならば 10 (=4.0+6)となります。このように、上への移動時間が1のところで上へ行くことがよいこともわかります。
なお、このモンテカルロダイクストラ法は、サンプリングが正確であっても厳密な最適解の保証はありません。
以上
",True,https://qiita.com/SaitoTsutomu/items/04cd878ca861696d95e9
"TensorFlow1.7で、TensorFlow Hubという新たなライブラリが追加されました。

https://www.youtube.com/watch?v=rirzJ-e68cw
https://www.tensorflow.org/hub/

これにより、学習済みの深層学習モデルを、より簡単に転移学習やFine-Tuningに利用できるようになり、さらに独自のモデルを、他のユーザーにTensorFlow Hub経由で共有できるようになりました。
この記事では、TensorFlow Hubを利用して、Inception-v3の転移学習のコードを作成してみたいと思います。

TensorFlow Hubのインストール
TensorFlow Hubを利用するには、TensorFlowを1.7以上にアップグレードし、別途パッケージをインストールする必要があります。
pip install ""tensorflow>=1.7.0""
pip install tensorflow-hub


TensorFlow HubによるInception-v3モジュールの使い方
TensorFlow Hubでは学習済みのモデルデータをモジュールと呼ばれる単位で扱います。
Inception-v3を読み込むには、Module google/‌imagenet/‌inception_v3/‌feature_vector/1  |  TensorFlowに従い、
import tensorflow_hub as hub
module = hub.Module(""https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1"")

とします。実行されると、モジュールがダウンロードされます。保存場所は環境変数TFHUB_CACHE_DIRにより指定することができます。
このmoduleに、画像データのTensorを入力で渡すと、Inception-v3のネットワークの出力のTensorを得ることが出来ます。このInception-v3への入力画像サイズは299×299のカラー画像、出力は2048次元のTensorです。
# imagesは[batch, 299, 299, 3]のTensor
# outputsは[batch, 2048]のTensor
outputs = module(images)

後は、この出力を解きたい問題に合わせて、続くネットワークの入力に使用できます。例えば、10クラス分類問題であれば、以下のように、各クラスへの分類確率を計算できます。
logits = tf.layers.dense(inputs=outputs, units=10)
predictions = {
    ""classes"": tf.argmax(input=logits, axis=1),
    ""probabilities"": tf.nn.softmax(logits)
}


Inception-v3による転移学習
実際に、MNIST画像をInception-v3で学習するコードを作成してみたいと思います。MNISTは28×28のグレースケール画像なので、Inception-v3への入力は299×299のカラー画像とは合わないですが、あくまでTensorFlow Hubを使った一連の処理を試すため、ここではコードサンプルの多いMNISTを使用します。
MNISTのチュートリアルにあるコードをベースにします。
Githubに作成したコードを置いてあります: https://github.com/shu-yusa/tensorflow-hub-sample

Estimatorの作成
tf.estimator.Estimatorのコンストラクタのmodel_fnに渡す関数を、Inception-v3を使ったものにします。
def inceptionv3_model_fn(features, labels, mode):
    # Load Inception-v3 model.
    module = hub.Module(""https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1"")
    input_layer = adjust_image(features[""x""])
    outputs = module(input_layer)

    logits = tf.layers.dense(inputs=outputs, units=10)

    predictions = {
        # Generate predictions (for PREDICT and EVAL mode)
        ""classes"": tf.argmax(input=logits, axis=1),
        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
        # `logging_hook`.
        ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
    }

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

    # Calculate Loss (for both TRAIN and EVAL modes)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

    # Configure the Training Op (for TRAIN mode)
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
        train_op = optimizer.minimize(
            loss=loss,
            global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    # Add evaluation metrics (for EVAL mode)
    eval_metric_ops = {
        ""accuracy"": tf.metrics.accuracy(
            labels=labels, predictions=predictions[""classes""])}
    return tf.estimator.EstimatorSpec(
        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)

logits計算以前の部分のコードを、Inception-v3を使ったものに書き換えています。adjust_image()という関数が、呼ばれていますが、これは、以下のように、バッチサイズ×784サイズのfeatures[""x""]を、Inception-v3の入力画像に合わせている処理です。
def adjust_image(data):
    # Reshape to [batch, height, width, channels].
    imgs = tf.reshape(data, [-1, 28, 28, 1])
    # Adjust image size to Inception-v3 input.
    imgs = tf.image.resize_images(imgs, (299, 299))
    # Convert to RGB image.
    imgs = tf.image.grayscale_to_rgb(imgs)
    return imgs

このinceptionv3_model_fnを使って、Estimatorを作成します。
# Create an estimator
classifier = tf.estimator.Estimator(
    model_fn=inceptionv3_model_fn, model_dir=""/tmp/convnet_model"")

残りの部分は、おおよそ、元のチュートリアルと同じです。

グラフの確認
実際にコードを実行し、TensorBoardでグラフを確認すると、TensorFlow Hubの部分は、以下のようになっていました。hub_inputに画像のTensorが渡され、内部でInceptionV3モデルを経て、hub_outputで出力されていることが確認できます。


まとめ
TensorFlow1.7で導入されたTensorFlow Hubを利用して、Inception-v3モデルの転移学習を行う簡単なコードを書いてみました。
これ以前に同様のことを行うには、Inception-v3のモデルの定義スクリプト、学習済みのチェックポイントファイルを持ってきて、グラフを抜き出したり、変数を学習から外すために固めたりする必要がありましたが、それらの処理を数行で簡単に行えるようになりました。
今後、転移学習を行うのであれば、これを使わない手はないと思います。この記事がその助けになれば幸いです。
",True,https://qiita.com/shu-yusa/items/8ae5326ff55f9dd674c7
"

0. はじめに
二分探索法は単純ながらも効果が大きく印象に残りやすいもので、アルゴリズム学習のスタート地点に彩られた花という感じです。二分探索というと「ソート済み配列の中から目的のものを高速に探索する」アルゴリズムを思い浮かべる方が多いと思います。巨大なサイズのデータを扱う場面の多い現代ではそれだけでも十分実用的ですが、二分探索法はもっとずっと広い適用範囲を持っています。
本記事では、二分探索法のエッセンスを抽象化して、適用範囲の広い「二分探索法の一般形」を紹介します。同時に無数にある二分探索の実装方法の中でも「めぐる式二分探索」がバグりにくいと感じているので、紹介したいと思います。
　

注意 1: 二分探索の計算時間について
二分探索の計算時間について簡単に触れておきたいと思います。例えば「$n$ 個の要素からなるソート済み配列から目的の値を探索する」というよく知られた設定であれば、

単純な線形探索では $O(n)$ の時間がかかる
二分探索を行えば $O(\log{n})$ の時間で済む

といった具合に、線形探索に比べて劇的に高速化することができます。ソート済み配列に関する探索の問題に限らず、他の問題設定に対しても「逐次的にやると $O(n)$ 回調べる必要があることを $O(\log{n})$ 回調べれば十分」という高速化を実現できることが多いです。

注意 2: 連続量に対する二分探索
本記事では離散量に対する二分探索のみを対象にしていますが、二分探索は連続的な問題に対しても有効です。それについては別途記事にしたいと思います。

1. 普通の二分探索から std::lower_bound() へ
二分探索は通常は「ソート済み配列の中から目的のものを探す」アルゴリズムを指します。そのような処理を実現する方法として、以下のような実装をよく目にします。仕様としては

配列 a の中に値 key があれば、その index を返す (複数ある場合はどれか 1 つを返す)
配列 a の中に値 key がなければ、-1 を返す

というものになっています。
#include <iostream>
#include <vector>
using namespace std;

vector<int> a = {1, 14, 32, 51, 51, 51, 243, 419, 750, 910};

// 目的の値 key の index を返すようにする (ない場合は -1)
int binary_search(int key) {
    int left = 0, right = (int)a.size() - 1; // 配列 a の左端と右端
    while (right >= left) {
        int mid = left + (right - left) / 2; // 区間の真ん中
        if (a[mid] == key) return mid;
        else if (a[mid] > key) right = mid - 1;
        else if (a[mid] < key) left = mid + 1;
    }
    return -1;
}

int main() {
    cout << binary_search(51) << endl; // a[4] = 51 (他に a[3], a[5] も)
    cout << binary_search(1) << endl; // a[0] = 1
    cout << binary_search(910) << endl; // a[9] = 910

    cout << binary_search(52) << endl; // -1
    cout << binary_search(0) << endl; // -1
    cout << binary_search(911) << endl; // -1
}

もう少し汎用的なものを考えてみましょう。例えば C++ の std::lower_bound() は


配列 a の index (正確には iterator) のうち key 以上となる最小の index を返す


ようにしています。そうすれば単に「配列 a の中に key があるかどうかを探す」よりも

配列 a の中に値 key がなくても、key が a の中で何番目に小さいかわかる
配列 a の中に値 key が複数あったとき、そのうちの最小の index を取って来れる
発展テクとして std::upper_bound() も併用すれば、配列 a の中に値 key をもつものが何個あるかもわかる

という風により多くの情報を得ることができます。次章でこの思想をさらに推し進めて、二分探索法を一般化します。

2. 一般化した二分探索法
std::lower_bound() がやりたいことは、結局は

a[index] >= key という条件を満たす最小の index を見つけたい

と言い表すことができます。
　

　
この std::lower_bound() をさらに一般化して、二分探索法は次のような場面で適用可能なアルゴリズムだということができます！

【やりたいこと】
x に関する「条件」がある。
以下のことがわかっているときに、条件を満たす最小の x を見つけたい

x = left は条件を満たさない
x = right は条件を満たす
left と right の間に条件を満たすようになる境目がある！


境目まではずっと条件を満たしておらず、境目から先はずっと条件を満たしている (単調性)





これは以下に示すような非常に汎用的なコードで実現することができます。いわゆるめぐる式二分探索です。実装のお気持ちですが、


left は「常に」条件を満たさない
right は「常に」条件を満たす
right - left = 1 になるまで範囲を狭める (最後は right が条件を満たす最小)



という明快なロジックです。left と right の初期値の定め方だけ注意しておけば、何も考えなくとも書けるようになります。以下に lower_bound() の実装をしてみます:
#include <iostream>
#include <vector>
using namespace std;

vector<int> a = {1, 14, 32, 51, 51, 51, 243, 419, 750, 910};

// index が条件を満たすかどうか
bool isOK(int index, int key) {
    if (a[index] >= key) return true;
    else return false;
}

// 汎用的な二分探索のテンプレ
int binary_search(int key) {
    int left = -1; //「index = 0」が条件を満たすこともあるので、初期値は -1
    int right = (int)a.size(); // 「index = a.size()-1」が条件を満たさないこともあるので、初期値は a.size()

    /* どんな二分探索でもここの書き方を変えずにできる！ */
    while (right - left > 1) {
        int mid = left + (right - left) / 2;

        if (isOK(mid, key)) right = mid;
        else left = mid;
    }

    /* left は条件を満たさない最大の値、right は条件を満たす最小の値になっている */
    return right;
}

int main() {
    cout << binary_search(51) << endl; // a[3] = 51 (さっきは 4 を返したが今回は「最小の index」なので 3)
    cout << binary_search(1) << endl; // a[0] = 1
    cout << binary_search(910) << endl; // a[9] = 910

    cout << binary_search(52) << endl; // 6
    cout << binary_search(0) << endl; // 0
    cout << binary_search(911) << endl; // 10 (場外)
}

二分探索をこのように一般化して考えることで、「ソート済配列の中での探索」に限らず様々な問題に活用することができます。注意点として、ここで挙げた一般化では

求めたい境目を境にして「左側はすべて偽で、右側はすべて真」である

という単調性を仮定しましたが、「条件を満たす最小」を求めることにこだわらなければこの仮定はなくてもよく

二分探索の初期値の一方が真、もう一方が偽になるようにしておいて、真偽の境目を 1 つ求める

という使い方もできます。単調性の仮定がなければ真偽の境目が複数あり得て、そのうちの 1 つが求まることになります。このような二分探索法フレームワークは、例えば方程式 f(x) = 0 の解をどれか 1 つ求めたい場面などで活用することができます。

3. めぐる式二分探索のさらなる利点
めぐるちゃんのツイートにも書いてあることですが、上の状態でもまだ


left は常に条件を満たさない
right は常に条件を満たす
right - left = 1 になるまで範囲を狭める (最後は right が条件を満たす最小)


という部分で、left と right のどちらが条件を満たしていたかを考える部分に思考リソースをとられます。また上記は「条件を満たす最小の index を求める」という処理でしたが、代わりに以下のようにして「条件を満たす最大の index を求める」という風にしたくなる場面もあります。


left は常に条件を満たす
right は常に条件を満たさない
right - left = 1 になるまで範囲を狭める (最後は left が条件を満たす最大)


めぐる式二分探索ではそれも含めた統一的な実装を可能にしています。left と right のどちらが大きいかも気にすることなく、以下のように書くことができます。
#include <iostream>
#include <vector>
using namespace std;

vector<int> a = {1, 14, 32, 51, 51, 51, 243, 419, 750, 910};

// index が条件を満たすかどうか
bool isOK(int index, int key) {
    if (a[index] >= key) return true;
    else return false;
}

// 汎用的な二分探索のテンプレ
int binary_search(int key) {
    int ng = -1; //「index = 0」が条件を満たすこともあるので、初期値は -1
    int ok = (int)a.size(); // 「index = a.size()-1」が条件を満たさないこともあるので、初期値は a.size()

    /* ok と ng のどちらが大きいかわからないことを考慮 */
    while (abs(ok - ng) > 1) {
        int mid = (ok + ng) / 2;

        if (isOK(mid, key)) ok = mid;
        else ng = mid;
    }
    return ok;
}

int main() {
    cout << binary_search(51) << endl; // a[3] = 51 (さっきは 4 を返したが今回は「最小の index」なので 3)
    cout << binary_search(1) << endl; // a[0] = 1
    cout << binary_search(910) << endl; // a[9] = 910

    cout << binary_search(52) << endl; // 6
    cout << binary_search(0) << endl; // 0
    cout << binary_search(911) << endl; // 10 (場外)
}

ここまで来れば、二分探索処理の実装にとられる思考リソースをかなり軽減できると思います。

4. 一般化した二分探索の適用例

4-1. 年齢当てゲーム
まずは

「アルゴリズム」とは何か、すべての人がわかるように解説！

でも挙げた年齢当てゲームを考えてみましょう。問題設定を再掲します。

A さんの年齢を当てようとしています。
A さんが、20 歳以上 36 歳未満だというのはわかっているとしましょう。あなたは A さんに 4 回だけ Yes / No で答えられる質問をすることができます。A さんの年齢を当てられるでしょうか？？？


年齢当てゲームの実装

20 歳以上 36 歳未満であることはわかっている A さんの年齢を 4 回の質問でズバリ当てます！
以下の実装のキーとなる変数 left, right についてですが、

x = left は「A さんの年齢は x 歳以上である」という条件を満たす
x = right は「A さんの年齢は x 歳以上である」という条件を満たさない

という状態を常に保つようにしています。言い換えれば、A さんの年齢の候補が常に left 歳以上 right 歳未満となるように定めています。初期状態は left = 20, right = 36 と 16 歳分の幅がありますが、質問を通して狭めて行きます。終了状態では right - left = 1 となっていて、A さんは left 歳以上 right 歳未満なので left 歳で確定します。
なお、上の年齢当てゲームでは簡単のために right - left が 2 の冪乗 (2^4 = 16) になるようにしていましたが、そうでなくても下の実装はきちんと動作します。例えば left = 0, right = 100 としてもきちんと動作します (その場合は最大で 7 回の質問で当てます)。
#include <iostream>
using namespace std;

int main() {
    cout << ""Start Game!"" << endl;

    /* A さんの年齢の候補を表す区間を、[left, right) と表します */
    /* A さんは、left 歳以上 right 歳未満、right 歳自体は候補に含まれないことに注意 */
    int left = 20, right = 36;

    /* A さんの年齢を 1 つに絞れないうちは繰り返す */
    while (right - left > 1) {
        int mid = left + (right - left) / 2; // 区間の真ん中

        /* mid 歳以上かを聞いて、回答を yes/no で受け取る */
        cout << ""Is the age same or more than "" << mid << "" ? (yes / no)"" << endl;
        string ans;
        cin >> ans;

        /* 回答の応じて、年齢を絞る */
        if (ans == ""yes"") left = mid; // mid 歳以上なら、mid 歳以上 right 歳以下になるように
        else right = mid; // mid 歳未満なら、left 歳以上 mid 歳未満になるように
    }

    /* ズバリ当てる！ */
    cout << ""The age is "" << left << ""!"" << endl;
}

",True,https://qiita.com/drken/items/97e37dd6143e33a64c8c
"

最初に
本ドキュメントでは関数型プログラミングで見受けられるコードのパターンを記述する。
関数型プログラミングでは、関数自体を通常のオブジェクトと同じように、変数に格納したり、引数に渡したり、データ構造の一部にできることにより、命令型プログラミングと比較して簡潔なコード記述ができる効果が見込まれる。
本ドキュメントでは、関数型プログラミングの理論よりサンプルを提示することにより、読者に視覚的に効果を見せるよう努めている。
対象となる読者は以下の通りである。

Java言語による命令型/オブジェクト指向型プログラミングが書ける/読める
関数型プログラミングに興味がある、あるいは、Java
8によるラムダ式による記述に興味がある

パターンは、命令型プログラミングの例として、Java言語での例を記載する。また、関数型プログラミングの例としてはScala言語に加えJava言語でも記載する。ただし、Java言語に関しては記載可能な関数型プログラミング・コード・パターンのみに限る。
Scala言語は、オブジェクト指向プログラミングと関数型プログラミングを組み合わせた設計思想として作られている。
一方で、Java言語は、Project Lambdaと呼ばれるプロジェクトにより、それまでのJava言語を拡張する形で実装がされ、Java
SEバージョン8により正式にJava言語に取り込まれた。
Scala言語とJava言語で、2つの整数値を乗数を算出する関数を比べてみる。
import java.util.function.IntBinaryOperator;
IntBinaryOperator intMultiplier = (a, b) -> a * b;

val intMultiplier: (Int, Int) => Int = (a, b) => a * b;

いくつか類似している部分とそうでない部分があることに気づくであろう。まず、関数は、aとbの2つの引数を渡し、それを乗算したものを返している部分の記述はJava言語とScala言語で共通している。この表記はラムダ式と呼ばれる。
Java言語では関数はIntBinaryOperatorというクラスになっている。Scalaでは整数型Intの引数を2つ取り、関数型Intを返すということがクラス(
(Int, Int) => Int )であることが分かる。
Java言語におけるクラスIntBinaryOperatorは、SAM(Single Abstract
Method)インターフェイスと呼ばれる単一の抽象メソッドを持つインターフェイスである。Java言語では、このSAMインターフェイスに対して、上記のような省略したコードを記述できる。上記のコードは実際には以下のコードを同等である。
IntBinaryOperator intMultiplier = new IntBinaryOperator() {
    @Override
    public int applyAsInt(int a, int b) {
        return a * b;
    }
};

一方で、Scala言語はクラスが関数の入出力を分かりやすく示したクラスであるが、Scala言語でも実際はFunction2[Int, Int, Int]というクラスのエイリアスになっている。そのため、以下のような記載もできる。
val intMultiplier: Function2[Int, Int, Int] = (a, b) => a * b;

Scala言語の場合は、関数へのクラスが決められているために、変数intMultiplierの型を明示しなくてもよい。(ただし、この例だと代わりに引数(a, b)への型を明示する必要がある)
val intMultiplier = (a: Int, b: Int) => a * b;

Java言語の場合は、必ず使用するSAMインターフェイスを明示する必要があるが、逆にメリットとしては、どのようなSAMインターフェイスに対してもラムダ式で記載できる点にある。
なお、Scala言語でもSAMインターフェイスに対するラムダ式を、Scala 2.12で正式サポートされている。
なお、本ドキュメントでは関数型プログラミングに関する部分に力点をおいていて、あまり言語の構文などは詳しくは書かない。これは読者自身で調査をしていただきたいと考えている。

パターン
パターンをプログラムで記述する際に、list 変数を使用するが、これは次の通りの変数定義が事前に行われている前提とする。
List<String> list = Arrays.asList(""apple"", ""orange"", ""grape"");

val list: Seq[String] = Seq(""apple"", ""orange"", ""grape"")


繰り返し
繰り返しは基本的なパターンではあるが、これにも関数型プログラミングの特徴が存在する。
最初に命令型プログラミングでの例を示す。

命令型プログラミング
for (String s : list)
    System.out.println(s);


拡張for文を使用しているが、それ以外は特筆すべき点はない。
これを関数型プログラミングであるScalaで書くと下記のようになる。最初は少し冗長に書く。

関数型プログラミング)
def output(s: String) = {
  println(s)
}
list.foreach(output)


1～3行目でoutput関数を定義している。これは、引数sを受け取り、その内容を標準出力に印字する。
4行目ではこのout関数をforeachメソッドの引数として渡している。このことにより、list変数内のひとつひとつの要素が順にoutput関数の引数sに渡されて実行がされる。
実際にはこのように関数を分けて定義は行わず、以下のようにforeachメソッドの引数に関数定義を直接記述する。

関数型プログラミング
list.foreach { s: String => println(s) }


Scalaに関しては、この記述は以下のようにfor表記でも記述が出来る。Scalaのfor表記は、Javaの拡張for文と見た目は似ているが、実際は異なり、この例でのfor表記は上記のforeachを使用したものに変換がされる。

関数型プログラミング
for (s <- list)
  println(s)


次にJava言語での関数型プログラミングの例を示す。まずはScala言語の例のように冗長な例から示す。

命令型プログラミング
Consumer<String> output = s -> System.out.println(s);
list.stream().forEach(output);


この1行目の書き方はラムダ式を使用した書き方で、実際は下記のようなコードと同等となる。
Consumer<String> output = new Consumer<String>() {
    public void accept(String s) {
        System.out.println(s);
    }
};

Java言語においても、Scala言語と同様に、一般的には下記のように記述を行う。

関数型プログラミング
list.stream().forEach(s -> System.out.println(s));


Java言語ではメソッド参照という書き方があり、以下の様な書き方もできる。

関数型プログラミング
list.stream().forEach(System.out::println);


同様な記述はScalaでも可能であるが、Scalaの場合はprintln関数そのものを渡している。

関数型プログラミング
list.foreach(println)



マップ
リスト内のデータ一つ一つを変換し、別のデータを作成するという例に移る。
以下はlist 変数内の各項目を、文字列の長さに変換する命令型プログラミングでの例である。

命令型プログラミング
List<Integer> eachLength = new ArrayList<>();
for (String s : list)
    eachLength.add(s.length());


命令型プログラミングでは、最初に文字列の長さが入った空のリスト( eachLength )を作成する。(1行目)そして、順に文字列ひとつひとつを長さに変換し、eachLength に加えていく。(2～3行目)
次に関数型プログラミングでの例を示す。

関数型プログラミング
val eachLength: Seq[Int] = list.map(s => s.length())


関数型プログラミングでは、リストのmap関数に、変換用の関数を渡すことで、リストそのものの変換を実現している。
Scala言語の場合は、これをfor表記でも記述できる。記述は繰り返しで示した例を似ているが、yieldというキーワードが付いている。このキーワードを付けることで、上記のmap関数で記述した内容と同じ動きとなる。

関数型プログラミング
val eachLength: Seq[Int] = for (s <- list) yield s.length()


次に、Java言語での関数型プログラミングの例を示す。

関数型プログラミング
List<Integer> eachLength =
  list.stream().map(s -> s.length()).collect(Collectors.toList());


Scala言語のmap関数を使用した例に似ているが、リストを一旦ストリームにしたあと、再度リストに戻しているために冗長にみえる。
メソッド参照を使用した場合は以下のようになる。

関数型プログラミング
List<Integer> eachLength = list.stream().map(String::length).collect(Collectors.toList());



合計
リストの文字列の長さの合計を算出する例を示す。Scala言語、Java言語ともに合計を算出する関数が用意されているが、最初はこれらを使用しない例から始める。
まずは命令型プログラミングでの例を示す。ここでは、先ほど使用したeachLength変数を再利用する。

命令型プログラミング
int summery = 0;
for (int length : eachLength)
    summery += length;


命令型プログラミングでは、最初に合計値を0で初期化した変数summeryを用意し、それに各文字列の長さの値を加えていく。
次に関数型プログラミングでの例を示す。

関数型プログラミング
val summery = eachLength.reduceLeft((x, y) => x + y)


このreduceLeft関数では、まず、eachLength変数のリストに格納された最初(0番目)と次(1番目)の項目値を足し算をする。次に、その足し算の結果値と、その次(2番目)の項目値の足し算をする。同様に3番目以降の値も足し算をeachLength変数のリストの最後まで繰り返し、最後の足し算の合計値を返す。
なお、Scala言語では次のような書き方もできる。これは左の’_’
(アンダースコア)が上記のx、右の’_’
(アンダースコア)が上記のyを表している。

関数型プログラミング
val summery = eachLength.reduceLeft(_ + _)


次に、Java言語での関数型プログラミングの例を示す。

関数型プログラミング
Integer summery = eachLength.stream().reduce(0, (a, b) -> a + b);


Scala言語の例によく似ているが、reduceメソッドの第一引数に0という数値を渡している違いがある。そして、上記のコードでは分からないが、この例のreduceメソッドの内部は必ずしも足し算をeachLength変数のリストの最初の項目値から順に足し算をするわけではないという点に注意が必要である。
たとえば、2, 5, 7, 3という順のリストがあったとする。Scala言語の例では必ず以下の計算が行われる。
((2 + 5) + 7) + 3
ところが、Java言語の例では以下のように最初の項目から順に計算が行われない場合がある。
((0 + 2) + 5) + ((0 + 7) + 3)
Java言語の場合は、順序が最初から行われないことに加え、最初に足し算が発生するときに必ず0を加えている。
これは数学的には単位元が存在すること、結合律を満たすことを意味し、この２つを満たすことをモノイドという。
最初の項目から順に計算が行われないのがどういうケースで発生するかというと、これは並列計算を実施するときに発生する。並列計算により、各プロセッサーに分散し計算を効率化することができる。
実はScala言語でもreduceLeftメソッドではなくreduceメソッドを使用するとJava言語のように並列計算に対応ができる。

関数型プログラミング
val summery = eachLength.reduce(_ + _)


また、foldメソッドを使用すれば、初期値を与えることも出来る。

関数型プログラミング
val summery = eachLength.fold(0)(_ + _)


さて、合計値を計算する方法だが、これはScala言語、Java言語共に特化した記述方法があり、最後にその記述を紹介する。
Scala言語は名前そのままのsumメソッドが準備されている。

関数型プログラミング
val summery = eachLength.sum


Java言語の場合は、以下の2つの方法がある。

関数型プログラミング
int summery = list.stream().mapToInt(String::length).sum();


この方法は、mapToIntメソッドで、IntStreamというint型に特化したストリームに変換することで実現している。

関数型プログラミング
Integer summery4 = list.stream().collect(Collectors.summingInt(String::length));


こちらの方法は、文字列の長さを計算したものを順次合計するやり方である。

エラー処理
ここではエラー処理を題材にする。ここではエラー処理ということを広義に捉え、エラーがない状態とある状態を持てることをエラー処理と考え、紹介する。

ヌル
C言語に影響を受けた言語は、変数にオブジェクトがアサインされていない状態のとき、ヌル(null)値が格納される。
ヌル値の問題点は、変数値がヌルが許されるかどうかがをドキュメント以外では判別できず、ヌル値のままその変数を使用しようとするとNullPointerExceptionが発生することである。
関数型プログラミングでは、ヌルは原則使用せず、以下のようなクラスを使用し、解決を行っている。

少し簡略したクラス図だが、抽象クラスOptionがあり、そこに抽象メソッドgetと抽象メソッドisEmptyが準備されている。(もちろん実際にはもっと多くのメソッドは存在する)その派生クラスとして、SomeクラスとNoneクラスが定義されていて、Someクラスでは実際の値が入った状態、Noneクラスは値がない状態を表す。getメソッドはSomeクラスでは対象の値が返るが、Noneクラスでは例外が発生する。isDefinedメソッドはSomeクラスではtrue、Noneクラスではfalseが返る。
これにより、まずOptionクラスの変数はヌル値になる可能性があることが明確に分かる。そして、その可能性がある場合は、以下のようにif文で分岐が出来る。

関数型プログラミング)
val valueOption: Option[T] = .....
if(valueOption.isDefined) {
  val value: T = valueOption.get
  // valueで何かをする
}


型TはOption(Optional)型の値の型である。
Javaでもほぼ同様の書き方ができる。JavaではSomeクラスやNoneクラスといった具象クラスを分ける実装はしていなく、Optionalクラスの内部状態で

関数型プログラミング
Optional<T> valueOption = .....
if (valueOption.isPresent()) {
    T value = valueOption.get();
    // valueで何かをする
}


おそらくここまでの説明では、Option(Optional)クラスは関数型プログラミングというよりオブジェクト指向であるという印象があるのではないかと思う。
まずは、Option(Optional)クラスにおけるマップを示す。
Opition(Optional)クラスにはマップ(map)メソッドが存在する。(上記のクラス図にはマップ・メソッドは省略している)

関数型プログラミング
val textOption: Option[String] = .....
val lengthOption: Option[Int] = textOption.map(s => s.length())



関数型プログラミング
Optional<String> textOption = .....
Optional<Integer> lengthOption = textOption.map(s -> s.length());


上記はマップ・メソッドではString型のオブジェクトを含む可能性があるtextOption変数を、String型オブジェクトの長さを含む可能性があるlengthOption変数に変換をしている。つまり、textOption変数のisDefined(isPresent)メソッドがtrueを返せば、lengthOption変数もtrueを返し、textOption変数のisDefined(isPresent)メソッドがfalseを返せば、lengthOption変数もfalseを返す。
Scalaの場合は、以下のfor表記でもマップ・メソッドと同等の書き方ができる。

関数型プログラミング
val textOption: Option[String] = .....
val lengthOption: Option[Int] = for(s <- textOption) yield s.length()


また、Scalaの場合にはOptionクラスにもforeachメソッドが存在する。

関数型プログラミング
textOption.foreach(println)


この例では、textOption変数の実体がSomeクラスであれば、つまり、中身のデータがヌルでないならば、中身のデータを印字し、Noneクラスであれば、何も実行しない(印字がされない)。
Javaの場合はメソッド名がifPresentという名称で同じことが実行できる。

関数型プログラミング
textOption.ifPresent(System.out::println);


Scalaのforeachメソッドの例も、以下のfor表記で同等の書き方が可能である。

関数型プログラミング
for(s <- textOption) println(s)


ここまで、Option(Optional)クラスにもリストと同じようにforeachメソッドやマップ・メソッドに相当するものがあることを示してきた。
次に関数型プログラミングを語る上では、もう一つの重要なflatMapメソッドを紹介する。(上記のクラス図にはflatMapメソッドは省略している)
ここでの例は、コマンドライン引数を解析し、コマンドライン引数に含まれるユーザーID、パスワードを元に対象サーバーにアクセスをする。(対象サーバーは、DBサーバーやsshでのLinuxサーバーなどが想定されるが、ここではあくまで例のために具体的なところまでは言及しない)
あらかじめ以下の3つのメソッドが定義されている。
それぞれがOption(Optional)クラスのインスタンスの戻り値であるのは、これらが値を持たない(持てない)可能性があるからである。getUserId/getPasswordメソッドは、適切な引数が渡されていない場合は値が持てないし、connectはユーザーIDやパスワードが誤っている場合や、サーバーに接続できない場合は値が持てない。

Scala
def getUserId(args: Array[String]): Option[String]
def getPassword(args: Array[String]): Option[String]
def connect(user: String, password: String): Option[Connection]



Java
Optional<String> getUserId(String[] args)
Optional<String> getPassword(String[] args)
Optional<Connection> connect(String userId, String password)


目指したい作成対象メソッドは以下の型のメソッドである。

Scala
def connect(args: Array[String]): Option[Connection]



Java
Optional<Connection> connect(String[] args)


まずはJavaの命令型プログラミングでの例を示す。

命令型プログラミング
Optional<Connection> connect(String[] args) {
    Optional<String> userIdOption = getUserId(args);
    Optional<String> passwordOption = getPassword(args);
    if (userIdOption.isPresent() && passwordOption.isPresent()) {
        String userId = userIdOption.get();
        String password = passwordOption.get();
        return connect(userId, password);
    } else
        return Optional.empty();
}


難易度は高くないため、説明は不要かと思う。おそらくこれでも十分分かりやすいと思うかもしれない。
さて、flatMapメソッドの説明に移る。
型Option[A]である変数optionAと、クラスAを引数とし、Option[B]
(Optional<B>)を返すメソッドactionが存在するとする。

関数型プログラミング(Scala)
val optionB: Option[B] = optionA.flatMap { a => action(a) }



関数型プログラミング(Java)
Optional<B> optionB = optionA.flatMap(a -> action(a)};


これは、optionAがヌル値ではない値が入った場合、かつ、その値でaction(a)を実行した結果がさらにヌル値ではない値が入った状態であるときにのみ、optionBはヌル値ではない値が入った状態になる。
つまり、以下と同じ動きをする。

関数型プログラミング(Scala)
val optionB: Option[B] = if(optionA.isDefined) action(optionA.get) else None



関数型プログラミング(Java)
Optional<B> optionB = optionA.isPresent() ? action(optionA.get()) : Optional.empty();


表にすると以下になる。下記でtrue/falseとはOption(Optional)のインスタンスのisDefined(isPresent)の結果がtrue/falseになることである。



optionA
action(a)の結果
flatMapの結果(optionB)




true
true
true


true
false
false


false
呼ばれない
false



このflatMapメソッドを使用したconnectメソッドの実装は下記のとおりとなる。

関数型プログラミング(Scala)
def connect(args: Array[String]): Option[Connection] =
  getUserId(args).flatMap { userId =>
    getPassword(args).flatMap { password => connect(userId, password) 
  }
}



関数型プログラミング(Java)
Optional<Connection> connect(String[] args) {
    return getUserId(args).flatMap(userId ->
        getPassword(args).flatMap(password -> connect(userId, password)));
}


これから見て分かる通り、命令型プログラミングでの条件分岐がこの例ではflatMapメソッドに吸収されている。
これだけだとむしろ複雑になったように見えるかもしれない。これをScala言語のfor表記で記載すると下記のようになる。

関数型プログラミング
def connect(args: Array[String]): Option[Connection] =
  for(userId <- getUserId(args);
      password <- getPassword(args);
      connection <- connect(userId, password))
    yield connection


この表記では、Option(Optional)クラスの中身の値を取り出し、その値で処理を順次実施しているイメージが掴みやすいのではなかろうか。
なお、このfor表記は、正確には次のように変換される。

関数型プログラミング
def connect3(args: Array[String]): Option[Connection] =
  getUserId(args).flatMap { userId =>
    getPassword(args).
      flatMap { password =>
        connect(userId, password).map { conn =>
          conn
        }
      }
  }



例外
例外に関してはScala言語のパターンのみになる。Java言語でも例外に関するライブラリをOSSで作成している例もあるが、現時点でメジャーではないために説明はしない。(https://github.com/jasongoodwin/better-java-monads)
まず、Java言語での例外処理の話から始める。Java言語ではtry/catchで例外処理を実施する。

関数型プログラミング
try {
    // 例外発生が伴う可能性がある処理
    .....
} catch(....) {
    // 例外発生時の対応処理
    ....
}


Scala言語でも同様のtry/catchで記述が可能であるが、ここでは他の関数型プログラミングらしい方法を紹介する。
Scala言語にはTryクラスが準備されている。Tが大文字である点に注意してほしい。Tryクラスは以下のような構造である。

まず一見にしてTryクラスがOptionクラスに類似していることが分かると思う。成功(Success)と失敗(Failure)のクラスが分かれているのは、OptionクラスのSomeクラス/Noneクラスがあるのと類似している。
さて、Tryクラスの使い方だが、これにはTry(コンパニオン)オブジェクトのapplyメソッドを使用する。Java言語風な言い方では、静的メソッドに近い。(違いがScala言語ではシングルトン・オブジェクトとなる)

関数型プログラミング
val result = Try.apply {
    // 例外発生が伴う可能性がある処理
    .....
}


まず、applyメソッドに失敗する可能性がある処理を渡している。また、その結果をresult変数で返している。result変数には、処理の成功/失敗にしたがい(Tryクラスから派生した)SuccessクラスかFailureクラスのインスタンスが格納される。
Scalaの(コンパニオン)オブジェクトのapplyメソッドは省略することが許されているために、下記のように書くこともできる。

関数型プログラミング
val result = Try {
    // 例外発生が伴う可能性がある処理
    .....
}


TryクラスもOptionクラスと同様に成功した場合は値を取り出すことができる。

関数型プログラミング
val result: Try[T] = .....
if(result.isSuccess) {
  val value: T = result.get
  // valueで何かをする
}


Option(Optional)クラスで説明した事項はTryクラスでも同様にできる。以下はOption(Optional)クラスと比較しながら見てほしい。

関数型プログラミング(Scala)
val textTry: Try[String] = .....
val lengthTry: Try[Int] = textTry.map(s => s.length())



関数型プログラミング(Scala)
val textTry: Try[String] = .....
val lengthTry: Try[Int] = for(s <- textTry) yield s.length()



関数型プログラミング(Scala)
textTry.foreach(println)



関数型プログラミング(Scala)
for(s <- textTry) println(s)



関数型プログラミング(Scala)
def getUserId(args: Array[String]): Try[String] = .....
def getPassword(args: Array[String]): Try[String] = .....
def connect(user: String, password: String): Try[Connection] = .....



関数型プログラミング(Scala)
def connect(args: Array[String]): Try[Connection] =
  getUserId(args).flatMap { userId =>
    getPassword(args).flatMap { password => connect(userId, password) }
  }



関数型プログラミング
def connect(args: Array[String]): Try[Connection] =
  for (
    userId <- getUserId(args);
    password <- getPassword(args);
    connection <- connect(userId, password)
  ) yield connection


Option(Optional)クラスと同じ記述ができるのが分かると思う。
Option(Optional)クラスのNoneは、TryクラスにおけるFailureクラスに相当する、つまり、エラー処理における失敗という状態に相当すると考えれば、共通点があることは納得できるのではないかと思う。

クラス定義
関数型プログラミングではオブジェクトはImmutable(不変)である必要がある。
一方、オブジェクト指向プログラミングにおいてはオブジェクトの内部を変化させることが基本となる。これはMutable(可変)といわれる。
Java SE(Standard Edition)の中で、Mutableなクラスの例を示す。

命令型プログラミング
Stack<String> stack = new Stack<>();
stack.push(""apple"");
stack.push(""orange"");
stack.push(""grape"");
stack.pop();


読者はStackクラスに関して既知であると想定し、Stackクラスそのものの説明はしない。この例の1行目から5行目までの全てのstack変数は同一のインスタンスを参照している。4行目が終わった時点では、スタックには上からgrape、orange、apppleが積まれている。5行目ではその一番上から値を削除するため、5行目実行後はorange、apppleになる。
Java SEの多くのクラスはこのようなMutableなクラスである。一方で、Java
SEも一部Immutableなクラスがある。典型的にはStringクラスがImmutableクラスである。

命令型プログラミング
String s = ""abcde"";
String t = s.substring(2);


1行目実行後の変数sと2行目実行後の変数tはそれぞれ”abcde”、”cde”の文字列だが、重要なのは2つは異なるStringインスタンスを参照している点と、変数s
は2行目の実行後も”abcde”のままであるという点である。
Immutableなクラスは自分自身のインスタンスの状態を変えるのではなく、状態ごとに別のインスタンスを生成する。
先ほどのStackの例をScala言語で実装すると、違いが明確に分かると思う。(なお、Scala言語でのStackクラスは現在は使用が推奨されず、Listクラスを代替として使うことが推奨されるが、説明のため、Stackクラスを使用する)

関数型プログラミング
import scala.collection.immutable.Stack
val stack = new Stack[String]()
stack.push(""apple"")
stack.push(""orange"")
stack.push(""grape"")
stack.pop


5行目を実行した後のstack変数が指すインスタンスには何が入っているだろうか。Javaのように3つ全ての文字列が積まれているように思うかもしれないが、Immutableなので、正解は5行目が終わっても空のままである。(このため、6行目にpopメソッドを実行した場合は、スタックが空のために、popする対象がない旨の例外が送出される)
では、Immutableなクラスではどうすればよいのか？以下がその例である。

関数型プログラミング
import scala.collection.immutable.Stack
val stack: Stack[String] = new Stack[String]()
val stack1: Stack[String] = stack.push(""apple"")
val stack2: Stack[String] = stack1.push(""orange"")
val stack3: Stack[String] = stack2.push(""grape"")
val (value: String, stack4: Stack[String]) = stack3.pop2


興味深いのは、Java言語ではpushメソッドでインスタンスの状態が変わる一方、Scala言語では状態が変わるのではなく、状態が変わったスタックのインスタンスを返す。
また、6行目のpopメソッド相当のpop2メソッドも注目すべき点で、一番上に積まれていた値と共に、その値を取り出した後のスタックも戻り値で返している。
このように関数型プログラミングでは、Immutableなクラスを実現するために、状態を変えたい場合、自身のインスタンスの状態を変えるのではなく、インスタンスの状態を変えた新しいインスタンスを返す。これにより、インスタンスの状態に一貫性を持たせることを可能にしている。
なお、先ほどの例では、stack変数がstack,
stack1～4を多く出てきて、コードが汚いと感じる方もいるかもしれない。関数型プログラミングにはこのためのStateモナドというものが存在する。ここではコードの説明はしないが、変数が多く出てこない書き方が出来る点だけ頭に留めておいてほしい。

関数型プログラミング
import scalaz._, Scalaz._
import scala.collection.immutable.Stack
// Stateモナド用のpush、pop定義
def push[T](t: T): State[Stack[T], Unit] =
  State { stack: Stack[T] => (stack.push(t), Unit) }
def pop[T](): State[Stack[T], T] =
  State { stack: Stack[T] => stack.pop2.swap }

// アクションの定義。stack変数が出てこない点がポイント
val actions: State[Stack[String], String] =
  for(
    _ <- push(""apple"");
    _ <- push(""orange"");
    _ <- push(""grape"");
    v <- pop[String]) yield v

val (stack: Stack[String], value: String) = actions(new Stack[String]())



繰り返し(再び)
命令型プログラミングでは通常繰り返しにはwhile文/do-while文のような繰り返し構文を使用する。一方、関数型プログラミングでは、while文/do-while文を使用することは推奨されない。
その理由のひとつとしては、while文/do-while文はImmutableな変数値を持つ可能性が高くなるためである。Java言語はもちろんScala言語でもwhile文/do-while文は戻り値を持たない。(正確にはScala言語はUnit型というそれ自体に意味を持たない値を返す)したがって、データをループにより加工する場合では、何らかのMutableな変数値を持つ必要がある。関数型言語ではMutableな変数値を持つことは可読性を低めるBad
Practiceの１つと看做される。
もう一つの理由としては関数型プログラミングにおいては、while文/do-while文は代替のコーディングが準備されているからである。命令型プログラミングとは出自が異なる関数型プログラミングでは、while文/do-while文以外の方法で繰り返しをコーディングする。今回はその方法を紹介する。

再帰呼出し
再帰呼出しに関しては、Scalaのみになり、Javaでは対象外となる。再帰呼出しそのものは命令型プログラミングでも古くから存在するが、Scalaでは末尾呼出し最適化という機能が働くことが大きく異なる点である。
まずは以前繰り返しを説明した合計値を取得する例から説明する。以下は再掲である。

命令型プログラミング
int summery = 0;
for (int length : eachLength)
    summery += length;


これと同様の合計値を返す再帰呼出しの例の最初の版は以下になる。

関数型プログラミング
def sum(eachLength: Seq[Int]): Int =
  eachLength match {
    case Seq(len) => len
    case Seq() => 0
    case Seq(len, remains@_*) => len + sum(remains)
  }


このコードは実は大きな問題がある。eachLengthがたとえば10,000個の要素がある場合、StackOverflowErrorエラーで失敗をする。これは関数呼出し時に有限の領域であるスタック領域を使用してしまうために、繰り返し関数が呼び出させた場合、このスタック領域を使い果たしてしまうためである。これはJava言語でも一般的に起こる事象であるが、Scalaのような一部の関数型言語では特定の条件では末尾呼出し最適化と呼ばれる機構が機能し、再帰呼出しを回避する。
まずは先ほどの例が末尾呼出し最適化が機能していないことを確認する。

関数型プログラミング
import scala.annotation.tailrec
@tailrec
def sum(eachLength: Seq[Int]): Int =
  eachLength match {
    case Seq(len) => len
    case Seq() => 0
    case Seq(len, remains@_*) => len + sum(remains)
  }


このコードをコンパイルすると以下のエラーが発生する。これは再帰呼出しが末尾にないために最適化ができないということを示している。実際に上の例の7行目はsum関数を再帰呼び出した後に加算が行われていて、最末尾に再帰呼出しが行われていないことが分かる。

関数型プログラミング
could not optimize @tailrec annotated method sum: it contains a recursive
 call not in tail positionimport scala.annotation.tailrec


では、末尾再帰最適化の実施をするためにはどういう実装にすればよいか。先ほどの再帰呼び出しをした後、加算を実施していたがその順序を変えればよい。具体的には以下のようになる。

関数型プログラミング
import scala.annotation.tailrec
@tailrec
def sum(eachLength: Seq[Int], value: Int = 0): Int =
  eachLength match {
    case Seq(len) => value + len
    case Seq() => value
    case Seq(len, remains@_*) => sum(remains, value + len)
  }


この例ではパラメータvalueが一つ増えている。ここに合計値を常に渡すようにしているが、このパラメータは隠したいと思うかもしれない。これには以下のように関数を隠すことで実現できる。

関数型プログラミング
import scala.annotation.tailrec
def sum(eachLength: Seq[Int]) = {
  @tailrec
  def _sum(_eachLength: Seq[Int], value: Int): Int =
    _eachLength match {
      case Seq(len) => value + len
      case Seq() => value
      case Seq(len, remains@_*) => _sum(remains, value + len)
    }
  _sum(eachLength, 0)
}



fold関数
fold関数は以前説明をしたが、ここでは少し視点を変えてみる。
再帰呼び出しにおける例を抽象化した場合どうなるでしょうか。まず、初期値を渡す必要があった。最後の例ではそれは隠蔽させたが、ここの例では復活させる。また、合計値と、配列(eachLength)の要素を元に新しい合計値を算出するロジックが必要となった。
まず第一版は以下の通りである。これを2つ前のコードと比べてほしい。変化点は本質的なものは2つ(のみ)である。１つは関数名をsumからrepeatに変えた。もうひとつは加算の関数(+)を、引数に渡した関数で処理をするようにした。小さいものとしてはvalueパラメータのデフォルトパタメータ(0)をなくした。

関数型プログラミング
// 関数定義
def repeat(list: Seq[Int], value: Int)(f: (Int, Int) => Int): Int = {
  list match {
    case Seq(elemValue) => f(elemValue, value)
    case Seq() => value
    case Seq(elemValue, remains@_*) =>
      repeat(remains, f(elemValue, value))(f)
  }
}



関数型プログラミング
// 呼出し例
repeat(eachLength, 0) { _ + _ }


これはIntに特化しているが型をさらに抽象化する。

関数型プログラミング
// 関数定義
def repeat[A, B](list: Seq[A], value: B)(f: (A, B) => B): B = {
  list match {
    case Seq(elemValue) => f(elemValue, value)
    case Seq() => value
    case Seq(elemValue, remains@_*) => repeat(remains, f(elemValue, value))(f)
  }
}


この例をみて賢明な方は気づいたかもしれないが、これはfold関数と非常に類似していることが分かる。

関数型プログラミング
// Seq#foldLeft関数の定義
def foldLeft[B](z: B)(op: (B, A) => B): B



関数型プログラミング
// 呼出し例(再掲)
val summery = eachLength.fold(0)(_ + _)


ここで特に伝えたいポイントは、fold関数は繰り返しを汎用的に実装できるテクニックの１つである点である。
おそらく、この説明だけだと、単に説明だけのトリック、つまり、応用性があまりないと感じるかもしれないが、fold/reduce関数は実は大変広い。
CSV解析ツールの例を考えてみよう。以下はScala言語でCSVファイル解析の例である。簡単のために、ダブルクオート文字(“)の考慮などはされていない。
以下はそのサンプルである。1行目はCSVのヘッダで2行目以降が実データになっている。

CSVファイル・サンプル
a, b, c
1, 2, 3
4, 5, 6


まずはCSV用の型を定義する。CSVのヘッダ用CsvHeaderクラス、CSVのボディー用CsvLineクラスおよびCSV(そのもの)を表すCsvクラスを定義する。

関数型プログラミング
object CsvHeader {
  def parse(line: String): CsvHeader = {
    CsvHeader(line.split(',').map(_.trim).toSeq)
  }
}
case class CsvHeader(colName: Seq[String])
case class CsvLine(items: Seq[String])
case class Csv(header: CsvHeader, lines: Seq[CsvLine] = Seq()) {
  def addLine(line: String) =
    Csv(header, lines :+ CsvLine(line.split(',').map(_.trim)))
}


全行をStringのリストの形で取得したものをCsvに変換する例は以下である。

関数型プログラミング
val lines = ....;
val Some(csv) = lines.foldLeft[Option[Csv]](None) {
  case (None, line) =>
    Some(Csv(CsvHeader.parse(line)))
  case (Some(csv), line) =>
    Some(csv.addLine(line))
}


この例のポイントは、1行目のヘッダを解析済みかどうかをOptionがSomeかNoneかで判断している。Optionは最初はNoneで渡されるが、1行目のヘッダを渡された時点で、Csv型を包んだSomeになる。つまり、状態管理をしている。このようにfold関数は、リストや配列などのひとつひとつの要素に対して状態を変化させながら結果を出す場合に使用できる。
この状態を持ちながら繰り返しが行えることは非常に重要で、これにより多くの繰り返し処理を、再帰呼出しではなく、fold関数により実現できる。関数プログラミングでは、再帰呼出しで実装を実現する前に、fold関数で実装を検討する方が望ましい。再帰呼出しは、その度に関数定義が必要なため、可読性が低くなることが多い。
ただし、fold関数によりむしろ可読性が悪くなったり、あるいは、パフォーマンスが悪くなる場合は、再帰呼出しを使用してほしい。
なお、先に説明したサンプルは(先ほどの例より若干難しくみえるかもしれないが)以下のようにももう少し短く書ける。

関数型プログラミング
val lines = ....;
val Some(csv) = lines.foldLeft[Option[Csv]](None) {
  case (optCsv, line) =>
    optCsv.map(_.addLine(line)).orElse(Some(Csv(CsvHeader.parse(line))))
}


これまでのfold関数の例は、Scala言語だったが、Java言語では以下のようになる。ここでは説明は省略するが、ほぼScala言語と同様に記述している。

関数型プログラミング
public class CsvHeader {
    private List<String> colNames;
    public CsvHeader(List<String> colNames) {
        this.colNames = colNames;
    }
    ....
}



関数型プログラミング
public class CsvLine {
    private List<String> items;
    public CsvLine(List<String> items) {
        this.items = items;
    }
    ....
}



関数型プログラミング
public class Csv {
    private CsvHeader header;
    private List<CsvLine> lines;
    public Csv(CsvHeader header, List<CsvLine> lines) {
        this.header = header;
        this.lines = lines;
    }
    public Csv(CsvHeader header) {
        this.header = header;
        this.lines = new ArrayList<CsvLine>();
    }
    public Csv addLine(String line) {
        return addLine(new CsvLine(parseLine(line)));
    }
    public Csv addLine(CsvLine line) {
        List<CsvLine> lines = new ArrayList<CsvLine>(this.lines);
        lines.add(line);
        return new Csv(header, lines);
    }
    ....
}



関数型プログラミング
List<String> parseLine(String line) {
    return Arrays.
        stream(line.split("","")).
        map(item ->   item.trim()).collect(Collectors.toList());
}



関数型プログラミング
List<String> lines = ....;
Csv result = lines.stream().reduce(
    Optional.<Csv>empty(),
    (optCsv, line) ->
    Optional.of(optCsv.map(
        csv -> csv.addLine(line)).orElse(
            new Csv(new CsvHeader(parseLine(line))))),
    (optCsv1, optCsv2) -> {throw new RuntimeException();}).get();



最後に
本ドキュメントでは私の経験から関数型プログラミングのパターンを書いた。関数型プログラミングでは、命令型プログラミングとは異なる書き方が多く可能なことが発見できたと思う。命令型プログラミングしか経験がない場合は、関数型プログラミングは分かりにくいと感じるものもあるかもしれないが、何度かコードを書いたり読んだりすることで、関数型プログラミングは多くの抽象化機構があり、その抽象化を理解すれば、関数型プログラミングの理解者には非常に分かりやすいコードになる。
このドキュメントに記載したパターンはその抽象化の基本的なものであるが、是非本ドキュメントを、関数型プログラミングの理解の入り口にしていただくと幸いである。
Hirofumi Arimoto
Java, Scala, JavaScriptや、機械学習に興味あり
実際の開発での知見を可能な限り記事にしたいと思っている
",True,https://qiita.com/hiro0107@github/items/1c9f7396ba0a308f7132
"現在ではSpring Bootの手軽さに軒並み飲み込まれた感がありますが、Javaのマイクロフレームワークはちょっとしたブームでした。
Javaのマイクロフレームワーク ― この新トレンドは見逃せない
enkanは、RackやExpress.jsで実装されているミドルウェアパターンをJavaで実装した、現在ファーストリリースへ向け開発中の新しいマイクロフレームワークです。
Java9のREPLを活用して開発・運用を楽にする機能も実装(予定)です。

なぜ今さら新しいWebフレームワークを?
Spring BootやJava EEは、高度なDIとたくさんのアノテーションでフレームワーク内部の動きはブラックボックス化されます。これを完全にブラックボックスのまま、実用的なWebアプリケーションを完成させるのは、実際のところ難しく、フレームワークの内部を覗きにいかなくてはなりませんが、これが結構ハマりポイントになるようです。
また、これらの高度なDIコンテナ技術は(デフォルトだと)大量のクラスをスキャンしたり、多くの設定をしたりで、アプリケーションの高速な起動から離れていく方向にあるように思えます。JVMの起動がだいぶん高速化された現代において、アプリケーション自体が高速に起動するフレームワークが必要だと考えたわけです。
そこでClojureにミニマルでSimple made easyを地で行く、ductというフレームワークがあるので、このフレームワークとアーキテクチャスタックをJavaに移植し、それをベースにその他必要なものを開発する至りました。(ringを和訳してenkan(円環)です)

特長
コンセプトは大きく分けて3つあります。

Minimal (Simple made easy)
Ease of development
Ease of operation


Minimal

 ミドルウェアパターン
Java以外の言語では、Webのフレームワークはミドルウェアパターンを実装したものが隆盛しています。Rack、Express/connect、ring-clojureのようなものはミドルウェアを適用していくところだけを基本機能と持っていて、ミドルウェアによって機能を足していくしかけです。Servlet APIのFilterと似ていますが、Session管理やHTTPリクエストのパース処理などもすべてミドルウェアで構成されるので、Webアプリケーションの構成が理解しやすく、拡張がより容易な点がメリットです。

特定のリクエストをSorryページに飛ばす
セッション管理
フラッシュスコープ
クッキーのデコード・エンコード
クエリーストリングの解析
トレースログの出力
Railsライクなルーティング (kotowari)
フォームへのバインディング (kotowari)
JSR-303 を使ったフォームのバリデーション (kotowari)
コントローラへのディスパッチ (kotowari)

などが、ミドルウェアとして提供され、これを組み合わせることでWebアプリケーションの基本機能が実現できます。

 設定ファイルレス
enkanでは一切の設定ファイルを無くし、必要ならばJavaで書く方針にしています。環境によって変わるものは12-factor appにしたがい、環境変数で指定します。
// Routing
Routes routes = Routes.define(r -> {
    r.get(""/"").to(ExampleController.class, ""index"");
    r.post(""/login"").to(LoginController.class, ""login"");
    r.resource(CustomerController.class);
}).compile();

app.use(builder(new MethodOverrideMiddleware())
        .set(MethodOverrideMiddleware::setGetterFunction, ""_method"")
        .build());
app.use(new ResourceMiddleware());
app.use(new RenderTemplateMiddleware());
app.use(new RoutingMiddleware(routes));
app.use(new DomaTransactionMiddleware<>());
app.use(new FormMiddleware());
app.use(new ValidateFormMiddleware());
app.use(new HtmlRenderer());
app.use(new ControllerInvokerMiddleware(injector));

デフォルトから挙動を変える場合も、汎用ビルダーを使って、そこそこスマートかつValidatableに設定を記述できます。

 ブラックボックスを避け、黒魔術を使わない
オブジェクトの生成が暗黙的に行われたり、書いた覚えがないけど設定がされていて上手く動いている、なんてのは便利な半面、ハマりのポイントになりかねません。
enkanでは必要最小限のDI機能をもち、それ以外は使う人が明示的にオブジェクト生成するように設計してあります。それでいながら、動的Mixinや汎用ビルダーなどによって、記述量を減らす設計になっています。

 アノテーションを最小限に
設定ファイルを少なくするために、アノテーションをコードに貼り付けるのでは問題は解決してはいないですよね。ものによっては設定が分散してしまって全体像が分かりにくくなる弊害のほうが大きくなるような気がしています。
特にルーティングの設定なんてものは、コントローラメソッド毎にぺたぺたマッピングを貼り付けるのではなく、全体像を見ながら決めたいですよね。
enkan+kotowariを使った典型的なCRUDのコントローラは以下のようなものですが、必要なアノテーションはコンポーネントをDIするための@Injectと、トランザクション制御用の@Transactionのみです。
public class CustomerController {
    @Inject
    private TemplateEngine templateEngine;

    @Inject
    private DomaProvider daoProvider;

    @Inject
    private BeansConverter beans;

    public HttpResponse index() {
        CustomerDao customerDao = daoProvider.getDao(CustomerDao.class);
        List<Customer> customers = customerDao.selectAll();
        return templateEngine.render(""customer/list"",
                ""customers"", customers);
    }

    public HttpResponse show(Parameters params) {
        CustomerDao customerDao = daoProvider.getDao(CustomerDao.class);
        Customer customer = customerDao.selectById(params.getLong(""id""));
        return templateEngine.render(""customer/show"", ""customer"", customer);
    }

    public HttpResponse newForm() {
        return templateEngine.render(""customer/new"",
                ""customer"", new CustomerForm());
    }

    @Transactional
    public HttpResponse create(CustomerForm form) {
        if (form.hasErrors()) {
            return templateEngine.render(""customer/new"", ""customer"", form);
        }
        CustomerDao customerDao = daoProvider.getDao(CustomerDao.class);
        Customer customer = beans.createFrom(form, Customer.class);
        customerDao.insert(customer);
        return redirect(getClass(), ""index"", SEE_OTHER);
    }

    public HttpResponse edit(Parameters params) {
        CustomerDao customerDao = daoProvider.getDao(CustomerDao.class);
        Customer customer = customerDao.selectById(params.getLong(""id""));
        CustomerForm form = beans.createFrom(customer, CustomerForm.class);
        return templateEngine.render(""customer/edit"",
                ""id"", params.getLong(""id""),
                ""customer"", form);
    }

    @Transactional
    public HttpResponse update(Parameters params, CustomerForm form) {
        if (form.hasErrors()) {
            return templateEngine.render(""customer/edit"",
                    ""id"", params.getLong(""id""),
                    ""customer"", form);
        }
        CustomerDao customerDao = daoProvider.getDao(CustomerDao.class);
        Customer customer = customerDao.selectById(params.getLong(""id""));
        beans.copy(form, customer);
        customerDao.update(customer);
        return redirect(getClass(), ""show?id="" + customer.getId(), SEE_OTHER);
    }

    @Transactional
    public HttpResponse delete(Parameters params) {
        CustomerDao customerDao = daoProvider.getDao(CustomerDao.class);
        Customer customer = customerDao.selectById(params.getLong(""id""));
        customerDao.delete(customer);

        return redirect(getClass(), ""index"", SEE_OTHER);
    }

}


 依存ライブラリを最小限に
フレームワーク自体がミニマルだからといって、多くのライブラリに依存していては、クラウドへのデプロイなどで結局時間を喰う原因になります。
コンポーネントを除く、enkanのコア機能の依存ライブラリは、JavaEE7のAPIとslf4jのAPIのみです。Undertowコンポーネントを使えば、Servlet APIにすら依存しなくなります。

 シングルインスタンス
ミドルウェアやコンポーネント、コントローラはすべてシングルインスタンスです。特にコンポーネントはJavaの従来のDIコンテナのように、複数のスコープを持てないので、スコープの違いによる問題は発生しないことになります。

Ease of Development (開発容易性)

 ホットデプロイ
Enkanはクラスローダー破棄型のクラスリローディング機能をもっています。REPLから/resetコマンドを打てば、1秒かからず最新のクラスに入れ替わります。/autoresetにしておけば、クラスの変更を自動検出してホットデプロイ出来るようになります。
現在の仕組みは、Seasar2のホットデプロイと同じで、原則開発向けの機能ですが、SO_REUSEPORT(Java9から通常APIで使えるようになる?)を利用した本番環境向けのホットデプロイ機能も実装予定です。

 設定ミスの警告
Webアプリを作っていると、フレームワークやルーティングの設定ミスで、それに気付かず時間を使っちゃうことがままあります。Enkanでは設定ミス専用のExceptionがあり、設定ミスがあると丁寧なメッセージですぐに修正することができるでしょう。

こういう具合に、設定ミスの場合は問題と解決策がセットで出るようになります。

Ease of Operation (運用容易性)

 高速な起動
REPLを立ち上げた状態から1秒〜3秒で起動し、HTTPリクエストを受付可能になります。
ポイントは一切のクラススキャンを廃止し、明示的なオブジェクトの生成と依存関係の宣言するようにしていいるところにあります。

 メトリクス
これはコンポーネントの機能の1つですが、DropwizardのMetrisを使って、システムの状態をREPLより取得できるようになります。
enkan> /metrics
-- Active Requests ----------------------------------
             count = 0
-- Errors ------------------------------------
             count = 0
         mean rate = 0.00 events/s
     1-minute rate = 0.00 events/s
     5-minute rate = 0.00 events/s
    15-minute rate = 0.00 events/s
-- Request Timer -----------------------------
             count = 408
         mean rate = 1.87 calls/sec
     1-minute rate = 1.87 calls/sec
     5-minute rate = 1.10 calls/sec
    15-minute rate = 0.42 calls/sec
               min = 0.00 sec
               max = 0.16 sec
              mean = 0.00 sec
            stddev = 0.01 sec
            median = 0.00 sec
              75% <= 0.00 sec
              95% <= 0.00 sec
              98% <= 0.00 sec
              99% <= 0.01 sec
            99.9% <= 0.16 sec


 REPL
もっとも他のフレームワークと違うところがREPLでしょうか。Enkanには標準でREPLが付いていて、これでアプリケーションの起動・停止、各種メトリクスのモニタリング、ミドルウェアの振る舞いの動的編集が可能になります。
現段階では、なんちゃってREPL(pseudo repl)が付いているのみですが、近い将来jshellが使えるようにする予定です。
REPLを起動し、enkanアプリケーションに接続し、/startコマンドを実行すると、サーバが瞬時に起動します。
REPL> /connect 18080
Connected to enkan.
REPL> /start
[pool-1-thread-1] INFO com.zaxxer.hikari.HikariDataSource - HikariPool-0 - is starting.
[pool-1-thread-1] INFO org.flywaydb.core.internal.util.VersionPrinter - Flyway 3.2.1 by Boxfuse
[pool-1-thread-1] INFO org.flywaydb.core.internal.dbsupport.DbSupportFactory - Database: jdbc:h2:mem:test (H2 1.4)
[pool-1-thread-1] INFO org.flywaydb.core.internal.command.DbValidate - Validated 1 migration (execution time 00:00.019s)
[pool-1-thread-1] INFO org.flywaydb.core.internal.metadatatable.MetaDataTableImpl - Creating Metadata table: ""PUBLIC"".""schema_version""
[pool-1-thread-1] INFO org.flywaydb.core.internal.command.DbMigrate - Current version of schema ""PUBLIC"": << Empty Schema >>
[pool-1-thread-1] INFO org.flywaydb.core.internal.command.DbMigrate - Migrating schema ""PUBLIC"" to version 1 - CreateCustomer
[pool-1-thread-1] INFO org.flywaydb.core.internal.command.DbMigrate - Successfully applied 1 migration to schema ""PUBLIC"" (execution time 00:00.059s).
2 02, 2016 7:58:35 午後 org.hibernate.validator.internal.util.Version <clinit>
INFO: HV000001: Hibernate Validator 5.2.2.Final
[pool-1-thread-1] INFO org.eclipse.jetty.util.log - Logging initialized @2688228ms
[pool-1-thread-1] INFO org.eclipse.jetty.server.Server - jetty-9.3.5.v20151012
REPL> [pool-1-thread-1] INFO org.eclipse.jetty.server.ServerConnector - Started ServerConnector@5325abc3{HTTP/1.1,[http/1.1]}{0.0.0.0:3000}
[pool-1-thread-1] INFO org.eclipse.jetty.server.Server - Started @2688295ms

ルーティングやミドルウェアの状態もコマンドから参照可能です。

ミドルウェアの一覧を表示する
REPL> /middleware app list
ANY   defaultCharset (enkan.middleware.DefaultCharsetMiddleware@4929dbc3)
NONE   serviceUnavailable (enkan.middleware.ServiceUnavailableMiddleware@2ee4fa3b)
ANY   stacktrace (enkan.middleware.StacktraceMiddleware@545872dd)
ANY   trace (enkan.middleware.TraceMiddleware@1c985ffd)
ANY   contentType (enkan.middleware.ContentTypeMiddleware@1b68686e)
ANY   httpStatusCat (enkan.middleware.HttpStatusCatMiddleware@12d47c1a)
ANY   params (enkan.middleware.ParamsMiddleware@58d3a07)
ANY   normalization (enkan.middleware.NormalizationMiddleware@5b34eafc)
ANY   cookies (enkan.middleware.CookiesMiddleware@347c2ec)
ANY   session (enkan.middleware.SessionMiddleware@32424a32)
ANY   resource (enkan.middleware.ResourceMiddleware@5e73037f)
ANY   routing (kotowari.middleware.RoutingMiddleware@226c7147)
ANY   domaTransaction (enkan.middleware.DomaTransactionMiddleware@1f819744)
ANY   form (kotowari.middleware.FormMiddleware@3f325d5c)
ANY   validateForm (kotowari.middleware.ValidateFormMiddleware@791cd93e)
ANY   htmlRenderer (enkan.middleware.HtmlRenderer@383b6913)
ANY   controllerInvoker (kotowari.middleware.ControllerInvokerMiddleware@2b13e2e7)


例えば、アプリケーションを停止せずに一時的にメンテ中にし503を返すようにしたい場合、よくデータベースにフラグをもって、それをみてSorryに飛ばすかどうか判定するみたいな作りをすることがあるようですが、enkanではデータベースに聴きにいく必要はありません。ミドルウェアの設定をREPLから変えるだけです。

Sorryページに飛ばすようにする
REPL> /middleware app predicate serviceUnavailable ANY
REPL> /middleware app list
ANY   defaultCharset (enkan.middleware.DefaultCharsetMiddleware@4929dbc3)
ANY   serviceUnavailable (enkan.middleware.ServiceUnavailableMiddleware@2ee4fa3b)
ANY   stacktrace (enkan.middleware.StacktraceMiddleware@545872dd)



構成
EnkanはClojureのductのJava移植なので、ductのアーキテクチャスタックに応じてプロジェクトが別れています。

enkan-core: ring相当
enkan-system: component + nrepl 相当
kotowari: compojure相当

componentは、Javaの世界のふつうのコンテナ管理されるコンポーネントと似た概念ですが、その目的がもう少し絞り込まれています。
componentとは、アプリケーション全体で状態管理する必要があるハコです。そしてcomponentにはstart/stopのインタフェースが実装される必要があります。これによってサーバの再起動を安全におこなうことが出来るようになります。


[WIP] コンポーネント
enkanで用意されているコンポーネントは、現在主要なもので以下が利用可能です。

Webサーバ: Undertow, Jetty
DataSource: HikariCP
ORマッパー: Doma2
DBマイグレーション: Flyway
HTMLテンプレート: FreeMarker


まとめ
Spring BootとJava EEの2強の時代に、今さら感のある新しいフレームワークですが、起動の速さと開発・運用のしやすさではアドバンテージがあります。
そしてClojurianが仕方なくJavaを使わなきゃいけないときのベストチョイスになるよう開発進めていきたいと思います。
https://github.com/kawasima/enkan
",True,https://qiita.com/kawasima/items/723646eb30a91d724352
"自然言語処理に前処理は不可欠です。テキストは文字の羅列であり構造化されていないため、そのままでは処理するのが難しいです。特にWebテキストの中には HTMLタグ や JavaScript のコードといったノイズが含まれています。このようなノイズは前処理して取り除かなければ期待する結果は得られないでしょう。

出典: Deep learning for computational biology
本記事では自然言語処理における前処理の種類とその威力について説明します。説明順序としては、はじめに前処理の種類を説明します。各前処理については、1.どんな処理なのか、2.なぜその処理をするのか、3.実装方法(なるべく) という観点から説明します。種類について説明した後、前処理の威力を測るために前処理をした場合としなかった場合での文書分類の結果を比較します。

前処理の種類と実装
この節では以下に示す5つの前処理について説明します。5つの前処理について、1.どんな処理なのか、2.なぜその処理をするのか、3.実装方法という観点から説明します。


テキストのクリーニング
テキストのクリーニングでは、テキスト内に含まれるノイズを除去します。よくあるノイズとして、JavaScriptのコードやHTMLタグが挙げられます。これらのノイズを除去することで、ノイズがタスクの結果に及ぼす悪影響を抑えることができます。以下のようなイメージです:

JavaScriptやHTMLタグの除去はよく行われるのですが、実際にはデータに応じて除去したいノイズは変わります。そのような場合に使える手として正規表現があります。正規表現を書く際には以下のようなオンラインエディタを使ってリアルタイムにパターンマッチを確認しながら行うと、作業が捗ります。
https://regex101.com/

Pythonには Beautiful Soup や lxml のようなクリーニングを行うのに便利なライブラリがあります。
BeautifulSoupを用いたテキストのクリーニング例はこちら:
preprocessings/ja/cleaning.py

単語の分割
日本語のように単語の区切りが明らかでない言語に対してまず行われるのが単語の分割処理です。単語を分割する理由として、ほとんどの自然言語処理システムでは入力を単語レベルで扱うことが挙げられます。分割は主に形態素解析器を用いて行います。主な形態素解析器として MeCab や Juman++、 Janome が挙げられます。
イメージとしては以下のように分割します。この際、語彙数を減らすために単語を原形にすることもあります:

形態素解析する際に問題となるのがデフォルトでは新語の解析に強くない点です。上の例を見ると「国立新美術館」を「国立」「新」「美術館」の3つに分割しています。このような結果になる原因として、解析に使っている辞書に「国立新美術館」が含まれていないことが挙げられます。特にWebには新語が多数含まれているのでこの問題はさらに深刻になります。
この問題は NEologd と呼ばれる辞書を追加することでことである程度解決できます。NEologd には通常の辞書と比べて多くの新語が含まれています。したがって、NEologdを使用することで新語の解析に強くなります。以下は上と同じ文をNEologdを使って解析した結果です:

以上のようにして単語の分割をした後、後続の処理を行なっていきます。以下はPythonによる実装です:
preprocessings/ja/tokenizer.py

単語の正規化
単語の正規化では単語の文字種の統一、つづりや表記揺れの吸収といった単語を置き換える処理をします。この処理を行うことで、全角の「ネコ」と半角の「ﾈｺ」やひらがなの「ねこ」を同じ単語として処理できるようになります。後続の処理における計算量やメモリ使用量の観点から見ても重要な処理です。
単語の正規化には様々な処理がありますが、この記事では以下の3つの処理を紹介します。

文字種の統一
数字の置き換え
辞書を用いた単語の統一


文字種の統一
文字種の統一ではアルファベットの大文字を小文字に変換する、半角文字を全角文字に変換するといった処理を行います。たとえば「Natural」の大文字部分を小文字に変換して「natural」にしたり、「ﾈｺ」を全角に変換して「ネコ」にします。このような処理をすることで、単語を文字種の区別なく同一の単語として扱えるようになります。


数字の置き換え
数字の置き換えではテキスト中に出現する数字を別の記号(たとえば0)に置き換えます。たとえば、あるテキスト中に「2017年1月1日」のような数字が含まれる文字列が出現したとしましょう。数字の置き換えではこの文字列中の数字を「0年0月0日」のように変換してしまいます。

数字の置き換えを行う理由は、数値表現が多様で出現頻度が高い割には自然言語処理のタスクに役に立たないことが多いからです。たとえば、ニュース記事を「スポーツ」や「政治」のようなカテゴリに分類するタスクを考えましょう。この時、記事中には多様な数字表現が出現するでしょうが、カテゴリの分類にはほとんど役に立たないと考えられます。そのため、数字を別の記号に置き換えて語彙数を減らしてしまいます。
そのため、数値表現が重要なタスク(情報抽出とか)では数字の置き換えは行いません。

辞書を用いた単語の統一
辞書を用いた単語の統一では単語を代表的な表現に置き換えます。たとえば、「ソニー」と「Sony」という表記が入り混じった文章を扱う時に「ソニー」を「Sony」に置き換えてしまいます。これにより、これ以降の処理で2つの単語を同じ単語として扱えるようになります。置き換える際には文脈を考慮して置き換える必要があることには注意する必要があります。

単語正規化の世界は奥深くて、以上で説明した正規化以外にもつづりの揺れ吸収(loooooooooooool -> lol)、省略語の処理(4eva -> forever)、口語表現の代表化(っす -> です)といった正規化もあります。データが大量にあれば後述する単語の分散表現である程度対応できる処理もあるとは思いますが、自分の解きたいタスクに必要な処理をするのが一番良いと思います。
以上で説明した単語の正規化の一部を実装しています:
preprocessings/ja/normalization.py

ストップワードの除去
ストップワードというのは自然言語処理する際に一般的で役に立たない等の理由で処理対象外とする単語のことです。たとえば、助詞や助動詞などの機能語(「は」「の」「です」「ます」など)が挙げられます。これらの単語は出現頻度が高い割に役に立たず、計算量や性能に悪影響を及ぼすため除去されます。
ストップワードの除去には様々な方式がありますが、この記事では以下の2つの方式を紹介します。

辞書による方式
出現頻度による方式


辞書による方式
辞書による方式では、あらかじめストップワードを辞書に定義しておき、辞書内に含まれる単語をテキストから除去します。辞書は自分で作成してもいいのですが、既に定義済みの辞書が存在しています。ここでは日本語のストップワード辞書の一つである Slothlib の中身を見てみましょう。300語ほどの単語が一行ごとに定義されています:
あそこ
あたり
あちら
あっち
あと
あな
あなた
あれ
いくつ
いつ
いま
いや
いろいろ
...

この辞書内に定義された単語をストップワードとして読み込み、使用します。具体的には読み込んだストップワードが単語に分割されたテキスト内に含まれていれば除去してしまいます。以下のようなイメージです:

辞書による方法は素朴な方法で簡単ですがいくつか欠点もあります。一つ目は辞書を作るためのコストがかかる点です。もう一つはドメインによっては役に立たない場合がある点です。そのため、自分の対象としているコーパスによって作り変える必要があります。

出現頻度による方式
出現頻度による方式では、テキスト内の単語頻度をカウントし、高頻度(時には低頻度)の単語をテキストから除去します。高頻度の単語を除去するのは、それらの単語がテキスト中で占める割合が高い一方、役に立たないからです。以下の図はある英語の本の最も頻出する50単語の累計頻度をプロットしたものです:

50単語を見てみると、the や of、カンマのような文書分類等に役に立たなさそうな単語がテキストの50%近くを占めていることがわかります。出現頻度による方式ではこれら高頻度語をストップワードとしてテキストから取り除きます。
ストップワードの除去をしている実装はこちら:
preprocessings/ja/stopwords.py

単語のベクトル表現
単語のベクトル表現では、文字列である単語をベクトルに変換する処理を行います。なぜ文字列からベクトルに変換するのかというと、文字列は可変長で扱いにくい、類似度の計算がしにくい等の理由が挙げられます。ベクトル表現するのにも様々な手法が存在するのですが、以下の2つについて紹介します。

one-hot表現
分散表現


One-hot表現
単語をベクトルで表現する方法としてまず考えられるのはone-hot表現です。one-hot表現というのはある要素のみが1でその他の要素が0であるような表現方法のことです。各次元に 1 か 0 を設定することで「その単語か否か」を表します。
たとえば、one-hot表現でpythonという単語を表すとしましょう。ここで、単語の集合であるボキャブラリは(nlp, python, word, ruby, one-hot)の5単語とします。そうすると、pythonを表すベクトルは以下のようになります。

one-hot表現はシンプルですが、ベクトル間の演算で何も意味のある結果を得られないという弱点があります。たとえば、単語間で類似度を計算するために内積を取るとしましょう。one-hot表現では異なる単語は別の箇所に1が立っていてその他の要素は0なので、異なる単語間の内積を取った結果は0になってしまいます。これは望ましい結果とは言えません。また、1単語に1次元を割り当てるので、ボキャブラリ数が増えると非常に高次元になってしまいます。

分散表現
それに対して分散表現は、単語を低次元の実数値ベクトルで表す表現です。だいたい50次元から300次元くらいで表現することが多いです。たとえば、先ほど挙げた単語を分散表現で表すと以下のように表せます。

分散表現を使うことでone-hot表現が抱えていた問題を解決できます。たとえば、ベクトル間の演算で単語間の類似度を計算することができるようになります。上のベクトルを見ると、python と ruby の類似度は python と word の類似度よりも高くなりそうです。また、ボキャブラリ数が増えても各単語の次元数を増やさずに済みます。
分散表現のベクトルを得るための実装はこちら:
preprocessings/ja/word_vector.py

前処理の威力
この説では前処理にどれほどの効果があるのか検証します。具体的には文書分類タスクに前処理を適用した場合としていない場合で分類性能と実行時間を比較しました。結果として、前処理をすることで分類性能が向上し、実行時間は半分ほどになりました。

文書分類に使用したコーパス
文書分類のデータセットとして livedoor ニュースコーパスを使いました。livedoor ニュースコーパスは livedoor ニュースの記事を収集し、HTMLタグを取り除いて作成されたものです。このデータセットには以下に挙げる9つのクラスが含まれています:

トピックニュース
Sports Watch
ITライフハック
家電チャンネル
MOVIE ENTER
独女通信
エスマックス
livedoor HOMME
Peachy


使用する前処理の種類
この節では前処理する場合の前処理の種類について簡単に述べておきます。
前処理しない場合は文章を形態素解析(ipadic)した後、BoWに変換しTF-IDFで重み付けしています。
それに対して、前処理した場合はまず文章に対してクリーニングを行います。行った処理は以下の3つです:

URLの除去
メンションの除去
いくつかの記号の除去

クリーニングした後は形態素解析の辞書に NEologd を使って文章を分かち書きします。そして分かち書きした単語に対してテキストの正規化を行っています。処理としては以下の2つを行っています:

文字種の統一
数字の置き換え

正規化した単語に対して出現頻度ベースでストップワードの除去を行い、最終的にはBoWにTF-IDFで重み付けしたベクトルを用いて分類を行います。分類には RandomForest を用いています。

結果
結果については分類性能と実行時間を比較してみます。まずは分類性能(accuracy)についてみてみます。



前処理あり
前処理なし




0.917
0.898



分類性能については前処理した場合としなかった場合でほとんど変化しませんでした。予想では性能向上すると思っていたのですが・・・。この辺はさらなる考察が必要です。
実装のミスを修正したところ、前処理した場合としなかった場合で 1.9ポイント差がつきました。この性能での1.9ポイント差なのでそれなりの差ではないでしょうか。
実行時間を比較したところ以下のような結果になりました。前処理しない場合は600秒程度かかっているのに対して、前処理した場合はだいたいその半分の時間で計算が終了しています。これはクリーニングや正規化、そして特にストップワードの除去で語彙数を減らしたことが実行時間の削減につながったと考えられます。

※前処理時間を含む
notebooks/document_classification.ipynb

おわりに
前処理は自然言語処理には欠かせない処理です。自然言語をコンピュータで扱うためには様々な処理が必要です。本記事ではそのうちのいくつかを紹介しました。この記事が皆様のお役に立てば幸いです。
私のTwitterアカウントでも機械学習や自然言語処理に関する情報をつぶやいています。
@Hironsan
この分野にご興味のある方のフォローをお待ちしています。

参考文献

Bag of Words Meets Bags of Popcorn
ゼロから始める自然言語処理 【FIT2016チュートリアル】
文字の正規化
簡易的な日本語ストップワードの取得メソッド
pythonによる文字列の正規化
scikit-learnとgensimでニュース記事を分類する

",True,https://qiita.com/Hironsan/items/2466fe0f344115aff177
"

動機
シャーロック・ホームズのまだらの紐をRDFでナレッジグラフにしてSPARQLで犯人を当てようというナレッジグラフ推論チャレンジ なる素敵な企画があるそうで。
word2vecみたいに何でもかんでもベクトルにしてふわっとやるのもいいんだけど、論理構造から推論して答えを導くというアプローチのほうが説明可能性という面でが望みがある気がしている。
いまいち流行っていないのは、word2vec系の技術は導入が簡単でチュートリアルもそれなりにある一方で、ナレッジグラフの類は、なにか検索したいものに対してこれを構築するのが困難だし検索に使うSPARQLも一癖あって、なかなか敷居が高いからだろう。
何はともあれまずは日本語テキストをRDFで構造化したい。誰が何をどうしたという情報を取ろうとするとmecab,cabochaでは足りなくて述語項構造解析という技術がいるのだけど、
COTOHA API の構文解析APIでその結果が得られるのでこれを使うことにする。
ただレスポンスがJSONで直感的に使いづらいので、まずはアクセサを作ることにした。

COTOHA-NLPライブラリ
cotoha-nlp で製造中。簡単に動くところまでできたので一旦こちらで紹介する。
そのうちpypiにも登録したい。

使い方(暫定版)
$ git clone https://github.com/obilixilido/cotoha-nlp
$ cd cotoha-nlp
$ pyenv local 3.6.5
$ pipenv --three
$ PIPENV_SHELL=/bin/bash pipenv shell
$ pip install -e .
$ echo ""兄の太郎は図書館に行きました"" | python example/example.py --client_id [client_id] --client_secret [client secret] --developer_api_base_url https://api.ce-cotoha.com/api/dev/nlp --access_token_publish_url https://api.ce-cotoha.com/v1/oauth/accesstokens

pyenv,pipenvは導入済みの想定で。
[client_id] と [client secret]  はCOTOHA APIのサイトにログインすると確認できる。

サンプルちょっと解説
# パーサを初期化
parser = Parser(client_id, client_secret, developer_api_base_url, access_token_publish_url)

#　パース
s = parser.parse(input())

## 使ってみる
# 入力文そのまま
print(s.form)
#-> 兄の太郎は図書館に行きました

# 述語の文節の表記を取得
print(s.get_predicate().form)
#-> 行きました

# 述語の動作主を取得
print(s.get_predicate().get_children(filter=[""agent""])[0].get_chunk_head_token().form)
#-> 太郎

# 述語の動作主を形容している語を取得
print(s.get_predicate().get_children(filter=[""agent""])[0].get_chunk_head_token().get_children(filter=[""nomd""])[0].form)
#-> 兄


なんとなく、親・子の関係が直感的でない気がするけど、可視化してみた結果に従った。
cabocha等の普通の係り受け構造の説明だとはツリーが逆転してる(これとか)けど、ツリー構造として捉えるならこっちのほうが自然だろうと思い。

今後
とりあえず誰が何をどうした情報が取れそうなので、適当なテキスト(青空文庫かwikipediaかな)で情報抽出してRDFにしてRDF DBに入れてみる予定。
以前 Apache Jenaを使ったことがあったけど今はAllegroGraphのほうが良いのかな？
あとcotoha-nlpもテスト等充実しないと...
",True,https://qiita.com/obilixilido/items/2eeb8bb89ef52a06e84d
"

CRF
自然言語処理で固有表現抽出するのにはCRFというものを使うらしい。
pythonにより簡単に実装できるそうなので、やってみました。
このサイトhttp://qiita.com/Hironsan/items/326b66711eb4196aa9d4
をとっても参考にしました。ありがとうございます。

対象
pythonわかる人
自分にいろいろ教えてくれる人

実装
だいたい2時間くらいかかりました。
CRFの学習はhttp://qiita.com/Hironsan/items/326b66711eb4196aa9d4#%E5%9B%BA%E6%9C%89%E8%A1%A8%E7%8F%BE%E6%8A%BD%E5%87%BA%E5%99%A8%E3%82%92%E6%A7%8B%E7%AF%89%E3%81%99%E3%82%8B%E3%81%AE%E3%81%AB%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B%E3%83%87%E3%83%BC%E3%82%BF
を参考にしました。ほとんどコピペです。本当に感謝しています。
modelを学習した後にJupyterでいろいろ試して遊びました。

jupyter
#上で学習しやすいようにstr型を学習しやすい形に変える。見辛過ぎますね。すみません。
def text2sent(text):
    sent =[]
    m = MeCab.Tagger (""-Ochasen"")
    tokens = [line.split('\t') for line in m.parse(text).split('\n')]
    for i in range(len(tokens)-2):
        token = tokens[i][0]
        if len(tokens[i])<=3:
            p1 = ""*""
            p2 = ""*""
            p3 = ""*""
            p4 = ""*""
        else:
            part = tokens[i][3].split('-')
            p1 = part[0]
            if len(part) == 2:
                p2 = part[1]
                p3 = ""*""
                p4 = ""*""
            if len(part) == 3:
                p2 = part[1]
                p3 = part[2]
                p4 = ""*""
            elif len(part) == 4:
                p2 = part[1]
                p3 = part[2]
                p4 = part[3]
            else:
                p2 = ""*""
                p3 = ""*""
                p4 = ""*""
            form = [tokens[i][4],tokens[i][5]]
            if form[0]=='':
                f1='*'
                f2='*'
            else:
                f1 = form[0]
                f2 = form[1]
        sent.append([token,p1,p2,p3,p4,f1,f2,tokens[i][2],tokens[i][1],tokens[i][1]])
    return sent

#上で学習したモデルをロード
tagger = pycrfsuite.Tagger()
tagger.open('model.crfsuite')

#適当にtextを綺麗にする。
text = """"   #好きなテキストを入れよう
text = text.replace(""\u3000"",""。"")
text = re.sub('[^(a-zA-Z0-9ぁ-んァ-ン一-龥 。ー)]',"""",text)
text = text.replace('br','。')

#textをsent（上で学習するための型?）へ変換する。
sent = text2sent(text)

#tokenと予測したTagをつけたものをlistで保存
tokenlist = sent2tokens(sent)
predictlist = tagger.tag(sent2features(sent))

#表示
wordlist = []
for i in range(len(tokenlist)):
    if predictlist[i][0] == ""B"":
        wordlist.append((tag,word))
        word = tokenlist[i]
        tag = predictlist[i][-3:]
    if predictlist[i][0] == ""I"":
        word+=tokenlist[i]
    if predictlist[i][0] == ""O"":
        pass
set(wordlist.sort())




結果

テキスト→「フランス革命」のWikipediaの文を引用
https://ja.wikipedia.org/wiki/%E3%83%95%E3%83%A9%E3%83%B3%E3%82%B9%E9%9D%A9%E5%91%BD
フランス革命（フランスかくめい、仏: Révolution française, 英: French Revolution）は、18世紀にフランス王国（ブルボン朝）で起きた市民革命。
世界史上の代表的な市民革命で、前近代的な社会体制を変革して近代ブルジョア社会を樹立した革命。
1787年にブルボン朝の王権に対する貴族の反抗に始まった擾乱は、1789年から全社会層を巻き込む本格的な革命となり、政治体制は絶対王政から立憲王政、そして共和制へと移り変わった。さらに1794年のテルミドール反動ののち退潮へ向かい、1799年にナポレオン・ボナパルトによるクーデターと帝政樹立に至る（1799年11月9日のブリュメール18日のクーデター[注 1]）。一般的には1787年の貴族の反抗から1799年のナポレオンによるクーデターまでが革命期とされている。
フランスの王政とアンシャン・レジームが崩壊する過程で、封建的諸特権が撤廃されて近代的所有権が確立される一方、アッシニア紙幣をめぐって混乱が起こった。
当時のフランスでは啓蒙思想家であるルソーや百科全書派であるヴォルテールにより、社会契約説が多くの知識人に影響を与えた。それに共感した国民が当時の社会体制（アンシャン・レジーム）に対する不満を鬱積させた。ブルボン朝政府、特にルイ16世はこれを緩和するために漸進的な改革を目指したが、特権階級と国民との乖離を埋めることはできなかった。
1789年7月14日のバスティーユ襲撃を契機としてフランス全土に騒乱が発生し、第三身分（平民）らによる国民議会（憲法制定国民議会）が発足し、革命の進展とともに絶対王政と封建制度は崩壊した。
革命の波及を恐れるヨーロッパ各国の君主たちはこれに干渉する動きを見せ、反発する革命政府との間でフランス革命戦争が勃発した。フランス国内でも、カトリック教会制度の見直し、ルイ16世の処刑などのギロチンの嵐、ヴァンデの反乱といった内乱、ジャコバン派の恐怖政治、繰り返されるクーデター、それらに伴った大量殺戮などによって混乱を極めた。革命は1794年のテルミドールのクーデターによるジャコバン派の粛清で転機を迎えたが、不安定な状況は1799年のブリュメールのクーデターや1801年にフランス政府がローマ教皇とコンコルダートを結んで和解するまで続いた。最終的な決着は、第三共和政の成立を待たねばならず、革命勃発より80数年がかかった。
その後多くの国家がフランス革命時に掲げられた理念を取り入れている。民法、メートル法など、フランス革命が生み出した制度も後世に受け継がれた。

結果
{('ART', 'ナポレオンボナパルトによるクーデターと帝政樹立'),
 ('ART', '憲法制定国民議会'),
 ('DAT', '1801年'),
 ('LOC', 'アンシャンレジーム'),
 ('LOC', 'カトリック'),
 ('LOC', 'ジャコバン'),
 ('LOC', 'フランス'),
 ('LOC', 'ヨーロッパ'),
 ('LOC', 'ローマ'),
 ('LOC', '仏'),
 ('LOC', '台湾'),
 ('LOC', '英'),
 ('ORG', 'French'),
 ('ORG', 'Rvolution'),
 ('ORG', 'ォルテール'),
 ('ORG', 'ブルボン'),
 ('PSN', 'テルミドール')}
ふむふむ。
学習がちゃんとできてない気がします。

今後の課題
・学習用データが圧倒的に少なさそう。
・自動で学習用データを作るプログラムを作りたい。それかもうできていたら教えて欲しい。
・word2vecの技術を応用できるともっと成功率が上がりそう。
・Tagの種類をもう少し実用的なものにする。

参考
http://qiita.com/Hironsan/items/326b66711eb4196aa9d4#%E5%9B%BA%E6%9C%89%E8%A1%A8%E7%8F%BE%E6%8A%BD%E5%87%BA%E5%99%A8%E3%82%92%E6%A7%8B%E7%AF%89%E3%81%99%E3%82%8B%E3%81%AE%E3%81%AB%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B%E3%83%87%E3%83%BC%E3%82%BF
ありがとうございます。
あー疲れた。目が痛いな。
",True,https://qiita.com/hiroto0227/items/93ff4d229eca19476b42
"現在JavaScriptで関数型プログラミングについて勉強中...
今回で第四弾です。
ファンクターとモナドの概念について学び、
手も動かさねばと、簡単なClassを作成してみた。
前回までの記事↓↓↓
【JavaScript】即時関数, 無名関数, クロージャについて
【JavaScript】関数を使いこなす(高階関数, 再帰関数)
【JavaScript】部分適用とカリー化

関数型プログラムにおける例外処理

命令型で有名な例外処理をひとまずおさらい
→ try - catch を使って処理する。以下が一例
try {
　// ここに本来の処理を書く
} catch(e) {
　// console.log('Error' + e.message);
}

しかし、関数型においては、例外を投げないことが多い。
理由としては、関数型プログラミングとして見た時
例外処理を行う関数には以下のような問題点がある。

合成やチェーン化ができない
参照透過性の原則に違反する

予期しない動作から守るために多数の行を書くことは個人的には好きではない。
try - catchやnullチェックを使うのもいいが、もっと良い書き方はないか？
そこでファンクターやモナドといった概念が必要となってくる。
本題に入る前に、ちょっと寄り道して、まずはファンクターやモナドを理解する上で必要となる
値をラッピング, マッピングするという処理というものがある。

ラッピングとマッピングという概念
関数型プログラミングの基礎となるデザインパターンの１つに
値(nullかもしれない)をラッピングすることがある。
以下がラッピング&マッピングするデザインパターンの特徴

値への直接のアクセスを禁止する
値へのアクセスは、ラッピングされている値に処理をマッピングすることで行う

以下はラッピング&マッピング処理の一例

wrap.js
class Wrapper {
    constructor(value) {
        this._value = value;
    }

    // マッピング処理。arrayに使うmapとは別物
    map(f) {
        return f(this._value);
    }

    toString() {
        return 'Wrapper (' + this._value + ')';
    }
}

// 引数の値をラッピングする
const wrap = (val) => new Wrapper(val);


ここで大切なのは、一度ラッピングした値は、直接取得, 操作ができないということ。

ファンクター
ファンクターとは、
「（ラッピングされていない）値から（ラッピングされていない）値への関数」
を
「ラッピングした値の中身に対して先の関数を適用し、そして、再度ラッピングする関数」
へ変換すること。
こうすることで、
処理をつなげて、連続処理が行えるようになる。
mapとかfilter等も
配列のそれぞれの要素に対して関数を適用し、配列にして返す
といった点でファンクターと言える。
先程のWrapperクラスにfmap関数を追加し、ファンクターとしての機能を取り入れてみる。

fanctor.js
class Wrapper {
    constructor(value) {
        this._value = value;
    }

    map(f) {
        return f(this._value);
    }

    toString() {
        return 'Wrapper (' + this._value + ')';
    }
}
const wrap = (val) => new Wrapper(val);

// fmap関数を追加。値に対して与えられた関数を適用し、再度ラッピンッグして返す。
// (A -> B) -> Wrapper(A) -> Wrapper(B)
Wrapper.prototype.fmap = function (f) {
    return wrap(f(this._value));
}


const plus3 = a => a + 3; // plus3関数を作成。
const two = wrap(2); // 値2をラッピングする。

// ラッピングされた値2に対して、plus3関数を適用
console.log(two.fmap(plus3)); // => Wrapper { _value: 5 } 関数適用後、ラッピングした値を返す

// nullとnullをラッピングした値に対して処理を行ってみる
const test = null
const test2 = wrap(test);
console.log(test.map(x => x)); // => もちろん、エラーをはく
console.log(test2.fmap(x => x)); // => Wrapper { _value: null } エラーにはならない.ラッピングされたnullが返される


上記のように、同じ型でラッピングした値を常に返すようにできる
結果、合成やチェーン化が可能となる

モナド
モナドとは、
ある場面において、処理を扱う際に、任意のロジックに処理をいじょうすることができる
ということ
少しわかりづらいため、実例を元にみていく。
前置きとして、ファンクターの問題点に触れる。
ファンクターは先程見たとおり、非常に便利だが、
以下の例のように、二重, 三重ラッピングを起こす可能性がある。

niju.js
// 関数型の便利なライブラリ
const R = require('ramda');

class Wrapper {
    constructor(value) {
        this._value = value;
    }

    map(f) {
        return f(this._value);
    }

    toString() {
        return 'Wrapper (' + this._value + ')';
    }
}

const wrap = (val) => new Wrapper(val);

const sample1 = R.curry((a, b) => wrap(a + b));
const sample2 = a => wrap(a);

const composeFunc = R.compose(sample2, sample1(1));
console.log(composeFunc(5)); // => Wrapper { _value: Wrapper { _value: 6 } }  


上記のように、ラッピングした値をラッピングするという二重ラッピングの可能性がある。
これを毎回マッピングしていくのもかなり面倒....
モナドを使うことで、この問題は解消される。
解決例の前に、モナドに必要な概念について押さえておく。
モナド型と呼ばれる、モナドインターフェースの実装に関するコンセプトがあり、
様々なモナド型が存在するが、全てが以下のインターフェースに準拠している。

型コンストラクタ：モナド型を生成する
ユニット関数(of関数)：任意の値をモナド構造に挿入する。
バインド関数：処理をチェーン化する(以降、mapと呼ぶ)
ジョイン処理：モナド構造のレイヤーをflatにする.

このインターフェースに従って、Wrapperクラスをモナドに変更する

monad.js
const R = require('ramda');

class Wrapper {
    // 型コンストラクタ
    constructor(value) {
        this._value = value;
    }

    // ユニット関数(of関数)
    static of(a) {
        return new Wrapper(a);
    }

    // バインド関数
    map(f) {
        return Wrapper.of(f(this._value));
    }

    // ジョイン処理
    join() {
        if(!(this._value instanceof Wrapper)) {
            return this;
        }
        return this._value.join();
    }

    get() {
        return this._value;
    } 
}

const sample1 = R.curry((a, b) => Wrapper.of(a + b));
const sample2 = a => Wrapper.of(a);

const composeFunc = R.compose(sample2, sample1(1));
console.log(composeFunc(5)); // => 先程同様、Wrapper { _value: Wrapper { _value: 6 } }  
console.log(composeFunc(5).join().get()); => 6


join処理のおかげで、平坦化された。
上記がモナドの例だが、あまりに抽象的なので、
モナド型の１つであるMaybeモナドを実装することでより理解を深める。

Maybeモナド
Maybeモナドによって、nullチェックを一元化できる。
Maybeモナドのインターフェースに従って、Maybeクラスをつくる。

maybemonad.js
const R = require('ramda');

class Maybe {
    static just(a) {
        return new Just(a);
    }

    static nothing() {
        return new Nothing();
    }

    static fromNullable(a) {
        return a !== null ? Maybe.just(a) : Maybe.nothing();
    }

    static of(a) {
        return just(a);
    }

    get isNothing() {
        return false;
    }

    get isJust() {
        return false;
    }
}

class Just extends Maybe {
    constructor(value) {
        super();
        this._value = value;
    }

    get value() {
        return this._value;
    }

    map(f) {
        return Maybe.fromNullable(f(this._value));
    }

    getOrElse(_) {
        return this._value;
    }

    filter(f) {
        return Maybe.fromNullable(f(this._value) ? this._value : null);
    }

    chain(f) {
        return f(this._value);
    }

    toString() {
        return `Maybe.Just(${this._value})`;
    }
}

class Nothing extends Maybe {
    map(f) {
        return this;
    }

    get value() {
        throw new TypeError(""Can't extract the value of a Nothing."");
    }

    getOrElse(other) {
        return other;
    }

    filter(f) {
        return this;
    }

    chain(f) {
        return this;
    }

    toString() 
    {
        return 'Maybe.Nothing';
    }
}


Maybeクラスを親クラスとして、JustクラスとNothingクラスを実装。
nullか否かによって、実際の挙動を振り分ける。
このMaybeモナドを基に具体的な例として、
人 → 職 → 場所という構造となるクラスを作り、人からその人が働く場所を取得するという実装行ってみる。

maybeMonad.js
const R = require('ramda');

class Maybe {
    static just(a) {
        return new Just(a);
    }

    static nothing() {
        return new Nothing();
    }

    static fromNullable(a) {
        return a !== null ? Maybe.just(a) : Maybe.nothing();
    }

    static of(a) {
        return just(a);
    }

    get isNothing() {
        return false;
    }

    get isJust() {
        return false;
    }
}

class Just extends Maybe {
    constructor(value) {
        super();
        this._value = value;
    }

    get value() {
        return this._value;
    }

    map(f) {
        return Maybe.fromNullable(f(this._value));
    }

    getOrElse(_) {
        return this._value;
    }

    filter(f) {
        return Maybe.fromNullable(f(this._value) ? this._value : null);
    }

    chain(f) {
        return f(this._value);
    }

    toString() {
        return `Maybe.Just(${this._value})`;
    }
}

class Nothing extends Maybe {
    map(f) {
        return this;
    }

    get value() {
        throw new TypeError(""Can't extract the value of a Nothing."");
    }

    getOrElse(other) {
        return other;
    }

    filter(f) {
        return this;
    }

    chain(f) {
        return this;
    }

    toString() 
    {
        return 'Maybe.Nothing';
    }
}

// jobクラスのインスタンスをプロパティに持つPersonクラス
class Person {
    constructor(job) {
        this.job = job;
    }

    job() {
        return this.job;
    }
}

// Areaプロパティを持つJobクラス
class Job {
    constructor(area) {
        this.area = area;
    }
}

// personをラッピングする関数
const safePerson = person => Maybe.fromNullable(person);

// personからチェーンでプロパティの値を取得していく。途中のいずれかのプロパティがない場合は、""not exist""となる
const getJobArea = person => person
                                .map(R.prop(""job""))
                                .map(R.prop(""area""))
                                .getOrElse('not exist');

// 関数合成 → personをラッピングし、areaを取得しにいく。
const area = R.compose(getJobArea, safePerson);

const job = new Job(""tokyo""); // area = tokyoとなるjobを生成
const taro = new Person(job); // 上記jobをプロパティにもつperson ""taro""を生成
const jiro = new Person(null); // job = null となるperson ""jiro""を生成

console.log(area(taro)); // => tokyo
console.log(area(jiro)); // => not exist


上記例からわかるように、nullチェックをせずとも、nullの可能性がある値を扱うことができた
ちなみにこのコードをnullチェックするカタチで書くと以下のようになる。

nullcheck.js
const saburo = new Person(job);

const area2 = person => {
    if(person.job !== null) {
        if(person.job.area !== null) {
            return person.job.area;
        }
    }
    return 'not exist';
};

console.log(area2(saburo));


なるほど、たしかに短い。
このMaybeモナドは、SwiftやJava8のoptional型に近しいと思う。

まとめ
もやもやしていたファンクター&モナドに関して、だいぶ整理することができた。
モナドを使うことで、関数型プログラミングにおける様々なコードを最小限にでき、
チェーン化して、わかりやすいコードを書くことができるようになると感じた。
次回は、promiseについて書く。

参考資料
書籍
JavaScript関数型プログラミング 複雑性を抑える発想と実践法を学ぶ (impress top gear) 
記事
ファンクタ, アプリカティブ, モナド
ファンクターについて勉強したことの整理
JavaScriptのモナド
JavaScriptでMonadを試した

",True,https://qiita.com/To_BB/items/aa68027a9f319bce7a33
"PySpa統合思念体です。これからJavaScriptを覚えるなら、「この書き方はもう覚えなくていい」（よりよい代替がある）というものを集めてみました。
ES6以降の難しさは、旧来の書き方にプラスが増えただけではなく、大量の「旧来の書き方は間違いを誘発しやすいから非推奨」というものを作り出した点にあります。5年前、10年前の本やウェブがあまり役に立たちません。なお、書き方が複数あるものは、好き嫌いは当然あると思いますが、あえて過激に1つに絞っているところもあります。なお、これはこれから新規に学ぶ人が、過去のドキュメントやコードを見た時に古い情報を選別するためのまとめです。残念ながら、今時の書き方のみで構成された書籍などが存在しないからです。
たぶん明示的に書いていても読み飛ばす人はいると思いますが、すでに書いている人向けではありません。これから書くコードをこのスタイルにしていくのは別にいいと思いますが、既存のコードをすべて書き直せとか（後方互換性があるのでその必要性はありません）そういうものではありません。コードベースの安定が第一です。ちなみに、古い例と書いてある例の中に、僕が昨日まで書いていたコードもありますが、PySpaの総意で古い判定がされたものもあります。
TL;DR
もはや別言語です。

本エントリーの位置づけ
12/30に追加

新しい機能の紹介というよりも、「もはやこのバッドノウハウはいらない」と、未来に引き継ぐべきではない切り捨てるテクニックの紹介です
新しくこれからJavaScriptを学ぶ人が知るべき文法という体裁にしています（制御構文等、変わらない部分は紹介していません）。
TypeScriptも基本的にESと歩調を合わせて機能追加されているので、TypeScriptを使う人もターゲットです。

また、近年はJavaScriptの新機能はコミュニティからの提案で行われるプロセスになっています。stage1からstage4まで受け入れのレベルがあり、stage4は次期新バージョン（毎年6月に発行)に内定したことを意味します。本エントリーではstage3以下の機能については基本的に触れません。

互換性
12/30に追加
これらの書き方ですが、実際にどれだけのブラウザで対応しているのかが気になる方も多いようです。調べるには次のサイトが役立ちます。

ECMAScript Compatibility Table

軽く整理するとこのエントリーで紹介している文法は、次の環境では動作します。

IEを除く現行ブラウザ（スマホもデスクトップも）
Node.js 6.5以降

IE11や、未だに10%ぐらいシェアのあるAndroid 4.4以下は未サポートのものが多いです。あと、GoogleのJavaScriptを解釈するBotはChrome 41相当という説明が中の人がしていたりしますが、Chrome 41だと機能が制約されます（リンク先は41より後に追加されたJS機能一覧）。const/letはできるが、class構文はないということで、おそらくIE11相当の機能にとどめているのではないかと推測されます。そのため、インターネット上で公開する場合は次に紹介するBabelを使うのが原則として必要でしょう。

環境構築編

Babel
使わないとかTypeScriptとかも選択肢は一応ありますが、たとえpercelを使っていても避けられないのがBabelです。
Babelは昔は単独のツールだったものが、プラグインで拡張できるようになり、preset-es2015などのいくつかの設定を統括するプラグインが登場し、最終的にpreset-envというものに集約されました。なので、preset-es2015など、別の書き方をしている本や資料は古いです。

古い.babelrc
{
  ""presets"": [""es2015""]
}


とりあえず最新の文法を解釈するようにするには、次のような設定ファイルを書きます。

今時の.babelrc
{
  ""presets"": [""@babel/preset-env""]
}


ブラウザのバージョンなど、「どの環境で動かしたいか？」という条件で出力フォーマットを設定できます。

今時の.babelrc
{
  ""presets"": [
    [""@babel/preset-env"", {
      ""targets"": {
        ""node"": ""6.10""
      }
    }]
  ]
}


当然、""ie"": ""11""という指定もできるので、指定された実行環境がES6ネイティブ対応じゃなくても、これから紹介するようなことはだいたいできるでしょう。
もちろん、JSそのものをいじりたい（新しい文法を考案して提案するために実験したい）という人はこの限りではありません（が初心者向けユースケースではないので省略）。

変数・リテラル

constをまず使う
昔は変数宣言で使えるのはvarのみでした。

古い書き方
var name = ""@wozozo"";


今後、まっさきに使うべきはconstです。varは全部とりあえずconstに置き換えて、上書きがどうしても必要なところだけletにします。

何はともあれconst
const name = ""@wozozo"";


関数型言語と同じで、変わる必要がないものは「もう変わらない」と宣言することで、脳内でコードをエミュレーションするときのメモリ消費を下げることができます。
変数を変更する場合はletを使います。

変更がある変数はlet
// 変更前
let name;
if (mode === ""slack"") {
    name = ""youichi"";
} else if (mode === ""twitter"") {
    name = ""@wozozo"";
}


なお、C++のconstを知っている人からすると違和感があるかもしれませんが、constで宣言した変数に格納された配列は、代入し直すのはできませんが、配列に要素を追加することはできます。オブジェクトの属性変更もできます。そのため、使える場所はかなり広いです。
なお、varも、グローバルスコープに変数を置く場合にはまだ使えますが、後述するように本当の意味でのグローバルスコープを扱う機会は減っているので基本的に「使わない」で良いでしょう。

変数のスコープ
以前は{、}はブロックにはなりましたが、変数の範囲とはリンクしていませんでした。

古いコード
for (var i = 0; i < 10; i++) {
    // do something
}
console.log(i); // -> 10


let、constはこのブロックの影響を受けます。

新しいコード
for (let i = 0; i < 10; i++) {
    // do something
}
console.log(i); // ReferenceError: i is not defined


ifとかforとか制御構文を使わずに、おもむろに{、}のブロックを書いてスコープを制限することもできます。スコープが狭くなると、変数の影響範囲が小さくなるのでコードの理解がしやすくなります。若者であれば記憶力は強いので良いですが、歳をとるとだんだん弱ってくるのです。また、若くても二日酔いの時もあるでしょうし、風邪ひいたり疲れている時もあると思うので、頑張らないで理解できるように常にしておくのは意味があります。

文字列の結合
従来は他の言語でいうprintf系のようなものがなく、文字列を+で結合したり、配列に入れて.join()で結合したりしましたが、これは古いコードです。

古いコード
console.log(""[Debug]:"" + variable);


いまどきは文字列テンプレートリテラルというのがありますので、これを使います。printfのような数値の変換などのフォーマットはなく、あくまでも文字列結合をスマートにやるためのものです。もちろん、数が決まらない配列などは従来どおり.join()を使います。

新しいコード
console.log(`[Debug]: ${variable}`);



オブジェクトのコピー
Reduxでimmutable.jsを使わない、という状況ではよくオブジェクトのコピーが発生します。

古いコード
var destObj = {};
for (var key in srcObj) {
  if (srcObj.hasOwnProperty(k)) {
    destObj[k] = srcObj[k];
  }
}


今時はObject.assign()というクラスメソッドを使います。

新しいコード
const destObj = {};
Object.assign(destObj, srcObj);


ECMAScript 2018では、オブジェクトのスプレッド演算子サポートが公式で入りました。さらに2018年6月からは次のようになります。このピリオド（スプレッド演算子)3つは後の配列や引数のところでも出てきます。また、このスプレッド演算子は最後に説明する分割代入の「残りの要素」を扱う時にも使います。

新しいコード
const destObj = {...srcObj};



クラス宣言
昔は関数とprototypeという属性をいじくり回してクラスを表現していました。正確には処理系的にはクラスではないのですが、コードのユーザー視点では他の言語のクラスと同等なのでクラスとしてしまいます。

古いクラスの表現
// 関数だけどコンストラクタ
function SmallAnimal() {
    this.animaltype = ""ポメラニアン"";
}

// こうやって継承
SmallAnimal.prototype = new Parent();

// こうやってメソッド
SmallAnimal.prototype.say = function() {
    console.log(this.animalType + ""だけどMSの中に永らく居たBOM信者の全身の毛をむしりたい"");
};

var smallAnimal = new SmallAnimal();
smallAnimal.say();
// ポメラニアンだけどMSの中に永らく居たBOM信者の全身の毛をむしりたい


なお、この仕組みをラップした自前のextends関数みたいなのを使ってクラスっぽいことを表現しようという、なんとかThe Good Partsに影響された一派も一時期いましたが、百害あって一利なしです。
今時の書き方は次のようなclassを使った書き方です。「これはシンタックスシュガーで云々」みたいに言ってくるオッサンの顔面にはパンチをしてもいいです。

新しいクラスの表現
class SmallAnimal extends Parent {
    constructor() {
        this.animaltype = ""ポメラニアン"";
    }

    say() {
        console.log(`${this.animalType}だけどMSの中に永らく居たBOM信者の全身の毛をむしりたい`);
    }
}



関数宣言

アロー関数のみにしていく
functionキーワードはもう捨てましょう！functionキーワードのthisの取り扱いはトラブルの元です。もう存在しなかったものとして歴史の闇に葬ります。次の書き方は古いfunctionキーワードを使っています。こういう説明を見かけたらゴミ箱にダンクシュートします。

古いの関数定義
function name(引数) {
    本体
}


今時はアロー関数を使って書いていきます。特に、今時の書き方は、JavaScriptでよく扱う、無名関数との相性が非常に高くなっています。

今時の関数定義
const name = (引数) => {
    本体
};


状況によってはカッコやreturnなどが省略できたりしますが、そこは割愛します。

即時実行関数はもう使わない
関数を作ってその場で実行することで、スコープ外に変数などが見えないようにする、というテクニックがかつてありました。即時実行関数と呼びます。今時であれば、WebPackなりBrowserifyなりRollupなりPercelなりでファイルを結合するでしょうし、昔のように1ファイルでライブラリを配ってそれを<script>タグで読み込む、というのは減ってきていると思います。
そのため、こういう書き方自体も減ってきています（もちろん、なくなってはいません）。ただ、タグ一つで読み込めるのは利用者にとっては便利なので、超絶良いフレームワーク的なライブラリができて公開したい、という気持ちが出てきてからで遅くはありません。今は無視しましょう。

古いテクニックである即時実行関数
var lib = (function() {
  var libBody = {};

  var localVariable;

  libBody.method = function() {
      console.log(localVariable);
  }
  return libBody;
})();


function(){}をかっこでくくって、その末尾に関数呼び出しのための()がさらに付いている感じです。これで、エクスポートしたい特定の変数だけをreturnで返して公開をしていました。
今時はES6スタイルであればexport { name1, name2, …, nameN };といった書き方が使えます。Browserify/Node.jsでもmodule.exports = { name1: name1, name2: name2... }となります。公開しているもの以外は非公開です。なので、堂々とグローバル空間に置いても問題ありません。

非同期処理
JavaScriptで中級以降になってくると避けられないのが非同期処理です。以前はコールバック地獄と揶揄されるような感じでした。（なお、エラー処理時にかっこなしのif文を一行で書く、というスタイルは好き嫌いの別れるところだと思うので、ここではあえて触れません）。

コールバック地獄
func1(引数, function(err, value) {
  if (err) return err;
  func2(引数, function(err, value) {
    if (err) return err;
    func3(引数, function(err, value) {
      if (err) return err;
      func4(引数, function(err, value) {
        if (err) return err;
        func5(引数, function(err, value) {
          // 最後に実行されるコードブロック
        });
      });
    });
  });
});


その後、非同期処理の待ちはPromiseを使うようになりました。これで、ネストはだいぶ浅くなるので、書きやすくなりました。

ちょっと今時の書き方
const getData = (url) => {
    fetch(url).then(resp => {
        return resp.json();
    }).then(json => {
        console.log(json);
    });
}; 


ただ、new Promise(...)と、Promiseの前にnewを付ける書き方も過去のものです。Promiseはasync関数がreturnとともに作るものです。
今時はasync/awaitキーワードをアロー関数の前に付与します。functionの前にも付けられますが、functionはもはやNGワードなので忘れてください。
asyncとつくと、その関数がPromiseというクラスのオブジェクトを返すようになります。Promiseはその名の通り「重たい仕事が終わったら、あとで呼びに来るからね」という約束です。awaitは、その約束が守られるのを粛々と待ちます。

今時の非同期処理
// 非同期処理をawaitで待つ
const fetchData = async (url) => {
    const resp = await fetch(url);
    const json = await resp.json();
    console.log(json);
};


Promiseを返したメソッドでは、Promiseのthen()メソッドを呼ぶことで、約束を待ちましたが、可能な限りawaitを使って、then()も滅ぼしましょう。
Promise.all()などのところでPromiseは使うので、この名前を禁止することはありません。

apply()

昔は、関数に引数セットを配列で引き渡したいときはapply()というメソッドを使っていました。

古い書き方
function f(a, b, c) {
    console.log(a, b, c);
}

// a=1, b=2, c=3として実行される
f.apply(null, [1, 2, 3]);


この関数の中のthisを最初の引数で指定したりとかありましたが、関数宣言をすべてアロー関数にするのであれば、もうそういうのも過去の話です。配列展開の文法のスプレッド演算子...を使うと同じようなことができます。

新しい書き方
const f = (a, b, c) => {
    console.log(a, b, c);
};

f(...[1, 2, 3]);


なお、apply()がよく出てきていた文脈としては、関数を可変長配列にしたい、というものでした。関数の中ではargumentsという名前のオブジェクトが関数の引数を持っているのですが、これが配列のようで配列でない、ちょっと配列っぽいオブジェクトです。ちょっと使いにくいので、一旦本物の配列にする時にapply()を使ったハックがよく利用されていました。何が起きているかは理解する必要はありません。

可変長配列の古いコード
function f() {
    var list = Array.prototype.slice.call(arguments);
    console.log(list);
}
f(1, 2, 3, 4, 5, 6);
// [1, 2, 3, 4, 5, 6];


これもスプレッド演算子を使うことでわかりやすくなります。もともとの方法は関数宣言だけを見ても実際の引数の数がわかりにくいという問題がありましたが、こちらの方がわかりやすいでしょう。

可変長配列の新しいコード
const f = (a, b, ...c) => {
    console.log(a, b, c);
};
f(1, 2, 3, 4, 5, 6);
// 1, 2, [3, 4, 5, 6];



デフォルト引数
1/20追記
JavaScriptは同じ動的言語のPythonとかよりもはるかにゆるく、宣言された引数を付けずに呼び出すこともでき、その場合には変数にundefinedが設定されました。そのため、undefinedの場合は省略されたとみなして、値を設定したりしていました。

デフォルト引数の古いコード
function f(a, b, c) {
    if (c === undefined) {
        c = ""default value"";
    }
}


やっかいなのは、コールバック関数が末尾にあって、途中の値を省略可能にするときです。JavaScriptでは最後の引数がコールバック、というのはだいたい統一された設計指針として広まりましたが（超古代のsetTimeoutとか例外はいる）、そのためにこういう引数処理が必要になったりました。

古くてやっかいな、コールバック関数の扱い
function f(a, b, cb) {
    if (typeof b === ""function"") {
        cb = b;
        b = undefined;
    }
    :
}


どの引数が省略可能で、省略したら引数を代入しなおしたり・・・とか面倒ですし、同じ型の引数があったら判別できなかったりとか・・・
今時は、他の言語と同じように関数宣言のところに書くことができ、複雑なロジックを手で実装する必要はなくなりました。楽ですね。あと、コールバックですが、すでに説明したとおりにPromiseを返す手法が一般的になったので、「末尾は関数だけど途中が省略」というケースがなくなりました。めでたいですね。

新しいデフォルト引数
const f = (name=""小動物"", favorite=""ストロングゼロ"")


配列やオブジェクトは、分割代入する機能が増えましたが、それと組み合わせると、オブジェクトで柔軟なパラメータを受け取れるがデフォルト値も設定される、みたいなこともできます。以前は、オプショナルな引数はoptsという名前のオブジェクトを渡すこともよくありましたよね？今時であれば、完全省略時でもデフォルト値が設定されるし、部分的に設定も可能みたいな引数は次のように書けます。オブジェクトじゃなくて、配列にすることもできます。

分割代入を使って配列やオブジェクトを変数に展開＆デフォルト値も設定
const f = ({name=""小動物"", drink=""ストロングゼロ""}={}) => {
    :
}



thisを操作するコードは書かない
12/30追記
以前は、prototypeを操作しないで済むように、自前で継承の関数を作ったり、いろいろなメタプログラミングが行われてきました。また、同一の関数実装を流用したり、というところとかで、thisを意識したコーディングがよく行われていました。むしろ、thisを実行時に差し替えることで柔軟性を獲得してきたのがES3〜5ぐらいまでに行われてきたことです。むしろ、これらのthisの違いを知り、使いこなせるのがJavaScript上級者の第一歩ぐらいでした。

古いthisを操作するコード

// apply()の第一引数でthisを外部から指定して実行
func.apply(newThis, [1, 2, 3]);

// call()の第一引数でthisを外部から指定して実行
func.call(newThis, 1, 2, 3);

// bind()でthisを固定
func.bind(newThis);

// オブジェクトに属する関数オブジェクトは、オブジェクトがthisに代入されて実行される
var obj = {
    method: function() {
    }
};

// 何も束縛されていないとglobal名前空間（ブラウザならwindowと同じ）を表す。
// グローバル名前空間に変数追加（汚染ともいう）ができてしまう。
function global() {
    this.bucho = ""show"";
}


とくに一番最後のものがやっかいで、イベントハンドラをオブジェクト内部で定義したときに、そのオブジェクトを参照する方法がなくなるため、次のようなコードが世界で何億回も書かれました。きっと誇張じゃないでしょう。

今後必要なくなるイディオム
var self=this;


コメントにもあるように、jQueryにもその名残があります。jQueryのthisは、選択されているカレントノードを表します。

jQueryはthisを使って、選択されている要素にアクセスする
$('.death-march').each(function () {
  $(this).text(""@moriyoshi参上"");
});


もちろん、使っているフレームワークが特定の流儀を期待しているなら、それをしない方があとあと問題が起きるので、そこは仕方がないところはあります。
このようなthisの操作は今後は不要です。アロー演算子を使えば、オブジェクトの中だろうがつねに、オブジェクトのインスタンスを表すようになります。var self=thisも不要です。
なお、オブジェクトにメソッドを追加するには次の構文が使えます。functionキーワードを使って書くのと実体としては大差がありませんが、書かなくてもいい、というのは楽です。クラスもオブジェクトも常にthisがインスタンスになれば、コードを読む時の「このthis」どこから来たんだろうか？というのに頭を悩ませる必要はありません。

ES6のオブジェクトのメソッドは省略記法がある
const obj = {
    method() {
        console.log(this);
    }
}



配列、辞書
ES6では、単なる配列以外にも、Map/Setなどが増えました。これらは子供のデータをフラットにたくさん入れられるデータ構造で、ループの中で一個ずつ子供を取得する（イテレーション）できるので、iterableと呼ばれます。そのため、配列固有の操作じゃなくて、iterable共通の操作にしていくことが、2018年のESの書き方になります。

ループはfor ... ofを使う
次のコードは古の時代からのコードです。

古いループ1
var iterable = [10, 20, 30];

for (var i = 0; i < iterable.length; i++) {
  var value = iterable[i];
  console.log(value);
}


次のコードは比較的新しいのですが今となってはより新しいコードもあります。一応、現在のiterable（Array, Set, Map）のすべてで使えます。ただパフォーマンス上は遅いとされています（関数呼び出しが挟まるので）。

古いループ2
var iterable = [10, 20, 30];

iterable.forEach(value => {
  console.log(value);
});


イテレータプロトコルという言語組み込みがこれです。今後も新しいiterableが出たとしてもずっと使い続けられます。ループを回して、要素をひとつずつ取り出す・・・というコードはfor ... ofを使います。MDNのサンプルをちょっと改変。

新しいループ
const iterable = [10, 20, 30];

for (const value of iterable) {
  console.log(value);
}


こちらは関数呼び出しを伴わないフラットなコードなので、async/awaitとも一緒に使えます。配列の要素を引数にして、1つずつasyncしたい場合などです。

asyncと新しいループ
const iterable = [10, 20, 30];

for (let value of iterable) {
  await doSomething(value);
}



辞書・ハッシュ用途はオブジェクトではなくてMapを使う
古のコードはオブジェクトを、他言語の辞書やハッシュのようにつかっていました。

古いコード
var map = {
  ""五反田"": ""約束の地"",
  ""戸越銀座"": ""TGSGNZ""
};

for (var key in map) {
    if (map.hasOwnProperty(key)) {
        console.log(key + "" : "" + map[key]);
    }
}


今時はMapを使います。他の言語のようにリテラルで簡単に初期化できないのはあれですが、最初の部分だけですので我慢してください。

新しいコード
const map = new Map([
  [""五反田"", ""約束の地""],
  [""戸越銀座"", ""TGSGNZ""]
]);

for (const [key, value] of map) {
    console.log(`${key} : ${value}`);
}


keyだけでループしたい場合（以前同様）はfor (const key of map.keys()), valueだけでループしたい場合はfor (const value of map.values())が使えます。
keys()メソッド、values()メソッドも、配列の実体を作っているわけではなくて、イテレータという小さいオブジェクトだけを返すので、要素数がどんなに大きくなろうとも、動作も軽いはずです。

分割代入（Destructuring Assignment）
1/20追記
オブジェクトや配列の中身を展開する方法としては、以前は一つずつ変数に代入するとかしていましたし、そもそもそういうことをしないでオブジェクトのまま扱うということをしていました。

一個ずつ取り出す。あるいは取り出さないで利用する古い記法
var thinking = {
    name: ""小動物"",
    mind: ""Python3と寝たい"",
    reason: ""`raise e from cause` べんりですよ""
};
console.log(thinking.name + ""だけど"" + thinking.reason + "" "" + thinking.mind + ""の理由の一つです"");

var name = thinking.name;
var mind = thinking.mind;
var reason = thinking.reason;
console.log(name + ""だけど"" + reason + "" "" + mind + ""理由の一つです"");


分割代入を使えば、オブジェクトや配列を一気に複数の変数に展開できます。オブジェクトの場合は変数名とキー名で引き当てられます。配列は順番ですね。対応する要素が存在しなかったときのデフォルト値も自由に設定できます。

分割代入するし、デフォルト値も一緒に設定できる
const thinking = {
    name: ""小動物"",
    mind: ""Python3と寝たい"",
    reason: ""`raise e from cause` べんりですよ""
};

const {name=""約束の地の住人"", mind, reason} = thinking;
console.log(`${name}だけど${reason} ${mind}理由の一つです`);


記法としては関数の引数のデフォルト値と同じです。
import文なり、requireなりも、一度の呼び出しで、一つしか値が帰ってきません。複数の値を取りたいときは、１つのまとめたオブジェクトをもらってから、属性アクセスしたり、何度も呼び出しをしていました。

分割代入を使わない古い記法
var path = require(""path"");
var readFileSync = require(""fs"").readFileSync;
var writeFileSync = require(""fs"").writeFileSync;


この記法を使えば、中間の変数を省略して、欲しいものだけを得ることもできるようになります。async/await（というか、その裏のPromise）は一つしか値を渡せませんので、この記法を使うと、複数の値を気軽に返せるようになります。

分割代入を使って一発でほしいものだけ取得
const { join } = require(""path"");
import { readFileSync, writeFileSync } from ""fs"";


5/23追記
分割代入の左辺にスプレッド演算子をおくことで、「残りの要素」を扱うこともできます。オブジェクトのスプレッド演算子はECMAScript 2018で公式の仕様に仲間入りしました。
旧来は、配列のsliceを使う方法ぐらいでしょう。オブジェクトの場合はおそらくシンプルな方法はなくて、オブジェクトをまるごとコピーしてから不要なものを削除、あるいはすべてのキーに対してループで回して必要な要素だけを新しいオブジェクトに割り当てる、といった操作が必要だったでしょう。

旧来の手法
// 配列
var rest = array.slice(2);


スプレッド演算子を使えば、よりわかりやすいく記述できます。

スプレッド演算子を利用した記法
// 配列
const [ a, b, ...rest ] = array;
// オブジェクト
const { a, b, ...rest } = obj;


",True,https://qiita.com/shibukawa/items/19ab5c381bbb2e09d0d9
"

Arch Linuxでポートを開放する．
sshdはsystemctl start sshdするだけでうまく行くのに，自分でウェブサーバを起動して外部からアクセスしようとしてもウンともスンとも言わないので戦った記録．
Arch Linuxで自作のサーバを建てたいんだけどポート開放がうまく行かない人向け．

結論
0.0.0.0を使ったら一瞬だった．0.0.0.0と127.0.0.1の違いを理解していなかった自分がアホだった．
恥ずかしいからこの記事も消したいけど一応ハマっている人がいるかもしれないから残す．かと言って127.0.0.1でハマる人なんかがサーバを立てるとも思えないので，この記事があっても生き恥をさらすだけで意味ないのかもしれない．

戦いの記録
Arch Linuxでは一般的な推奨事項に


ファイアウォールの設定
ファイアウォールは Linux のネットワークスタックの上部で拡張保護レイヤーとして働きます。Linux カーネルには Netfilter プロジェクトのひとつで、ステートフルファイアウォールである iptables があります。利用するにはフロントエンドを使うか、直接設定します。Arch では全てのポートが閉じられており、ネットワークデーモンは設定をしないかぎり自動で起動しないため、保護すべきサービスがない限りファイアウォールはあまり意味がありません。

とあるので，iptablesを起動して全部ACCEPTにしたりしてもポートがオープンになりませんし，そもそもArchはデフォルトでiptablesを起動しないとiptablesに書いてあります．
これに気づくまでにだいぶ時間がかかりましたが気づいてからは早かったです．
まず，

ネットワークデーモンは設定をしないかぎり自動で起動しないため、

とありますが，対偶を取ると，「自動で起動しているネットワークデーモンは設定をしている」ということになります．
よって，冒頭で記述したsshdはパッケージに設定ファイルが入っているので，何もしなくても22番ポートが開放されネットワークデーモンが起動しています．
そこで，/usr/lib/systemd/system/sshdを調べると，sshd.serviceとsshd.socketという怪しげなファイルが見つかります．
中を見ると

/usr/lib/systemd/system/sshd.service
[Unit]
Description=OpenSSH Daemon
Wants=sshdgenkeys.service
After=sshdgenkeys.service
After=network.target

[Service]
ExecStart=/usr/bin/sshd -D
ExecReload=/bin/kill -HUP $MAINPID
KillMode=process
Restart=always

[Install]
WantedBy=multi-user.target

# This service file runs an SSH daemon that forks for each incoming connection.
# If you prefer to spawn on-demand daemons, use sshd.socket and sshd@.service.



/usr/lib/systemd/system/sshd.socket
[Unit]
Conflicts=sshd.service
Wants=sshdgenkeys.service

[Socket]
ListenStream=22
Accept=yes

[Install]
WantedBy=sockets.target


と書かれています．
しかし，ここで私は疑問が出てきました．自分のsshdではポートを22番から変えているのです．
試しにsystemctl status sshd.socketしてみるとinactive，つまり起動していません．このため22番ポートという設定が無視されてポート開放できているのです．
(余談ですが，この話もSecure_Shellに書いてあります)
つまり，なんかうまいこと.socket ファイルや.serviceファイルを作ってポートを開放させてあげれば自作サーバをArch Linux上で動かせるはずです．
いろいろググるとこんなページがヒットしまして，あと，Systemdのページをちょろちょろ参考にすれば，

/etc.systemd/system/service-name.service
[Unit]
Description=Simple HTTP Proxy

[Service]
ExecStart=/usr/lib/systemd/systemd-socket-proxyd localhost:接続したいポート
PrivateTmp=yes



/etc.systemd/system/service-name.socket
[Unit]
Description=Simple HTTP Proxy Socket

[Socket]
ListenStream=接続を受け付けるポート


systemctl start service-name.socket

を書くだけです．
詳しくはman systemd-socket-proxyd
例えば，httpサーバを8080番で受け付けて，外部から80番ポートにアクセスしてコンテンツを取得したい場合，接続したいポートに8080，受け付けるポートに80を書けば良い．
この方法が正しいかは知らないので，もしシステムをぶっ壊しても自己責任でお願いします．
書いてsystemd daemon-reloadしてsystemd service.socketとかすると普通に動いて今までのiptables諸々の苦労は何だったんだ…って感じになった．
ちなみに，sshd.socketが起動していないのになんで正しいポートを開放できてるのかは謎です．これがわかったらもっと簡単になるかもしれません．
わかったので更新しました．
",True,https://qiita.com/amama/items/78f4545b49c53d006308
"以下の設定ファイルを作成することで、USBメモリとSDカードを自動マウントすることを確認している。
$ vi /etc/udev/rules.d/11-media-by-label-auto-mount.rules
KERNEL!=""sd[a-z][0-9]"", GOTO=""media_by_label_auto_mount_end""

# Import FS infos
IMPORT{program}=""/sbin/blkid -o udev -p %N""

# Get a label if present, otherwise specify one
ENV{ID_FS_LABEL}!="""", ENV{dir_name}=""%E{ID_FS_LABEL}""
ENV{ID_FS_LABEL}=="""", ENV{dir_name}=""usbhd-%k""

# Global mount options
ACTION==""add"", ENV{mount_options}=""relatime""
# Filesystem-specific mount options
ACTION==""add"", ENV{ID_FS_TYPE}==""vfat|ntfs"", ENV{mount_options}=""$env{mount_options},utf8,gid=100,umask=002""

# Mount the device
ACTION==""add"", RUN+=""/bin/mkdir -p /media/%E{dir_name}"", RUN+=""/bin/mount -o         $env{mount_options} /dev/%k /media/%E{dir_name}""

# Clean up after removal
ACTION==""remove"", ENV{dir_name}!="""", RUN+=""/bin/umount -l /media/%E{dir_name}"",     RUN+=""/bin/rmdir /media/%E{dir_name}""

# Exit
LABEL=""media_by_label_auto_mount_end""

ArchLinux: Auto Mounting USB Drives
",True,https://qiita.com/rbtnn/items/f41791fd9975d00225f8
"

発端
この記事の補足に追記した/etc/adjtimeの位置に関するFHS3.0における変更ですが、その理由は

In FHS 2.1, this file was /etc/adjtime, but as hwclock updates it, that was obviously incorrect.

とあまりにも文句なく「そりゃそーだよな」だったので手元のGentooについては修正しちまうことにしました。

どうやって変更する？
ググるとLFSではsed(1)でソースそのものを書き換えているみたいな情報が出てきますが、現在のutil-linuxではADJTIME_PATHがhwclock(8)の実行時だけではなくconfigureスクリプトの実行時にも指定できるようなので、こいつを使います。

Gentooパッケージに新しいビルドオプションをつける方法

ebuildに書いてあるケース
USEフラグを使います。Gentoo使いにはお馴染みの方法ですし、ここでは扱いません。

ebuild的に全く関知していないオプションを足したい
こういう時に便利なのがpackage.envとEXTRA_ECONFです。
今回の場合は、以下2つのファイルを作ります。ディレクトリがなかったら適宜作りましょう。

/etc/portage/env/adjtime-location.conf
EXTRA_ECONF=""ADJTIME_PATH=/var/lib/hwclock/adjtime""



/etc/portage/package.env
sys-apps/util-linux adjtime-location.conf


構造としては、

Portageの設定をひとまとめにしたテキストファイルを/etc/portage/env下に配置する



EXTRA_ECONF以外にもPortageの機能に関する変数、例えばFEATURESやCXXFLAGSなどが指定可能。



/etc/portage/package.envというテキストファイルに適用したいパッケージ名と設定ファイル名をスペース区切りで指定する


パッケージごとに1行、設定ファイルはその行内においてスペース区切りで複数書いてOK



といった感じです。

つけた後にやること
# emerge sys-apps/util-linuxでビルドをやり直し、configureスクリプト実行コマンドラインの出力の末尾にEXTRA_ECONFで指定した行が加わっていることを確認すればOKです。
もちろん、追加したオプションの内容によっては、それが原因でビルドに失敗する場合もあるので気をつけましょう。
最後に、今回はadjtimeファイルの参照先が変わったので
# mkdir /var/lib/hwclock
# mv /etc/adjtime /var/lib/hwclock/adjtime
# ln -s /var/lib/hwclock/adjtime /etc/adjtime

とフォローを入れておきました。
こういうのはGentooの醍醐味ですね。お疲れ様でした。
",True,https://qiita.com/slug/items/49398805540a41e57e80
"この文章は「プログラマのための量子アルゴリズム入門」の一部です。
全体の目次・記載方針を確認する場合は、次の画像をクリックしてください。


今回の内容

まずは、量子アルコリズムを学ぶ上で基本となる量子ビット、量子状態、測定、ユニタリ発展に関する説明を1量子ビットの世界で説明します。

イントロダクション

古典コンピュータで表現可能な情報の最小単位はビット(bit)です。量子コンピュータのビットと区別のため、古典ビットともいいます。
1ビットの情報には0または1で表される、2種類の状態があります。2ビットであれば $00, 01, 10, 11$ の4通りの状態があります。このようにして、nビットの情報は $2^n$ 通りの状態があります。
また、このようなビットの状態はいつでも測定することができ、同じ操作を行えば毎回同じ結果を得ることができます。
この古典ビットの性質と比べると、量子コンピュータで使われる量子ビットは不思議な性質を持っています。
まず、量子力学の法則である重ね合わせの原理(principle of superposition)により、複数のビットが重なり合った状態となっています。
また、量子ビットの状態は測定するとき、返ってくる結果は確率的に決定されます。そのため、同じ操作をしても結果が異なることがあります。 例えば、同じ計算をしても0が返ってきたり、1が返ってきたりすることがあります。
この章では、量子力学のこの事実を前提に、それをどのように量子ビットとして表現するか説明します。
それでは、1量子ビットの世界を見てみましょう。

量子ビット、量子状態

まず、古典ビットに対応する量子ビットや量子状態について説明します。

定義
$\mathbb{C}^2$ の単位ベクトルを量子状態(quantum state)といい、量子状態の集合を $Q$ と書く。  
\begin{align}
Q := \left\{ \left(
  \begin{array}{c}
    a \\
    b
  \end{array}
\right)
\in \mathbb{C}^2 \ \middle| \ |a|^2 + |b|^2 = 1 \right\}
\end{align}


古典ビットの0, 1に対応する量子ビット(quantum bit, qubit)を $\mid0\rangle, \mid1\rangle$ と記します。
$Q$ の要素としては、次のものに対応します。
\begin{align}
\mid0\rangle :=
\left(
  \begin{array}{c}
    1 \\
    0
  \end{array}
\right) \\
\mid1\rangle :=
\left(
  \begin{array}{c}
    0 \\
    1
  \end{array}
\right)
\end{align}

$\mid0\rangle$ はゼロベクトルでないことに、注意が必要です。
\begin{align}
\mid0\rangle \neq
\left(
  \begin{array}{c}
    0 \\
    0
  \end{array}
\right)
\end{align}

この記法を利用することで、
\begin{align}
\left(
  \begin{array}{c}
    a \\
    b
  \end{array}
\right) = 
a\left(
  \begin{array}{c}
    1 \\
    0
  \end{array}
\right) + 
b\left(
  \begin{array}{c}
    0 \\
    1
  \end{array}
\right) = 
a \mid 0 \rangle + b \mid 1 \rangle 
\end{align}

となります。そのため、$Q$ を次のように書くことができます。
Q = \left\{ a \mid 0 \rangle + b \mid 1 \rangle \ \middle| \ a,b \in \mathbb{C}^2 で |a|^2 + |b|^2 = 1 を満たす\right\}

扱いに便利であるため、$Q$の要素は $\mid0\rangle$ と $\mid1\rangle$ を使った記法で書くことが多いです。
例えば、次のようなものは量子状態です。
\begin{align}
\mid0\rangle, \quad \mid1\rangle, \quad \frac{1}{\sqrt{5}}\mid0\rangle + \frac{2}{\sqrt{5}}\mid1\rangle, \quad  -\frac{1}{\sqrt{3}}\mid0\rangle + \frac{1+i}{\sqrt{3}}\mid1\rangle
\end{align}

逆に、次のものは量子状態ではありません。
\mid 0 \rangle + \mid 1 \rangle \quad (1^2+1^2 = 2 \neq 1となるため)

$Q$ の要素を文字で表すとき、$\varphi$ ではなく、$\mid\varphi\rangle$ という書き方をすることが多いです。量子力学では列ベクトルを表すときにこの記法を使用するため、それに従っています。
量子状態は、内積と呼ばれる演算を使用して表すこともできます。
$\mathbb{C}^2$ の要素
\begin{align}
\mid\varphi\rangle =
\left(
  \begin{array}{c}
    a_1 \\
    b_1
  \end{array}
\right) , \quad
\mid\psi\rangle =
\left(
  \begin{array}{c}
    a_2 \\
    b_2
  \end{array}
\right)
\in \mathbb{C}^2
\end{align}

に対し、内積 $\langle\varphi\mid\psi\rangle$ を次のように定義します。
\begin{align}
\langle\varphi\mid\psi\rangle := \varphi^*\psi = 
\left(
  \begin{array}{cc}
    \overline{a_1} & \overline{b_1}
  \end{array}
\right)
\left(
  \begin{array}{c}
    a_2 \\
    b_2
  \end{array}
\right)
= \overline{a_1}a_2 + \overline{b_1}b_2
\end{align}

$\varphi^*$ は $\varphi$ の随伴行列、$\overline{a_1}$ は $a_1$ の複素共役を表します。
これを利用して、$\varphi$ と $\varphi$ の内積を計算すると、
\begin{align}
\langle\varphi\mid\varphi\rangle = \overline{a_1}a_1 + \overline{b_1}b_1 = |a_1|^2 + |b_1|^2
\end{align}

となります。これにより、次のように $Q$ を表すことができます。
\begin{align}
Q = \left\{ \mid\varphi\rangle \in \mathbb{C}^2 \ \middle| \ \langle\varphi\mid\varphi\rangle = 1 \right\}
\end{align}

量子状態 $Q$ を表す方法はいくつかあるため、そのときどきで扱いやすい方法を使います。

測定

量子力学の世界には不確定性原理というものがあり、量子の状態は測定するまで分かりません。これは、量子コンピュータの世界でも同じであり、量子状態を測定するまでは計算結果がどうなったのか分かりません。
ここでは、量子コンピュータにおける測定について説明します。

定義
量子状態を測定(measurement)することにより、$\mid0\rangle$または$\mid1\rangle$を得ることできる。
このことを「測定値0を得る」とか「量子ビット0を得る」という。
ただし、$a\mid0\rangle + b\mid1\rangle \in Q$ を観測したときに、測定値0を得る確率は $|a|^2$ であり、測定値1を得る確率は $|b|
^2$ である。
また、量子状態は測定結果に合わせて、$\mid0\rangle$または$\mid1\rangle$ に変化する。

測定により得られる結果が確率的であることは、量子コンピュータが現在のコンピュータと大きく異なる点です。例えば、
$$
\begin{align}
\mid\varphi\rangle = \frac{1}{\sqrt{5}}\mid0\rangle + \frac{2}{\sqrt{5}}\mid1\rangle
\end{align}
$$
として $\mid\varphi\rangle$ を測定するとき、量子ビット0を得る確率は $\left|\frac{1}{\sqrt{5}}\right|^2 = \frac{1}{5}$ となり、量子ビット1を得る確率は $\left|\frac{2}{\sqrt{5}}\right|^2 = \frac{4}{5}$ となります。  
また、
$$
\begin{align}
\mid\psi\rangle = \frac{i}{\sqrt{5}}\mid0\rangle + \frac{2}{\sqrt{5}}\mid1\rangle
\end{align}
$$
とし、$\mid\psi\rangle$ を測定するとき、量子ビット0を得る確率は $\left|\frac{i}{\sqrt{5}}\right|^2 = \frac{1}{5}$ となり、量子ビット1を得る確率は $\left|\frac{2}{\sqrt{5}}\right|^2 = \frac{4}{5}$ となります。そのため、$\mid\varphi\rangle$ と $\mid\psi\rangle$ は異なる量子状態ですが、単純に測定するだけでは区別できません。
もう少し一般的な説明で言い換えると次のようになります。
複素数 $w, z$ が $|w|=1, |z|=1$ を満たすとします。2つの量子状態
$$
\begin{align}
  & a\mid0\rangle + b\mid1\rangle, 
  & aw\mid0\rangle + bz\mid1\rangle
\end{align}
$$
を測定するとき、どちらの量子状態も量子ビット0を得る確率は $|a|^2$、量子ビット1を得る確率は $|b|^2$ となります。そのため、これらは区別できません。  
確率的にしか測定できないし、異なる量子状態でも区別がつかないケースがあるため、使いづらそうと感じるかしれません。
その一方で、量子状態は2つの量子ビットが重なり合った状態を表すことができ、これは古典ビットにはなかった特長です。量子アルゴリズムでは、この特長を有効に活用して古典ビットでは実現できなかったことを実現します。

ユニタリ発展

量子ビットの状態について説明し、それを測定できることも分かりました。
しかし、これだけでは計算することができません。古典コンピュータでは、ビット演算を行う素子(AND, OR, XOR, NOT等)を使い、ビットの状態を変化させることで計算を行います。
古典コンピュータの演算回路に相当するものが、量子コンピュータのユニタリ発展です。  

定義
量子状態 $\mid\varphi\rangle$ は、$2\times2$ ユニタリ行列 $U$ を使って $U\mid\varphi\rangle$ という量子状態に写ることができる。
このような量子状態の変化をユニタリ発展(unitary evolution)という。

量子コンピュータでは、ユニタリ発展を行う回路を実装することで量子ビットの状態を変化させ、計算することができます。
ユニタリ発展については、いくつか説明します。  

1. 量子状態をユニタリ発展させたものは、量子状態になるのか
ユニタリ発展したもの $U\mid\varphi\rangle$ が量子状態にならないと、この定義は意味がありません。
$U\mid\varphi\rangle$ が量子状態になることを、実際に確認してみましょう。
\begin{align}
\varphi =
\left(
  \begin{array}{c}
    a \\
    b
  \end{array}
\right) \in Q
\end{align}

とし、$U$ をユニタリ行列とすると、
\begin{align}
\langle U\varphi \mid U\varphi\rangle = (U\varphi)^*(U\varphi) = \varphi^* U^* U \varphi = \varphi^* \varphi
= 
\left(
  \begin{array}{cc}
    \overline{a} & \overline{b}
  \end{array}
\right)
\left(
  \begin{array}{c}
    a \\
    b
  \end{array}
\right)
= \overline{a}a + \overline{b}b = |a|^2 + |b|^2 = 1 
\end{align}

となります。そのため、$U \mid \varphi\rangle \in Q$ です。
また、これにより、ユニタリ行列を $Q$ から $Q$ への関数 $U:Q \rightarrow Q$ と見ることができます。
(これは、全単射です。また、数学用語で「ユニタリ群 $U(2)$ が $Q$ に作用している」といいます)

2. どうしてユニタリ行列なのか
量子状態の発展の仕方は量子力学に登場するシュレーディンガー方程式を解くことで分かり、実際にシュレーディンガー方程式を解くとユニタリ行列をかけることにより発展することが分かるそうです。
ここでは、そのことを認め、詳細には立ち入りません。

3. 任意のユニタリ発展は、ハード実装可能なのか
代表的なユニタリ発展についてはハード実装できているようですが、任意のユニタリ発展をハード実装できている訳ではないと思います。
この文章では、ハード実装できると仮定して、量子アルゴリズムについて説明することがあります。

4. ユニタリ発展の可逆性
ユニタリ行列は必ず逆行列を持ちます。そのため、$U \mid \varphi \rangle$ を $U^{-1}$ でユニタリ発展させると、
$$
\begin{align}
U^{-1}U \mid \varphi \rangle = \mid \varphi \rangle 
\end{align}
$$
となり、$\mid \varphi \rangle$ に戻ります。そのため、任意のユニタリ発展は可逆です。

次回の内容

さて、今回は量子ビット、量子状態、測定、ユニタリ発展について、1量子ビットの世界で説明しました。
が、正直言って、「これで何ができるか分からない」という感じだと思います。
次回は1量子ビットの量子回路について説明し、もう少し手触りを感じられるようにしたいと思います。
それではまた、次回もよろしくお願いします。
",True,https://qiita.com/snuffkin/items/f7ca92d00560b9e6c7b8
"最近は組み込み用CPUにも色々と高度な機能が付くようになったおかげで、
よほどそのCPUに熟知している人を除けば、自分でアセンブラを書くより
コンパイラに最適化を任せた方が早くなる事が増えてきたと思います。
と言う事で、僕が知っている組み込みソフトウェアの
最適化の定石やノウハウをまとめていきたいと思います。
wikipediaの最適化の項目が結構充実しているので、
こちらから目を通すと良いかも知れません。
https://ja.wikipedia.org/wiki/最適化_(情報工学)

前置き
ここでは速度の最適化のみを取り扱います。
サイズの最適化についてはスコープ外なので悪しからず。
基本的にARM向けの記述が多いです。
ARMについては以下を見ると良さそうです。
[アセンブラ] ARMの仕様を見てみる 
アセンブラの出力例は以下のコンパイラがデフォルト設定で出力したものを使っています。
arm-none-eabi-gcc.exe (GNU Tools for ARM Embedded Processors) 5.4.1 20160919 (release) [ARM/embedded-5-branch revision 240496]

最適化の定石
一般的に言って、こんなところではないでしょうか。

1.性能を測定できる所まで保守性重視で実装する
この段階では保守性を最優先で進めた方が良いです。
高凝集・低結合で変化に強いコードを書きましょう。
https://ja.wikipedia.org/wiki/結合度
https://ja.wikipedia.org/wiki/凝集度

2.性能を測定してボトルネックを探し出す
まずはボトルネックがどこにあるのかを探します。
これをしないと、性能に全く影響しない所を延々と最適化してしまったりします。
なお、CPUによっては性能測定のための機能があったりします。
ARMだとパフォーマンスモニタが便利です。
http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0363fj/Bgbfdfgh.html
あとはETMなんてのもあります。

3.大域最適化を行う
ボトルネックの修正のためには大きな設計変更が必要かも知れません。
この時に保守性の低いコードを書いていたりすると悲惨な目に遭います。

4.局所最適化を行う
関数単位の局所的な最適化を行います。
ここで得られる効果は微々たるものという事が多いのではないでしょうか。

最適化テクニック

関数をinline化する
常套手段ですね。
良く言われるのが関数の呼び出しコストです。
そのほかにもコンパイラによる最適化の自由度が上がるため、
処理の順番を変えてパイプラインハザードを解消してくれる時もあります。
https://ja.wikipedia.org/wiki/パイプライン処理
なお、「処理速度のため」と非常に長い関数を書く人も居ますが、
inline化で代替できるケースは多いです。

constを付けられる変数にはconstを付けてみる
(追記)
.rodataのデフォルト配置先が低速なメモリになっている場合、
単純にconstを付けると逆に遅くなる可能性もあります。
@st1011 さんご指摘ありがとうございました。
constもコンパイラへの最適化のヒントとなりパイプラインハザードが無くなる場合があります。
有識者にとっては意外でもなかったりするのかもしれません。

空間的局所性の高いアクセスには構造体を使う
(追記)
最近のコンパイラは頭が良いので、構造体を使わずとも
オフセット付きロード/ストアにしてくれるようです。
@fujitanozomu さん検証して頂きありがとうございました。
構造体を使えば、オフセット付きロード/ストア命令によって高速化できる可能性があります。
0x40001000とその隣0x40001004を4Byteずつ取ってくる処理を考えます。
#include <stdint.h>

uint32_t get_temperature(void){
    uint32_t *alpha = (uint32_t *)0x40001000;
    uint32_t *bravo = (uint32_t *)0x40001004;
    return *alpha + *bravo;
}

これのコンパイル結果はこのようになるはずです(レジスタ名はARMのものを使用)。
1. r2に0x40001000をセット
2. r2のアドレスの値をr0にロード
3. r2の値を+4
4. r2のアドレスの値をr1にロード
5. r0にr0とr1の和を入れる
構造体を使う場合、コードはこうなると思います。
#include <stdint.h>

struct temperature{
    uint32_t alpha;
    uint32_t bravo;
};

uint32_t get_temperature(void){
    struct temperature *temperature = (struct temperature *)0x40001000;
    return temperature->alpha + temperature->bravo;
}

これのコンパイル結果は以下の様になります。
1. r2に0x40001000をセット
2. r2のアドレスの値をr0にロード
3. r2+4のアドレスの値をr1にロード
4. r0にr0とr1の和を入れる
Bravoにオフセット付きロードが使える分、1命令減ります。

constなローカルの配列変数をスタックにコピーしたくない場合、宣言にstaticを付ける
この現象は原因が良く分からないのですが、以下のような配列変数がある場合
以下の様に関数内にconstなローカル配列変数があった場合、
#include <stdint.h>
uint32_t read_const_table(uint32_t select)
{
    const uint32_t someTable[] = {0,1,2,3};
    return someTable[select];
}

以下の様に、配列の中身を初期化用のrodata領域から一度全て
スタックにコピーした後、そこから値を取りだすコードが出力されます。
(コメントでの指摘どおり、これは自体は言語規格通りの動作でした)。
mov ip, r0
str lr, [sp, #-4]!
ldr r3, .L3
sub sp, sp, #20
ldmia   r3, {r0, r1, r2, r3}
add lr, sp, #16
stmdb   lr, {r0, r1, r2, r3}
add ip, lr, ip, lsl #2
ldr r0, [ip, #-16]
add sp, sp, #20
ldr lr, [sp], #4
bx  lr

この例ではコンパイラの最適化によって、スタックを介さずに初期化用のrodata領域から
直接r0に格納できそうな気もしますが、どうやらやってくれない様です（？）。
対策としては配列変数をcostからstatic constにします。
#include <stdint.h>
uint32_t read_const_table(uint32_t select)
{
    static const uint32_t someTable[] = {0,1,2,3};
    return someTable[select];
}

こうすれば、スタックへのコピーを介さず直に読みだしてくれます。
ldr    r3, .L2
ldr    r0, [r3, r0, asl #2]
bx     lr


コンパイラ付属のライブラリを使う
ガリガリに最適化されたライブラリが入っていることが多いので
車輪の再発明をせず素直につかってしまいましょう。
ただし、変なsection指定attributeが付いている事があるので要注意です。
僕は昔これで痛い目にあいました。
Linkerの出力から見かけないオブジェクトが変な所に配置されてないか確認しましょう。

既に確立されたアルゴリズムを使う
「実はこれってただのソートじゃないか」などと気づく事はありませんか。
高速に処理できるアルゴリズムが既にあるなら、それを採用しましょう。

コンパイラの最適化オプションを変える
僕は割と最近までは知らなかったのですが、コンパイラの-O2と言ったものは
個別の最適化オプションの組み合わせでできています。
参照：
https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#Optimize-Options
これを変えるとぐっと高速になるかもしれません。
ただし、今まで最適化がかかって無かったために隠れていた
バグが表面化する事もあります。

CPUの仕様書をじっくり読み込む
色々書きましたが、結局はこれが一番重要です。
パイプラインの段数によってパイプラインハザードをどの程度恐れる必要があるか変わりますし、
フォワーディング機能があればデータハザードは気にしなくて良いでしょう。
むしろ、早いコードの書き方がそのものズバリで書いてあったりします。

アンチパターン

8bit,16bitのローカル変数を使ってしまう
32bit CPUを使う前提で書きます。
基本的に32bit CPUは32bit演算しかできません。
(64bit演算もできるものはあったりしますが)
では8bit,16bit変数の演算はどうなるかと言うと、
32bitで演算した結果に対して0xFF (8bit) / 0xFFFF (16bit)の
ビットマスクを掛けるので、その分遅くなります。
スタックの消費量の観点でも、スタックエントリはCPUの
レジスタサイズと同じというケースが多く、この場合8bit,16bit変数も
32bitでスタックに積まれます。

register指定子を使ってしまう
これはコンパイラによる最適化の自由度を下げるので避けた方が良いです。
僕の経験からすると、むしろ遅くなる場合が多いです。

最初の実装で最適化してしまう
一般的に、プログラムの実行時間の80%は20%のコードが占有すると言われています。
僕が昔携わった製品では5%のコードが95%の実行時間を使っていました。
そのため、せっかく手間暇かけて最適化しても、プロダクトの性能に影響しなかったりします。
また、この段階で行える最適化となると局所最適化ですが、これによって保守性を下げてしまうと
大域最適化の障壁になり、目標性能を達成できなかったという事になりかねません。

命令数が減ったから早くなったと判断してしまう
逆アセンブラにかければ、その関数の命令数が分かります。
https://ja.wikipedia.org/wiki/逆アセンブラ
確かに命令数が減ったならその分早くなるケースは多いです。
しかし、パイプラインハザードを考えると、必ずしもそうとは言えません。
実行サイクル数を図ってみると逆に増えているケースもあります。

まとめ
長々と最適化のテクニックについて書いていきましたが、
結局、保守性や可読性の高い素直で普通なコードを書いていれば、
大した話ではありません。
何か分かりづらい事や、あるいは間違っている点があれば気軽にコメントしてください。
",True,https://qiita.com/YankeeDeltaBravo225/items/274b92735fbafc060a75
"

NTZとNLZ
NTZは最下位ビットから連続する立っていないビットの数、NLZは最上位ビットから連続する立っていないビットの数のことを言います。
具体的に言うと、8bitの
00101000
というビット列があった場合、最下位ビットからは0が3つ連続しているのでNTZは3、最上位ビットからは0が2つ連続しているのでNLZは2になります。
アルゴリズムの高速化などを考えると割と必要になることがあるのですが、いまいちライブラリが整理されていない感があります。
CPUの命令セットにも含まれていたりするようですが、全てのビットが0だった時の結果がバラバラだったりもするので、あまり使い物になりません。
という訳で、NTZとNLZを実装してみたいと思います。
とは言え、どういう方法で実装すればいいのかについては、既にネット上で情報を探れば色々出てくるので、今更私が解説しても仕方ありません。
この記事ではなるべく汎用的に使えるようにテンプレート関数を定義しようと思います。

方法
まずは何も言わずにここを読みましょう。
当面C#と.NETな記録: [C#] 一番右端の立っているビット位置を求める「ものすごい」コード
上記で紹介されている黒魔術を使います。これはC#のものですが、C++で実装します。
上記の記事では64bitに関して扱っていますが、全ての整数型に対応しようと思ったら、32bit、16bit、8bitのそれぞれについて実装する必要があります。

謎の数値と配列
上記で紹介した記事に出ていたマジックナンバーとテーブルを拡張して、以下の数値と配列を使用します。

ntz_nlz.hpp
//8bit版
static constexpr auto magic8 = 0x1DU;
static constexpr int ntz_table8[15] = {8,  0, -1,  1,  6, -1, -1,  2,  7, -1,  5, -1, -1,  4,  3};
static constexpr int nlz_table8[15] = {8,  7, -1,  6,  1, -1, -1,  5,  0, -1,  2, -1, -1,  3,  4};
//16bit版
static constexpr auto magic16 = 0x0F2DU;
static constexpr int ntz_table16[31] = {
    16,  0, -1,  1, -1,  8, -1,  2, 14, -1, -1,  9, -1, 11, -1,  3,
    15, -1,  7, -1, 13, -1, 10, -1, -1,  6, 12, -1,  5, -1,  4,
};
static constexpr int nlz_table16[31] = {
    16, 15, -1, 14, -1,  7, -1, 13,  1, -1, -1,  6, -1,  4, -1, 12,
     0, -1,  8, -1,  2, -1,  5, -1, -1,  9,  3, -1, 10, -1, 11,
};
// 32bit版
static constexpr auto magic32 = 0x07C56E99U;
static constexpr int ntz_table32[63] = {
    32,  0, -1,  1, -1, 10, -1,  2, 29, -1, 11, -1, 25, -1, -1,  3,
    30, -1, -1, 23, -1, 12, 14, -1, -1, 26, -1, 16, -1, 19, -1,  4,
    31, -1,  9, -1, 28, -1, 24, -1, -1, 22, -1, 13, -1, 15, 18, -1,
    -1,  8, 27, -1, 21, -1, -1, 17,  7, -1, 20, -1,  6, -1,  5
};
static constexpr int nlz_table32[63] = {
    32, 31, -1, 30, -1, 21, -1, 29,  2, -1, 20, -1,  6, -1, -1, 28,
     1, -1, -1,  8, -1, 19, 17, -1, -1,  5, -1, 15, -1, 12, -1, 27,
     0, -1, 22, -1,  3, -1,  7, -1, -1,  9, -1, 18, -1, 16, 13, -1,
    -1, 23,  4, -1, 10, -1, -1, 14, 24, -1, 11, -1, 25, -1, 26
};
// 64bit版
static constexpr auto magic64 = 0x03F0A933ADCBD8D1ULL;
static constexpr int ntz_table64[127] = {
    64,  0, -1,  1, -1, 12, -1,  2, 60, -1, 13, -1, -1, 53, -1,  3,
    61, -1, -1, 21, -1, 14, -1, 42, -1, 24, 54, -1, -1, 28, -1,  4,
    62, -1, 58, -1, 19, -1, 22, -1, -1, 17, 15, -1, -1, 33, -1, 43,
    -1, 50, -1, 25, 55, -1, -1, 35, -1, 38, 29, -1, -1, 45, -1,  5,
    63, -1, 11, -1, 59, -1, 52, -1, -1, 20, -1, 41, 23, -1, 27, -1,
    -1, 57, 18, -1, 16, -1, 32, -1, 49, -1, -1, 34, 37, -1, 44, -1,
    -1, 10, -1, 51, -1, 40, -1, 26, 56, -1, -1, 31, 48, -1, 36, -1,
     9, -1, 39, -1, -1, 30, 47, -1,  8, -1, -1, 46,  7, -1,  6,
};
static constexpr int nlz_table64[127] = {
    64, 63, -1, 62, -1, 51, -1, 61,  3, -1, 50, -1, -1, 10, -1, 60,
     2, -1, -1, 42, -1, 49, -1, 21, -1, 39,  9, -1, -1, 35, -1, 59,
     1, -1,  5, -1, 44, -1, 41, -1, -1, 46, 48, -1, -1, 30, -1, 20,
    -1, 13, -1, 38,  8, -1, -1, 28, -1, 25, 34, -1, -1, 18, -1, 58,
     0, -1, 52, -1,  4, -1, 11, -1, -1, 43, -1, 22, 40, -1, 36, -1,
    -1,  6, 45, -1, 47, -1, 31, -1, 14, -1, -1, 29, 26, -1, 19, -1,
    -1, 53, -1, 12, -1, 23, -1, 37,  7, -1, -1, 32, 15, -1, 27, -1,
    54, -1, 24, -1, -1, 33, 16, -1, 55, -1, -1, 17, 56, -1, 57,
};


1bit分拡張することで、0をチェックする必要性をなくしています。その分テーブルのサイズは増えてしまうのですが、せいぜい1KB程度なので、大抵の場合は気にすることはないでしょう。
ちなみに、-1で埋められている部分は使用されません。

NTZ, NLZ用の型特性クラス
テンプレートで扱う必要があるので、以下のように特性クラスを実装します

ntz_nlz.hpp
template<std::size_t size>
struct ntz_traits;

template<>
struct ntz_traits<1>
{
    using type = std::uint8_t;
    static constexpr int shift = 4;
    static constexpr auto magic = magic8;
    static constexpr auto ntz_table = ntz_table8;
    static constexpr auto nlz_table = nlz_table8;
};

template<>
struct ntz_traits<2>
{
    using type = std::uint16_t;
    static constexpr int shift = 11;
    static constexpr auto magic = magic16;
    static constexpr auto ntz_table = ntz_table16;
    static constexpr auto nlz_table = nlz_table16;
};

template<>
struct ntz_traits<4>
{
    using type = std::uint32_t;
    static constexpr int shift = 26;
    static constexpr auto magic = magic32;
    static constexpr auto ntz_table = ntz_table32;
    static constexpr auto nlz_table = nlz_table32;
};

template<>
struct ntz_traits<8>
{
    using type = std::uint64_t;
    static constexpr int shift = 57;
    static constexpr auto magic = magic64;
    static constexpr auto ntz_table = ntz_table64;
    static constexpr auto nlz_table = nlz_table64;
};


このように定義を行うことで、整数型Tに対して、ntz_traits<sizeof(T)>を使用してマジックナンバーやテーブルを取得できます。
整数型のビット深度が8, 16, 32, 64でない環境に関しては、とりあえず考えません。面倒なので。

NTZの実装
さて、これだけ定義してしまえば、NTZの実装は簡単です。

ntz_nlz.hpp
// SFINAEに必要なもろもろ
extern void* enabler;
template<bool condition, typename T = void>
using enable_if_type = typename std::enable_if<condition, T>::type;
template<typename T>
using make_unsigned_type = typename std::make_unsigned<T>::type;

// unsigned型のNTZ。例の黒魔術
template<typename T, enable_if_type<std::is_unsigned<T>{}>*& = enabler>
inline constexpr int ntz(T val) noexcept {
    using tr= ntz_traits<sizeof(T)>;
    using type = typename tr::type;
    return tr::ntz_table[static_cast<type>(tr::magic*static_cast<type>(val&-val))>>tr::shift];
}

// signdな型はunsignedな型にキャストする
template<typename T, enable_if_type<std::is_signed<T>{}>*& = enabler>
inline constexpr int ntz(T val) noexcept {
    return ntz(static_cast<make_unsigned_type<T>>(val));
}

// enum型は同じサイズの整数型にキャストする
template<typename T, enable_if_type<std::is_enum<T>{}>*& = enabler>
inline constexpr int ntz(T val) noexcept {
    return ntz(static_cast<typename ntz_traits<sizeof(T)>::type>(val));
}

// ポインタ型はuintptr_tにキャストして整数型として扱う
template<typename T>
inline int ntz(T* val) noexcept { return ntz(reinterpret_cast<std::uintptr_t>(val)); }

// bool型は1bitとみなす
inline constexpr int ntz(bool val) noexcept { return val?0:1; }

// nullptrは0bitとみなす。多分使わないけど
inline constexpr int ntz(std::nullptr_t) noexcept { return 0; }



符号なしの整数型を基本に、符号付きの型、列挙型、ポインタ型に関しては、同じサイズの符号付き整数型にキャストして呼び出すオーバーロード関数を用意しています。
また、bool型とnullptr_t型に関しては特殊なので別のオーバーロードを用意しました。

NLZの実装
さて、ntzではval&-valという式を使って一番下位の立っているビットを残すという計算を行ったわけですが、一番上位の立っているビットを残すのは少し手間のかかる操作です。
考え方は単純で、例えばvalが64bitの型なら、
val = val | (val>>32);
val = val | (val>>16);
val = val | (val>>8);
val = val | (val>>4);
val = val | (val>>2);
val = val | (val>>1);
val = val ^ (val>>1);

と、一番上位の立っているビットから下を全部1にしてから、一番上位以外のビットを全て反転します。
テンプレートで、かつC++11のconstexprでも利用可能な再帰関数にするために、以下の様にします。

ntz_nlz.hpp
template<int n>
struct highest_bit
{
    template<typename T>
    static inline constexpr T get(T val) noexcept {
        return highest_bit<n/2>::get(static_cast<T>(val | (val >> n)));
    }
};
template<>
struct highest_bit<1>
{
    template<typename T>
    static inline constexpr T get_2(T val) noexcept { return static_cast<T>(val ^ (val >> 1)); }
    template<typename T>
    static inline constexpr T get(T val) noexcept { return get_2(static_cast<T>(val | (val >> 1))); }
};
template<typename T>
inline constexpr T get_highest_bit(T val) noexcept { return highest_bit<sizeof(T)*4>::get(val); }


これさえ定義されれば、後はntzとほぼ同じです。

ntz_nlz.hpp

// unsigned型のNLZ
template<typename T, enable_if_type<std::is_unsigned<T>{}>*& = enabler>
inline constexpr int nlz(T val) noexcept {
    using tr= ntz_traits<sizeof(T)>;
    using type = typename tr::type;
    return tr::nlz_table[static_cast<type>(tr::magic*get_highest_bit(val))>>tr::shift];
}

template<typename T, enable_if_type<std::is_signed<T>{}>*& = enabler>
inline constexpr int nlz(T val) noexcept {
    return nlz(static_cast<make_unsigned_type<T>>(val));
}

template<typename T, enable_if_type<std::is_enum<T>{}>*& = enabler>
inline constexpr int nlz(T val) noexcept {
    return nlz(static_cast<typename ntz_traits<sizeof(T)>::type>(val));
}

template<typename T>
inline int nlz(T* val) noexcept { return nlz(reinterpret_cast<std::uintptr_t>(val)); }

inline constexpr int nlz(bool val) noexcept { return val?0:1; }

inline constexpr int nlz(std::nullptr_t) noexcept { return 0; }




終わりに
冒頭で紹介した記事を読んで、「こりゃスゲエ！」と思って実装してみたものの、全てのスカラー型に対応しようなんて思ったら意外と面倒でした。
あ、でもこれ全てじゃないですね。floatとdoubleとlong doubleに対応していないので。
浮動小数点型も、reinterpret_castを使用して無理やりIEEE754の内部表現をビット列と見做してNTZとNLZをとることは可能ですが、特にlong double辺りは64bitより大きい環境もあったりして面倒ですし、面倒な割には使いどころがなさそうなので今回は省きました。
NTZは整数型を素因数分解した時の2の次数です。これが分かると、ユークリッドの互除法より早い（除算を使わない）最小公倍数アルゴリズムが使えるようになったりします。
NLZを使えば、2つの整数型に対して、「上位ビットから見て初めて異なるビットが現れる位置」を求めることができます。
それが分かって何になるんだと思われるかもしれませんが、それに関しては後日別の記事で書こうと思います。
それでは本日はこの辺で。
",True,https://qiita.com/kazatsuyu/items/38203287c19890a2b7c6
